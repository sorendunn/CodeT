{"prompt": "Array,\n    ) -> Tuple[jnp.ndarray, Dict[str, jnp.ndarray]]:\n        log_joint_probs, aux = fun(\n            params=state.params,\n            targets=targets,\n            outputs=outputs,\n            mutable=state.mutable,\n            rng=rng,\n            return_aux=[\"outputs\"],\n        )\n        return -log_joint_probs, aux\n\n    def val_metrics_step(\n        self,\n        aux: Dict[str, jnp.ndarray],\n        targets: Array,\n        metrics: Optional[\n            Tuple[Callable[[jnp.ndarray, jnp.ndarray, Array], Array],...]\n        ] = None,\n    ) -> Dict[str, jnp.ndarray]:\n        if metrics is not None:\n            val_metrics = self.compute_metrics(\n                self.predict_fn(aux[\"outputs\"]),\n                self.uncertainty_fn(aux[\"outputs\"]),\n                targets,\n                metrics,\n            )\n            return {f\"val_{m}\": v for m, v in val_metrics.items()}\n        else:\n            return {}\n\n    def training_epoch_end(\n        self, training_losses_and_metrics_current_epoch: List[Dict[str, jnp.ndarray]]\n    ) -> Dict[str, float]:\n        return self._get_mean_losses_and_metrics(\n            training_losses_and_metrics_current_epoch\n        )\n\n    def val_epoch_end(\n        self,\n        val_losses_and_metrics_current_epoch: List[Dict[str, jnp.ndarray]],\n        state: CalibState,\n    ) -> Dict[str, float]:\n        val_losses_and_metrics_current_epoch = self._get_mean_losses_and_metrics(\n            val_losses_and_metrics_current_epoch\n        )\n        # early stopping\n        improved = self.early_stopping_update(val_losses_and_metrics_current_epoch)\n        if improved and self.save_checkpoint_dir:\n            self.save_checkpoint(state, self.save_checkpoint_dir, force_save=True)\n        return val_losses_and_metrics_current_epoch\n\n    def _get_mean_losses_and_metrics(\n        self, losses_and_metrics: List[Dict[str, jnp.ndarray]]\n    ) -> Dict[str, float]:\n        losses_and_metrics = stack_forest(losses_and_metrics)\n        losses_and_metrics = tree_map(lambda x: x.mean(), losses_and_metrics)\n        return losses_and_metrics\n\n    def should_perform_validation(\n        self, val_data_loader: Optional[DataLoader], epoch: int\n    ) -> bool:\n        return (\n            val_data_loader is not None\n            and self.eval_every_n_epochs > 0\n            and epoch % self.eval_every_n_epochs == 0\n        )\n\n    @staticmethod\n    def sync_gradients_and_loss(\n        grad: jnp.ndarray, loss: jnp.ndarray\n    ) -> Tuple[jnp.ndarray, jnp.ndarray]:\n        return grad, loss\n\n    def on_train_start(\n        self,\n        state: CalibState,\n        targets: List[Array],\n        outputs: List[Array],\n        rng: PRNGKeyArray,\n    ) -> Tuple[CalibState, List[Array], List[Array], PRNGKeyArray]:\n        return state, targets, outputs, rng\n\n    def on_train_end(self, state: CalibState) -> CalibState:\n        self.save_checkpoint(\n            state,\n            save_checkpoint_dir=self.save_checkpoint_dir,\n            keep=self.keep_top_n_checkpoints,\n            force_save=True,\n        )\n        return state\n\n    def on_val_start(self, state: CalibState) -> CalibState:\n        return state\n\n    def compute_metrics(\n        self,\n        preds: Array,\n        uncertainties: Array,\n        targets: Array,\n        metrics: Optional[\n            Tuple[Callable[[jnp.ndarray, jnp.ndarray, Array], Array],...]\n        ],\n    ) -> Dict[str, Array]:\n        metrics_vals = {}\n        for metric in metrics:\n            metrics_vals[metric.__name__] = metric(preds, uncertainties, targets)\n        return metrics_vals\n\n\nclass JittedMixin:\n    @partial(jax.jit, static_argnums=(0, 4))\n    def training_step(\n        self,\n        state: CalibState,\n        targets: Array,\n        outputs: Array,\n        fun: Callable,\n        rng: PRNGKeyArray,\n    ) -> Tuple[CalibState, Dict[str, Any]]:\n        return super().training_step(state, targets, outputs, fun, rng)\n\n    @partial(jax.jit, static_argnums=(0, 4))\n    def val_loss_step(\n        self,\n        state: CalibState,\n        targets: Array,\n        outputs: Array,\n        fun: Callable,\n        rng: PRNGKeyArray,\n    ) -> Dict[str, jnp.ndarray]:\n        return super().val_loss_step(state, targets, outputs, fun, rng)\n\n\nclass MultiDeviceMixin:\n    all_reduce_mean = jax.pmap(lambda x: lax.pmean(x, \"x\"), \"x\")\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.multi_device = True\n\n    @staticmethod\n    def _add_device_dim_to_array(arr: Array) -> Array:\n        n_devices = jax.local_device_count()\n        if arr.shape[0] % n_devices!= 0:\n            raise ValueError(\n                f\"The number of data points of all outputs and targets must be a multiple of {n_devices}, \"\n                f\"that is the number of available devices. However, {arr.shape[0]} were found.\"\n            )\n        return arr.reshape((n_devices, -1) + arr.shape[1:]) if arr is not None else arr\n\n    @staticmethod\n    def sync_mutable(state: CalibState) -> CalibState:\n        return (\n            state.replace(mutable=MultiDeviceMixin.all_reduce_mean(state.mutable))\n            if state.mutable[\"output_calibrator\"] is not None\n            else state\n        )\n\n    @staticmethod\n    def sync_gradients_and_loss(\n        grads: jnp.ndarray, loss: jnp.ndarray\n    ) -> Tuple[jnp.ndarray, jnp.ndarray]:\n        grad = lax.pmean(grads, axis_name=\"batch\")\n        loss = lax.pmean(loss, axis_name=\"batch\")\n        return grad, loss\n\n    def save_checkpoint(\n        self,\n        state: CalibState,\n        save_checkpoint_dir: Path,\n        keep: int = 1,\n        force_save: bool = False,\n        prefix: str = \"checkpoint_\",\n    ) -> None:\n        state = self.sync_mutable(state)\n        state = jax.device_get(tree_map(lambda x: x[0], state))", "metadata": {"task_id": "awslabs_fortuna/141", "ground_truth": "        return super(MultiDeviceMixin, self).save_checkpoint(\n            state, save_checkpoint_dir, keep, force_save, prefix\n        )", "fpath_tuple": ["awslabs_fortuna", "fortuna", "calib_model", "calib_model_calibrator.py"], "context_start_lineno": 364, "line_no": 541}}
{"prompt": "LD momentum to be added to the safety guidance at each diffusion step. If set to 0.0\n                momentum will be disabled. Momentum is already built up during warmup, i.e. for diffusion steps smaller\n                than `sld_warmup_steps`. `sld_momentum_scale` is defined as `sm` of Eq. 7 in [Safe Latent\n                Diffusion](https://arxiv.org/abs/2211.05105).\n            sld_mom_beta (`float`, *optional*, defaults to 0.4):\n                Defines how safety guidance momentum builds up. `sld_mom_beta` indicates how much of the previous\n                momentum will be kept. Momentum is already built up during warmup, i.e. for diffusion steps smaller\n                than `sld_warmup_steps`. `sld_mom_beta` is defined as `beta m` of Eq. 8 in [Safe Latent\n                Diffusion](https://arxiv.org/abs/2211.05105).\n        Returns:\n            [`~pipelines.stable_diffusion.StableDiffusionPipelineOutput`] or `tuple`:\n            [`~pipelines.stable_diffusion.StableDiffusionPipelineOutput`] if `return_dict` is True, otherwise a `tuple.\n            When returning a tuple, the first element is a list with the generated images, and the second element is a\n            list of `bool`s denoting whether the corresponding generated image likely represents \"not-safe-for-work\"\n            (nsfw) content, according to the `safety_checker`.\n        \"\"\"\n        # 0. Default height and width to unet\n        height = height or self.unet.config.sample_size * self.vae_scale_factor\n        width = width or self.unet.config.sample_size * self.vae_scale_factor\n\n        # 1. Check inputs. Raise error if not correct\n        self.check_inputs(prompt, height, width, callback_steps)\n\n        # 2. Define call parameters\n        batch_size = 1 if isinstance(prompt, str) else len(prompt)\n        device = self._execution_device\n\n        # here `guidance_scale` is defined analog to the guidance weight `w` of equation (2)\n        # of the Imagen paper: https://arxiv.org/pdf/2205.11487.pdf. `guidance_scale = 1`\n        # corresponds to doing no classifier free guidance.\n        do_classifier_free_guidance = guidance_scale > 1.0\n\n        enable_safety_guidance = sld_guidance_scale > 1.0 and do_classifier_free_guidance\n        if not enable_safety_guidance:\n            warnings.warn(\"Safety checker disabled!\")\n\n        # 3. Encode input prompt\n        prompt_embeds = self._encode_prompt(\n            prompt, device, num_images_per_prompt, do_classifier_free_guidance, negative_prompt, enable_safety_guidance\n        )\n\n        # 4. Prepare timesteps\n        self.scheduler.set_timesteps(num_inference_steps, device=device)\n        timesteps = self.scheduler.timesteps\n\n        # 5. Prepare latent variables\n        num_channels_latents = self.unet.in_channels\n        latents = self.prepare_latents(\n            batch_size * num_images_per_prompt,\n            num_channels_latents,\n            height,\n            width,\n            prompt_embeds.dtype,\n            device,\n            generator,\n            latents,\n        )\n\n        # 6. Prepare extra step kwargs.\n        extra_step_kwargs = self.prepare_extra_step_kwargs(generator, eta)\n\n        safety_momentum = None\n\n        num_warmup_steps = len(timesteps) - num_inference_steps * self.scheduler.order\n        with self.progress_bar(total=num_inference_steps) as progress_bar:\n            for i, t in enumerate(timesteps):\n                # expand the latents if we are doing classifier free guidance\n                latent_model_input = (\n                    torch.cat([latents] * (3 if enable_safety_guidance else 2))\n                    if do_classifier_free_guidance\n                    else latents\n                )\n                latent_model_input = self.scheduler.scale_model_input(latent_model_input, t)\n\n                # predict the noise residual\n                noise_pred = self.unet(latent_model_input, t, encoder_hidden_states=prompt_embeds).sample\n\n                # perform guidance\n                if do_classifier_free_guidance:\n                    noise_pred_out = noise_pred.chunk((3 if enable_safety_guidance else 2))\n                    noise_pred_uncond, noise_pred_text = noise_pred_out[0], noise_pred_out[1]\n\n                    # default classifier free guidance\n                    noise_guidance = noise_pred_text - noise_pred_uncond\n\n                    # Perform SLD guidance\n                    if enable_safety_guidance:\n                        if safety_momentum is None:\n                            safety_momentum = torch.zeros_like(noise_guidance)\n                        noise_pred_safety_concept = noise_pred_out[2]\n\n                        # Equation 6\n                        scale = torch.clamp(\n                            torch.abs((noise_pred_text - noise_pred_safety_concept)) * sld_guidance_scale, max=1.0\n                        )\n\n                        # Equation 6\n                        safety_concept_scale = torch.where(\n                            (noise_pred_text - noise_pred_safety_concept) >= sld_threshold,\n                            torch.zeros_like(scale),\n                            scale,\n                        )\n\n                        # Equation 4\n                        noise_guidance_safety = torch.mul(\n                            (noise_pred_safety_concept - noise_pred_uncond), safety_concept_scale\n                        )\n\n                        # Equation 7\n                        noise_guidance_safety = noise_guidance_safety + sld_momentum_scale * safety_momentum\n\n                        # Equation 8\n                        safety_momentum = sld_mom_beta * safety_momentum + (1 - sld_mom_beta) * noise_guidance_safety\n\n                        if i >= sld_warmup_steps:  # Warmup\n                            # Equation 3\n                            noise_guidance = noise_guidance - noise_guidance_safety\n\n                    noise_pred = noise_pred_uncond + guidance_scale * noise_guidance\n\n                    # compute the previous noisy sample x_t -> x_t-1\n                latents = self.scheduler.step(noise_pred, t, latents, **extra_step_kwargs).prev_sample\n\n                # call the callback, if provided\n                if i == len(timesteps) - 1 or ((i + 1) > num_warmup_steps and (i + 1) % self.scheduler.order == 0):\n                    progress_bar.update()\n                    if callback is not None and i % callback_steps == 0:\n                        callback(i, t, latents)\n\n        # 8. Post-processing\n        image = self.decode_latents(latents)\n\n        # 9. Run safety checker\n        image, has_nsfw_concept, flagged_images = self.run_safety_checker(\n            image, device, prompt_embeds.dtype, enable_safety_guidance\n        )\n\n        # 10. Convert to PIL\n        if output_type == \"pil\":\n            image = self.numpy_to_pil(image)\n            if flagged_images is not None:", "metadata": {"task_id": "huggingface_diffusers/88", "ground_truth": "                flagged_images = self.numpy_to_pil(flagged_images)", "fpath_tuple": ["huggingface_diffusers", "src", "diffusers", "pipelines", "stable_diffusion_safe", "pipeline_stable_diffusion_safe.py"], "context_start_lineno": 580, "line_no": 721}}
{"prompt": "# This type's active player number\n                    name = '{}_{}_{}'.format(k, cate, i)\n                    ckpt_path = osp.join(self.path_policy, '{}_ckpt.pth'.format(name))\n                    player = create_player(\n                        self.cfg, k, self.cfg[k], cate, self.payoff, ckpt_path, name, 0, self.metric_env.create_rating()\n                    )\n                    if self.cfg.use_pretrain:\n                        self.save_checkpoint(self.cfg.pretrain_checkpoint_path[cate], ckpt_path)\n                    self.active_players.append(player)\n                    self.payoff.add_player(player)\n\n        # Add pretrain player as the initial HistoricalPlayer for each player category.\n        if self.cfg.use_pretrain_init_historical:\n            for cate in self.cfg.player_category:\n                main_player_name = [k for k in self.cfg.keys() if'main_player' in k]\n                assert len(main_player_name) == 1, main_player_name\n                main_player_name = main_player_name[0]\n                name = '{}_{}_0_pretrain_historical'.format(main_player_name, cate)\n                parent_name = '{}_{}_0'.format(main_player_name, cate)\n                hp = HistoricalPlayer(\n                    self.cfg.get(main_player_name),\n                    cate,\n                    self.payoff,\n                    self.cfg.pretrain_checkpoint_path[cate],\n                    name,\n                    0,\n                    self.metric_env.create_rating(),\n                    parent_id=parent_name\n                )\n                self.historical_players.append(hp)\n                self.payoff.add_player(hp)\n\n        # Save active players' ``player_id``` & ``player_ckpt```.\n        self.active_players_ids = [p.player_id for p in self.active_players]\n        self.active_players_ckpts = [p.checkpoint_path for p in self.active_players]\n        # Validate active players are unique by ``player_id``.\n        assert len(self.active_players_ids) == len(set(self.active_players_ids))\n\n    def get_job_info(self, player_id: str = None, eval_flag: bool = False) -> dict:\n        \"\"\"\n        Overview:\n            Get info dict of the job which is to be launched to an active player.\n        Arguments:\n            - player_id (:obj:`str`): The active player's id.\n            - eval_flag (:obj:`bool`): Whether this is an evaluation job.\n        Returns:\n            - job_info (:obj:`dict`): Job info.\n        ReturnsKeys:\n            - necessary: ``launch_player`` (the active player)\n        \"\"\"\n        if player_id is None:\n            player_id = self.active_players_ids[0]\n        with self._active_players_lock:\n            idx = self.active_players_ids.index(player_id)\n            player = self.active_players[idx]\n            job_info = self._get_job_info(player, eval_flag)\n            assert 'launch_player' in job_info.keys() and job_info['launch_player'] == player.player_id\n        return job_info\n\n    @abstractmethod\n    def _get_job_info(self, player: ActivePlayer, eval_flag: bool = False) -> dict:\n        \"\"\"\n        Overview:\n            Real `get_job` method. Called by ``_launch_job``.\n        Arguments:\n            - player (:obj:`ActivePlayer`): The active player to be launched a job.\n            - eval_flag (:obj:`bool`): Whether this is an evaluation job.\n        Returns:\n            - job_info (:obj:`dict`): Job info. Should include keys ['lauch_player'].\n        \"\"\"\n        raise NotImplementedError\n\n    def judge_snapshot(self, player_id: str, force: bool = False) -> bool:\n        \"\"\"\n        Overview:\n            Judge whether a player is trained enough for snapshot. If yes, call player's ``snapshot``, create a\n            historical player(prepare the checkpoint and add it to the shared payoff), then mutate it, and return True.\n            Otherwise, return False.\n        Arguments:\n            - player_id (:obj:`ActivePlayer`): The active player's id.\n        Returns:\n            - snapshot_or_not (:obj:`dict`): Whether the active player is snapshotted.\n        \"\"\"\n        with self._active_players_lock:\n            idx = self.active_players_ids.index(player_id)\n            player = self.active_players[idx]\n            if force or player.is_trained_enough():\n                # Snapshot\n                hp = player.snapshot(self.metric_env)\n                self.save_checkpoint(player.checkpoint_path, hp.checkpoint_path)\n                self.historical_players.append(hp)\n                self.payoff.add_player(hp)\n                # Mutate\n                self._mutate_player(player)\n                return True\n            else:\n                return False\n\n    @abstractmethod\n    def _mutate_player(self, player: ActivePlayer) -> None:\n        \"\"\"\n        Overview:\n            Players have the probability to mutate, e.g. Reset network parameters.\n            Called by ``self.judge_snapshot``.\n        Arguments:\n            - player (:obj:`ActivePlayer`): The active player that may mutate.\n        \"\"\"\n        raise NotImplementedError\n\n    def update_active_player(self, player_info: dict) -> None:\n        \"\"\"\n        Overview:\n            Update an active player's info.\n        Arguments:\n            - player_info (:obj:`dict`): Info dict of the player which is to be updated.\n        ArgumentsKeys:\n            - necessary: `player_id`, `train_iteration`\n        \"\"\"\n        try:\n            idx = self.active_players_ids.index(player_info['player_id'])\n            player = self.active_players[idx]\n            return self._update_player(player, player_info)\n        except ValueError as e:\n            print(e)\n\n    @abstractmethod\n    def _update_player(self, player: ActivePlayer, player_info: dict) -> None:\n        \"\"\"\n        Overview:\n            Update an active player. Called by ``self.update_active_player``.\n        Arguments:\n            - player (:obj:`ActivePlayer`): The active player that will be updated.\n            - player_info (:obj:`dict`): Info dict of the active player which is to be updated.\n        \"\"\"\n        raise NotImplementedError\n\n    def finish_job(self, job_info: dict) -> None:\n        \"\"\"\n        Overview:\n            Finish current job. Update shared payoff to record the game results.\n        Arguments:\n            - job_info (:obj:`dict`): A dict containing job result information.\n        \"\"\"\n        # TODO(nyz) more fine-grained job info\n        self.payoff.update(job_info)\n        if 'eval_flag' in job_info and job_info['eval_flag']:\n            home_id, away_id = job_info['player_id']\n            home_player, away_player = self.get_player_by_id(home_id), self.get_player_by_id(away_id)\n            job_info_result = job_info['result']\n            if isinstance(job_info_result[0], list):\n                job_info_result = sum(job_info_result, [])", "metadata": {"task_id": "opendilab_ACE/94", "ground_truth": "            home_player.rating, away_player.rating = self.metric_env.rate_1vs1(\n                home_player.rating, away_player.rating, result=job_info_result\n            )", "fpath_tuple": ["opendilab_ACE", "ding", "league", "base_league.py"], "context_start_lineno": 97, "line_no": 248}}
{"prompt": "ndarray = attr.field(validator=[\n      attr.validators.instance_of(np.ndarray),\n      _shape_equals(lambda s: [len(s), None])\n  ])\n  ys: np.ndarray = attr.field(validator=[\n      attr.validators.instance_of(np.ndarray),\n      _shape_equals(lambda s: [len(s), None])\n  ])\n  cs: np.ndarray = attr.field(validator=[\n      attr.validators.instance_of(np.ndarray),\n      _shape_equals(lambda s: [len(s), None])\n  ])\n  ages: np.ndarray = attr.field(validator=[\n      attr.validators.instance_of(np.ndarray),\n      _shape_equals(lambda s: [len(s)])\n  ])\n  generations: np.ndarray = attr.field(validator=[\n      attr.validators.instance_of(np.ndarray),\n      _shape_equals(lambda s: [len(s)])\n  ])\n  ids: np.ndarray = attr.field(validator=[\n      attr.validators.instance_of(np.ndarray),\n      _shape_equals(lambda s: [len(s)])\n  ])\n  trial_ids: np.ndarray = attr.field(validator=[\n      attr.validators.instance_of(np.ndarray),\n      _shape_equals(lambda s: [len(s)])\n  ])\n\n  def __len__(self) -> int:\n    return self.ys.shape[0]\n\n  def __getitem__(\n      self,\n      index: Any,\n     \n  ) -> 'Population':\n    return Population(**{k: v[index] for k, v in attr.asdict(self).items()})\n\n  def __add__(self, other: 'Population') -> 'Population':\n    return Population(\n        _concat(self.xs, other.xs), _concat(self.ys, other.ys),\n        _concat(self.cs, other.cs), _concat(self.ages, other.ages),\n        _concat(self.generations, other.generations),\n        _concat(self.ids, other.ids), _concat(self.trial_ids, other.trial_ids))\n\n  @classmethod\n  def recover(cls, metadata: vz.Metadata) -> 'Population':\n    encoded = metadata.get('values', default='', cls=str)\n    try:\n      decoded = json.loads(encoded, object_hook=json_utils.numpy_hook)\n    except json.JSONDecodeError as e:\n      raise serializable.DecodeError('Failed to recover state.') from e\n    return cls(**decoded)\n\n  def dump(self) -> vz.Metadata:\n    encoded = json.dumps(attr.asdict(self), cls=json_utils.NumpyEncoder)\n    return vz.Metadata({'values': encoded})\n\n  def empty_like(self) -> 'Population':\n    \"\"\"Creates an empty population that has the same shape as this.\"\"\"\n\n    return Population(**{\n        k: np.zeros([0] + list(v.shape[1:]))\n        for k, v in attr.asdict(self).items()\n    })\n\n\ndef _create_parameter_converters(\n    search_space: vz.SearchSpace\n) -> Collection[converters.DefaultModelInputConverter]:\n  \"\"\"Returns parameter converters.\"\"\"\n  if search_space.is_conditional:\n    raise ValueError('Cannot handle conditional search space!')\n\n  def create_input_converter(\n      pc: vz.ParameterConfig) -> converters.DefaultModelInputConverter:\n    return converters.DefaultModelInputConverter(\n        pc, scale=True, max_discrete_indices=0, onehot_embed=True)\n\n  return [create_input_converter(pc) for pc in search_space.parameters]\n\n\ndef _create_metric_converter(\n    mc: vz.MetricInformation) -> converters.DefaultModelOutputConverter:\n  # TODO: Do something other than raising an error\n  return converters.DefaultModelOutputConverter(\n      mc,\n      flip_sign_for_minimization_metrics=True,\n      shift_safe_metrics=True,\n      raise_errors_for_missing_metrics=True)\n\n\nclass PopulationConverter(templates.PopulationConverter):\n  \"\"\"Population converter.\"\"\"\n\n  def __init__(self,\n               search_space: vz.SearchSpace,\n               metrics: Collection[vz.MetricInformation],\n               *,\n               metadata_ns: str = 'population'):\n    self._objective_metrics, self._safe_metrics = _filter_and_split(metrics)\n    self._num_objective_metrics = len(self._objective_metrics)\n    self._num_safe_metrics = len(self._safe_metrics)\n    self._metrics = self._objective_metrics + self._safe_metrics\n    self._metadata_ns = metadata_ns\n\n    self._trial_converter = converters.DefaultTrialConverter(\n        _create_parameter_converters(search_space),\n        [_create_metric_converter(mc) for mc in self._metrics])\n    self._empty_feature_dict = converters.DictOf2DArrays(\n        self._trial_converter.to_features([]))\n\n  def to_suggestions(  # pytype: disable=signature-mismatch  # overriding-parameter-type-checks\n      self, offsprings: Offspring) -> Collection[vz.TrialSuggestion]:\n    parameters_list = self._trial_converter.to_parameters(\n        self._empty_feature_dict.dict_like(offsprings.xs))\n    suggestions = [vz.TrialSuggestion(p) for p in parameters_list]\n    for idx, t in enumerate(suggestions):\n      t.metadata.ns(self._metadata_ns).update(offsprings[idx:idx + 1].dump())\n    return suggestions\n\n  def _empty_offsprings(self) -> Offspring:\n    return Offspring(self._empty_feature_dict.asarray(), np.zeros([0]),\n                     np.zeros([0]))\n\n  def to_population(self, completed: Sequence[vz.CompletedTrial]) -> Population:\n    \"\"\"Converts trials into population. Accepts an empty list.\"\"\"\n    offsprings = self._empty_offsprings()  # create empty\n    # Each Trial should contain its genes as metadata. (Note that\n    # genes-to-trial mapping is many-to-one). We try to load the genes.\n    for t in completed:\n      metadata = t.metadata.ns(self._metadata_ns)\n      try:\n        offsprings += Offspring.load(metadata)\n      except serializable.DecodeError:\n        # Upon failure, arbitrarily choose one set of genes that map to the\n        # current trial.\n        offsprings += Offspring(\n            converters.DictOf2DArrays(self._trial_converter.to_features(\n                [t])).asarray(), np.zeros([1]), np.zeros([1]))\n\n    ys = self._trial_converter.to_labels_array(completed)\n    return Population(offsprings.xs, ys[:, :self._num_objective_metrics],\n                      ys[:, self._num_objective_metrics:],\n                      np.zeros([ys.shape[0]]), offsprings.generations,\n                      offsprings.ids,", "metadata": {"task_id": "google_vizier/140", "ground_truth": "                      np.asarray([t.id for t in completed], dtype=np.int32))", "fpath_tuple": ["google_vizier", "vizier", "_src", "algorithms", "evolution", "numpy_populations.py"], "context_start_lineno": 171, "line_no": 318}}