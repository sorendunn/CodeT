{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First implementing prompting-based versions of the ideas to see if they have potential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pass at k = ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def replicate_jsonl_lines(input_file, output_file, at):\n",
    "    with open(input_file, 'r', encoding='utf-8') as infile, open(output_file, 'w', encoding='utf-8') as outfile:\n",
    "        for line in infile:\n",
    "            line_data = json.loads(line)\n",
    "\n",
    "            # Replicate the line with different instance IDs\n",
    "            for i in range(at):\n",
    "                new_line_data = line_data.copy()\n",
    "                new_line_data['metadata']['instance_id'] = f\"{line_data['metadata']['task_id']}/instance_{i}\"\n",
    "                json.dump(new_line_data, outfile)\n",
    "                outfile.write('\\n')\n",
    "\n",
    "# Usage\n",
    "                \n",
    "at = 10\n",
    "\n",
    "input_jsonl_path = 'temp_subsets/rg-one-gram-ws-20-ss-2-one-line_0.1_instruct.jsonl'  # Update this to your input file path\n",
    "output_jsonl_path = input_jsonl_path.replace(\".jsonl\",f\"_pass_at_{at}.jsonl\")\n",
    "\n",
    "replicate_jsonl_lines(input_jsonl_path, output_jsonl_path, at)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subsets of retreivals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import re\n",
    "\n",
    "def random_combinations_of_fragments(fragments, num_subsets):\n",
    "    \n",
    "    # First and last fragments assumed to be the header and footer\n",
    "    header_fragment = fragments[0]\n",
    "    footer_fragment = fragments[-1]\n",
    "    \n",
    "    # The central fragments will be randomized and distributed\n",
    "    central_fragments = fragments[1:-1]\n",
    "    random.shuffle(central_fragments)\n",
    "\n",
    "    # If the number of subsets is greater than or equal to the number of fragments, \n",
    "    # warn and copy existing fragments to make up the numbers\n",
    "    if num_subsets > len(central_fragments):\n",
    "        print(\"Warning: The number of subsets must be less than the number of unique code fragments.\")\n",
    "        print(\"Existing fragments will be copied randomly to ensure sufficient fragments.\")\n",
    "        while num_subsets > len(central_fragments):\n",
    "            central_fragments.append(random.choice(fragments[1:-1]))\n",
    "    \n",
    "    # Calculate the number of fragments in each subset\n",
    "    fragments_per_subset = len(central_fragments) // num_subsets\n",
    "\n",
    "    # Generate the random code snippets\n",
    "    snippets = []\n",
    "    for i in range(num_subsets):\n",
    "        # Calculate the start and end index of the fragments for this subset\n",
    "        start_idx = i * fragments_per_subset\n",
    "        # For the last subset, take all remaining fragments to handle cases not evenly divisible.\n",
    "        end_idx = (start_idx + fragments_per_subset) if i < num_subsets - 1 else None\n",
    "\n",
    "        # Subset of fragments for this snippet (distribution)\n",
    "        subset_fragments = central_fragments[start_idx:end_idx]\n",
    "        \n",
    "        # Concatenate header_fragment, subset_fragments, and footer_fragment to create the full snippet\n",
    "        snippet = header_fragment + '\\n' + '\\n'.join(subset_fragments) + '\\n' + footer_fragment\n",
    "        \n",
    "        # Add the completed snippet to the list of snippets\n",
    "        snippets.append(snippet)\n",
    "\n",
    "    return snippets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fragment 1:\n",
      "# Here are some relevant code fragments from other files of the repo:\n",
      "\n",
      "Fragment 2:\n",
      "# --------------------------------------------------\n",
      "# the below code fragment can be found in:\n",
      "# torchrl/envs/vec_env.py\n",
      "# --------------------------------------------------\n",
      "#     @_check_start\n",
      "#     def _shutdown_workers(self) -> None:\n",
      "#         if self.is_closed:\n",
      "#             raise RuntimeError(\n",
      "#                 \"calling {self.__class__.__name__}._shutdown_workers only allowed when env.is_closed = False\"\n",
      "#             )\n",
      "#         for i, channel in enumerate(self.parent_channels):\n",
      "#             if self._verbose:\n",
      "#                 print(f\"closing {i}\")\n",
      "#             # try:\n",
      "#             channel.send((\"close\", None))\n",
      "#             # except:\n",
      "#             #     raise RuntimeError(f\"closing {channel} number {i} failed\")\n",
      "#             msg, _ = channel.recv()\n",
      "#             if msg != \"closing\":\n",
      "#                 raise RuntimeError(\n",
      "#                     f\"Expected 'closing' but received {msg} from worker {i}\"\n",
      "#                 )\n",
      "# \n",
      "#         del self.shared_tensordicts, self.shared_tensordict_parent\n",
      "\n",
      "Fragment 3:\n",
      "# --------------------------------------------------\n",
      "# the below code fragment can be found in:\n",
      "# torchrl/envs/vec_env.py\n",
      "# --------------------------------------------------\n",
      "#         ).clone()\n",
      "# \n",
      "#     @_check_start\n",
      "#     def _shutdown_workers(self) -> None:\n",
      "#         if self.is_closed:\n",
      "#             raise RuntimeError(\n",
      "#                 \"calling {self.__class__.__name__}._shutdown_workers only allowed when env.is_closed = False\"\n",
      "#             )\n",
      "#         for i, channel in enumerate(self.parent_channels):\n",
      "#             if self._verbose:\n",
      "#                 print(f\"closing {i}\")\n",
      "#             # try:\n",
      "#             channel.send((\"close\", None))\n",
      "#             # except:\n",
      "#             #     raise RuntimeError(f\"closing {channel} number {i} failed\")\n",
      "#             msg, _ = channel.recv()\n",
      "#             if msg != \"closing\":\n",
      "#                 raise RuntimeError(\n",
      "#                     f\"Expected 'closing' but received {msg} from worker {i}\"\n",
      "#                 )\n",
      "\n",
      "Fragment 4:\n",
      "# --------------------------------------------------\n",
      "# the below code fragment can be found in:\n",
      "# build_tools/setup_helpers/extension.py\n",
      "# --------------------------------------------------\n",
      "#     and \"ATen parallel backend: OpenMP\" in torch.__config__.parallel_info()\n",
      "# )\n",
      "# _TORCH_CUDA_ARCH_LIST = os.environ.get(\"TORCH_CUDA_ARCH_LIST\", None)\n",
      "# \n",
      "# \n",
      "# def get_ext_modules():\n",
      "#     return [\n",
      "#         Extension(name=\"torchrl._torchrl\", sources=[]),\n",
      "#     ]\n",
      "# \n",
      "# \n",
      "# # Based off of\n",
      "# # https://github.com/pybind/cmake_example/blob/580c5fd29d4651db99d8874714b07c0c49a53f8a/setup.py\n",
      "# class CMakeBuild(build_ext):\n",
      "#     def run(self):\n",
      "#         try:\n",
      "#             subprocess.check_output([\"cmake\", \"--version\"])\n",
      "#         except OSError:\n",
      "#             raise RuntimeError(\"CMake is not available.\") from None\n",
      "#         super().run()\n",
      "\n",
      "Fragment 5:\n",
      "# --------------------------------------------------\n",
      "# the below code fragment can be found in:\n",
      "# build_tools/setup_helpers/extension.py\n",
      "# --------------------------------------------------\n",
      "#     return [\n",
      "#         Extension(name=\"torchrl._torchrl\", sources=[]),\n",
      "#     ]\n",
      "# \n",
      "# \n",
      "# # Based off of\n",
      "# # https://github.com/pybind/cmake_example/blob/580c5fd29d4651db99d8874714b07c0c49a53f8a/setup.py\n",
      "# class CMakeBuild(build_ext):\n",
      "#     def run(self):\n",
      "#         try:\n",
      "#             subprocess.check_output([\"cmake\", \"--version\"])\n",
      "#         except OSError:\n",
      "#             raise RuntimeError(\"CMake is not available.\") from None\n",
      "#         super().run()\n",
      "# \n",
      "#     def build_extension(self, ext):\n",
      "#         # Since two library files (libtorchrl and _torchrl) need to be\n",
      "#         # recognized by setuptools, we instantiate `Extension` twice. (see `get_ext_modules`)\n",
      "#         # This leads to the situation where this `build_extension` method is called twice.\n",
      "#         # However, the following `cmake` command will build all of them at the same time,\n",
      "\n",
      "Fragment 6:\n",
      "# --------------------------------------------------\n",
      "# the below code fragment can be found in:\n",
      "# build_tools/setup_helpers/extension.py\n",
      "# --------------------------------------------------\n",
      "# \n",
      "# def get_ext_modules():\n",
      "#     return [\n",
      "#         Extension(name=\"torchrl._torchrl\", sources=[]),\n",
      "#     ]\n",
      "# \n",
      "# \n",
      "# # Based off of\n",
      "# # https://github.com/pybind/cmake_example/blob/580c5fd29d4651db99d8874714b07c0c49a53f8a/setup.py\n",
      "# class CMakeBuild(build_ext):\n",
      "#     def run(self):\n",
      "#         try:\n",
      "#             subprocess.check_output([\"cmake\", \"--version\"])\n",
      "#         except OSError:\n",
      "#             raise RuntimeError(\"CMake is not available.\") from None\n",
      "#         super().run()\n",
      "# \n",
      "#     def build_extension(self, ext):\n",
      "#         # Since two library files (libtorchrl and _torchrl) need to be\n",
      "#         # recognized by setuptools, we instantiate `Extension` twice. (see `get_ext_modules`)\n",
      "\n",
      "Fragment 7:\n",
      "# --------------------------------------------------\n",
      "# the below code fragment can be found in:\n",
      "# build_tools/setup_helpers/extension.py\n",
      "# --------------------------------------------------\n",
      "#                 stderr=STDOUT,\n",
      "#             )\n",
      "#         except CalledProcessError as exc:\n",
      "#             print(exc.output)\n",
      "# \n",
      "#         try:\n",
      "#             check_output(\n",
      "#                 [\"cmake\", \"--build\", \".\"] + build_args,\n",
      "#                 cwd=self.build_temp,\n",
      "#                 stderr=STDOUT,\n",
      "#             )\n",
      "#         except CalledProcessError as exc:\n",
      "#             print(exc.output)\n",
      "# \n",
      "#     def get_ext_filename(self, fullname):\n",
      "#         ext_filename = super().get_ext_filename(fullname)\n",
      "#         ext_filename_parts = ext_filename.split(\".\")\n",
      "#         without_abi = ext_filename_parts[:-2] + ext_filename_parts[-1:]\n",
      "#         ext_filename = \".\".join(without_abi)\n",
      "#         return ext_filename\n",
      "\n",
      "Fragment 8:\n",
      "# --------------------------------------------------\n",
      "\"\"\"Based on the above, complete the following code from the main script file:\"\"\"\n",
      "# build_tools/setup_helpers/extension.py\n",
      "# --------------------------------------------------\n",
      "#                 [\"cmake\", str(_ROOT_DIR)] + cmake_args,\n",
      "#                 cwd=self.build_temp,\n",
      "#                 stderr=STDOUT,\n",
      "#             )\n",
      "#         except CalledProcessError as exc:\n",
      "#             print(exc.output)\n",
      "# \n",
      "#         try:\n",
      "#             check_output(\n",
      "#                 [\"cmake\", \"--build\", \".\"] + build_args,\n",
      "#                 cwd=self.build_temp,\n",
      "#                 stderr=STDOUT,\n",
      "#             )\n",
      "#         except CalledProcessError as exc:\n",
      "#             print(exc.output)\n",
      "# \n",
      "#     def get_ext_filename(self, fullname):\n",
      "#         ext_filename = super().get_ext_filename(fullname)\n",
      "#         ext_filename_parts = ext_filename.split(\".\")\n",
      "#         without_abi = ext_filename_parts[:-2] + ext_filename_parts[-1:]\n",
      "# --------------------------------------------------\n",
      "\"\"\"Based on the above, complete the following code:\"\"\"\n",
      "\n",
      "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
      "#\n",
      "# This source code is licensed under the MIT license found in the\n",
      "# LICENSE file in the root directory of this source tree.\n",
      "import argparse\n",
      "import distutils.command.clean\n",
      "import glob\n",
      "import os\n",
      "import shutil\n",
      "import subprocess\n",
      "import sys\n",
      "from datetime import date\n",
      "from pathlib import Path\n",
      "from typing import List\n",
      "\n",
      "from setuptools import find_packages, setup\n",
      "from torch.utils.cpp_extension import BuildExtension, CppExtension\n",
      "\n",
      "cwd = os.path.dirname(os.path.abspath(__file__))\n",
      "try:\n",
      "    sha = (\n",
      "        subprocess.check_output([\"git\", \"rev-parse\", \"HEAD\"], cwd=cwd)\n",
      "        .decode(\"ascii\")\n",
      "        .strip()\n",
      "    )\n",
      "except Exception:\n",
      "    sha = \"Unknown\"\n",
      "\n",
      "\n",
      "def get_version():\n",
      "    version_txt = os.path.join(cwd, \"version.txt\")\n",
      "    with open(version_txt, \"r\") as f:\n",
      "        version = f.readline().strip()\n",
      "    if os.getenv(\"BUILD_VERSION\"):\n",
      "        version = os.getenv(\"BUILD_VERSION\")\n",
      "    elif sha != \"Unknown\":\n",
      "        version += \"+\" + sha[:7]\n",
      "    return version\n",
      "\n",
      "\n",
      "ROOT_DIR = Path(__file__).parent.resolve()\n",
      "\n",
      "\n",
      "package_name = \"torchrl\"\n",
      "\n",
      "\n",
      "def get_nightly_version():\n",
      "    today = date.today()\n",
      "    return f\"{today.year}.{today.month}.{today.day}\"\n",
      "\n",
      "\n",
      "def parse_args(argv: List[str]) -> argparse.Namespace:\n",
      "    parser = argparse.ArgumentParser(description=\"torchrl setup\")\n",
      "    parser.add_argument(\n",
      "        \"--package_name\",\n",
      "        type=str,\n",
      "        default=\"torchrl\",\n",
      "        help=\"the name of this output wheel\",\n",
      "    )\n",
      "    return parser.parse_known_args(argv)\n",
      "\n",
      "\n",
      "def write_version_file(version):\n",
      "    version_path = os.path.join(cwd, \"torchrl\", \"version.py\")\n",
      "    with open(version_path, \"w\") as f:\n",
      "        f.write(\"__version__ = '{}'\n",
      "\".format(version))\n",
      "        f.write(\"git_version = {}\n",
      "\".format(repr(sha)))\n",
      "\n",
      "\n",
      "def _get_pytorch_version():\n",
      "    # if \"PYTORCH_VERSION\" in os.environ:\n",
      "    #     return f\"torch=={os.environ['PYTORCH_VERSION']}\"\n",
      "    return \"torch\"\n",
      "\n",
      "\n",
      "def _get_packages():\n",
      "    exclude = [\n",
      "        \"build*\",\n",
      "        \"test*\",\n",
      "        \"torchrl.csrc*\",\n",
      "        \"third_party*\",\n",
      "        \"tools*\",\n",
      "    ]\n",
      "    return find_packages(exclude=exclude)\n",
      "\n",
      "\n",
      "ROOT_DIR = Path(__file__).parent.resolve()\n",
      "\n",
      "\n",
      "class clean(distutils.command.clean.clean):\n",
      "    def run(self):\n",
      "        # Run default behavior first\n",
      "        distutils.command.clean.clean.run(self)\n",
      "\n",
      "        # Remove torchrl extension\n",
      "        for path in (ROOT_DIR / \"torchrl\").glob(\"**/*.so\"):\n",
      "            print(f\"removing '{path}'\")\n",
      "            path.unlink()\n",
      "        # Remove build directory\n",
      "        build_dirs = [\n",
      "            ROOT_DIR / \"build\",\n",
      "        ]\n",
      "        for path in build_dirs:\n",
      "            if path.exists():\n",
      "                print(f\"removing '{path}' (and everything under it)\")\n",
      "                shutil.rmtree(str(path), ignore_errors=True)\n",
      "\n",
      "\n",
      "# def _run_cmd(cmd):\n",
      "#     try:\n",
      "#         return subprocess.check_output(cmd, cwd=ROOT_DIR).decode(\"ascii\").strip()\n",
      "#     except Exception:\n",
      "#         return None\n",
      "\n",
      "\n",
      "def get_extensions():\n",
      "    extension = CppExtension\n",
      "\n",
      "    extra_link_args = []\n",
      "    extra_compile_args = {\n",
      "        \"cxx\": [\n",
      "            \"-O3\",\n",
      "            \"-std=c++14\",\n",
      "            \"-fdiagnostics-color=always\",\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "ENDING_COMMAND = '# --------------------------------------------------\\n\"\"\"Based on the above, complete the following code from the main script file:\"\"\"'\n",
    "\n",
    "def split_code_fragments(input_string):\n",
    "    # Define the pattern for delimiters which starts and ends with a line of '-'\n",
    "    delimiter_pattern = r'# -+\\s+# the below code fragment can be found in:|' + ENDING_COMMAND\n",
    "\n",
    "    # Use re.split() to split the input string into parts using the pattern\n",
    "    fragments = re.split(delimiter_pattern, input_string)\n",
    "\n",
    "    # Filter out any empty strings that may have been added during the split\n",
    "    fragments = [fragment.strip() for fragment in fragments if fragment.strip()]\n",
    "\n",
    "    # Add back the delimiter header for the second fragment onwards\n",
    "    intro_line = '# --------------------------------------------------\\n# the below code fragment can be found in:'\n",
    "    for i in range(1, len(fragments)):\n",
    "        if i < len(fragments) - 1:\n",
    "            fragments[i] = intro_line + '\\n' + fragments[i]\n",
    "        else:\n",
    "            fragments[i] = ENDING_COMMAND + '\\n' + fragments[i]\n",
    "\n",
    "    return fragments\n",
    "\n",
    "input_string = '''# Here are some relevant code fragments from other files of the repo:\n",
    "# --------------------------------------------------\n",
    "# the below code fragment can be found in:\n",
    "# torchrl/envs/vec_env.py\n",
    "# --------------------------------------------------\n",
    "#     @_check_start\n",
    "#     def _shutdown_workers(self) -> None:\n",
    "#         if self.is_closed:\n",
    "#             raise RuntimeError(\n",
    "#                 \"calling {self.__class__.__name__}._shutdown_workers only allowed when env.is_closed = False\"\n",
    "#             )\n",
    "#         for i, channel in enumerate(self.parent_channels):\n",
    "#             if self._verbose:\n",
    "#                 print(f\"closing {i}\")\n",
    "#             # try:\n",
    "#             channel.send((\"close\", None))\n",
    "#             # except:\n",
    "#             #     raise RuntimeError(f\"closing {channel} number {i} failed\")\n",
    "#             msg, _ = channel.recv()\n",
    "#             if msg != \"closing\":\n",
    "#                 raise RuntimeError(\n",
    "#                     f\"Expected 'closing' but received {msg} from worker {i}\"\n",
    "#                 )\n",
    "# \n",
    "#         del self.shared_tensordicts, self.shared_tensordict_parent\n",
    "# --------------------------------------------------\n",
    "# the below code fragment can be found in:\n",
    "# torchrl/envs/vec_env.py\n",
    "# --------------------------------------------------\n",
    "#         ).clone()\n",
    "# \n",
    "#     @_check_start\n",
    "#     def _shutdown_workers(self) -> None:\n",
    "#         if self.is_closed:\n",
    "#             raise RuntimeError(\n",
    "#                 \"calling {self.__class__.__name__}._shutdown_workers only allowed when env.is_closed = False\"\n",
    "#             )\n",
    "#         for i, channel in enumerate(self.parent_channels):\n",
    "#             if self._verbose:\n",
    "#                 print(f\"closing {i}\")\n",
    "#             # try:\n",
    "#             channel.send((\"close\", None))\n",
    "#             # except:\n",
    "#             #     raise RuntimeError(f\"closing {channel} number {i} failed\")\n",
    "#             msg, _ = channel.recv()\n",
    "#             if msg != \"closing\":\n",
    "#                 raise RuntimeError(\n",
    "#                     f\"Expected 'closing' but received {msg} from worker {i}\"\n",
    "#                 )\n",
    "# --------------------------------------------------\n",
    "# the below code fragment can be found in:\n",
    "# build_tools/setup_helpers/extension.py\n",
    "# --------------------------------------------------\n",
    "#     and \"ATen parallel backend: OpenMP\" in torch.__config__.parallel_info()\n",
    "# )\n",
    "# _TORCH_CUDA_ARCH_LIST = os.environ.get(\"TORCH_CUDA_ARCH_LIST\", None)\n",
    "# \n",
    "# \n",
    "# def get_ext_modules():\n",
    "#     return [\n",
    "#         Extension(name=\"torchrl._torchrl\", sources=[]),\n",
    "#     ]\n",
    "# \n",
    "# \n",
    "# # Based off of\n",
    "# # https://github.com/pybind/cmake_example/blob/580c5fd29d4651db99d8874714b07c0c49a53f8a/setup.py\n",
    "# class CMakeBuild(build_ext):\n",
    "#     def run(self):\n",
    "#         try:\n",
    "#             subprocess.check_output([\"cmake\", \"--version\"])\n",
    "#         except OSError:\n",
    "#             raise RuntimeError(\"CMake is not available.\") from None\n",
    "#         super().run()\n",
    "# --------------------------------------------------\n",
    "# the below code fragment can be found in:\n",
    "# build_tools/setup_helpers/extension.py\n",
    "# --------------------------------------------------\n",
    "#     return [\n",
    "#         Extension(name=\"torchrl._torchrl\", sources=[]),\n",
    "#     ]\n",
    "# \n",
    "# \n",
    "# # Based off of\n",
    "# # https://github.com/pybind/cmake_example/blob/580c5fd29d4651db99d8874714b07c0c49a53f8a/setup.py\n",
    "# class CMakeBuild(build_ext):\n",
    "#     def run(self):\n",
    "#         try:\n",
    "#             subprocess.check_output([\"cmake\", \"--version\"])\n",
    "#         except OSError:\n",
    "#             raise RuntimeError(\"CMake is not available.\") from None\n",
    "#         super().run()\n",
    "# \n",
    "#     def build_extension(self, ext):\n",
    "#         # Since two library files (libtorchrl and _torchrl) need to be\n",
    "#         # recognized by setuptools, we instantiate `Extension` twice. (see `get_ext_modules`)\n",
    "#         # This leads to the situation where this `build_extension` method is called twice.\n",
    "#         # However, the following `cmake` command will build all of them at the same time,\n",
    "# --------------------------------------------------\n",
    "# the below code fragment can be found in:\n",
    "# build_tools/setup_helpers/extension.py\n",
    "# --------------------------------------------------\n",
    "# \n",
    "# def get_ext_modules():\n",
    "#     return [\n",
    "#         Extension(name=\"torchrl._torchrl\", sources=[]),\n",
    "#     ]\n",
    "# \n",
    "# \n",
    "# # Based off of\n",
    "# # https://github.com/pybind/cmake_example/blob/580c5fd29d4651db99d8874714b07c0c49a53f8a/setup.py\n",
    "# class CMakeBuild(build_ext):\n",
    "#     def run(self):\n",
    "#         try:\n",
    "#             subprocess.check_output([\"cmake\", \"--version\"])\n",
    "#         except OSError:\n",
    "#             raise RuntimeError(\"CMake is not available.\") from None\n",
    "#         super().run()\n",
    "# \n",
    "#     def build_extension(self, ext):\n",
    "#         # Since two library files (libtorchrl and _torchrl) need to be\n",
    "#         # recognized by setuptools, we instantiate `Extension` twice. (see `get_ext_modules`)\n",
    "# --------------------------------------------------\n",
    "# the below code fragment can be found in:\n",
    "# build_tools/setup_helpers/extension.py\n",
    "# --------------------------------------------------\n",
    "#                 stderr=STDOUT,\n",
    "#             )\n",
    "#         except CalledProcessError as exc:\n",
    "#             print(exc.output)\n",
    "# \n",
    "#         try:\n",
    "#             check_output(\n",
    "#                 [\"cmake\", \"--build\", \".\"] + build_args,\n",
    "#                 cwd=self.build_temp,\n",
    "#                 stderr=STDOUT,\n",
    "#             )\n",
    "#         except CalledProcessError as exc:\n",
    "#             print(exc.output)\n",
    "# \n",
    "#     def get_ext_filename(self, fullname):\n",
    "#         ext_filename = super().get_ext_filename(fullname)\n",
    "#         ext_filename_parts = ext_filename.split(\".\")\n",
    "#         without_abi = ext_filename_parts[:-2] + ext_filename_parts[-1:]\n",
    "#         ext_filename = \".\".join(without_abi)\n",
    "#         return ext_filename\n",
    "# --------------------------------------------------\n",
    "# the below code fragment can be found in:\n",
    "# build_tools/setup_helpers/extension.py\n",
    "# --------------------------------------------------\n",
    "#                 [\"cmake\", str(_ROOT_DIR)] + cmake_args,\n",
    "#                 cwd=self.build_temp,\n",
    "#                 stderr=STDOUT,\n",
    "#             )\n",
    "#         except CalledProcessError as exc:\n",
    "#             print(exc.output)\n",
    "# \n",
    "#         try:\n",
    "#             check_output(\n",
    "#                 [\"cmake\", \"--build\", \".\"] + build_args,\n",
    "#                 cwd=self.build_temp,\n",
    "#                 stderr=STDOUT,\n",
    "#             )\n",
    "#         except CalledProcessError as exc:\n",
    "#             print(exc.output)\n",
    "# \n",
    "#     def get_ext_filename(self, fullname):\n",
    "#         ext_filename = super().get_ext_filename(fullname)\n",
    "#         ext_filename_parts = ext_filename.split(\".\")\n",
    "#         without_abi = ext_filename_parts[:-2] + ext_filename_parts[-1:]\n",
    "# --------------------------------------------------\n",
    "\"\"\"Based on the above, complete the following code:\"\"\"\n",
    "\n",
    "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
    "#\n",
    "# This source code is licensed under the MIT license found in the\n",
    "# LICENSE file in the root directory of this source tree.\n",
    "import argparse\n",
    "import distutils.command.clean\n",
    "import glob\n",
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "import sys\n",
    "from datetime import date\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "from setuptools import find_packages, setup\n",
    "from torch.utils.cpp_extension import BuildExtension, CppExtension\n",
    "\n",
    "cwd = os.path.dirname(os.path.abspath(__file__))\n",
    "try:\n",
    "    sha = (\n",
    "        subprocess.check_output([\"git\", \"rev-parse\", \"HEAD\"], cwd=cwd)\n",
    "        .decode(\"ascii\")\n",
    "        .strip()\n",
    "    )\n",
    "except Exception:\n",
    "    sha = \"Unknown\"\n",
    "\n",
    "\n",
    "def get_version():\n",
    "    version_txt = os.path.join(cwd, \"version.txt\")\n",
    "    with open(version_txt, \"r\") as f:\n",
    "        version = f.readline().strip()\n",
    "    if os.getenv(\"BUILD_VERSION\"):\n",
    "        version = os.getenv(\"BUILD_VERSION\")\n",
    "    elif sha != \"Unknown\":\n",
    "        version += \"+\" + sha[:7]\n",
    "    return version\n",
    "\n",
    "\n",
    "ROOT_DIR = Path(__file__).parent.resolve()\n",
    "\n",
    "\n",
    "package_name = \"torchrl\"\n",
    "\n",
    "\n",
    "def get_nightly_version():\n",
    "    today = date.today()\n",
    "    return f\"{today.year}.{today.month}.{today.day}\"\n",
    "\n",
    "\n",
    "def parse_args(argv: List[str]) -> argparse.Namespace:\n",
    "    parser = argparse.ArgumentParser(description=\"torchrl setup\")\n",
    "    parser.add_argument(\n",
    "        \"--package_name\",\n",
    "        type=str,\n",
    "        default=\"torchrl\",\n",
    "        help=\"the name of this output wheel\",\n",
    "    )\n",
    "    return parser.parse_known_args(argv)\n",
    "\n",
    "\n",
    "def write_version_file(version):\n",
    "    version_path = os.path.join(cwd, \"torchrl\", \"version.py\")\n",
    "    with open(version_path, \"w\") as f:\n",
    "        f.write(\"__version__ = '{}'\\n\".format(version))\n",
    "        f.write(\"git_version = {}\\n\".format(repr(sha)))\n",
    "\n",
    "\n",
    "def _get_pytorch_version():\n",
    "    # if \"PYTORCH_VERSION\" in os.environ:\n",
    "    #     return f\"torch=={os.environ['PYTORCH_VERSION']}\"\n",
    "    return \"torch\"\n",
    "\n",
    "\n",
    "def _get_packages():\n",
    "    exclude = [\n",
    "        \"build*\",\n",
    "        \"test*\",\n",
    "        \"torchrl.csrc*\",\n",
    "        \"third_party*\",\n",
    "        \"tools*\",\n",
    "    ]\n",
    "    return find_packages(exclude=exclude)\n",
    "\n",
    "\n",
    "ROOT_DIR = Path(__file__).parent.resolve()\n",
    "\n",
    "\n",
    "class clean(distutils.command.clean.clean):\n",
    "    def run(self):\n",
    "        # Run default behavior first\n",
    "        distutils.command.clean.clean.run(self)\n",
    "\n",
    "        # Remove torchrl extension\n",
    "        for path in (ROOT_DIR / \"torchrl\").glob(\"**/*.so\"):\n",
    "            print(f\"removing '{path}'\")\n",
    "            path.unlink()\n",
    "        # Remove build directory\n",
    "        build_dirs = [\n",
    "            ROOT_DIR / \"build\",\n",
    "        ]\n",
    "        for path in build_dirs:\n",
    "            if path.exists():\n",
    "                print(f\"removing '{path}' (and everything under it)\")\n",
    "                shutil.rmtree(str(path), ignore_errors=True)\n",
    "\n",
    "\n",
    "# def _run_cmd(cmd):\n",
    "#     try:\n",
    "#         return subprocess.check_output(cmd, cwd=ROOT_DIR).decode(\"ascii\").strip()\n",
    "#     except Exception:\n",
    "#         return None\n",
    "\n",
    "\n",
    "def get_extensions():\n",
    "    extension = CppExtension\n",
    "\n",
    "    extra_link_args = []\n",
    "    extra_compile_args = {\n",
    "        \"cxx\": [\n",
    "            \"-O3\",\n",
    "            \"-std=c++14\",\n",
    "            \"-fdiagnostics-color=always\",\n",
    "'''\n",
    "\n",
    "# Get the list of code fragments and other information\n",
    "fragments = split_code_fragments(input_string)\n",
    "\n",
    "# Now you can process, output, or use the fragments as needed\n",
    "for i, fragment in enumerate(fragments):\n",
    "    print(f\"Fragment {i+1}:\\n{fragment}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Here are some relevant code fragments from other files of the repo:\n",
      "# --------------------------------------------------\n",
      "# the below code fragment can be found in:\n",
      "# build_tools/setup_helpers/extension.py\n",
      "# --------------------------------------------------\n",
      "#     return [\n",
      "#         Extension(name=\"torchrl._torchrl\", sources=[]),\n",
      "#     ]\n",
      "# \n",
      "# \n",
      "# # Based off of\n",
      "# # https://github.com/pybind/cmake_example/blob/580c5fd29d4651db99d8874714b07c0c49a53f8a/setup.py\n",
      "# class CMakeBuild(build_ext):\n",
      "#     def run(self):\n",
      "#         try:\n",
      "#             subprocess.check_output([\"cmake\", \"--version\"])\n",
      "#         except OSError:\n",
      "#             raise RuntimeError(\"CMake is not available.\") from None\n",
      "#         super().run()\n",
      "# \n",
      "#     def build_extension(self, ext):\n",
      "#         # Since two library files (libtorchrl and _torchrl) need to be\n",
      "#         # recognized by setuptools, we instantiate `Extension` twice. (see `get_ext_modules`)\n",
      "#         # This leads to the situation where this `build_extension` method is called twice.\n",
      "#         # However, the following `cmake` command will build all of them at the same time,\n",
      "# --------------------------------------------------\n",
      "# the below code fragment can be found in:\n",
      "# torchrl/envs/vec_env.py\n",
      "# --------------------------------------------------\n",
      "#         ).clone()\n",
      "# \n",
      "#     @_check_start\n",
      "#     def _shutdown_workers(self) -> None:\n",
      "#         if self.is_closed:\n",
      "#             raise RuntimeError(\n",
      "#                 \"calling {self.__class__.__name__}._shutdown_workers only allowed when env.is_closed = False\"\n",
      "#             )\n",
      "#         for i, channel in enumerate(self.parent_channels):\n",
      "#             if self._verbose:\n",
      "#                 print(f\"closing {i}\")\n",
      "#             # try:\n",
      "#             channel.send((\"close\", None))\n",
      "#             # except:\n",
      "#             #     raise RuntimeError(f\"closing {channel} number {i} failed\")\n",
      "#             msg, _ = channel.recv()\n",
      "#             if msg != \"closing\":\n",
      "#                 raise RuntimeError(\n",
      "#                     f\"Expected 'closing' but received {msg} from worker {i}\"\n",
      "#                 )\n",
      "# --------------------------------------------------\n",
      "\"\"\"Based on the above, complete the following code:\"\"\"'\n",
      "\n",
      "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
      "#\n",
      "# This source code is licensed under the MIT license found in the\n",
      "# LICENSE file in the root directory of this source tree.\n",
      "import argparse\n",
      "import distutils.command.clean\n",
      "import glob\n",
      "import os\n",
      "import shutil\n",
      "import subprocess\n",
      "import sys\n",
      "from datetime import date\n",
      "from pathlib import Path\n",
      "from typing import List\n",
      "\n",
      "from setuptools import find_packages, setup\n",
      "from torch.utils.cpp_extension import BuildExtension, CppExtension\n",
      "\n",
      "cwd = os.path.dirname(os.path.abspath(__file__))\n",
      "try:\n",
      "    sha = (\n",
      "        subprocess.check_output([\"git\", \"rev-parse\", \"HEAD\"], cwd=cwd)\n",
      "        .decode(\"ascii\")\n",
      "        .strip()\n",
      "    )\n",
      "except Exception:\n",
      "    sha = \"Unknown\"\n",
      "\n",
      "\n",
      "def get_version():\n",
      "    version_txt = os.path.join(cwd, \"version.txt\")\n",
      "    with open(version_txt, \"r\") as f:\n",
      "        version = f.readline().strip()\n",
      "    if os.getenv(\"BUILD_VERSION\"):\n",
      "        version = os.getenv(\"BUILD_VERSION\")\n",
      "    elif sha != \"Unknown\":\n",
      "        version += \"+\" + sha[:7]\n",
      "    return version\n",
      "\n",
      "\n",
      "ROOT_DIR = Path(__file__).parent.resolve()\n",
      "\n",
      "\n",
      "package_name = \"torchrl\"\n",
      "\n",
      "\n",
      "def get_nightly_version():\n",
      "    today = date.today()\n",
      "    return f\"{today.year}.{today.month}.{today.day}\"\n",
      "\n",
      "\n",
      "def parse_args(argv: List[str]) -> argparse.Namespace:\n",
      "    parser = argparse.ArgumentParser(description=\"torchrl setup\")\n",
      "    parser.add_argument(\n",
      "        \"--package_name\",\n",
      "        type=str,\n",
      "        default=\"torchrl\",\n",
      "        help=\"the name of this output wheel\",\n",
      "    )\n",
      "    return parser.parse_known_args(argv)\n",
      "\n",
      "\n",
      "def write_version_file(version):\n",
      "    version_path = os.path.join(cwd, \"torchrl\", \"version.py\")\n",
      "    with open(version_path, \"w\") as f:\n",
      "        f.write(\"__version__ = '{}'\n",
      "\".format(version))\n",
      "        f.write(\"git_version = {}\n",
      "\".format(repr(sha)))\n",
      "\n",
      "\n",
      "def _get_pytorch_version():\n",
      "    # if \"PYTORCH_VERSION\" in os.environ:\n",
      "    #     return f\"torch=={os.environ['PYTORCH_VERSION']}\"\n",
      "    return \"torch\"\n",
      "\n",
      "\n",
      "def _get_packages():\n",
      "    exclude = [\n",
      "        \"build*\",\n",
      "        \"test*\",\n",
      "        \"torchrl.csrc*\",\n",
      "        \"third_party*\",\n",
      "        \"tools*\",\n",
      "    ]\n",
      "    return find_packages(exclude=exclude)\n",
      "\n",
      "\n",
      "ROOT_DIR = Path(__file__).parent.resolve()\n",
      "\n",
      "\n",
      "class clean(distutils.command.clean.clean):\n",
      "    def run(self):\n",
      "        # Run default behavior first\n",
      "        distutils.command.clean.clean.run(self)\n",
      "\n",
      "        # Remove torchrl extension\n",
      "        for path in (ROOT_DIR / \"torchrl\").glob(\"**/*.so\"):\n",
      "            print(f\"removing '{path}'\")\n",
      "            path.unlink()\n",
      "        # Remove build directory\n",
      "        build_dirs = [\n",
      "            ROOT_DIR / \"build\",\n",
      "        ]\n",
      "        for path in build_dirs:\n",
      "            if path.exists():\n",
      "                print(f\"removing '{path}' (and everything under it)\")\n",
      "                shutil.rmtree(str(path), ignore_errors=True)\n",
      "\n",
      "\n",
      "# def _run_cmd(cmd):\n",
      "#     try:\n",
      "#         return subprocess.check_output(cmd, cwd=ROOT_DIR).decode(\"ascii\").strip()\n",
      "#     except Exception:\n",
      "#         return None\n",
      "\n",
      "\n",
      "def get_extensions():\n",
      "    extension = CppExtension\n",
      "\n",
      "    extra_link_args = []\n",
      "    extra_compile_args = {\n",
      "        \"cxx\": [\n",
      "            \"-O3\",\n",
      "            \"-std=c++14\",\n",
      "            \"-fdiagnostics-color=always\",\n",
      "# Here are some relevant code fragments from other files of the repo:\n",
      "# --------------------------------------------------\n",
      "# the below code fragment can be found in:\n",
      "# build_tools/setup_helpers/extension.py\n",
      "# --------------------------------------------------\n",
      "#     and \"ATen parallel backend: OpenMP\" in torch.__config__.parallel_info()\n",
      "# )\n",
      "# _TORCH_CUDA_ARCH_LIST = os.environ.get(\"TORCH_CUDA_ARCH_LIST\", None)\n",
      "# \n",
      "# \n",
      "# def get_ext_modules():\n",
      "#     return [\n",
      "#         Extension(name=\"torchrl._torchrl\", sources=[]),\n",
      "#     ]\n",
      "# \n",
      "# \n",
      "# # Based off of\n",
      "# # https://github.com/pybind/cmake_example/blob/580c5fd29d4651db99d8874714b07c0c49a53f8a/setup.py\n",
      "# class CMakeBuild(build_ext):\n",
      "#     def run(self):\n",
      "#         try:\n",
      "#             subprocess.check_output([\"cmake\", \"--version\"])\n",
      "#         except OSError:\n",
      "#             raise RuntimeError(\"CMake is not available.\") from None\n",
      "#         super().run()\n",
      "# --------------------------------------------------\n",
      "# the below code fragment can be found in:\n",
      "# build_tools/setup_helpers/extension.py\n",
      "# --------------------------------------------------\n",
      "#                 [\"cmake\", str(_ROOT_DIR)] + cmake_args,\n",
      "#                 cwd=self.build_temp,\n",
      "#                 stderr=STDOUT,\n",
      "#             )\n",
      "#         except CalledProcessError as exc:\n",
      "#             print(exc.output)\n",
      "# \n",
      "#         try:\n",
      "#             check_output(\n",
      "#                 [\"cmake\", \"--build\", \".\"] + build_args,\n",
      "#                 cwd=self.build_temp,\n",
      "#                 stderr=STDOUT,\n",
      "#             )\n",
      "#         except CalledProcessError as exc:\n",
      "#             print(exc.output)\n",
      "# \n",
      "#     def get_ext_filename(self, fullname):\n",
      "#         ext_filename = super().get_ext_filename(fullname)\n",
      "#         ext_filename_parts = ext_filename.split(\".\")\n",
      "#         without_abi = ext_filename_parts[:-2] + ext_filename_parts[-1:]\n",
      "# --------------------------------------------------\n",
      "\"\"\"Based on the above, complete the following code:\"\"\"'\n",
      "\n",
      "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
      "#\n",
      "# This source code is licensed under the MIT license found in the\n",
      "# LICENSE file in the root directory of this source tree.\n",
      "import argparse\n",
      "import distutils.command.clean\n",
      "import glob\n",
      "import os\n",
      "import shutil\n",
      "import subprocess\n",
      "import sys\n",
      "from datetime import date\n",
      "from pathlib import Path\n",
      "from typing import List\n",
      "\n",
      "from setuptools import find_packages, setup\n",
      "from torch.utils.cpp_extension import BuildExtension, CppExtension\n",
      "\n",
      "cwd = os.path.dirname(os.path.abspath(__file__))\n",
      "try:\n",
      "    sha = (\n",
      "        subprocess.check_output([\"git\", \"rev-parse\", \"HEAD\"], cwd=cwd)\n",
      "        .decode(\"ascii\")\n",
      "        .strip()\n",
      "    )\n",
      "except Exception:\n",
      "    sha = \"Unknown\"\n",
      "\n",
      "\n",
      "def get_version():\n",
      "    version_txt = os.path.join(cwd, \"version.txt\")\n",
      "    with open(version_txt, \"r\") as f:\n",
      "        version = f.readline().strip()\n",
      "    if os.getenv(\"BUILD_VERSION\"):\n",
      "        version = os.getenv(\"BUILD_VERSION\")\n",
      "    elif sha != \"Unknown\":\n",
      "        version += \"+\" + sha[:7]\n",
      "    return version\n",
      "\n",
      "\n",
      "ROOT_DIR = Path(__file__).parent.resolve()\n",
      "\n",
      "\n",
      "package_name = \"torchrl\"\n",
      "\n",
      "\n",
      "def get_nightly_version():\n",
      "    today = date.today()\n",
      "    return f\"{today.year}.{today.month}.{today.day}\"\n",
      "\n",
      "\n",
      "def parse_args(argv: List[str]) -> argparse.Namespace:\n",
      "    parser = argparse.ArgumentParser(description=\"torchrl setup\")\n",
      "    parser.add_argument(\n",
      "        \"--package_name\",\n",
      "        type=str,\n",
      "        default=\"torchrl\",\n",
      "        help=\"the name of this output wheel\",\n",
      "    )\n",
      "    return parser.parse_known_args(argv)\n",
      "\n",
      "\n",
      "def write_version_file(version):\n",
      "    version_path = os.path.join(cwd, \"torchrl\", \"version.py\")\n",
      "    with open(version_path, \"w\") as f:\n",
      "        f.write(\"__version__ = '{}'\n",
      "\".format(version))\n",
      "        f.write(\"git_version = {}\n",
      "\".format(repr(sha)))\n",
      "\n",
      "\n",
      "def _get_pytorch_version():\n",
      "    # if \"PYTORCH_VERSION\" in os.environ:\n",
      "    #     return f\"torch=={os.environ['PYTORCH_VERSION']}\"\n",
      "    return \"torch\"\n",
      "\n",
      "\n",
      "def _get_packages():\n",
      "    exclude = [\n",
      "        \"build*\",\n",
      "        \"test*\",\n",
      "        \"torchrl.csrc*\",\n",
      "        \"third_party*\",\n",
      "        \"tools*\",\n",
      "    ]\n",
      "    return find_packages(exclude=exclude)\n",
      "\n",
      "\n",
      "ROOT_DIR = Path(__file__).parent.resolve()\n",
      "\n",
      "\n",
      "class clean(distutils.command.clean.clean):\n",
      "    def run(self):\n",
      "        # Run default behavior first\n",
      "        distutils.command.clean.clean.run(self)\n",
      "\n",
      "        # Remove torchrl extension\n",
      "        for path in (ROOT_DIR / \"torchrl\").glob(\"**/*.so\"):\n",
      "            print(f\"removing '{path}'\")\n",
      "            path.unlink()\n",
      "        # Remove build directory\n",
      "        build_dirs = [\n",
      "            ROOT_DIR / \"build\",\n",
      "        ]\n",
      "        for path in build_dirs:\n",
      "            if path.exists():\n",
      "                print(f\"removing '{path}' (and everything under it)\")\n",
      "                shutil.rmtree(str(path), ignore_errors=True)\n",
      "\n",
      "\n",
      "# def _run_cmd(cmd):\n",
      "#     try:\n",
      "#         return subprocess.check_output(cmd, cwd=ROOT_DIR).decode(\"ascii\").strip()\n",
      "#     except Exception:\n",
      "#         return None\n",
      "\n",
      "\n",
      "def get_extensions():\n",
      "    extension = CppExtension\n",
      "\n",
      "    extra_link_args = []\n",
      "    extra_compile_args = {\n",
      "        \"cxx\": [\n",
      "            \"-O3\",\n",
      "            \"-std=c++14\",\n",
      "            \"-fdiagnostics-color=always\",\n",
      "# Here are some relevant code fragments from other files of the repo:\n",
      "# --------------------------------------------------\n",
      "# the below code fragment can be found in:\n",
      "# build_tools/setup_helpers/extension.py\n",
      "# --------------------------------------------------\n",
      "# \n",
      "# def get_ext_modules():\n",
      "#     return [\n",
      "#         Extension(name=\"torchrl._torchrl\", sources=[]),\n",
      "#     ]\n",
      "# \n",
      "# \n",
      "# # Based off of\n",
      "# # https://github.com/pybind/cmake_example/blob/580c5fd29d4651db99d8874714b07c0c49a53f8a/setup.py\n",
      "# class CMakeBuild(build_ext):\n",
      "#     def run(self):\n",
      "#         try:\n",
      "#             subprocess.check_output([\"cmake\", \"--version\"])\n",
      "#         except OSError:\n",
      "#             raise RuntimeError(\"CMake is not available.\") from None\n",
      "#         super().run()\n",
      "# \n",
      "#     def build_extension(self, ext):\n",
      "#         # Since two library files (libtorchrl and _torchrl) need to be\n",
      "#         # recognized by setuptools, we instantiate `Extension` twice. (see `get_ext_modules`)\n",
      "# --------------------------------------------------\n",
      "# the below code fragment can be found in:\n",
      "# torchrl/envs/vec_env.py\n",
      "# --------------------------------------------------\n",
      "#     @_check_start\n",
      "#     def _shutdown_workers(self) -> None:\n",
      "#         if self.is_closed:\n",
      "#             raise RuntimeError(\n",
      "#                 \"calling {self.__class__.__name__}._shutdown_workers only allowed when env.is_closed = False\"\n",
      "#             )\n",
      "#         for i, channel in enumerate(self.parent_channels):\n",
      "#             if self._verbose:\n",
      "#                 print(f\"closing {i}\")\n",
      "#             # try:\n",
      "#             channel.send((\"close\", None))\n",
      "#             # except:\n",
      "#             #     raise RuntimeError(f\"closing {channel} number {i} failed\")\n",
      "#             msg, _ = channel.recv()\n",
      "#             if msg != \"closing\":\n",
      "#                 raise RuntimeError(\n",
      "#                     f\"Expected 'closing' but received {msg} from worker {i}\"\n",
      "#                 )\n",
      "# \n",
      "#         del self.shared_tensordicts, self.shared_tensordict_parent\n",
      "# --------------------------------------------------\n",
      "\"\"\"Based on the above, complete the following code:\"\"\"'\n",
      "\n",
      "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
      "#\n",
      "# This source code is licensed under the MIT license found in the\n",
      "# LICENSE file in the root directory of this source tree.\n",
      "import argparse\n",
      "import distutils.command.clean\n",
      "import glob\n",
      "import os\n",
      "import shutil\n",
      "import subprocess\n",
      "import sys\n",
      "from datetime import date\n",
      "from pathlib import Path\n",
      "from typing import List\n",
      "\n",
      "from setuptools import find_packages, setup\n",
      "from torch.utils.cpp_extension import BuildExtension, CppExtension\n",
      "\n",
      "cwd = os.path.dirname(os.path.abspath(__file__))\n",
      "try:\n",
      "    sha = (\n",
      "        subprocess.check_output([\"git\", \"rev-parse\", \"HEAD\"], cwd=cwd)\n",
      "        .decode(\"ascii\")\n",
      "        .strip()\n",
      "    )\n",
      "except Exception:\n",
      "    sha = \"Unknown\"\n",
      "\n",
      "\n",
      "def get_version():\n",
      "    version_txt = os.path.join(cwd, \"version.txt\")\n",
      "    with open(version_txt, \"r\") as f:\n",
      "        version = f.readline().strip()\n",
      "    if os.getenv(\"BUILD_VERSION\"):\n",
      "        version = os.getenv(\"BUILD_VERSION\")\n",
      "    elif sha != \"Unknown\":\n",
      "        version += \"+\" + sha[:7]\n",
      "    return version\n",
      "\n",
      "\n",
      "ROOT_DIR = Path(__file__).parent.resolve()\n",
      "\n",
      "\n",
      "package_name = \"torchrl\"\n",
      "\n",
      "\n",
      "def get_nightly_version():\n",
      "    today = date.today()\n",
      "    return f\"{today.year}.{today.month}.{today.day}\"\n",
      "\n",
      "\n",
      "def parse_args(argv: List[str]) -> argparse.Namespace:\n",
      "    parser = argparse.ArgumentParser(description=\"torchrl setup\")\n",
      "    parser.add_argument(\n",
      "        \"--package_name\",\n",
      "        type=str,\n",
      "        default=\"torchrl\",\n",
      "        help=\"the name of this output wheel\",\n",
      "    )\n",
      "    return parser.parse_known_args(argv)\n",
      "\n",
      "\n",
      "def write_version_file(version):\n",
      "    version_path = os.path.join(cwd, \"torchrl\", \"version.py\")\n",
      "    with open(version_path, \"w\") as f:\n",
      "        f.write(\"__version__ = '{}'\n",
      "\".format(version))\n",
      "        f.write(\"git_version = {}\n",
      "\".format(repr(sha)))\n",
      "\n",
      "\n",
      "def _get_pytorch_version():\n",
      "    # if \"PYTORCH_VERSION\" in os.environ:\n",
      "    #     return f\"torch=={os.environ['PYTORCH_VERSION']}\"\n",
      "    return \"torch\"\n",
      "\n",
      "\n",
      "def _get_packages():\n",
      "    exclude = [\n",
      "        \"build*\",\n",
      "        \"test*\",\n",
      "        \"torchrl.csrc*\",\n",
      "        \"third_party*\",\n",
      "        \"tools*\",\n",
      "    ]\n",
      "    return find_packages(exclude=exclude)\n",
      "\n",
      "\n",
      "ROOT_DIR = Path(__file__).parent.resolve()\n",
      "\n",
      "\n",
      "class clean(distutils.command.clean.clean):\n",
      "    def run(self):\n",
      "        # Run default behavior first\n",
      "        distutils.command.clean.clean.run(self)\n",
      "\n",
      "        # Remove torchrl extension\n",
      "        for path in (ROOT_DIR / \"torchrl\").glob(\"**/*.so\"):\n",
      "            print(f\"removing '{path}'\")\n",
      "            path.unlink()\n",
      "        # Remove build directory\n",
      "        build_dirs = [\n",
      "            ROOT_DIR / \"build\",\n",
      "        ]\n",
      "        for path in build_dirs:\n",
      "            if path.exists():\n",
      "                print(f\"removing '{path}' (and everything under it)\")\n",
      "                shutil.rmtree(str(path), ignore_errors=True)\n",
      "\n",
      "\n",
      "# def _run_cmd(cmd):\n",
      "#     try:\n",
      "#         return subprocess.check_output(cmd, cwd=ROOT_DIR).decode(\"ascii\").strip()\n",
      "#     except Exception:\n",
      "#         return None\n",
      "\n",
      "\n",
      "def get_extensions():\n",
      "    extension = CppExtension\n",
      "\n",
      "    extra_link_args = []\n",
      "    extra_compile_args = {\n",
      "        \"cxx\": [\n",
      "            \"-O3\",\n",
      "            \"-std=c++14\",\n",
      "            \"-fdiagnostics-color=always\",\n"
     ]
    }
   ],
   "source": [
    "for i in random_combinations_of_fragments(fragments, 3):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import random\n",
    "import json\n",
    "from typing import List\n",
    "\n",
    "def split_snippets(text: str, num_splits) -> List[str]:\n",
    "    fragments = split_code_fragments(text)\n",
    "    return random_combinations_of_fragments(fragments, num_splits)\n",
    "\n",
    "\n",
    "def process_jsonl_and_write(file_path, output_jsonl_path, num_splits):\n",
    "    output_variants = []\n",
    "\n",
    "    with open(file_path, 'r') as jsonl_file:\n",
    "        for line in jsonl_file: # Read line by line\n",
    "            data = json.loads(line)\n",
    "            prompt = data['prompt']\n",
    "            task_id = data['metadata']['task_id']\n",
    "\n",
    "            # Generate variants\n",
    "            prompt_variants = split_snippets(prompt, num_splits)\n",
    "\n",
    "            # Construct new data entries for each variant and add to output list\n",
    "            for i, variant in enumerate(prompt_variants):\n",
    "                new_data = {\n",
    "                    \"prompt\": variant,\n",
    "                    \"metadata\": {\n",
    "                        \"task_id\": f\"{task_id}\",\n",
    "                        \"instance_id\": f\"{task_id}_{i}\",\n",
    "                        \"ground_truth\": data['metadata']['ground_truth'], # Keep the same ground truth\n",
    "                        \"fpath_tuple\": data['metadata']['fpath_tuple'],\n",
    "                        \"context_start_lineno\": data['metadata']['context_start_lineno'],\n",
    "                        \"line_no\": data['metadata']['line_no'],\n",
    "                    }\n",
    "                }\n",
    "                output_variants.append(new_data)\n",
    "\n",
    "    # Write the output variants to a new JSONL file\n",
    "    with open(output_jsonl_path, 'w') as outfile:\n",
    "        for variant in output_variants:\n",
    "            json.dump(variant, outfile)\n",
    "            outfile.write('\\n') # Write each JSON object on a new line\n",
    "\n",
    "num_variants = 10\n",
    "\n",
    "input_jsonl_path = 'subsets/rg-one-gram-ws-20-ss-2-fixed_0.1_with_instructions_temp_0.jsonl'  # Update this to your input file path\n",
    "output_jsonl_path = f'temp_subsets/rg-one-gram-ws-20-ss-2-one-line_0.1_instruct_{num_variants}_variants.jsonl' # Update this to your desired output file path\n",
    "\n",
    "# Use the function with the path to your JSONL file\n",
    "process_jsonl_and_write(input_jsonl_path, output_jsonl_path, num_variants)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge Overlapping Segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fragment: 0\n",
      "# --------------------------------------------------\n",
      "# the below code fragment can be found in:\n",
      "# torchrl/envs/vec_env.py\n",
      "# --------------------------------------------------\n",
      "#         ).clone()\n",
      "# \n",
      "#     @_check_start\n",
      "#     def _shutdown_workers(self) -> None:\n",
      "#         if self.is_closed:\n",
      "#             raise RuntimeError(\n",
      "#                 \"calling {self.__class__.__name__}._shutdown_workers only allowed when env.is_closed = False\"\n",
      "#             )\n",
      "#         for i, channel in enumerate(self.parent_channels):\n",
      "#             if self._verbose:\n",
      "#                 print(f\"closing {i}\")\n",
      "#             # try:\n",
      "#             channel.send((\"close\", None))\n",
      "#             # except:\n",
      "#             #     raise RuntimeError(f\"closing {channel} number {i} failed\")\n",
      "#             msg, _ = channel.recv()\n",
      "#             if msg != \"closing\":\n",
      "#                 raise RuntimeError(\n",
      "#                     f\"Expected 'closing' but received {msg} from worker {i}\"\n",
      "#                 )\n",
      "# \n",
      "#         del self.shared_tensordicts, self.shared_tensordict_parent\n",
      "fragment: 1\n",
      "# --------------------------------------------------\n",
      "# the below code fragment can be found in:\n",
      "# build_tools/setup_helpers/extension.py\n",
      "# --------------------------------------------------\n",
      "#     and \"ATen parallel backend: OpenMP\" in torch.__config__.parallel_info()\n",
      "# )\n",
      "# _TORCH_CUDA_ARCH_LIST = os.environ.get(\"TORCH_CUDA_ARCH_LIST\", None)\n",
      "# \n",
      "# \n",
      "# def get_ext_modules():\n",
      "#     return [\n",
      "#         Extension(name=\"torchrl._torchrl\", sources=[]),\n",
      "#     ]\n",
      "# \n",
      "# \n",
      "# # Based off of\n",
      "# # https://github.com/pybind/cmake_example/blob/580c5fd29d4651db99d8874714b07c0c49a53f8a/setup.py\n",
      "# class CMakeBuild(build_ext):\n",
      "#     def run(self):\n",
      "#         try:\n",
      "#             subprocess.check_output([\"cmake\", \"--version\"])\n",
      "#         except OSError:\n",
      "#             raise RuntimeError(\"CMake is not available.\") from None\n",
      "#         super().run()\n",
      "# \n",
      "#     def build_extension(self, ext):\n",
      "#         # Since two library files (libtorchrl and _torchrl) need to be\n",
      "#         # recognized by setuptools, we instantiate `Extension` twice. (see `get_ext_modules`)\n",
      "#         # This leads to the situation where this `build_extension` method is called twice.\n",
      "#         # However, the following `cmake` command will build all of them at the same time,\n",
      "fragment: 2\n",
      "# --------------------------------------------------\n",
      "# the below code fragment can be found in:\n",
      "# build_tools/setup_helpers/extension.py\n",
      "# --------------------------------------------------\n",
      "#                 [\"cmake\", str(_ROOT_DIR)] + cmake_args,\n",
      "#                 cwd=self.build_temp,\n",
      "#                 stderr=STDOUT,\n",
      "#             )\n",
      "#         except CalledProcessError as exc:\n",
      "#             print(exc.output)\n",
      "# \n",
      "#         try:\n",
      "#             check_output(\n",
      "#                 [\"cmake\", \"--build\", \".\"] + build_args,\n",
      "#                 cwd=self.build_temp,\n",
      "#                 stderr=STDOUT,\n",
      "#             )\n",
      "#         except CalledProcessError as exc:\n",
      "#             print(exc.output)\n",
      "# \n",
      "#     def get_ext_filename(self, fullname):\n",
      "#         ext_filename = super().get_ext_filename(fullname)\n",
      "#         ext_filename_parts = ext_filename.split(\".\")\n",
      "#         without_abi = ext_filename_parts[:-2] + ext_filename_parts[-1:]\n",
      "#         ext_filename = \".\".join(without_abi)\n",
      "#         return ext_filename\n"
     ]
    }
   ],
   "source": [
    "def merge_overlapping_strings_with_header(strings):\n",
    "    def get_header_and_body(s):\n",
    "        # Split the string into header and body\n",
    "        header, body = s.split('\\n# --------------------------------------------------\\n', 1)\n",
    "        return header, body.strip()\n",
    "\n",
    "    def can_merge(body_a, body_b):\n",
    "        # Check if the ending of body_a overlaps with the start of body_b\n",
    "        min_length = min(len(body_a), len(body_b))\n",
    "        for i in range(min_length, 0, -1):\n",
    "            if body_a.endswith(body_b[:i]):\n",
    "                return i\n",
    "            \n",
    "        if (body_a in body_b) or (body_b in body_a):\n",
    "            return min_length\n",
    "        return 0\n",
    "\n",
    "    def merge_two_strings(body_a, body_b, overlap_length):\n",
    "        # Merge the bodies, keeping the overlapping part only once\n",
    "        return body_a + body_b[overlap_length:]\n",
    "\n",
    "    # Group strings by their header\n",
    "    grouped_by_header = {}\n",
    "    for s in strings:\n",
    "        header, body = get_header_and_body(s)\n",
    "        if header in grouped_by_header:\n",
    "            grouped_by_header[header].append(body)\n",
    "        else:\n",
    "            grouped_by_header[header] = [body]\n",
    "\n",
    "    # Merge strings within the same header group\n",
    "    merged_strings = []\n",
    "    for header, bodies in grouped_by_header.items():\n",
    "        # Attempt to merge strings until no further merges are possible\n",
    "        for i in range(10):\n",
    "            while len(bodies) > 1:\n",
    "                merged = False\n",
    "                for i in range(len(bodies)):\n",
    "                    for j in range(len(bodies)):\n",
    "                        if i != j:\n",
    "                            overlap_length = can_merge(bodies[i], bodies[j])\n",
    "                            if overlap_length > 0:\n",
    "                                bodies[i] = merge_two_strings(bodies[i], bodies[j], overlap_length)\n",
    "                                del bodies[j]\n",
    "                                merged = True\n",
    "                                break\n",
    "                    if merged:\n",
    "                        break\n",
    "                if not merged:\n",
    "                    break\n",
    "        \n",
    "        # Add all merged bodies with the common header\n",
    "        for body in bodies:\n",
    "            merged_strings.append(header + '\\n# --------------------------------------------------\\n' + body)\n",
    "\n",
    "    return merged_strings\n",
    "\n",
    "# # Example usage:\n",
    "# strings = [\n",
    "#     \"# --------------------------------------------------\\n# the below code fragment can be found in:\\n# torchrl/envs/vec_env.py\\n# --------------------------------------------------\\nhello, wor\",\n",
    "#     \"# --------------------------------------------------\\n# the below code fragment can be found in:\\n# torchrl/envs/vec_env.py\\n# --------------------------------------------------\\nworld!\",\n",
    "#     \"# --------------------------------------------------\\n# the below code fragment can be found in:\\n# torchrl/envs/vec_env.py\\n# --------------------------------------------------\\nwor\",\n",
    "#     \"# --------------------------------------------------\\n# the below code fragment can be found in:\\n# torchrl/envs/vec_env.py\\n# --------------------------------------------------\\norld! My name is John.\",\n",
    "#     \"# --------------------------------------------------\\n# the below code fragment can be found in:\\n# torchrl/envs/vec_env.py\\n# --------------------------------------------------\\n I am a programmer.\"\n",
    "# ]\n",
    "\n",
    "# merged_strings = merge_overlapping_strings_with_header(strings)\n",
    "# for ms in merged_strings:\n",
    "#     print(\"BEGINNING\")\n",
    "#     print(ms)\n",
    "\n",
    "input_fragments = fragments\n",
    "\n",
    "merged_fragments =  merge_overlapping_strings_with_header(input_fragments[1:-1])\n",
    "for i, fragment in enumerate(merged_fragments):\n",
    "    print(f\"fragment: {i}\")\n",
    "    print(fragment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Here are some relevant code fragments from other files of the repo:\n",
      "# --------------------------------------------------\n",
      "# the below code fragment can be found in:\n",
      "# torchrl/envs/vec_env.py\n",
      "# --------------------------------------------------\n",
      "#         ).clone()\n",
      "# \n",
      "#     @_check_start\n",
      "#     def _shutdown_workers(self) -> None:\n",
      "#         if self.is_closed:\n",
      "#             raise RuntimeError(\n",
      "#                 \"calling {self.__class__.__name__}._shutdown_workers only allowed when env.is_closed = False\"\n",
      "#             )\n",
      "#         for i, channel in enumerate(self.parent_channels):\n",
      "#             if self._verbose:\n",
      "#                 print(f\"closing {i}\")\n",
      "#             # try:\n",
      "#             channel.send((\"close\", None))\n",
      "#             # except:\n",
      "#             #     raise RuntimeError(f\"closing {channel} number {i} failed\")\n",
      "#             msg, _ = channel.recv()\n",
      "#             if msg != \"closing\":\n",
      "#                 raise RuntimeError(\n",
      "#                     f\"Expected 'closing' but received {msg} from worker {i}\"\n",
      "#                 )\n",
      "# \n",
      "#         del self.shared_tensordicts, self.shared_tensordict_parent\n",
      "# --------------------------------------------------\n",
      "# the below code fragment can be found in:\n",
      "# build_tools/setup_helpers/extension.py\n",
      "# --------------------------------------------------\n",
      "#     and \"ATen parallel backend: OpenMP\" in torch.__config__.parallel_info()\n",
      "# )\n",
      "# _TORCH_CUDA_ARCH_LIST = os.environ.get(\"TORCH_CUDA_ARCH_LIST\", None)\n",
      "# \n",
      "# \n",
      "# def get_ext_modules():\n",
      "#     return [\n",
      "#         Extension(name=\"torchrl._torchrl\", sources=[]),\n",
      "#     ]\n",
      "# \n",
      "# \n",
      "# # Based off of\n",
      "# # https://github.com/pybind/cmake_example/blob/580c5fd29d4651db99d8874714b07c0c49a53f8a/setup.py\n",
      "# class CMakeBuild(build_ext):\n",
      "#     def run(self):\n",
      "#         try:\n",
      "#             subprocess.check_output([\"cmake\", \"--version\"])\n",
      "#         except OSError:\n",
      "#             raise RuntimeError(\"CMake is not available.\") from None\n",
      "#         super().run()\n",
      "# \n",
      "#     def build_extension(self, ext):\n",
      "#         # Since two library files (libtorchrl and _torchrl) need to be\n",
      "#         # recognized by setuptools, we instantiate `Extension` twice. (see `get_ext_modules`)\n",
      "#         # This leads to the situation where this `build_extension` method is called twice.\n",
      "#         # However, the following `cmake` command will build all of them at the same time,\n",
      "# --------------------------------------------------\n",
      "# the below code fragment can be found in:\n",
      "# build_tools/setup_helpers/extension.py\n",
      "# --------------------------------------------------\n",
      "#                 [\"cmake\", str(_ROOT_DIR)] + cmake_args,\n",
      "#                 cwd=self.build_temp,\n",
      "#                 stderr=STDOUT,\n",
      "#             )\n",
      "#         except CalledProcessError as exc:\n",
      "#             print(exc.output)\n",
      "# \n",
      "#         try:\n",
      "#             check_output(\n",
      "#                 [\"cmake\", \"--build\", \".\"] + build_args,\n",
      "#                 cwd=self.build_temp,\n",
      "#                 stderr=STDOUT,\n",
      "#             )\n",
      "#         except CalledProcessError as exc:\n",
      "#             print(exc.output)\n",
      "# \n",
      "#     def get_ext_filename(self, fullname):\n",
      "#         ext_filename = super().get_ext_filename(fullname)\n",
      "#         ext_filename_parts = ext_filename.split(\".\")\n",
      "#         without_abi = ext_filename_parts[:-2] + ext_filename_parts[-1:]\n",
      "#         ext_filename = \".\".join(without_abi)\n",
      "#         return ext_filename\n",
      "# --------------------------------------------------\n",
      "\"\"\"Based on the above, complete the following code:\"\"\"'\n",
      "\n",
      "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
      "#\n",
      "# This source code is licensed under the MIT license found in the\n",
      "# LICENSE file in the root directory of this source tree.\n",
      "import argparse\n",
      "import distutils.command.clean\n",
      "import glob\n",
      "import os\n",
      "import shutil\n",
      "import subprocess\n",
      "import sys\n",
      "from datetime import date\n",
      "from pathlib import Path\n",
      "from typing import List\n",
      "\n",
      "from setuptools import find_packages, setup\n",
      "from torch.utils.cpp_extension import BuildExtension, CppExtension\n",
      "\n",
      "cwd = os.path.dirname(os.path.abspath(__file__))\n",
      "try:\n",
      "    sha = (\n",
      "        subprocess.check_output([\"git\", \"rev-parse\", \"HEAD\"], cwd=cwd)\n",
      "        .decode(\"ascii\")\n",
      "        .strip()\n",
      "    )\n",
      "except Exception:\n",
      "    sha = \"Unknown\"\n",
      "\n",
      "\n",
      "def get_version():\n",
      "    version_txt = os.path.join(cwd, \"version.txt\")\n",
      "    with open(version_txt, \"r\") as f:\n",
      "        version = f.readline().strip()\n",
      "    if os.getenv(\"BUILD_VERSION\"):\n",
      "        version = os.getenv(\"BUILD_VERSION\")\n",
      "    elif sha != \"Unknown\":\n",
      "        version += \"+\" + sha[:7]\n",
      "    return version\n",
      "\n",
      "\n",
      "ROOT_DIR = Path(__file__).parent.resolve()\n",
      "\n",
      "\n",
      "package_name = \"torchrl\"\n",
      "\n",
      "\n",
      "def get_nightly_version():\n",
      "    today = date.today()\n",
      "    return f\"{today.year}.{today.month}.{today.day}\"\n",
      "\n",
      "\n",
      "def parse_args(argv: List[str]) -> argparse.Namespace:\n",
      "    parser = argparse.ArgumentParser(description=\"torchrl setup\")\n",
      "    parser.add_argument(\n",
      "        \"--package_name\",\n",
      "        type=str,\n",
      "        default=\"torchrl\",\n",
      "        help=\"the name of this output wheel\",\n",
      "    )\n",
      "    return parser.parse_known_args(argv)\n",
      "\n",
      "\n",
      "def write_version_file(version):\n",
      "    version_path = os.path.join(cwd, \"torchrl\", \"version.py\")\n",
      "    with open(version_path, \"w\") as f:\n",
      "        f.write(\"__version__ = '{}'\n",
      "\".format(version))\n",
      "        f.write(\"git_version = {}\n",
      "\".format(repr(sha)))\n",
      "\n",
      "\n",
      "def _get_pytorch_version():\n",
      "    # if \"PYTORCH_VERSION\" in os.environ:\n",
      "    #     return f\"torch=={os.environ['PYTORCH_VERSION']}\"\n",
      "    return \"torch\"\n",
      "\n",
      "\n",
      "def _get_packages():\n",
      "    exclude = [\n",
      "        \"build*\",\n",
      "        \"test*\",\n",
      "        \"torchrl.csrc*\",\n",
      "        \"third_party*\",\n",
      "        \"tools*\",\n",
      "    ]\n",
      "    return find_packages(exclude=exclude)\n",
      "\n",
      "\n",
      "ROOT_DIR = Path(__file__).parent.resolve()\n",
      "\n",
      "\n",
      "class clean(distutils.command.clean.clean):\n",
      "    def run(self):\n",
      "        # Run default behavior first\n",
      "        distutils.command.clean.clean.run(self)\n",
      "\n",
      "        # Remove torchrl extension\n",
      "        for path in (ROOT_DIR / \"torchrl\").glob(\"**/*.so\"):\n",
      "            print(f\"removing '{path}'\")\n",
      "            path.unlink()\n",
      "        # Remove build directory\n",
      "        build_dirs = [\n",
      "            ROOT_DIR / \"build\",\n",
      "        ]\n",
      "        for path in build_dirs:\n",
      "            if path.exists():\n",
      "                print(f\"removing '{path}' (and everything under it)\")\n",
      "                shutil.rmtree(str(path), ignore_errors=True)\n",
      "\n",
      "\n",
      "# def _run_cmd(cmd):\n",
      "#     try:\n",
      "#         return subprocess.check_output(cmd, cwd=ROOT_DIR).decode(\"ascii\").strip()\n",
      "#     except Exception:\n",
      "#         return None\n",
      "\n",
      "\n",
      "def get_extensions():\n",
      "    extension = CppExtension\n",
      "\n",
      "    extra_link_args = []\n",
      "    extra_compile_args = {\n",
      "        \"cxx\": [\n",
      "            \"-O3\",\n",
      "            \"-std=c++14\",\n",
      "            \"-fdiagnostics-color=always\",\n"
     ]
    }
   ],
   "source": [
    "new_frag = [fragments[0]] + merged_fragments + [fragments[-1]]\n",
    "print(\"\\n\".join(new_frag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def merge_overlapping_jsonl_lines(input_file, output_file):\n",
    "    with open(input_file, 'r', encoding='utf-8') as infile, open(output_file, 'w', encoding='utf-8') as outfile:\n",
    "        for line in infile:\n",
    "            line_data = json.loads(line)\n",
    "            new_line_data = line_data.copy()\n",
    "            old_prompt = line_data[\"prompt\"]\n",
    "            fragments = split_code_fragments(old_prompt)\n",
    "            merged_fragments = merge_overlapping_strings_with_header(fragments[1:-1])\n",
    "            new_fragments = [fragments[0]] + merged_fragments + [fragments[-1]]\n",
    "            new_prompt = \"\\n\".join(new_fragments)\n",
    "            new_line_data[\"prompt\"] = new_prompt\n",
    "            json.dump(new_line_data, outfile)\n",
    "            outfile.write('\\n')\n",
    "\n",
    "# Usage\n",
    "input_jsonl_path = 'subsets/rg-one-gram-ws-20-ss-2-one-line_0.1_instruct.jsonl'  # Update this to your input file path\n",
    "output_jsonl_path = input_jsonl_path.replace(\".jsonl\",\"_merged.jsonl\")\n",
    "\n",
    "merge_overlapping_jsonl_lines(input_jsonl_path, output_jsonl_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge and then create subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n"
     ]
    }
   ],
   "source": [
    "input_jsonl_path = 'subsets/rg-one-gram-ws-20-ss-2-one-line_0.1_instruct_merged.jsonl'  # Update this to your input file path\n",
    "output_jsonl_path = 'subsets/rg-one-gram-ws-20-ss-2-one-line_0.1_instruct_merged_3_variants.jsonl' # Update this to your desired output file path\n",
    "\n",
    "# Use the function with the path to your JSONL file\n",
    "process_jsonl_and_write(input_jsonl_path, output_jsonl_path, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subset the in-file context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The bottom 40% of lines:\n",
      "Line 4\n",
      "Line 5\n"
     ]
    }
   ],
   "source": [
    "def get_bottom_percent_lines(input_string, percent):\n",
    "    if not 0 <= percent <= 100:\n",
    "        raise ValueError(\"The percent must be between 0 and 100\")\n",
    "\n",
    "    # Split the string into lines\n",
    "    lines = input_string.strip().split('\\n')\n",
    "    \n",
    "    # Determine the number of lines that corresponds to the bottom x%\n",
    "    number_of_lines = len(lines)\n",
    "    cutoff_index = max(0, int((percent / 100) * number_of_lines))\n",
    "\n",
    "    # Select the bottom x% lines\n",
    "    bottom_lines = lines[-cutoff_index:] if cutoff_index > 0 else []\n",
    "\n",
    "    # Join the selected lines back into a string\n",
    "    return '\\n'.join(bottom_lines)\n",
    "\n",
    "# Example usage\n",
    "input_string = \"Line 1\\nLine 2\\nLine 3\\nLine 4\\nLine 5\"\n",
    "percent = 40  # Suppose we want the bottom 40% of lines\n",
    "\n",
    "result = get_bottom_percent_lines(input_string, percent)\n",
    "print(\"The bottom {}% of lines:\".format(percent))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['# Here are some relevant code fragments from other files of the repo:',\n",
       " '# --------------------------------------------------\\n# the below code fragment can be found in:\\n# torchrl/envs/vec_env.py\\n# --------------------------------------------------\\n#     @_check_start\\n#     def _shutdown_workers(self) -> None:\\n#         if self.is_closed:\\n#             raise RuntimeError(\\n#                 \"calling {self.__class__.__name__}._shutdown_workers only allowed when env.is_closed = False\"\\n#             )\\n#         for i, channel in enumerate(self.parent_channels):\\n#             if self._verbose:\\n#                 print(f\"closing {i}\")\\n#             # try:\\n#             channel.send((\"close\", None))\\n#             # except:\\n#             #     raise RuntimeError(f\"closing {channel} number {i} failed\")\\n#             msg, _ = channel.recv()\\n#             if msg != \"closing\":\\n#                 raise RuntimeError(\\n#                     f\"Expected \\'closing\\' but received {msg} from worker {i}\"\\n#                 )\\n# \\n#         del self.shared_tensordicts, self.shared_tensordict_parent',\n",
       " '# --------------------------------------------------\\n# the below code fragment can be found in:\\n# torchrl/envs/vec_env.py\\n# --------------------------------------------------\\n#         ).clone()\\n# \\n#     @_check_start\\n#     def _shutdown_workers(self) -> None:\\n#         if self.is_closed:\\n#             raise RuntimeError(\\n#                 \"calling {self.__class__.__name__}._shutdown_workers only allowed when env.is_closed = False\"\\n#             )\\n#         for i, channel in enumerate(self.parent_channels):\\n#             if self._verbose:\\n#                 print(f\"closing {i}\")\\n#             # try:\\n#             channel.send((\"close\", None))\\n#             # except:\\n#             #     raise RuntimeError(f\"closing {channel} number {i} failed\")\\n#             msg, _ = channel.recv()\\n#             if msg != \"closing\":\\n#                 raise RuntimeError(\\n#                     f\"Expected \\'closing\\' but received {msg} from worker {i}\"\\n#                 )',\n",
       " '# --------------------------------------------------\\n# the below code fragment can be found in:\\n# build_tools/setup_helpers/extension.py\\n# --------------------------------------------------\\n#     and \"ATen parallel backend: OpenMP\" in torch.__config__.parallel_info()\\n# )\\n# _TORCH_CUDA_ARCH_LIST = os.environ.get(\"TORCH_CUDA_ARCH_LIST\", None)\\n# \\n# \\n# def get_ext_modules():\\n#     return [\\n#         Extension(name=\"torchrl._torchrl\", sources=[]),\\n#     ]\\n# \\n# \\n# # Based off of\\n# # https://github.com/pybind/cmake_example/blob/580c5fd29d4651db99d8874714b07c0c49a53f8a/setup.py\\n# class CMakeBuild(build_ext):\\n#     def run(self):\\n#         try:\\n#             subprocess.check_output([\"cmake\", \"--version\"])\\n#         except OSError:\\n#             raise RuntimeError(\"CMake is not available.\") from None\\n#         super().run()',\n",
       " '# --------------------------------------------------\\n# the below code fragment can be found in:\\n# build_tools/setup_helpers/extension.py\\n# --------------------------------------------------\\n#     return [\\n#         Extension(name=\"torchrl._torchrl\", sources=[]),\\n#     ]\\n# \\n# \\n# # Based off of\\n# # https://github.com/pybind/cmake_example/blob/580c5fd29d4651db99d8874714b07c0c49a53f8a/setup.py\\n# class CMakeBuild(build_ext):\\n#     def run(self):\\n#         try:\\n#             subprocess.check_output([\"cmake\", \"--version\"])\\n#         except OSError:\\n#             raise RuntimeError(\"CMake is not available.\") from None\\n#         super().run()\\n# \\n#     def build_extension(self, ext):\\n#         # Since two library files (libtorchrl and _torchrl) need to be\\n#         # recognized by setuptools, we instantiate `Extension` twice. (see `get_ext_modules`)\\n#         # This leads to the situation where this `build_extension` method is called twice.\\n#         # However, the following `cmake` command will build all of them at the same time,',\n",
       " '# --------------------------------------------------\\n# the below code fragment can be found in:\\n# build_tools/setup_helpers/extension.py\\n# --------------------------------------------------\\n# \\n# def get_ext_modules():\\n#     return [\\n#         Extension(name=\"torchrl._torchrl\", sources=[]),\\n#     ]\\n# \\n# \\n# # Based off of\\n# # https://github.com/pybind/cmake_example/blob/580c5fd29d4651db99d8874714b07c0c49a53f8a/setup.py\\n# class CMakeBuild(build_ext):\\n#     def run(self):\\n#         try:\\n#             subprocess.check_output([\"cmake\", \"--version\"])\\n#         except OSError:\\n#             raise RuntimeError(\"CMake is not available.\") from None\\n#         super().run()\\n# \\n#     def build_extension(self, ext):\\n#         # Since two library files (libtorchrl and _torchrl) need to be\\n#         # recognized by setuptools, we instantiate `Extension` twice. (see `get_ext_modules`)',\n",
       " '# --------------------------------------------------\\n# the below code fragment can be found in:\\n# build_tools/setup_helpers/extension.py\\n# --------------------------------------------------\\n#                 stderr=STDOUT,\\n#             )\\n#         except CalledProcessError as exc:\\n#             print(exc.output)\\n# \\n#         try:\\n#             check_output(\\n#                 [\"cmake\", \"--build\", \".\"] + build_args,\\n#                 cwd=self.build_temp,\\n#                 stderr=STDOUT,\\n#             )\\n#         except CalledProcessError as exc:\\n#             print(exc.output)\\n# \\n#     def get_ext_filename(self, fullname):\\n#         ext_filename = super().get_ext_filename(fullname)\\n#         ext_filename_parts = ext_filename.split(\".\")\\n#         without_abi = ext_filename_parts[:-2] + ext_filename_parts[-1:]\\n#         ext_filename = \".\".join(without_abi)\\n#         return ext_filename',\n",
       " '# --------------------------------------------------\\n# the below code fragment can be found in:\\n# build_tools/setup_helpers/extension.py\\n# --------------------------------------------------\\n#                 [\"cmake\", str(_ROOT_DIR)] + cmake_args,\\n#                 cwd=self.build_temp,\\n#                 stderr=STDOUT,\\n#             )\\n#         except CalledProcessError as exc:\\n#             print(exc.output)\\n# \\n#         try:\\n#             check_output(\\n#                 [\"cmake\", \"--build\", \".\"] + build_args,\\n#                 cwd=self.build_temp,\\n#                 stderr=STDOUT,\\n#             )\\n#         except CalledProcessError as exc:\\n#             print(exc.output)\\n# \\n#     def get_ext_filename(self, fullname):\\n#         ext_filename = super().get_ext_filename(fullname)\\n#         ext_filename_parts = ext_filename.split(\".\")\\n#         without_abi = ext_filename_parts[:-2] + ext_filename_parts[-1:]',\n",
       " '# --------------------------------------------------\\n\"\"\"Based on the above, complete the following code:\"\"\"\\'\\n\\n# Copyright (c) Meta Platforms, Inc. and affiliates.\\n#\\n# This source code is licensed under the MIT license found in the\\n# LICENSE file in the root directory of this source tree.\\nimport argparse\\nimport distutils.command.clean\\nimport glob\\nimport os\\nimport shutil\\nimport subprocess\\nimport sys\\nfrom datetime import date\\nfrom pathlib import Path\\nfrom typing import List\\n\\nfrom setuptools import find_packages, setup\\nfrom torch.utils.cpp_extension import BuildExtension, CppExtension\\n\\ncwd = os.path.dirname(os.path.abspath(__file__))\\ntry:\\n    sha = (\\n        subprocess.check_output([\"git\", \"rev-parse\", \"HEAD\"], cwd=cwd)\\n        .decode(\"ascii\")\\n        .strip()\\n    )\\nexcept Exception:\\n    sha = \"Unknown\"\\n\\n\\ndef get_version():\\n    version_txt = os.path.join(cwd, \"version.txt\")\\n    with open(version_txt, \"r\") as f:\\n        version = f.readline().strip()\\n    if os.getenv(\"BUILD_VERSION\"):\\n        version = os.getenv(\"BUILD_VERSION\")\\n    elif sha != \"Unknown\":\\n        version += \"+\" + sha[:7]\\n    return version\\n\\n\\nROOT_DIR = Path(__file__).parent.resolve()\\n\\n\\npackage_name = \"torchrl\"\\n\\n\\ndef get_nightly_version():\\n    today = date.today()\\n    return f\"{today.year}.{today.month}.{today.day}\"\\n\\n\\ndef parse_args(argv: List[str]) -> argparse.Namespace:\\n    parser = argparse.ArgumentParser(description=\"torchrl setup\")\\n    parser.add_argument(\\n        \"--package_name\",\\n        type=str,\\n        default=\"torchrl\",\\n        help=\"the name of this output wheel\",\\n    )\\n    return parser.parse_known_args(argv)\\n\\n\\ndef write_version_file(version):\\n    version_path = os.path.join(cwd, \"torchrl\", \"version.py\")\\n    with open(version_path, \"w\") as f:\\n        f.write(\"__version__ = \\'{}\\'\\n\".format(version))\\n        f.write(\"git_version = {}\\n\".format(repr(sha)))\\n\\n\\ndef _get_pytorch_version():\\n    # if \"PYTORCH_VERSION\" in os.environ:\\n    #     return f\"torch=={os.environ[\\'PYTORCH_VERSION\\']}\"\\n    return \"torch\"\\n\\n\\ndef _get_packages():\\n    exclude = [\\n        \"build*\",\\n        \"test*\",\\n        \"torchrl.csrc*\",\\n        \"third_party*\",\\n        \"tools*\",\\n    ]\\n    return find_packages(exclude=exclude)\\n\\n\\nROOT_DIR = Path(__file__).parent.resolve()\\n\\n\\nclass clean(distutils.command.clean.clean):\\n    def run(self):\\n        # Run default behavior first\\n        distutils.command.clean.clean.run(self)\\n\\n        # Remove torchrl extension\\n        for path in (ROOT_DIR / \"torchrl\").glob(\"**/*.so\"):\\n            print(f\"removing \\'{path}\\'\")\\n            path.unlink()\\n        # Remove build directory\\n        build_dirs = [\\n            ROOT_DIR / \"build\",\\n        ]\\n        for path in build_dirs:\\n            if path.exists():\\n                print(f\"removing \\'{path}\\' (and everything under it)\")\\n                shutil.rmtree(str(path), ignore_errors=True)\\n\\n\\n# def _run_cmd(cmd):\\n#     try:\\n#         return subprocess.check_output(cmd, cwd=ROOT_DIR).decode(\"ascii\").strip()\\n#     except Exception:\\n#         return None\\n\\n\\ndef get_extensions():\\n    extension = CppExtension\\n\\n    extra_link_args = []\\n    extra_compile_args = {\\n        \"cxx\": [\\n            \"-O3\",\\n            \"-std=c++14\",\\n            \"-fdiagnostics-color=always\",']"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fragments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def subset_infile_jsonl_lines(input_file, output_file, percent):\n",
    "    with open(input_file, 'r', encoding='utf-8') as infile, open(output_file, 'w', encoding='utf-8') as outfile:\n",
    "        for line in infile:\n",
    "            line_data = json.loads(line)\n",
    "            new_line_data = line_data.copy()\n",
    "\n",
    "            old_prompt = line_data[\"prompt\"]\n",
    "            fragments = split_code_fragments(old_prompt)\n",
    "            get_first_lines = fragments[-1].strip().split('\\n')[:3]\n",
    "            subset_infile_context = \"\\n\".join(get_first_lines) + \"\\n\" + get_bottom_percent_lines(fragments[-1], percent)\n",
    "\n",
    "            fragments = fragments[:-1]\n",
    "            new_fragments = fragments + [subset_infile_context]\n",
    "            new_prompt = \"\\n\".join(new_fragments)\n",
    "            new_line_data[\"prompt\"] = new_prompt\n",
    "\n",
    "            json.dump(new_line_data, outfile)\n",
    "            outfile.write('\\n')\n",
    "\n",
    "# Usage\n",
    "percent = 50\n",
    "input_jsonl_path = 'subsets/rg-one-gram-ws-20-ss-2-one-line_0.1_instruct_merged.jsonl'  # Update this to your input file path\n",
    "output_jsonl_path = input_jsonl_path.replace(\".jsonl\",f\"_infile_subset_{percent}.jsonl\")\n",
    "\n",
    "subset_infile_jsonl_lines(input_jsonl_path, output_jsonl_path, percent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Multiple In-File Context Subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Here are some relevant code fragments from other files of the repo:\n",
      "# --------------------------------------------------\n",
      "# the below code fragment can be found in:\n",
      "# src/diffusers/configuration_utils.py\n",
      "# --------------------------------------------------\n",
      "# \n",
      "#         if cls.has_compatibles:\n",
      "#             compatible_classes = [c for c in cls._get_compatibles() if not isinstance(c, DummyObject)]\n",
      "#         else:\n",
      "#             compatible_classes = []\n",
      "# \n",
      "#         expected_keys_comp_cls = set()\n",
      "#         for c in compatible_classes:\n",
      "#             expected_keys_c = cls._get_init_keys(c)\n",
      "#             expected_keys_comp_cls = expected_keys_comp_cls.union(expected_keys_c)\n",
      "#         expected_keys_comp_cls = expected_keys_comp_cls - cls._get_init_keys(cls)\n",
      "#         config_dict = {k: v for k, v in config_dict.items() if k not in expected_keys_comp_cls}\n",
      "# \n",
      "#         # remove attributes from orig class that cannot be expected\n",
      "#         orig_cls_name = config_dict.pop(\"_class_name\", cls.__name__)\n",
      "#         if orig_cls_name != cls.__name__ and hasattr(diffusers_library, orig_cls_name):\n",
      "#             orig_cls = getattr(diffusers_library, orig_cls_name)\n",
      "#             unexpected_keys_from_orig = cls._get_init_keys(orig_cls) - expected_keys\n",
      "#             config_dict = {k: v for k, v in config_dict.items() if k not in unexpected_keys_from_orig}\n",
      "#\n",
      "# --------------------------------------------------\n",
      "# the below code fragment can be found in:\n",
      "# src/diffusers/configuration_utils.py\n",
      "# --------------------------------------------------\n",
      "#         # load diffusers library to import compatible and original scheduler\n",
      "#         diffusers_library = importlib.import_module(__name__.split(\".\")[0])\n",
      "# \n",
      "#         if cls.has_compatibles:\n",
      "#             compatible_classes = [c for c in cls._get_compatibles() if not isinstance(c, DummyObject)]\n",
      "#         else:\n",
      "#             compatible_classes = []\n",
      "# \n",
      "#         expected_keys_comp_cls = set()\n",
      "#         for c in compatible_classes:\n",
      "#             expected_keys_c = cls._get_init_keys(c)\n",
      "#             expected_keys_comp_cls = expected_keys_comp_cls.union(expected_keys_c)\n",
      "#         expected_keys_comp_cls = expected_keys_comp_cls - cls._get_init_keys(cls)\n",
      "#         config_dict = {k: v for k, v in config_dict.items() if k not in expected_keys_comp_cls}\n",
      "# \n",
      "#         # remove attributes from orig class that cannot be expected\n",
      "#         orig_cls_name = config_dict.pop(\"_class_name\", cls.__name__)\n",
      "#         if orig_cls_name != cls.__name__ and hasattr(diffusers_library, orig_cls_name):\n",
      "#             orig_cls = getattr(diffusers_library, orig_cls_name)\n",
      "#             unexpected_keys_from_orig = cls._get_init_keys(orig_cls) - expected_keys\n",
      "# --------------------------------------------------\n",
      "# the below code fragment can be found in:\n",
      "# src/diffusers/configuration_utils.py\n",
      "# --------------------------------------------------\n",
      "# \n",
      "#         return config_dict\n",
      "# \n",
      "#     @staticmethod\n",
      "#     def _get_init_keys(cls):\n",
      "#         return set(dict(inspect.signature(cls.__init__).parameters).keys())\n",
      "# \n",
      "#     @classmethod\n",
      "#     def extract_init_dict(cls, config_dict, **kwargs):\n",
      "#         # 0. Copy origin config dict\n",
      "#         original_dict = {k: v for k, v in config_dict.items()}\n",
      "# \n",
      "#         # 1. Retrieve expected config attributes from __init__ signature\n",
      "#         expected_keys = cls._get_init_keys(cls)\n",
      "#         expected_keys.remove(\"self\")\n",
      "#         # remove general kwargs if present in dict\n",
      "#         if \"kwargs\" in expected_keys:\n",
      "#             expected_keys.remove(\"kwargs\")\n",
      "#         # remove flax internal keys\n",
      "#         if hasattr(cls, \"_flax_internal_args\"):\n",
      "# --------------------------------------------------\n",
      "# the below code fragment can be found in:\n",
      "# src/diffusers/configuration_utils.py\n",
      "# --------------------------------------------------\n",
      "#             compatible_classes = []\n",
      "# \n",
      "#         expected_keys_comp_cls = set()\n",
      "#         for c in compatible_classes:\n",
      "#             expected_keys_c = cls._get_init_keys(c)\n",
      "#             expected_keys_comp_cls = expected_keys_comp_cls.union(expected_keys_c)\n",
      "#         expected_keys_comp_cls = expected_keys_comp_cls - cls._get_init_keys(cls)\n",
      "#         config_dict = {k: v for k, v in config_dict.items() if k not in expected_keys_comp_cls}\n",
      "# \n",
      "#         # remove attributes from orig class that cannot be expected\n",
      "#         orig_cls_name = config_dict.pop(\"_class_name\", cls.__name__)\n",
      "#         if orig_cls_name != cls.__name__ and hasattr(diffusers_library, orig_cls_name):\n",
      "#             orig_cls = getattr(diffusers_library, orig_cls_name)\n",
      "#             unexpected_keys_from_orig = cls._get_init_keys(orig_cls) - expected_keys\n",
      "#             config_dict = {k: v for k, v in config_dict.items() if k not in unexpected_keys_from_orig}\n",
      "# \n",
      "#         # remove private attributes\n",
      "#         config_dict = {k: v for k, v in config_dict.items() if not k.startswith(\"_\")}\n",
      "# \n",
      "#         # 3. Create keyword arguments that will be passed to __init__ from expected keyword arguments\n",
      "# --------------------------------------------------\n",
      "# the below code fragment can be found in:\n",
      "# src/diffusers/configuration_utils.py\n",
      "# --------------------------------------------------\n",
      "#             compatible_classes = [c for c in cls._get_compatibles() if not isinstance(c, DummyObject)]\n",
      "#         else:\n",
      "#             compatible_classes = []\n",
      "# \n",
      "#         expected_keys_comp_cls = set()\n",
      "#         for c in compatible_classes:\n",
      "#             expected_keys_c = cls._get_init_keys(c)\n",
      "#             expected_keys_comp_cls = expected_keys_comp_cls.union(expected_keys_c)\n",
      "#         expected_keys_comp_cls = expected_keys_comp_cls - cls._get_init_keys(cls)\n",
      "#         config_dict = {k: v for k, v in config_dict.items() if k not in expected_keys_comp_cls}\n",
      "# \n",
      "#         # remove attributes from orig class that cannot be expected\n",
      "#         orig_cls_name = config_dict.pop(\"_class_name\", cls.__name__)\n",
      "#         if orig_cls_name != cls.__name__ and hasattr(diffusers_library, orig_cls_name):\n",
      "#             orig_cls = getattr(diffusers_library, orig_cls_name)\n",
      "#             unexpected_keys_from_orig = cls._get_init_keys(orig_cls) - expected_keys\n",
      "#             config_dict = {k: v for k, v in config_dict.items() if k not in unexpected_keys_from_orig}\n",
      "# \n",
      "#         # remove private attributes\n",
      "#         config_dict = {k: v for k, v in config_dict.items() if not k.startswith(\"_\")}\n",
      "# --------------------------------------------------\n",
      "# the below code fragment can be found in:\n",
      "# the main script file\n",
      "# --------------------------------------------------\n",
      "# \n",
      "# = 0.1 * sample\n",
      "# \n",
      "#             with tempfile.TemporaryDirectory() as tmpdirname:\n",
      "#                 scheduler.save_config(tmpdirname)\n",
      "#                 new_scheduler = scheduler_class.from_pretrained(tmpdirname)\n",
      "# \n",
      "#             if num_inference_steps is not None and hasattr(scheduler, \"set_timesteps\"):\n",
      "#                 scheduler.set_timesteps(num_inference_steps)\n",
      "#                 new_scheduler.set_timesteps(num_inference_steps)\n",
      "#             elif num_inference_steps is not None and not hasattr(scheduler, \"set_timesteps\"):\n",
      "#                 kwargs[\"num_inference_steps\"] = num_inference_steps\n",
      "# \n",
      "#             # Set the seed before step() as some schedulers are stochastic like EulerAncestralDiscreteScheduler, EulerDiscreteScheduler\n",
      "#             if \"generator\" in set(inspect.signature(scheduler.step).parameters.keys()):\n",
      "#                 kwargs[\"generator\"] = torch.manual_seed(0)\n",
      "#             output = scheduler.step(residual, time_step, sample, **kwargs).prev_sample\n",
      "# \n",
      "#             if \"generator\" in set(inspect.signature(scheduler.step).parameters.keys()):\n",
      "#                 kwargs[\"generator\"] = torch.manual_seed(0)\n",
      "# --------------------------------------------------\n",
      "# the below code fragment can be found in:\n",
      "# the main script file\n",
      "# --------------------------------------------------\n",
      "#             new_output = new_scheduler.step(residual, time_step, sample, **kwargs).prev_sample\n",
      "# \n",
      "#             assert torch.sum(torch.abs(output - new_output)) < 1e-5, \"Scheduler outputs are not identical\"\n",
      "# \n",
      "#     def check_over_forward(self, time_step=0, **forward_kwargs):\n",
      "#         kwargs = dict(self.forward_default_kwargs)\n",
      "#         kwargs.update(forward_kwargs)\n",
      "# \n",
      "#         num_inference_steps = kwargs.pop(\"num_inference_steps\", None)\n",
      "# \n",
      "#         for scheduler_class in self.scheduler_classes:\n",
      "#             if scheduler_class in (EulerAncestralDiscreteScheduler, EulerDiscreteScheduler, LMSDiscreteScheduler):\n",
      "#                 time_step = float(time_step)\n",
      "# \n",
      "#             scheduler_config = self.get_scheduler_config()\n",
      "#             scheduler = scheduler_class(**scheduler_config)\n",
      "# \n",
      "#             if scheduler_class == VQDiffusionScheduler:\n",
      "#                 num_vec_classes = scheduler_config[\"num_vec_classes\"]\n",
      "#                 sample = self.dummy_sample(num_vec_classes)\n",
      "# --------------------------------------------------\n",
      "# the below code fragment can be found in:\n",
      "# the main script file\n",
      "# --------------------------------------------------\n",
      "#                 model = self.dummy_model(num_vec_classes)\n",
      "#                 residual = model(sample, time_step)\n",
      "#             else:\n",
      "#                 sample = self.dummy_sample\n",
      "#                 residual = 0.1 * sample\n",
      "# \n",
      "#             with tempfile.TemporaryDirectory() as tmpdirname:\n",
      "#                 scheduler.save_config(tmpdirname)\n",
      "#                 new_scheduler = scheduler_class.from_pretrained(tmpdirname)\n",
      "# \n",
      "#             if num_inference_steps is not None and hasattr(scheduler, \"set_timesteps\"):\n",
      "#                 scheduler.set_timesteps(num_inference_steps)\n",
      "#                 new_scheduler.set_timesteps(num_inference_steps)\n",
      "#             elif num_inference_steps is not None and not hasattr(scheduler, \"set_timesteps\"):\n",
      "#                 kwargs[\"num_inference_steps\"] = num_inference_steps\n",
      "# \n",
      "#             if \"generator\" in set(inspect.signature(scheduler.step).parameters.keys()):\n",
      "#                 kwargs[\"generator\"] = torch.manual_seed(0)\n",
      "#             output = scheduler.step(residual, time_step, sample, **kwargs).prev_sample\n",
      "# \n",
      "# --------------------------------------------------\n",
      "# the below code fragment can be found in:\n",
      "# the main script file\n",
      "# --------------------------------------------------\n",
      "#             if \"generator\" in set(inspect.signature(scheduler.step).parameters.keys()):\n",
      "#                 kwargs[\"generator\"] = torch.manual_seed(0)\n",
      "#             new_output = new_scheduler.step(residual, time_step, sample, **kwargs).prev_sample\n",
      "# \n",
      "#             assert torch.sum(torch.abs(output - new_output)) < 1e-5, \"Scheduler outputs are not identical\"\n",
      "# \n",
      "#     def test_from_save_pretrained(self):\n",
      "#         kwargs = dict(self.forward_default_kwargs)\n",
      "# \n",
      "#         num_inference_steps = kwargs.pop(\"num_inference_steps\", None)\n",
      "# \n",
      "#         for scheduler_class in self.scheduler_classes:\n",
      "#             timestep = 1\n",
      "#             if scheduler_class in (EulerAncestralDiscreteScheduler, EulerDiscreteScheduler, LMSDiscreteScheduler):\n",
      "#                 timestep = float(timestep)\n",
      "# \n",
      "#             scheduler_config = self.get_scheduler_config()\n",
      "#             scheduler = scheduler_class(**scheduler_config)\n",
      "# \n",
      "#             if scheduler_class == VQDiffusionScheduler:\n",
      "# --------------------------------------------------\n",
      "# the below code fragment can be found in:\n",
      "# the main script file\n",
      "# --------------------------------------------------\n",
      "#                 num_vec_classes = scheduler_config[\"num_vec_classes\"]\n",
      "#                 sample = self.dummy_sample(num_vec_classes)\n",
      "#                 model = self.dummy_model(num_vec_classes)\n",
      "#                 residual = model(sample, timestep)\n",
      "#             else:\n",
      "#                 sample = self.dummy_sample\n",
      "#                 residual = 0.1 * sample\n",
      "# \n",
      "#             with tempfile.TemporaryDirectory() as tmpdirname:\n",
      "#                 scheduler.save_config(tmpdirname)\n",
      "#                 new_scheduler = scheduler_class.from_pretrained(tmpdirname)\n",
      "# \n",
      "#             if num_inference_steps is not None and hasattr(scheduler, \"set_timesteps\"):\n",
      "#                 scheduler.set_timesteps(num_inference_steps)\n",
      "#                 new_scheduler.set_timesteps(num_inference_steps)\n",
      "#             elif num_inference_steps is not None and not hasattr(scheduler, \"set_timesteps\"):\n",
      "#                 kwargs[\"num_inference_steps\"] = num_inference_steps\n",
      "# \n",
      "#             if \"generator\" in set(inspect.signature(scheduler.step).parameters.keys()):\n",
      "#                 kwargs[\"generator\"] = torch.manual_seed(0)\n",
      "# --------------------------------------------------\n",
      "# the below code fragment can be found in:\n",
      "# the main script file\n",
      "# --------------------------------------------------\n",
      "#             output = scheduler.step(residual, timestep, sample, **kwargs).prev_sample\n",
      "# \n",
      "#             if \"generator\" in set(inspect.signature(scheduler.step).parameters.keys()):\n",
      "#                 kwargs[\"generator\"] = torch.manual_seed(0)\n",
      "#             new_output = new_scheduler.step(residual, timestep, sample, **kwargs).prev_sample\n",
      "# \n",
      "#             assert torch.sum(torch.abs(output - new_output)) < 1e-5, \"Scheduler outputs are not identical\"\n",
      "# \n",
      "#     def test_compatibles(self):\n",
      "#         for scheduler_class in self.scheduler_classes:\n",
      "#             scheduler_config = self.get_scheduler_config()\n",
      "# \n",
      "#             scheduler = scheduler_class(**scheduler_config)\n",
      "# \n",
      "#             assert all(c is not None for c in scheduler.compatibles)\n",
      "# \n",
      "#             for comp_scheduler_cls in scheduler.compatibles:\n",
      "#                 comp_scheduler = comp_scheduler_cls.from_config(scheduler.config)\n",
      "#                 assert comp_scheduler is not None\n",
      "# \n",
      "# --------------------------------------------------\n",
      "# the below code fragment can be found in:\n",
      "# the main script file\n",
      "# --------------------------------------------------\n",
      "#             new_scheduler = scheduler_class.from_config(comp_scheduler.config)\n",
      "# \n",
      "#             new_scheduler_config = {k: v for k, v in new_scheduler.config.items() if k in scheduler.config}\n",
      "#             scheduler_diff = {k: v for k, v in new_scheduler.config.items() if k not in scheduler.config}\n",
      "# --------------------------------------------------\n",
      "\"\"\"Based on the above, complete the following code from the main script file:\"\"\"\n",
      "\n",
      "            # make sure that configs are essentially identical\n",
      "            assert new_scheduler_config == dict(scheduler.config)\n",
      "\n",
      "            # make sure that only differences are for configs that are not in init\n",
      "            init_keys = inspect.signature(scheduler_class.__init__).parameters.keys()\n",
      "            assert set(scheduler_diff.keys()).intersection(set(init_keys)) == set()\n",
      "\n",
      "    def test_from_pretrained(self):\n",
      "        for scheduler_class in self.scheduler_classes:\n"
     ]
    }
   ],
   "source": [
    "def format_code_context(input_fragments):\n",
    "    # Split the string into lines\n",
    "\n",
    "    context_string = \"\\n\".join(input_fragments[-1].split('\\n')[2:])\n",
    "\n",
    "    lines = context_string.splitlines()\n",
    "    \n",
    "    # Initialize list for blocks\n",
    "    blocks = []\n",
    "    \n",
    "    # Extract the last 10 lines for the bottom part\n",
    "    last_10_lines = lines[-10:]\n",
    "    \n",
    "    # Process the rest of the lines in 20-line blocks\n",
    "    remaining_lines = lines[:-10]\n",
    "\n",
    "    # Create 20-line blocks\n",
    "    for i in range(0, len(remaining_lines), 20):\n",
    "        block = remaining_lines[i:i+20]\n",
    "        blocks.append(block)\n",
    "\n",
    "    # Format each block\n",
    "    formatted_blocks = []\n",
    "    for block in blocks:\n",
    "        block_string = \"\\n# \".join(block)\n",
    "        formatted_block = (\n",
    "            \"# --------------------------------------------------\\n\"\n",
    "            \"# the below code fragment can be found in:\\n\"\n",
    "            \"# the main script file\\n\"\n",
    "            \"# --------------------------------------------------\\n\"\n",
    "            f\"# {block_string}\"\n",
    "        )\n",
    "        formatted_blocks.append(formatted_block)\n",
    "\n",
    "    # Format the last 10 lines block\n",
    "    last_10_block = (\n",
    "        '''\\n# --------------------------------------------------\\n'''\n",
    "        '\"\"\"Based on the above, complete the following code from the main script file:\"\"\"\\n'\n",
    "        + \"\\n\".join([f\"{line}\" for line in last_10_lines])\n",
    "    )\n",
    "\n",
    "    # Combine all formatted blocks into a single string\n",
    "    final_output =  \"\\n\".join(input_fragments[:-1]) +\"\\n\" + \"\\n\".join(formatted_blocks) + last_10_block\n",
    "\n",
    "    return final_output\n",
    "\n",
    "# Example usage:\n",
    "# Assuming 'in_file_context' variable contains the long in-file context itself\n",
    "prompt = '''# Here are some relevant code fragments from other files of the repo:\\n# --------------------------------------------------\\n# the below code fragment can be found in:\\n# src/diffusers/configuration_utils.py\\n# --------------------------------------------------\\n# \\n#         if cls.has_compatibles:\\n#             compatible_classes = [c for c in cls._get_compatibles() if not isinstance(c, DummyObject)]\\n#         else:\\n#             compatible_classes = []\\n# \\n#         expected_keys_comp_cls = set()\\n#         for c in compatible_classes:\\n#             expected_keys_c = cls._get_init_keys(c)\\n#             expected_keys_comp_cls = expected_keys_comp_cls.union(expected_keys_c)\\n#         expected_keys_comp_cls = expected_keys_comp_cls - cls._get_init_keys(cls)\\n#         config_dict = {k: v for k, v in config_dict.items() if k not in expected_keys_comp_cls}\\n# \\n#         # remove attributes from orig class that cannot be expected\\n#         orig_cls_name = config_dict.pop(\\\"_class_name\\\", cls.__name__)\\n#         if orig_cls_name != cls.__name__ and hasattr(diffusers_library, orig_cls_name):\\n#             orig_cls = getattr(diffusers_library, orig_cls_name)\\n#             unexpected_keys_from_orig = cls._get_init_keys(orig_cls) - expected_keys\\n#             config_dict = {k: v for k, v in config_dict.items() if k not in unexpected_keys_from_orig}\\n# \\n# --------------------------------------------------\\n# the below code fragment can be found in:\\n# src/diffusers/configuration_utils.py\\n# --------------------------------------------------\\n#         # load diffusers library to import compatible and original scheduler\\n#         diffusers_library = importlib.import_module(__name__.split(\\\".\\\")[0])\\n# \\n#         if cls.has_compatibles:\\n#             compatible_classes = [c for c in cls._get_compatibles() if not isinstance(c, DummyObject)]\\n#         else:\\n#             compatible_classes = []\\n# \\n#         expected_keys_comp_cls = set()\\n#         for c in compatible_classes:\\n#             expected_keys_c = cls._get_init_keys(c)\\n#             expected_keys_comp_cls = expected_keys_comp_cls.union(expected_keys_c)\\n#         expected_keys_comp_cls = expected_keys_comp_cls - cls._get_init_keys(cls)\\n#         config_dict = {k: v for k, v in config_dict.items() if k not in expected_keys_comp_cls}\\n# \\n#         # remove attributes from orig class that cannot be expected\\n#         orig_cls_name = config_dict.pop(\\\"_class_name\\\", cls.__name__)\\n#         if orig_cls_name != cls.__name__ and hasattr(diffusers_library, orig_cls_name):\\n#             orig_cls = getattr(diffusers_library, orig_cls_name)\\n#             unexpected_keys_from_orig = cls._get_init_keys(orig_cls) - expected_keys\\n# --------------------------------------------------\\n# the below code fragment can be found in:\\n# src/diffusers/configuration_utils.py\\n# --------------------------------------------------\\n# \\n#         return config_dict\\n# \\n#     @staticmethod\\n#     def _get_init_keys(cls):\\n#         return set(dict(inspect.signature(cls.__init__).parameters).keys())\\n# \\n#     @classmethod\\n#     def extract_init_dict(cls, config_dict, **kwargs):\\n#         # 0. Copy origin config dict\\n#         original_dict = {k: v for k, v in config_dict.items()}\\n# \\n#         # 1. Retrieve expected config attributes from __init__ signature\\n#         expected_keys = cls._get_init_keys(cls)\\n#         expected_keys.remove(\\\"self\\\")\\n#         # remove general kwargs if present in dict\\n#         if \\\"kwargs\\\" in expected_keys:\\n#             expected_keys.remove(\\\"kwargs\\\")\\n#         # remove flax internal keys\\n#         if hasattr(cls, \\\"_flax_internal_args\\\"):\\n# --------------------------------------------------\\n# the below code fragment can be found in:\\n# src/diffusers/configuration_utils.py\\n# --------------------------------------------------\\n#             compatible_classes = []\\n# \\n#         expected_keys_comp_cls = set()\\n#         for c in compatible_classes:\\n#             expected_keys_c = cls._get_init_keys(c)\\n#             expected_keys_comp_cls = expected_keys_comp_cls.union(expected_keys_c)\\n#         expected_keys_comp_cls = expected_keys_comp_cls - cls._get_init_keys(cls)\\n#         config_dict = {k: v for k, v in config_dict.items() if k not in expected_keys_comp_cls}\\n# \\n#         # remove attributes from orig class that cannot be expected\\n#         orig_cls_name = config_dict.pop(\\\"_class_name\\\", cls.__name__)\\n#         if orig_cls_name != cls.__name__ and hasattr(diffusers_library, orig_cls_name):\\n#             orig_cls = getattr(diffusers_library, orig_cls_name)\\n#             unexpected_keys_from_orig = cls._get_init_keys(orig_cls) - expected_keys\\n#             config_dict = {k: v for k, v in config_dict.items() if k not in unexpected_keys_from_orig}\\n# \\n#         # remove private attributes\\n#         config_dict = {k: v for k, v in config_dict.items() if not k.startswith(\\\"_\\\")}\\n# \\n#         # 3. Create keyword arguments that will be passed to __init__ from expected keyword arguments\\n# --------------------------------------------------\\n# the below code fragment can be found in:\\n# src/diffusers/configuration_utils.py\\n# --------------------------------------------------\\n#             compatible_classes = [c for c in cls._get_compatibles() if not isinstance(c, DummyObject)]\\n#         else:\\n#             compatible_classes = []\\n# \\n#         expected_keys_comp_cls = set()\\n#         for c in compatible_classes:\\n#             expected_keys_c = cls._get_init_keys(c)\\n#             expected_keys_comp_cls = expected_keys_comp_cls.union(expected_keys_c)\\n#         expected_keys_comp_cls = expected_keys_comp_cls - cls._get_init_keys(cls)\\n#         config_dict = {k: v for k, v in config_dict.items() if k not in expected_keys_comp_cls}\\n# \\n#         # remove attributes from orig class that cannot be expected\\n#         orig_cls_name = config_dict.pop(\\\"_class_name\\\", cls.__name__)\\n#         if orig_cls_name != cls.__name__ and hasattr(diffusers_library, orig_cls_name):\\n#             orig_cls = getattr(diffusers_library, orig_cls_name)\\n#             unexpected_keys_from_orig = cls._get_init_keys(orig_cls) - expected_keys\\n#             config_dict = {k: v for k, v in config_dict.items() if k not in unexpected_keys_from_orig}\\n# \\n#         # remove private attributes\\n#         config_dict = {k: v for k, v in config_dict.items() if not k.startswith(\\\"_\\\")}\\n# --------------------------------------------------\\n\\\"\\\"\\\"Based on the above, complete the following code:\\\"\\\"\\\"\\n\\n = 0.1 * sample\\n\\n            with tempfile.TemporaryDirectory() as tmpdirname:\\n                scheduler.save_config(tmpdirname)\\n                new_scheduler = scheduler_class.from_pretrained(tmpdirname)\\n\\n            if num_inference_steps is not None and hasattr(scheduler, \\\"set_timesteps\\\"):\\n                scheduler.set_timesteps(num_inference_steps)\\n                new_scheduler.set_timesteps(num_inference_steps)\\n            elif num_inference_steps is not None and not hasattr(scheduler, \\\"set_timesteps\\\"):\\n                kwargs[\\\"num_inference_steps\\\"] = num_inference_steps\\n\\n            # Set the seed before step() as some schedulers are stochastic like EulerAncestralDiscreteScheduler, EulerDiscreteScheduler\\n            if \\\"generator\\\" in set(inspect.signature(scheduler.step).parameters.keys()):\\n                kwargs[\\\"generator\\\"] = torch.manual_seed(0)\\n            output = scheduler.step(residual, time_step, sample, **kwargs).prev_sample\\n\\n            if \\\"generator\\\" in set(inspect.signature(scheduler.step).parameters.keys()):\\n                kwargs[\\\"generator\\\"] = torch.manual_seed(0)\\n            new_output = new_scheduler.step(residual, time_step, sample, **kwargs).prev_sample\\n\\n            assert torch.sum(torch.abs(output - new_output)) < 1e-5, \\\"Scheduler outputs are not identical\\\"\\n\\n    def check_over_forward(self, time_step=0, **forward_kwargs):\\n        kwargs = dict(self.forward_default_kwargs)\\n        kwargs.update(forward_kwargs)\\n\\n        num_inference_steps = kwargs.pop(\\\"num_inference_steps\\\", None)\\n\\n        for scheduler_class in self.scheduler_classes:\\n            if scheduler_class in (EulerAncestralDiscreteScheduler, EulerDiscreteScheduler, LMSDiscreteScheduler):\\n                time_step = float(time_step)\\n\\n            scheduler_config = self.get_scheduler_config()\\n            scheduler = scheduler_class(**scheduler_config)\\n\\n            if scheduler_class == VQDiffusionScheduler:\\n                num_vec_classes = scheduler_config[\\\"num_vec_classes\\\"]\\n                sample = self.dummy_sample(num_vec_classes)\\n                model = self.dummy_model(num_vec_classes)\\n                residual = model(sample, time_step)\\n            else:\\n                sample = self.dummy_sample\\n                residual = 0.1 * sample\\n\\n            with tempfile.TemporaryDirectory() as tmpdirname:\\n                scheduler.save_config(tmpdirname)\\n                new_scheduler = scheduler_class.from_pretrained(tmpdirname)\\n\\n            if num_inference_steps is not None and hasattr(scheduler, \\\"set_timesteps\\\"):\\n                scheduler.set_timesteps(num_inference_steps)\\n                new_scheduler.set_timesteps(num_inference_steps)\\n            elif num_inference_steps is not None and not hasattr(scheduler, \\\"set_timesteps\\\"):\\n                kwargs[\\\"num_inference_steps\\\"] = num_inference_steps\\n\\n            if \\\"generator\\\" in set(inspect.signature(scheduler.step).parameters.keys()):\\n                kwargs[\\\"generator\\\"] = torch.manual_seed(0)\\n            output = scheduler.step(residual, time_step, sample, **kwargs).prev_sample\\n\\n            if \\\"generator\\\" in set(inspect.signature(scheduler.step).parameters.keys()):\\n                kwargs[\\\"generator\\\"] = torch.manual_seed(0)\\n            new_output = new_scheduler.step(residual, time_step, sample, **kwargs).prev_sample\\n\\n            assert torch.sum(torch.abs(output - new_output)) < 1e-5, \\\"Scheduler outputs are not identical\\\"\\n\\n    def test_from_save_pretrained(self):\\n        kwargs = dict(self.forward_default_kwargs)\\n\\n        num_inference_steps = kwargs.pop(\\\"num_inference_steps\\\", None)\\n\\n        for scheduler_class in self.scheduler_classes:\\n            timestep = 1\\n            if scheduler_class in (EulerAncestralDiscreteScheduler, EulerDiscreteScheduler, LMSDiscreteScheduler):\\n                timestep = float(timestep)\\n\\n            scheduler_config = self.get_scheduler_config()\\n            scheduler = scheduler_class(**scheduler_config)\\n\\n            if scheduler_class == VQDiffusionScheduler:\\n                num_vec_classes = scheduler_config[\\\"num_vec_classes\\\"]\\n                sample = self.dummy_sample(num_vec_classes)\\n                model = self.dummy_model(num_vec_classes)\\n                residual = model(sample, timestep)\\n            else:\\n                sample = self.dummy_sample\\n                residual = 0.1 * sample\\n\\n            with tempfile.TemporaryDirectory() as tmpdirname:\\n                scheduler.save_config(tmpdirname)\\n                new_scheduler = scheduler_class.from_pretrained(tmpdirname)\\n\\n            if num_inference_steps is not None and hasattr(scheduler, \\\"set_timesteps\\\"):\\n                scheduler.set_timesteps(num_inference_steps)\\n                new_scheduler.set_timesteps(num_inference_steps)\\n            elif num_inference_steps is not None and not hasattr(scheduler, \\\"set_timesteps\\\"):\\n                kwargs[\\\"num_inference_steps\\\"] = num_inference_steps\\n\\n            if \\\"generator\\\" in set(inspect.signature(scheduler.step).parameters.keys()):\\n                kwargs[\\\"generator\\\"] = torch.manual_seed(0)\\n            output = scheduler.step(residual, timestep, sample, **kwargs).prev_sample\\n\\n            if \\\"generator\\\" in set(inspect.signature(scheduler.step).parameters.keys()):\\n                kwargs[\\\"generator\\\"] = torch.manual_seed(0)\\n            new_output = new_scheduler.step(residual, timestep, sample, **kwargs).prev_sample\\n\\n            assert torch.sum(torch.abs(output - new_output)) < 1e-5, \\\"Scheduler outputs are not identical\\\"\\n\\n    def test_compatibles(self):\\n        for scheduler_class in self.scheduler_classes:\\n            scheduler_config = self.get_scheduler_config()\\n\\n            scheduler = scheduler_class(**scheduler_config)\\n\\n            assert all(c is not None for c in scheduler.compatibles)\\n\\n            for comp_scheduler_cls in scheduler.compatibles:\\n                comp_scheduler = comp_scheduler_cls.from_config(scheduler.config)\\n                assert comp_scheduler is not None\\n\\n            new_scheduler = scheduler_class.from_config(comp_scheduler.config)\\n\\n            new_scheduler_config = {k: v for k, v in new_scheduler.config.items() if k in scheduler.config}\\n            scheduler_diff = {k: v for k, v in new_scheduler.config.items() if k not in scheduler.config}\\n\\n            # make sure that configs are essentially identical\\n            assert new_scheduler_config == dict(scheduler.config)\\n\\n            # make sure that only differences are for configs that are not in init\\n            init_keys = inspect.signature(scheduler_class.__init__).parameters.keys()\\n            assert set(scheduler_diff.keys()).intersection(set(init_keys)) == set()\\n\\n    def test_from_pretrained(self):\\n        for scheduler_class in self.scheduler_classes:'''\n",
    "input_fragments = split_code_fragments(prompt)\n",
    "formatted_code = format_code_context(input_fragments)\n",
    "print(formatted_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First Write a function to convert the in-file context into the same format as the out-of-file context\n",
    "import json\n",
    "\n",
    "def merge_overlapping_jsonl_lines(input_file, output_file):\n",
    "    with open(input_file, 'r', encoding='utf-8') as infile, open(output_file, 'w', encoding='utf-8') as outfile:\n",
    "        for line in infile:\n",
    "            line_data = json.loads(line)\n",
    "            new_line_data = line_data.copy()\n",
    "            old_prompt = line_data[\"prompt\"]\n",
    "            fragments = split_code_fragments(old_prompt)\n",
    "            new_prompt = format_code_context(fragments)\n",
    "            new_line_data[\"prompt\"] = new_prompt\n",
    "            json.dump(new_line_data, outfile)\n",
    "            outfile.write('\\n')\n",
    "\n",
    "# Usage\n",
    "input_jsonl_path = 'temp_subsets/rg-one-gram-ws-20-ss-2-one-line_0.1_instruct.jsonl'  # Update this to your input file path\n",
    "output_jsonl_path = input_jsonl_path.replace(\".jsonl\",\"_infile_snippets.jsonl\")\n",
    "\n",
    "merge_overlapping_jsonl_lines(input_jsonl_path, output_jsonl_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n",
      "Warning: The number of subsets must be less than the number of unique code fragments.\n",
      "Existing fragments will be copied randomly to ensure sufficient fragments.\n"
     ]
    }
   ],
   "source": [
    "at = 10\n",
    "input_jsonl_path = 'temp_subsets/rg-one-gram-ws-20-ss-2-one-line_0.1_instruct_infile_snippets.jsonl'\n",
    "output_jsonl_path = input_jsonl_path.replace('.jsonl', f'_{at}.jsonl')\n",
    "\n",
    "# Use the function with the path to your JSONL file\n",
    "process_jsonl_and_write(input_jsonl_path, output_jsonl_path, at)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Infile Contexts with Consistent Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import re\n",
    "\n",
    "def random_combinations_of_fragments_fixed_length(fragments, num_subsets):\n",
    "    # First and last fragments assumed to be the header and footer\n",
    "    header_fragment = fragments[0]\n",
    "    footer_fragment = fragments[-1]\n",
    "    # The central fragments will be randomized\n",
    "    central_fragments = fragments[1:-1]\n",
    "    \n",
    "    if num_subsets > 10:\n",
    "        # Return 3 subsets, each with ten randomly sampled central fragments without replacement\n",
    "        snippets = []\n",
    "        for _ in range(3):\n",
    "            sample_fragments = random.sample(central_fragments, 10)\n",
    "            snippet = header_fragment + '\\n' + '\\n'.join(sample_fragments) + '\\n' + footer_fragment\n",
    "            snippets.append(snippet)\n",
    "        return snippets\n",
    "    else:\n",
    "        # Return a snippet containing all of the fragments\n",
    "        snippet = header_fragment + '\\n' + '\\n'.join(central_fragments) + '\\n' + footer_fragment\n",
    "        return [snippet]*3  # wrapped in a list for consistent return type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "import json\n",
    "from typing import List\n",
    "\n",
    "def split_snippets(text: str, num_splits) -> List[str]:\n",
    "    fragments = split_code_fragments(text)\n",
    "    return random_combinations_of_fragments_fixed_length(fragments, num_splits)\n",
    "\n",
    "\n",
    "def process_jsonl_and_write(file_path, output_jsonl_path, num_splits):\n",
    "    output_variants = []\n",
    "\n",
    "    with open(file_path, 'r') as jsonl_file:\n",
    "        for line in jsonl_file: # Read line by line\n",
    "            data = json.loads(line)\n",
    "            prompt = data['prompt']\n",
    "            task_id = data['metadata']['task_id']\n",
    "\n",
    "            # Generate variants\n",
    "            prompt_variants = split_snippets(prompt, num_splits)\n",
    "\n",
    "            # Construct new data entries for each variant and add to output list\n",
    "            for i, variant in enumerate(prompt_variants):\n",
    "                new_data = {\n",
    "                    \"prompt\": variant,\n",
    "                    \"metadata\": {\n",
    "                        \"task_id\": f\"{task_id}\",\n",
    "                        \"instance_id\": f\"{task_id}_{i}\",\n",
    "                        \"ground_truth\": data['metadata']['ground_truth'], # Keep the same ground truth\n",
    "                        \"fpath_tuple\": data['metadata']['fpath_tuple'],\n",
    "                        \"context_start_lineno\": data['metadata']['context_start_lineno'],\n",
    "                        \"line_no\": data['metadata']['line_no'],\n",
    "                    }\n",
    "                }\n",
    "                output_variants.append(new_data)\n",
    "\n",
    "    # Write the output variants to a new JSONL file\n",
    "    with open(output_jsonl_path, 'w') as outfile:\n",
    "        for variant in output_variants:\n",
    "            json.dump(variant, outfile)\n",
    "            outfile.write('\\n') # Write each JSON object on a new line\n",
    "\n",
    "num_variants = 10\n",
    "\n",
    "input_jsonl_path = 'subsets/rg-one-gram-ws-20-ss-2-fixed_0.1_with_instructions_temp_0.jsonl'  # Update this to your input file path\n",
    "output_jsonl_path = f'temp_subsets/rg-one-gram-ws-20-ss-2-one-line_0.1_instruct_{num_variants}_variants.jsonl' # Update this to your desired output file path\n",
    "\n",
    "# Use the function with the path to your JSONL file\n",
    "process_jsonl_and_write(input_jsonl_path, output_jsonl_path, num_variants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "at = 10\n",
    "input_jsonl_path = 'temp_subsets/rg-one-gram-ws-20-ss-2-one-line_0.1_instruct_infile_snippets.jsonl'\n",
    "output_jsonl_path = input_jsonl_path.replace('.jsonl', f'_{at}.jsonl')\n",
    "\n",
    "# Use the function with the path to your JSONL file\n",
    "process_jsonl_and_write(input_jsonl_path, output_jsonl_path, at)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subset to only short responses for pass@100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def num_tokens_from_string(string: str) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens\n",
    "num_tokens_from_string(\"This is an example string with several tokens.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "line 85 of subsets/rg-one-gram-ws-20-ss-2-one-line_0.1_instruct.jsonl written to subsets/rg-one-gram-ws-20-ss-2-one-line_0.1_instruct_small_gt.jsonl\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def merge_overlapping_jsonl_lines(input_file, output_file):\n",
    "    with open(input_file, 'r', encoding='utf-8') as infile, open(output_file, 'w', encoding='utf-8') as outfile:\n",
    "\n",
    "        i = 0\n",
    "\n",
    "        for line in infile:\n",
    "            line_data = json.loads(line)\n",
    "            new_line_data = line_data.copy()\n",
    "            ground_truth = line_data[\"metadata\"][\"ground_truth\"]\n",
    "            if num_tokens_from_string(ground_truth) < 10:\n",
    "                i += 1\n",
    "                json.dump(new_line_data, outfile)\n",
    "                outfile.write('\\n')\n",
    "\n",
    "        print(f\"line {i} of {input_file} written to {output_file}\")\n",
    "\n",
    "# Usage\n",
    "input_jsonl_path = 'subsets/rg-one-gram-ws-20-ss-2-one-line_0.1_instruct.jsonl'  # Update this to your input file path\n",
    "output_jsonl_path = input_jsonl_path.replace(\".jsonl\",\"_small_gt.jsonl\")\n",
    "\n",
    "merge_overlapping_jsonl_lines(input_jsonl_path, output_jsonl_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Syntax-Checking Before Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
