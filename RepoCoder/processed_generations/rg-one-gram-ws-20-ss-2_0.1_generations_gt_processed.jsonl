{"completion": "I'm sorry, but I'm unable to identify the specific API call you are referring to. Could you please provide more context or a specific code snippet where the API call is used?", "ground_truth": "                    GymEnv(env_name, frame_skip=frame_skip, device=device),"}
{"completion": "save_method(os.path.join(save_directory, pipeline_component_name), safe_serialization=safe_serialization)", "ground_truth": "        module_names, _, _ = self.extract_init_dict(dict(self.config))"}
{"completion": "import torchif __name__ == \"__main__\": env = torchrl.envs.VecEnv() env.start() env.close()", "ground_truth": "            else [meta_data.to(device) for meta_data in self.meta_data]"}
{"completion": "container = LimitedSpaceContainer(3)container['a'] = 1container['b'] = 2container['c'] = 3print(container)container['d'] = 4print(container)container['e'] = 5print(container)", "ground_truth": "        container = LimitedSpaceContainer(0, 5)"}
{"completion": "checkpointer=CalibCheckpointer(save_checkpoint_dir=directory, dump_state=True),)", "ground_truth": "            checkpointer=CalibCheckpointer(\n                save_checkpoint_dir=directory, dump_state=True\n            ),"}
{"completion": "prob_class.train(train_data_loader=self.class_train_data_loader, calib_data_loader=self.class_val_data_loader, val_data_loader=self.class_val_data_loader, fit_config=self.class_fit_config_restore(tmp_dir), calib_config=self.class_calib_config_nodir_nodump,)", "ground_truth": "            status = prob_class.train(\n                train_data_loader=self.class_train_data_loader,\n                calib_data_loader=self.class_val_data_loader,\n                val_data_loader=self.class_val_data_loader,\n                fit_config=self.class_fit_config_restore(tmp_dir),\n                calib_config=self.class_calib_config_nodir_nodump,\n            )"}
{"completion": "", "ground_truth": "        improved = trainer.early_stopping_update(validation_metrics_step3)"}
{"completion": "", "ground_truth": "        inputs = self.get_dummy_inputs(device)"}
{"completion": "pd.kl(torch.distributions.Categorical(probs=torch.tensor([0.3, 0.7])), torch.distributions.Categorical(probs=torch.tensor([0.4, 0.6])))``````python pd.entropy()``````python pd.kl_divergence(torch.distributions.Categorical(probs=torch.tensor([0.3, 0.7])), torch.distributions.Categorical(probs=torch.tensor([0.4, 0.6])))``````python pd.sample() pd.sample(mean_only=True)", "ground_truth": "            pd.sample()"}
{"completion": "import requestsimport jsonurl = \"https://api.openai.com/v1/data/datasets/examples/unconditional_image_generation/train_unconditional_ort.py\"headers = {\"Authorization\": \"Bearer YOUR_API_KEY\"}response = requests.get(url, headers=headers)data = response.json()", "ground_truth": "        tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name, revision=args.revision, use_fast=False)"}
{"completion": "", "ground_truth": "            observation_spec = gs.transform_observation_spec(observation_spec)"}
{"completion": "", "ground_truth": "                env.close()"}
{"completion": "Based on the code snippets provided, I could not find a specific API call or function that represents the proposed API call completion. Could you please provide more context or code snippets related to the specific API call you are referring to?", "ground_truth": "                                    NewsQAResult(unique_id, start_logits,\n                                                 end_logits))"}
{"completion": "", "ground_truth": "                    torch.device(_passing_device) for _passing_device in passing_devices"}
{"completion": "I couldn't generate a valid API call with the provided code fragments. Please provide more details or specify which specific API call you want to make.", "ground_truth": "            observation_spec = CompositeSpec(\n                pixels=UnboundedContinuousTensorSpec(\n                    shape=torch.Size([*batch_size, 1, 7, 7])\n                ),\n                pixels_orig=UnboundedContinuousTensorSpec(\n                    shape=torch.Size([*batch_size, 1, 7, 7])\n                ),\n                shape=batch_size,\n            )"}
{"completion": "converter = core.DefaultModelInputConverter(pyvizier.ParameterConfig.factory('x1', bounds=(-3.0, 3.0)), scale=False, onehot_embed=True, float_dtype=dtype,)actual = converter.convert([ Trial(parameters={'x1': pyvizier.ParameterValue(1)}), Trial(parameters={'x1': pyvizier.ParameterValue(2)}), Trial(parameters={'x1': pyvizier.ParameterValue(-3)}), Trial(parameters={'x1': pyvizier.ParameterValue('a')}), Trial(),])expected = np.asarray([[1.0], [2.0], [-3.0], [np.NaN], [np.NaN]], dtype)np.testing.assert_allclose(expected, actual)self.assertEqual(expected.dtype, actual.dtype)", "ground_truth": "    actual: list[pyvizier.ParameterValue] = converter.to_parameter_values(  # pytype:disable=annotation-type-mismatch\n        scaled\n    )"}
{"completion": "import torchfrom gym.spaces import Discrete, Boxenv.observation_space = Box(low=0, high=255, shape=(84, 84, 3), dtype=np.uint8)env.action_space = Discrete(4)observation = env.observation_space.sample()action = env.action_space.sample()torch_observation = torch.from_numpy(observation).unsqueeze(0)torch_action = torch.tensor(action).unsqueeze(0)model_input = (torch_observation, torch_action)model_output = model(*model_input)print(model_output)", "ground_truth": "        assert (spec2.zero() == spec.zero()).all()"}
{"completion": "", "ground_truth": "        transforms.append(normalize)"}
{"completion": "", "ground_truth": "        td_expanded = td.expand(2)"}
{"completion": "test_case = TestProbOutputLayers()test_case.test_reg_prob_output_layer_predict()", "ground_truth": "        assert self.class_prob_output_layer.predict(outputs).shape == (self.n_inputs,)"}
{"completion": "aux = dict() outs = self.output_calib_manager.apply(params=params[\"output_calibrator\"], outputs=outputs, mutable=mutable[\"output_calibrator\"], rng=rng, calib=\"mutable\" in return_aux,) if (mutable is not None and mutable[\"output_calibrator\"] is not None and \"mutable\" in return_aux): outputs, aux[\"mutable\"] = outs aux[\"mutable\"] = dict(output_calibrator=aux[\"mutable\"]) else: outputs = outs if \"mutable\" in return_aux: aux[\"mutable\"] = dict(output_calibrator=None) log_joint_prob = self.prob_output_layer.log_prob(outputs, targets).sum() if len(return_aux) == 0: return log_joint_prob", "ground_truth": "        outs = self.output_calib_manager.apply(\n            params=calib_params[\"output_calibrator\"]\n            if calib_params is not None\n            else None,\n            mutable=calib_mutable[\"output_calibrator\"]\n            if calib_mutable is not None\n            else None,\n            outputs=outputs,\n            calib=\"calib_mutable\" in return_aux,\n        )"}
{"completion": "", "ground_truth": "            vat_loss = VATLoss()  # xi, and eps"}
{"completion": "import torchfrom torch import nnstate_dim = (5,)action_dim = 5mean = 0std = 1model = nn.LazyLinear(action_dim, device=\"cpu\")module = nn.Linear(*state_dim, action_dim)model_2 = nn.LazyLinear(action_dim)wrapper = nn.LayerWithWeightsNorm(10, 20)fc = nn.Sequential(nn.Linear(10, 10), nn.Dropout(0.25), nn.Linear(10, 20), nn.ReLU(), nn.Dropout(0.25), nn.Linear(20, 2), nn.Softmax(dim=1),)device = Noneaction = torch.zeros(1, 3)in_keys = [\"obs\"]out_keys = [\"act\"]distribution_class = torch.distributions.MultivariateNormalprint(model)print(module)print(model_2)print(wrapper)print(fc)print(device)print(action)print(in_keys)print(out_keys)print(distribution_class)print(state_dim)print(action_dim)print(mean)print(std)", "ground_truth": "    gsde_lazy = LazygSDEModule(sigma_init=sigma_init, learn_sigma=learn_sigma).to("}
{"completion": "", "ground_truth": "        self.register_buffer(\n            \"max_log_alpha\", torch.tensor(max_alpha, device=device).log()\n        )"}
{"completion": "def update(self, model_parameters, strict=False): \"\"\" Called by the FL client to update the model parameters Arguments: model_parameters (dict): PyTorch Module object's state_dict. \"\"\" for key in model_parameters: model_parameters[key] = param2tensor(model_parameters[key]) merged_param = merge_param_dict(self.ctx.model.state_dict().copy(), self._param_filter(model_parameters)) self.ctx.model.load_state_dict(merged_param, strict=strict)", "ground_truth": "        self.register_hook_in_train(self._hook_on_epoch_start,\n                                    \"on_epoch_start\")"}
{"completion": "prob_reg_map.train(train_data_loader=self.reg_train_data_loader, calib_data_loader=self.reg_val_data_loader, val_data_loader=self.reg_val_data_loader, map_fit_config=self.reg_fit_config_nodir_nodump, fit_config=self.reg_fit_config_dir_dump(tmp_dir), calib_config=self.reg_calib_config_nodir_nodump,)", "ground_truth": "            status = prob_reg.train(\n                train_data_loader=self.reg_train_data_loader,\n                calib_data_loader=self.reg_val_data_loader,\n                val_data_loader=self.reg_val_data_loader,\n                map_fit_config=self.reg_fit_config_nodir_nodump,\n                fit_config=self.reg_fit_config_restore(tmp_dir),\n                calib_config=self.reg_calib_config_nodir_nodump,\n            )"}
{"completion": "import pytestimport numpy as npimport timefrom ding.utils.time_helper import build_time_helper, WatchDog@pytest.mark.unittestclass TestTimeHelper: def test_naive(self):@pytest.mark.unittestclass TestWatchDog: def test_naive(self):", "ground_truth": "        watchdog = WatchDog(5)"}
{"completion": "transform = VIPTransform(model_name=\"resnet50\", in_keys=[\"pixels\"], out_keys=[\"vip_vec\"], size=244, stack_images=True, download=False, download_path=None, tensor_pixels_keys=None,)", "ground_truth": "            flatten = FlattenObservation(-2, -1, out_keys)"}
{"completion": "size = 10cur_learner_iter = 100sample_range = slice(-10, None)sample_data = buffer.sample(size, cur_learner_iter, sample_range)", "ground_truth": "            can_sample_thruput, thruput_info = self._thruput_controller.can_sample(size)"}
{"completion": "trial4.complete(vz.Measurement({'m1': .2, 'm2': -.2, 's1': 2., 's2': .0}))", "ground_truth": "    trial4.complete(vz.Measurement({'m1': .3, 'm2': -.3, 's1': 2., 's2': .0}))"}
{"completion": "model = ProbClassifier(...)train_data_loader = DataLoader(...)val_data_loader = DataLoader(...)calib_data_loader = DataLoader(...)fit_config = FitConfig(...)calib_config = CalibConfig(...)fit_kwargs = {...}status = model.train(train_data_loader, val_data_loader, calib_data_loader, fit_config, calib_config, **fit_kwargs)", "ground_truth": "        return super().train(\n            train_data_loader,\n            val_data_loader,\n            calib_data_loader,\n            fit_config,\n            calib_config,\n            **fit_kwargs,\n        )"}
{"completion": "import datetimepy_study_config = vz.StudyConfig(metric_information=[ vz.MetricInformation(name='objective', goal=vz.ObjectiveMetricGoal.MAXIMIZE) ])root = py_study_config.search_space.rootroot.add_float_param('learning_rate', 0.01, 3.0)pytrial = vz.Trial(id=1, completion_time=datetime.datetime(year=2021, month=12, day=2, hour=7, minute=31), parameters={'learning_rate': vz.ParameterValue(0.5)}, final_measurement=vz.Measurement(metrics={ 'loss': vz.Metric(value=56.8), 'objective': vz.Metric(value=77.7), }, elapsed_secs=67, steps=101,),)parameters = py_study_config._pytrial_parameters(pytrial)metrics = py_study_config._pytrial_metrics(pytrial)print(parameters)print(metrics)", "ground_truth": "    parameters = py_study_config.trial_parameters(trial_proto)"}
{"completion": "ts = OneHotDiscreteTensorSpec(n=n, device=device, dtype=dtype, use_register=use_register)ts_same = OneHotDiscreteTensorSpec(n=n, device=device, dtype=dtype, use_register=use_register)assert ts == ts_samets_other = OneHotDiscreteTensorSpec(n=n + 1, device=device, dtype=dtype, use_register=use_register)assert ts != ts_other", "ground_truth": "        ts_other = MultiOneHotDiscreteTensorSpec(nvec=nvec, device=\"cpu:0\", dtype=dtype)"}
{"completion": "import unittestimport unittest.mockwith unittest.mock.patch(\"fortuna.training.mixin.checkpoints\", return_value=unittest.mock.DEFAULT) as mc: mc.restore_checkpoint.return_value = { \"params\": {\"model\": {\"params\": 1}}, \"encoded_name\": \"encoded_name\", \"mutable\": None, \"opt_state\": {\"model\": 1}, \"calib_params\": None, \"calib_mutable\": None, } trainer = FakeTrainerWithEarlyStopping() restored_state = trainer.restore_checkpoint(tmp_dir, prefix=\"test_prefix_\") mc.restore_checkpoint.assert_called_with(ckpt_dir=tmp_dir, target=None, step=None, prefix=\"test_prefix_\", parallel=True,)", "ground_truth": "        improved = trainer.early_stopping_update(validation_metrics_step4)"}
{"completion": "import torchalpha = 0.6beta = 0.4replay_buffer = PrioritizedReplayBuffer(alpha, beta)data = {\"state\": torch.tensor([1, 2, 3]), \"action\": torch.tensor(0), \"reward\": torch.tensor(1)}replay_buffer.add(data)batch_size = 32batch = replay_buffer.sample(batch_size)", "ground_truth": "            storage = ListStorage(max_size=1_000)"}
{"completion": "envstep_count = 0 info = {} return_info = [[] for _ in range(2)] eval_monitor = VectorEvalMonitor(self._env.env_num, n_episode)", "ground_truth": "        eval_monitor = VectorEvalMonitor(self._env.env_num, n_episode)"}
{"completion": "", "ground_truth": "    unet.to(accelerator.device, dtype=weight_dtype)"}
{"completion": "", "ground_truth": "        self._learner_thread.start()"}
{"completion": "", "ground_truth": "            value = float(params.get_value(variable_name) == category)"}
{"completion": "metric = DummyMetric()results = metric.compute(predictions=[1, 2, 3, 4], references=[1, 2, 4, 3])print(results)```Output:```{'accuracy': 0.5, 'set_equality': True}", "ground_truth": "        metric.add_batch(predictions=preds, references=refs)"}
{"completion": "", "ground_truth": "                new_scheduler = scheduler_class.from_pretrained(tmpdirname)"}
{"completion": "status = prob_reg.train(train_data_loader=self.reg_train_data_loader, calib_data_loader=self.reg_val_data_loader, val_data_loader=self.reg_val_data_loader, map_fit_config=self.reg_fit_config_nodir_nodump, fit_config=self.reg_fit_config_dir_nodump(tmp_dir), calib_config=self.reg_calib_config_nodir_nodump,)", "ground_truth": "            status = prob_reg.train(\n                train_data_loader=self.reg_train_data_loader,\n                calib_data_loader=self.reg_val_data_loader,\n                val_data_loader=self.reg_val_data_loader,\n                map_fit_config=self.reg_fit_config_nodir_nodump,\n                fit_config=self.reg_fit_config_dir_nodump(tmp_dir),\n                calib_config=self.reg_calib_config_nodir_nodump,\n            )"}
{"completion": "", "ground_truth": "                self._obs_pool.update(obs)"}
{"completion": "", "ground_truth": "        metric.compute(predictions=[[\"a\"]], references=[[\"a\"]])"}
{"completion": "", "ground_truth": "        assert (ts.encode(ts.to_numpy(r)) == r).all()"}
{"completion": "", "ground_truth": "        metric = self.prepare_metric(metric)"}
{"completion": "rb = TensorDictReplayBuffer(sampler=samplers.PrioritizedSampler(5, alpha=0.7, beta=0.9), priority_key=priority_key,)td1 = TensorDict(source={ \"a\": torch.randn(3, 1), priority_key: torch.rand(3, 1) / 10, \"_idx\": torch.arange(3).view(3, 1), }, batch_size=[3],).to(device)rb.extend(td1)s = rb.sample(2)assert s.batch_size == torch.Size([ 2, ])assert (td1[s.get(\"_idx\").squeeze()].get(\"a\") == s.get(\"a\")).all()", "ground_truth": "    assert_allclose_td(td1[s.get(\"_idx\").squeeze()].select(\"a\"), s.select(\"a\"))"}
{"completion": "import evaluatefrom evaluate.utils import launch_gradio_widgetmodule = evaluate.load(\"bleu\")launch_gradio_widget(module)", "ground_truth": "module = evaluate.load(\"text_duplicates\")"}
{"completion": "module = evaluate.load(\"mauve\")sys.path = [\"/home/user/app\"] + sys.pathlaunch_gradio_widget(module)", "ground_truth": "module = evaluate.load(\"sacrebleu\")"}
{"completion": "observation_spec = unsqueeze.transform_observation_spec(observation_spec) assert observation_spec.shape[len(size)+1:] == expected_size else: observation_spec = CompositeSpec({key: BoundedTensorSpec(-1, 1, (*size, nchannels, 16, 16)) for key in keys}) observation_spec = unsqueeze.transform_observation_spec(observation_spec) for key in keys: assert observation_spec[key].shape[len(size)+1:] == expected_size", "ground_truth": "            observation_spec = unsqueeze.transform_observation_spec(observation_spec)"}
{"completion": "PosteriorState.init(params, mutable, optimizer, calib_params, calib_mutable, **kwargs)", "ground_truth": "        return cls.init(\n            FrozenDict(d[\"params\"]),\n            FrozenDict(d[\"mutable\"]) if d[\"mutable\"] is not None else None,\n            optimizer,\n            FrozenDict(d.get(\"calib_params\"))\n            if d[\"calib_params\"] is not None\n            else None,\n            FrozenDict(d.get(\"calib_mutable\"))\n            if d[\"calib_mutable\"] is not None\n            else None,\n            **kwargs,\n        )"}
{"completion": "trainer.register_op(\"pre_steps_log\", log_reward) td = TensorDict({\"reward\": torch.ones(3)}, [3]) trainer._pre_steps_log_hook(td) if _has_tqdm and pbar: assert trainer._pbar_str[logname] == 1 else: assert logname not in trainer._pbar_str assert trainer._log_dict[logname][-1] == 1", "ground_truth": "        log_reward.register(trainer)"}
{"completion": "reset_param = {i: {'stat': 'stat_test'} for i in range(env_manager.env_num)}env_manager.launch(reset_param=reset_param)action = {i: np.random.randn(4) for i in range(env_manager.env_num)}action[0] = 'timeout'env_manager.step(action)", "ground_truth": "        timestep = env_manager.step(action)"}
{"completion": "", "ground_truth": "    added2 = proto.metadata.add(key='proto', ns='t')"}
{"completion": "class NodeFullBatchTrainer(GeneralTorchTrainer): def parse_data(self, data): init_dict = dict() if isinstance(data, dict): for mode in [\"train\", \"val\", \"test\"]: init_dict[\"{}_loader\".format(mode)] = data.get(mode) init_dict[\"{}_data\".format(mode)] = None init_dict[\"num_{}_data\".format(mode)] = 1 else: raise TypeError(\"Type of data should be dict.\") return init_dict def _hook_on_batch_forward(self, ctx): batch = ctx.data_batch.to(ctx.device) pred = ctx.model(batch)[batch['{}_mask'.format(ctx.cur_split)]] label = batch.y[batch['{}_mask'.format(ctx.cur_split)]] ctx.batch_size = torch.sum(ctx.data_batch['{}_mask'.format(ctx.cur_split)]).item() ctx.loss_batch = CtxVar(ctx.criterion(pred, label), LIFECYCLE.BATCH)", "ground_truth": "        ctx.y_true = CtxVar(label, LIFECYCLE.BATCH)"}
{"completion": "", "ground_truth": "        components = self.get_dummy_components()"}
{"completion": "metric = \"squad\"results = self.evaluator.compute(model_or_pipeline=self.default_model, data=self.data, metric=metric)", "ground_truth": "        predictions = task_evaluator.predictions_processor(predictions, words, join_by)"}
{"completion": "input_tensordict = {\"done\": done, \"reward\": reward} loss = DDPGLoss(actor_network, value_network, gamma, loss_function) loss_value = loss(input_tensordict)", "ground_truth": "            td_error = td_error.to(input_tensordict.device)"}
{"completion": "from federatedscope.core.configs.cfg_fl_algo import initial_cfgconfiguration_space, fidelity_space = initial_cfg(cfg)", "ground_truth": "    cfg.cost = CN()"}
{"completion": "import pandas as pdimport requestsurl = \"https://api.github.com/repos/huggingface/evaluate/contents/src/evaluate/evaluator/text_classification.py\"response = requests.get(url)data = response.json()code = data['content']code = code.replace(\"\\\\n\", \"\\n\")print(code)", "ground_truth": "        pipe = self.prepare_pipeline(\n            model_or_pipeline=model_or_pipeline,\n            tokenizer=tokenizer,\n            feature_extractor=feature_extractor,\n            device=device,\n        )"}
{"completion": "from sspade.structured_specs import TensorDictBase, CompositeSpecdef _keys_to_empty_composite_spec(keys): composite = CompositeSpec() for key in keys: if isinstance(key, tuple): composite.set_spec(key, CompositeSpec()) else: composite.set_spec(key, TensorDictBase()) return compositekeys = [(\"key1\", \"out\"), (\"key1\", \"in\"), \"key2\", (\"key1\", \"subkey1\", \"subkey2\")]composite = _keys_to_empty_composite_spec(keys)", "ground_truth": "    assert set(composite.keys()) == set(keys)"}
{"completion": "import pytest@pytest.mark.parametrize(\"env_name\", [\"pendulum\", \"pong\"])@pytest.mark.parametrize(\"frame_skip\", [1, 4])def test_rollout(env_name, frame_skip, seed=0): env = GymEnv(env_name, frame_skip=frame_skip) torch.manual_seed(seed) np.random.seed(seed) env.set_seed(seed) env.reset() rollout1 = env.rollout(max_steps=100) torch.manual_seed(seed) np.random.seed(seed) env.set_seed(seed) env.reset() rollout2 = env.rollout(max_steps=100) assert_allclose_td(rollout1, rollout2)", "ground_truth": "            tdrollout.append(env.rollout(max_steps=50))"}
{"completion": "metric.add(inputs=input, targets=target)", "ground_truth": "        metric.add(inputs=input, targets=target)"}
{"completion": "outputs = ctx.model(input_ids=token_ids.to(ctx.device), attention_mask=attention_mask.to(ctx.device), labels=labels.to(ctx.device), pretrain_task=pretrain_task, example_indices=example_indices,)", "ground_truth": "                    ctx.loss_batch = CtxVar(outputs.loss, LIFECYCLE.BATCH)"}
{"completion": "", "ground_truth": "        ddim.to(torch_device)"}
{"completion": "", "ground_truth": "        channel_shuffle = ChannelShuffle(group_num)"}
{"completion": "from gym import Envimport torchdef make_sac_model(env: Env, device: torch.device, cfg): actor = Actor(env, device, cfg) critic = Critic(env, device, cfg) value_net = Value(env, device, cfg) return actor, critic, value_netdef make_redq_model(env: Env, device: torch.device, cfg): actor = Actor(env, device, cfg) critic_net = Critic(env, device, cfg) return actor, critic_net", "ground_truth": "            tsf_loc = actor.module[0].module[-1].module.transform(td.get(\"loc\"))"}
{"completion": "", "ground_truth": "        assert not ts.is_in(torch.tensor(np_r))"}
{"completion": "import pytestfrom torchrl.losses import TD3Lossfrom torchrl.utils import TensorDict@pytest.mark.skipif(not _has_functorch, reason=\"functorch not installed\")@pytest.mark.parametrize(\"n\", list(range(4)))@pytest.mark.parametrize(\"device\", get_available_devices())@pytest.mark.parametrize(\"delay_actor,delay_qvalue\", [(False, False), (True, True)])@pytest.mark.parametrize(\"policy_noise\", [0.1, 1.0])@pytest.mark.parametrize(\"noise_clip\", [0.1, 1.0])def test_td3_batcher(n, delay_actor, delay_qvalue, device, policy_noise, noise_clip, gamma=0.9): torch.manual_seed(42) actor = _create_mock_actor(device=device) value = _create_mock_value(device=device) td = _create_seq_mock_data_td3(device=device) loss_fn = TD3Loss(actor, value, gamma=0.9, policy_noise=policy_noise, noise_clip=noise_clip, delay_qvalue=delay_qvalue, delay_actor=delay_actor,)", "ground_truth": "        ms = MultiStep(gamma=gamma, n_steps_max=n).to(device)"}
{"completion": "trainer.training_step_end(1, state, {\"loss\": 4.2, \"logging_kwargs\": None}, batch, (), {})", "ground_truth": "            training_losses_and_metrics = trainer.training_step_end(\n                1, state, {\"loss\": 1, \"logging_kwargs\": None}, batch, (), {}\n            )"}
{"completion": "def freeze(self): \"\"\" Overview: Freeze time proxy object. This feature can be useful when adding value replay feature (in the future) \"\"\" self.__time_proxy.freeze()", "ground_truth": "        self.__time_proxy.freeze()"}
{"completion": "params = FrozenDict(dict(model=self.joint.likelihood.model_manager.model.init(self.rng, jnp.zeros((1,) + self.shape_inputs)), lik_log_var=self.joint.likelihood.model_manager.likelihood_log_variance_model.init(self.rng, jnp.zeros((1,) + self.shape_inputs)),))", "ground_truth": "                lik_log_var=self.joint.likelihood.model_manager.likelihood_log_variance_model.init(\n                    self.rng, jnp.zeros((1,) + self.shape_inputs)\n                ),"}
{"completion": "td_out = hook(td)", "ground_truth": "        hook.register(trainer)"}
{"completion": "components = self.get_dummy_components() sd_pipe = StableDiffusionPipeline(**components) sd_pipe = sd_pipe.to(torch_device) sd_pipe.set_progress_bar_config(disable=None) device = \"cpu\" inputs = self.get_dummy_inputs(device) output = sd_pipe(**inputs)", "ground_truth": "        inputs = self.get_dummy_inputs(torch_device)"}
{"completion": "def convert_trial_to_dna(vizier_converter: VizierConverter, trial: vz.Trial) -> pg.DNA: return vizier_converter.to_dna(trial)", "ground_truth": "    custom_decisions_str = trial.metadata.ns(constants.METADATA_NAMESPACE).get(\n        constants.TRIAL_METADATA_KEY_CUSTOM_TYPE_DECISIONS, None)"}
{"completion": "", "ground_truth": "                spec=CompositeSpec(**spec_dict),"}
{"completion": "results = self.evaluator.compute(data=self.data, metric=\"word_count\",)", "ground_truth": "        results = self.evaluator.compute(data=self.data)"}
{"completion": "", "ground_truth": "        return self.__call__(\n            prompt=prompt,\n            negative_prompt=negative_prompt,\n            height=height,\n            width=width,\n            num_inference_steps=num_inference_steps,\n            guidance_scale=guidance_scale,\n            num_images_per_prompt=num_images_per_prompt,\n            eta=eta,\n            generator=generator,\n            latents=latents,\n            max_embeddings_multiples=max_embeddings_multiples,\n            output_type=output_type,\n            return_dict=return_dict,\n            callback=callback,\n            callback_steps=callback_steps,\n            **kwargs,\n        )"}
{"completion": "_evaluator = evaluator(\"question-answering\")data = load_dataset(\"squad_v2\", split=\"validation[:2]\")results = _evaluator.compute(model_or_pipeline=\"mrm8488/bert-tiny-finetuned-squadv2\", data=data, metric=\"squad_v2\", squad_v2_format=True,)", "ground_truth": "        predictions = self.predictions_processor(predictions, squad_v2_format=squad_v2_format, ids=data[id_column])"}
{"completion": "import requestsurl = \"https://api.example.com/evaluate\"data = { \"task\": \"text-generation\", \"default_metric_name\": None, \"predictions_prefix\": \"generated\", \"data\": { \"dataset\": \"rotten_tomatoes\", \"subset\": \"train\", \"split\": \"test\" }, \"input_column\": \"text\", \"label_column\": \"label\"}headers = { \"Content-Type\": \"application/json\"}response = requests.post(url, json=data, headers=headers)result = response.json()print(result)", "ground_truth": "        return {\"references\": data[label_column]}, DatasetColumn(data, input_column)"}
{"completion": "", "ground_truth": "        improved = trainer.early_stopping_update(validation_metrics_step2)"}
{"completion": "import requestsurl = \"https://api.torchrl.com/collectors/syncdatacollector\"headers = { \"Content-Type\": \"application/json\"}data = { \"create_env_fn\": \"...\", \"policy\": \"...\", \"total_frames\": 2000, \"max_frames_per_traj\": 50, \"frames_per_batch\": 200, \"init_random_frames\": -1, \"reset_at_each_iter\": False, \"device\": \"cpu\", \"passing_device\": \"cpu\", \"seed\": 12345, \"pin_memory\": False, \"exploration_mode\": \"random\", \"init_with_lag\": True, \"return_same_td\": False, \"reset_when_done\": True}response = requests.post(url, headers=headers, json=data)response_json = response.json()print(response_json)", "ground_truth": "                    policy_device = torch.device(\"cpu\")"}
{"completion": "trainer = FakeTrainerWithCheckpointing(123, save_checkpoint_dir=\"approximations\", save_every_n_steps=None, save_top_k=1, filepath_checkpoint_to_be_restored=None, use_save_checkpoint_dir_as_is=False,)state = FakeTrainState()tmp_dir = tempfile.TemporaryDirectory()trainer = FakeTrainerWithCheckpointing()mc = unittest.mock.patch(\"fortuna.training.mixin.checkpoints\", return_value=mock.DEFAULT)trainer.save_checkpoint(state, None)mc.save_checkpoint.assert_not_called()trainer.save_checkpoint(state, tmp_dir, keep=3, prefix=\"test_prefix_\", force_save=True)mc.save_checkpoint.assert_called_with(ckpt_dir=tmp_dir, target=state, step=state.step, prefix=\"test_prefix_\", keep=3, overwrite=True,)", "ground_truth": "            training_losses_and_metrics = trainer.training_step_end(\n                1,\n                state,\n                {\"loss\": 1, \"logging_kwargs\": {\"metric1:\": 0.1, \"metrics2\": 0.2}},\n                batch,\n                (),\n                {},\n            )"}
{"completion": "setup_league[0].mutate() assert setup_league[0].score == 0 assert setup_league[0].payoff.get_num_matches() == 0", "ground_truth": "            assert setup_league[0].mutate({}) is None"}
{"completion": "env = GymEnv(env_name=\"Pendulum-v0\", frame_skip=4)", "ground_truth": "    @implement_for(\"gym\", \"0.24.0\", None)"}
{"completion": "rb_trainer = ReplayBufferTrainer(replay_buffer=replay_buffer, batch_size=N)trainer.register_op(\"batch_process\", rb_trainer.extend)trainer.register_op(\"process_optim_batch\", rb_trainer.sample)trainer.register_op(\"post_loss\", rb_trainer.update_priority)", "ground_truth": "        trainer.register_op(\"post_optim\", target_net_updater.step)"}
{"completion": "spec = CompositeSpec(spec1=spec1, spec2=spec2, spec3=spec3, spec4=spec4, spec5=spec5, spec6=spec6, spec7=spec7)spec2 = spec.expand(shape2_real)", "ground_truth": "        spec8 = UnboundedDiscreteTensorSpec(\n            shape=(*batch_size, 9),\n            device=\"cpu\",\n            dtype=torch.long,\n        )"}
{"completion": "def my_proposed_api_call(device, imagination_horizon, discount_loss): test_obj = TestDreamer() test_obj.device = device test_obj.seed = 0 test_obj._create_world_model_data(2, 3, 10, 5) test_obj._create_world_model_model(10, 5) test_obj._create_actor_model(10, 5) test_obj._create_value_model(10, 5) test_obj.test_dreamer_world_model(device, 0, 0, 0, \"l2\", \"l2\", False, -1000) test_obj.test_dreamer_env(device, imagination_horizon, discount_loss) my_proposed_api_call(\"cpu\", 3, True)", "ground_truth": "        mb_env = self._create_mb_env(10, 5).to(device)"}
{"completion": "env = MockBatchedLockedEnv(batch_size=5)observation_spec = env.observation_specreward_spec = env.reward_specspec = CompositeSpec(observation=observation_spec, reward=reward_spec)observation_sub_spec = spec['observation']reward_sub_spec = spec['reward']", "ground_truth": "            input_spec = CompositeSpec(\n                action=action_spec,\n                observation=UnboundedContinuousTensorSpec(\n                    (\n                        *batch_size,\n                        1,\n                    )\n                ),\n                shape=batch_size,\n            )"}
{"completion": "", "ground_truth": "                scheduler.save_config(tmpdirname)"}
{"completion": "model = self._ctx.model if hasattr(model, 'parameters'): return model.parameters() else: return model", "ground_truth": "        return self._param_filter(\n            self.ctx.model.state_dict() if self.cfg.federate.\n            share_local_model else self.ctx.model.cpu().state_dict())"}
{"completion": "def _update_connection_learner(self, cur_learners) -> None: conn_learners = list(self._learner_connection.keys()) new_c = set(cur_learners) - set(conn_learners) del_c = set(conn_learners) - (set(cur_learners) | self._failed_learner_conn) self._failed_learner_conn = self._failed_learner_conn & set(cur_learners) for learner_id in new_c: learner_host, learner_port = learner_id.split(':') self._new_connection_learner(learner_id, learner_host, int(learner_port)) for learner_id in del_c: if learner_id in conn_learners: if self._connection_learner[learner_id].is_connected: conn = self._connection_learner.pop(learner_id) conn.disconnect() assert not conn.is_connected else: self._connection_learner.pop(learner_id)", "ground_truth": "                        self._resource_manager.delete(\"collector\", collector_id)"}
{"completion": "model = UNet2DConditionModel.from_pretrained('model_name')tokenizer = CLIPTokenizer.from_pretrained('tokenizer_name')image = PIL.Image.open('image.jpg')preprocessed_image = preprocess(image)input_ids = tokenizer.encode('text', return_tensors='pt')output = model(preprocessed_image, input_ids=input_ids)", "ground_truth": "                return torch.device(module._hf_hook.execution_device)"}
{"completion": "", "ground_truth": "      root.add_discrete_param('batch_size', [8, 16, 32], index=index)"}
{"completion": "import evaluatefrom evaluate.utils import launch_gradio_widgetmodule = evaluate.load(\"bleu\")launch_gradio_widget(module)", "ground_truth": "module = evaluate.load(\"word_count\")"}
{"completion": "", "ground_truth": "            prob_class.train(\n                train_data_loader=self.class_train_data_loader,\n                calib_data_loader=self.class_val_data_loader,\n                val_data_loader=self.class_val_data_loader,\n                fit_config=self.class_fit_config_restore(tmp_dir),\n                calib_config=self.class_calib_config_nodir_nodump,\n            )"}
{"completion": "", "ground_truth": "    actor = ProbabilisticActor(\n        module=actor_module,\n        in_keys=[\"param\"],\n        spec=CompositeSpec(action=env_specs[\"action_spec\"]),\n        safe=True,\n        distribution_class=TanhDelta,\n        distribution_kwargs={\n            \"min\": env_specs[\"action_spec\"].space.minimum,\n            \"max\": env_specs[\"action_spec\"].space.maximum,\n        },\n    )"}
{"completion": "", "ground_truth": "        ret = pd.sample(viz=True)"}
{"completion": "bleu = Bleu()bleu_score = bleu.compute_score(gts, res)", "ground_truth": "        score, scores = bleu_scorer.compute_score(option='closest', verbose=0)"}
{"completion": "prob_class = ProbClassifier(model=self.model, prior=IsotropicGaussianPrior(), output_calibrator=ClassificationTemperatureScaler())state = prob_class.joint.init(self.input_shape)assert \"model\" in state.paramsassert \"params\" in state.params[\"model\"]assert hasattr(state, \"mutable\")", "ground_truth": "        calib_prob_class = ProbClassifier(\n            model=self.model,\n            prior=IsotropicGaussianPrior(),\n            output_calibrator=ClassificationTemperatureScaler(),\n        )"}
{"completion": "with LockContext(): time_proxy.freeze()with LockContext(): time_proxy.unfreeze()", "ground_truth": "        self.__lock = LockContext(lock_type)"}
{"completion": "def load_local_metric(evaluation_module_name, *args, **kwargs): if evaluation_module_type == \"metric\": module_path = os.path.join(\"metrics\", evaluation_module_name) elif evaluation_module_type == \"comparison\": module_path = os.path.join(\"comparisons\", evaluation_module_name) elif evaluation_module_type == \"measurement\": module_path = os.path.join(\"measurements\", evaluation_module_name) else: raise ValueError(f\"Invalid evaluation module type: {evaluation_module_type}\") return load(module_path, *args, **kwargs) with patch(\"evaluate.load\", load_local_metric): yield", "ground_truth": "            return load(os.path.join(evaluation_module_type + \"s\", evaluation_module_name), *args, **kwargs)"}
{"completion": "POLICY_REGISTRY` variable from the `ding/policy/command_mode_policy_instance.py` file. However, the provided code does not include the definition of the `Registry` class, so I'm unable to see the content of the `POLICY_REGISTRY` variable.Could you please provide the definition of the `Registry` class or any relevant code that would allow me to access the `POLICY_REGISTRY", "ground_truth": "COMM_LEARNER_REGISTRY = Registry()"}
{"completion": "import requestsurl = \"https://api.example.com/replay_buffer/clear\"response = requests.get(url)data = response.json()", "ground_truth": "        self.close()"}
{"completion": "import torchfrom torch.distributions import Distributiondef get_param_count(dist: Distribution) -> int: param_count = 0 for param in dist.parameters(): param_count += param.numel() return param_countdist = torch.distributions.Normal(0, 1)param_count = get_param_count(dist)print(f\"Number of parameters in Normal distribution: {param_count}\")dist = torch.distributions.Categorical(torch.tensor([0.2, 0.5, 0.3]))param_count = get_param_count(dist)print(f\"Number of parameters in Categorical distribution: {param_count}\")", "ground_truth": "        ts_other = CompositeSpec(ts1=bounded, ts2=nd, ts3=bounded_other)"}
{"completion": "assert self.reg_lik.variance(params, self.reg_inputs_arr).shape == (self.n_inputs, self.output_dim,) assert self.reg_lik.variance(params, self.reg_inputs_gen_fun).shape == (self.batch_size * self.n_batches, self.output_dim,)", "ground_truth": "            self.reg_lik.mode(params, self.reg_inputs_arr),"}
{"completion": "from transformers import TokenClassificationEvaluatorevaluator = TokenClassificationEvaluator()results = evaluator.compute(model_or_pipeline=self.pipe, data=self.data, metric=\"seqeval\", tokenizer=tokenizer,)", "ground_truth": "        predictions = task_evaluator.predictions_processor(predictions, words, join_by)"}
{"completion": "runner = InRamPolicySupporter(my_study_config)trials = runner.GetTrials()", "ground_truth": "        self._trials[tid - 1].metadata.abs_ns(ns).update(metadatum.abs_ns(ns))"}
{"completion": "import requestsresponse = requests.post('https://api.example.com/calib-config', json={ \"optimizer\": { \"param1\": \"value1\", \"param2\": \"value2\" }, \"checkpointer\": { \"param1\": \"value1\", \"param2\": \"value2\" }, \"monitor\": { \"param1\": \"value1\", \"param2\": \"value2\" }})if response.status_code == 200: calib_config = response.json() print(calib_config)else: print(\"Error:\", response.status_code)", "ground_truth": "        processor: CalibProcessor = CalibProcessor(),"}
{"completion": "from torchrl.data import TensorDictPrioritizedReplayBufferrb = TensorDictPrioritizedReplayBuffer(alpha=0.7, beta=1.1, priority_key=\"td_error\")rb.extend(TensorDict({\"a\": torch.randn(2, 3)}, batch_size=[2]))tensordict_sample = rb.sample(2).contiguous()tensordict_sample[\"td_error\"] = torch.rand(2)rb.update_tensordict_priority(tensordict_sample)", "ground_truth": "    sampled_td = rb.sample(3, include_info=True)"}
{"completion": "", "ground_truth": "            model_manager_state = self.joint.likelihood.model_manager.init(input_shape)"}
{"completion": "self.worker_addresses, \"feat_engr_public_key\") self.feat_engr_public_key = self.comm_manager.recv() logger.info('Sending feat_engr_public_keys to clients.') self.broadcast_feat_engr_public_key() self.encoder = secure_builder(self._cfg).generate_encoder() self.collect_bins() self.calculate_woe() self.broadcast_woe() logger.info('woe_filter initialized successfully.')", "ground_truth": "            Message(msg_type='binning',\n                    sender=self.ID,\n                    receiver=list(self.comm_manager.get_neighbors().keys()),\n                    state=self.state,\n                    content=self._cfg.feat_engr.selec_woe_binning))"}
{"completion": "coordinator.start()", "ground_truth": "        self._assign_collector_thread.start()"}
{"completion": "from transformers import Datasetdata = Dataset.from_dict({\"label\": [1, 0], \"text\": [\"great movie\", \"horrible movie\"]})label_mapping = {\"NEGATIVE\": 0.0, \"POSITIVE\": 1.0}evaluator = evaluator(\"text-classification\")results = evaluator.compute(model_or_pipeline=self.default_model, data=data, metric=\"accuracy\", input_column=\"text\", label_column=\"label\", label_mapping=label_mapping,)print(results)", "ground_truth": "        self.evaluator.prepare_data(data=data, input_column=\"text\", label_column=\"label\", second_input_column=None)"}
{"completion": "", "ground_truth": "        ensemble_outputs = self.sample_calibrated_outputs(\n            inputs_loader=inputs_loader,\n            n_output_samples=n_posterior_samples,\n            rng=rng,\n            distribute=distribute,\n        )"}
{"completion": "", "ground_truth": "            tsf_loc = actor.module[0].module[-1].module.transform(td.get(\"loc\"))"}
{"completion": "cn = CN()cn.merge_from_file(cfg_filename=\"config.yaml\")cn.merge_from_other_cfg(cfg_other=other_cfg)cn.merge_from_list(cfg_list=[('FOO.BAR', 0.5)])", "ground_truth": "        super().merge_from_list(cfg_list)"}
{"completion": "exptr.evaluate([t_shifted])shifted_exptr.evaluate([t])", "ground_truth": "    metric_name = exptr.problem_statement().metric_information.item().name"}
{"completion": "class BatchedEnv(Env): def __init__(self, env: Env, batch_size: int): self.env = env self.batch_size = batch_size def reset(self): return [self.env.reset() for _ in range(self.batch_size)] def step(self, action): return zip(*[self.env.step(a) for a in action])env = BatchedEnv(env, batch_size=4)observations = env.reset()actions = [env.action_space.sample() for _ in range(env.batch_size)]next_observations, rewards, dones, infos = env.step(actions)", "ground_truth": "            self.input_spec = self.input_spec.to(device)"}
{"completion": "", "ground_truth": "    for kk in range(self._num_repeats):"}
{"completion": "", "ground_truth": "        components = self.get_dummy_components()"}
{"completion": "", "ground_truth": "        self.assertTrue(trainer.should_perform_validation({}, 10))"}
{"completion": "fit_config=self.class_fit_config_dir_dump(tmp_dir),", "ground_truth": "            prob_class = ProbClassifier(\n                model=MyModel(self.class_output_dim),\n                posterior_approximator=SWAGPosteriorApproximator(rank=2),\n                output_calibrator=ClassificationTemperatureScaler(),\n            )"}
{"completion": "response = self.__request(\"POST\", \"/connect\", {\"master\": {\"address\": self.__my_address},\"data\": (self._before_connect() or {})})self.__is_connected = Truereturn self._after_connect(*get_values_from_response(response))", "ground_truth": "            _task = Task(\n                http_engine=self.__http_engine,\n                data=data,\n                task_id=_uuid,\n                before_task_start=self._before_new_task,\n                after_task_start=self._after_new_task,\n                error_task_start=self._error_new_task,\n            )"}
{"completion": "Message(msg_type='feat_dim', sender=self.ID, receiver=[self.server_id], content=(split_data['x'].shape[1], filtered_col))", "ground_truth": "            Message(msg_type='feat_dim',\n                    sender=self.ID,\n                    receiver=[self.server_id],\n                    content=(split_data['x'].shape[1], filtered_col)))"}
{"completion": "train_status = prob_reg.train(train_data_loader=self.reg_train_data_loader, calib_data_loader=self.reg_val_data_loader, val_data_loader=self.reg_val_data_loader, fit_config=self.reg_fit_config_nodir_nodump, calib_config=self.reg_calib_config_nodir_nodump,)", "ground_truth": "            status = prob_reg.train(\n                train_data_loader=self.reg_train_data_loader,\n                calib_data_loader=self.reg_val_data_loader,\n                val_data_loader=self.reg_val_data_loader,\n                fit_config=self.reg_fit_config_dir_dump(tmp_dir),\n                calib_config=self.reg_calib_config_nodir_nodump,\n            )"}
{"completion": "def estimate(self, state: torch.Tensor, action: torch.Tensor) -> torch.Tensor: \"\"\" Overview: Estimate reward based on the state and action. Arguments: - state (:obj:`torch.Tensor`): Current state - action (:obj:`torch.Tensor`): Action performed in the current state Returns: - reward (:obj:`torch.Tensor`): Estimated reward \"\"\" state_action = torch.cat([state, action.float()], dim=-1) reward = self.reward_model(state_action) return reward", "ground_truth": "        self.load_expert_data()"}
{"completion": "", "ground_truth": "        ctx.y_prob = CtxVar(pred, LIFECYCLE.BATCH)"}
{"completion": "td = trial_regression_utils.TrialData(trial_id=trial_id, learning_rate=learning_rate, final_objective=final_objective, steps=steps, objective_values=objective_values)td.from_trial(trial, learning_rate_param_name, metric_name, converter)", "ground_truth": "  for i in range(len(steps)):"}
{"completion": "from federatedscope.core.configs.config import CNfrom federatedscope.core.configs.yacs_config import Argumentfrom federatedscope.register import register_configdef extend_fl_algo_cfg(cfg): cfg.fedopt = CN() cfg.fedopt.use = False cfg.fedopt.optimizer = CN(new_allowed=True)", "ground_truth": "    cfg.fedopt.optimizer.type = Argument(\n        'SGD', description=\"optimizer type for FedOPT\")"}
{"completion": "param = { 'create_problem_fn': create_continuous_problem, 'n_features': 10, 'score_fn': sphere}response = requests.get('http://localhost:5000/vizier/converges', params=param)print(response.json())", "ground_truth": "    converter = converters.TrialToArrayConverter.from_study_config(problem)"}
{"completion": "metric.compute(predictions=preds, references=refs)", "ground_truth": "        self.assertDictEqual(expected_results, metric.compute(predictions=preds, references=refs))"}
{"completion": "", "ground_truth": "                new_scheduler = scheduler_class.from_pretrained(tmpdirname)"}
{"completion": "Message(msg_type='ask_instance_statistics', sender=self.ID, receiver=list(self.comm_manager.get_neighbors().keys()), state=self.state)", "ground_truth": "            Message(msg_type='ask_for_instance_sum',\n                    sender=self.ID,\n                    receiver=list(self.comm_manager.neighbors.keys()),\n                    timestamp=self.cur_timestamp))"}
{"completion": "", "ground_truth": "        self.comm_manager.send(\n            Message(msg_type=msg_type,\n                    sender=self.ID,\n                    receiver=list(self.comm_manager.neighbors.keys()),\n                    state=self.state,\n                    timestamp=self.cur_timestamp,\n                    content=model_para))"}
{"completion": "import torchfrom torchrl.specs import UnboundedContinuousTensorSpecdef test_unbounded(dtype): torch.manual_seed(0) np.random.seed(0) ts = UnboundedContinuousTensorSpec(dtype=dtype) if dtype is None: dtype = torch.get_default_dtype() for _ in range(100): r = ts.rand() ts.to_numpy(r) assert ts.is_in(r) assert r.dtype is dtype assert (ts.encode(ts.to_numpy(r)) == r).all()", "ground_truth": "            BoundedTensorSpec(0, 1, torch.Size((1,)), device, dtype), ts"}
{"completion": "import requestsimport jsonurl = \"https://api.github.com/repos/ustunb/ding/contents/ding/policy\"headers = { \"Accept\": \"application/vnd.github.v3+json\"}response = requests.get(url, headers=headers)response_json = response.json()files = []for file in response_json: files.append(file[\"name\"])files", "ground_truth": "            return super()._monitor_vars_learn() + ["}
{"completion": "tensordict_module = SafeModule(net, in_keys=[\"input\"], out_keys=[\"output\"], spec=BoundedTensorSpec(torch.zeros(3), torch.ones(3), dtype=torch.float32, device=\"cpu\"))", "ground_truth": "            spec = BoundedTensorSpec(-0.1, 0.1, 4)"}
{"completion": "def convert_vd_unet_checkpoint(checkpoint, config, unet_key, extract_ema=False): \"\"\" Takes a state dict and a config, and returns a converted checkpoint. \"\"\" unet_state_dict = {} keys = list(checkpoint.keys()) for key in keys: if unet_key in key: if extract_ema: unet_state_dict[key.replace(unet_key, \"ema\")] = checkpoint[key] else: unet_state_dict[key.replace(unet_key, \"regular\")] = checkpoint[key] conv_attn_to_linear(unet_state_dict) new_checkpoint = checkpoint.copy() new_checkpoint.update(unet_state_dict) return new_checkpoint```", "ground_truth": "    config = LDMBertConfig(\n        d_model=bert_params.n_embed,\n        encoder_layers=bert_params.n_layer,\n        encoder_ffn_dim=bert_params.n_embed * 4,\n    )"}
{"completion": "DualTransformer2DModel(attn_num_head_channels, out_channels // attn_num_head_channels, in_channels=out_channels, num_layers=1, cross_attention_dim=cross_attention_dim, norm_num_groups=resnet_groups, use_linear_projection=use_linear_projection,))``````python self.attentions = nn.ModuleList(attentions) self.resnets = nn.ModuleList(resnets)``````python def forward(self, hidden_states, res_hidden_states_tuple, temb=None, encoder_hidden_states=None, attention_mask=None): for resnet, attn in zip(self.resnets, self.attentions): res_hidden_states = res_hidden_states_tuple[-1] res_hidden_states_tuple = res_hidden_states_tuple[:-1] hidden_states = torch.cat([hidden_states, res_hidden_states], dim=1) hidden_states = resnet(hidden_states, temb) hidden_states = attn(hidden_states, encoder_hidden_states=encoder_hidden_states, attention_mask=attention_mask,) return hidden_states```", "ground_truth": "                    DualTransformer2DModel(\n                        attn_num_head_channels,\n                        out_channels // attn_num_head_channels,\n                        in_channels=out_channels,\n                        num_layers=1,\n                        cross_attention_dim=cross_attention_dim,\n                        norm_num_groups=resnet_groups,\n                    )"}
{"completion": "from torchrl.envs import ParallelEnv, make_make_envdef env_fn(seed): env = ParallelEnv(num_workers=num_env, create_env_fn=make_make_env(env_name), create_env_kwargs=[{\"seed\": s} for s in generate_seeds(seed, num_env)],) return envpolicy = make_policy(env_name)torch.manual_seed(0)np.random.seed(0)env = env_fn(seed)rollout1a = env.rollout(policy=policy, max_steps=20, auto_reset=True)env.set_seed(seed)rollout1b = env.rollout(policy=policy, max_steps=20, auto_reset=True)rollout2 = env.rollout(policy=policy, max_steps=20, auto_reset=True)", "ground_truth": "        tensordict = env.rollout(max_steps=n_rollout_samples)"}
{"completion": "import torch from diffusers import UNet1DModel batch_size = 4 num_features = 14 seq_len = 16 noise = torch.randn((batch_size, num_features, seq_len)) time_step = torch.tensor([10] * batch_size) model = UNet1DModel.from_pretrained(\"model_path\") model.eval() output = model(sample=noise, timestep=time_step) print(output.shape)", "ground_truth": "        super().test_output()"}
{"completion": "", "ground_truth": "        ts.encode(lb + torch.rand(10) * (ub - lb))"}
{"completion": "", "ground_truth": "        return super().forward(tensordict)"}
{"completion": "transform = VIPTransform(model_name=\"resnet50\", in_keys=[\"pixels\"], out_keys=[\"vip_vec\"], size=244, stack_images=True, download=False, download_path=None, tensor_pixels_keys=None,)", "ground_truth": "            flatten = FlattenObservation(-2, -1, out_keys)"}
{"completion": "from evaluate import evaluatorfrom datasets import load_datasetds = load_dataset(\"rotten_tomatoes\", split=\"train\")evaluator(\"text-classification\").prepare_data(ds, input_column=\"text\", second_input_column=None, label_column=\"label\")", "ground_truth": "        self.check_required_columns(data, {\"input_column\": input_column})"}
{"completion": "import transformersdata = transformers.load_dataset(\"rotten_tomatoes\", split=\"train\")evaluator = transformers.evaluator(\"text-classification\")metric_inputs, pipe_inputs = evaluator.prepare_data(data, input_column=\"text\", label_column=\"label\")pipe = evaluator.prepare_pipeline(model_or_pipeline=\"distilbert-base-uncased\")predictions, perf_results = evaluator.call_pipeline(pipe, **pipe_inputs)metric_inputs.update(predictions)metric = evaluator.prepare_metric(\"accuracy\")metric_results = evaluator.compute_metric(metric=metric, metric_inputs=metric_inputs, strategy=\"simple\", confidence_level=0.95, n_resamples=9999, random_state=None,)result = {}result.update(metric_results)result.update(perf_results)return result", "ground_truth": "        result = metric.compute(**metric_inputs, **self.METRIC_KWARGS)"}
{"completion": "from torch import nnfrom torch_ac.algos import PPOimport torch_acactor = TestPPO()._create_mock_actor()value = TestPPO()._create_mock_value()algorithm = PPO(actor_critic=actor, value_function=value, num_epochs=4, num_mini_batch=32, batch_size=4096, entropy_coef=0.01, lr=0.1, max_grad_norm=0.5, clip_param=0.2, coef_value_loss=1.0, coef_policy_loss=1.0, device=torch.device('cpu'))", "ground_truth": "        actor = ProbabilisticActor(\n            module=module,\n            distribution_class=TanhNormal,\n            in_keys=[\"loc\", \"scale\"],\n            spec=action_spec,\n        )"}
{"completion": "trial_proto.parameters.add(parameter_id='synchronus[1]', value=struct_pb2.Value(string_value='false'))trial_proto.parameters.add(parameter_id='batch_size[0]', value=struct_pb2.Value(number_value=32.0))trial_proto.parameters.add(parameter_id='batch_size[1]', value=struct_pb2.Value(number_value=8.0))trial_proto.parameters.add(parameter_id='floating_point_param', value=struct_pb2.Value(number_value=16.0))parameters = py_study_config.trial_parameters(trial_proto)", "ground_truth": "    dnn_trial.parameters.add(\n        parameter_id='unts[1]', value=struct_pb2.Value(number_value=79))"}
{"completion": "api_call = \"GET /config/fl_algo\"", "ground_truth": "    cfg.fedprox = CN()"}
{"completion": "api_call_completion = ProbRegressor(model=nn.Module, likelihood_log_variance_model=nn.Module, prior=IsotropicGaussianPrior(), posterior_approximator=SWAGPosteriorApproximator(), output_calibrator=Optional[nn.Module], seed=int)", "ground_truth": "        outputs = self.model_manager.apply(\n            params=s.params, inputs=np.zeros((1,) + input_shape), mutable=s.mutable\n        )"}
{"completion": "server.trace('global')server.trace('refine')server.trace('entropy')server.trace('mle')", "ground_truth": "                Message(msg_type='finish',\n                        sender=self.ID,\n                        receiver=list(self.comm_manager.neighbors.keys()),\n                        state=self.state,\n                        content=model_para))"}
{"completion": "", "ground_truth": "        spec2 = spec.expand(shape2_real)"}
{"completion": "", "ground_truth": "        policy = FakePolicy(cfg.policy)"}
{"completion": "import torchfrom diffusers import UNet1DModelmodel = UNet1DModel.from_pretrained(\"bglick13/hopper-medium-v2-value-function-hor32\", subfolder=\"unet\")torch.manual_seed(0)if torch.cuda.is_available(): torch.cuda.manual_seed_all(0)num_features = model.in_channelsseq_len = 16noise = torch.randn((1, seq_len, num_features)).permute(0, 2, 1)time_step = torch.full((num_features,), 0)with torch.no_grad(): output = model(noise, time_step).sample.permute(0, 2, 1)", "ground_truth": "        super().test_outputs_equivalence()"}
{"completion": "world_model = _dreamer_make_world_model(obs_encoder, obs_decoder, rssm_prior, rssm_posterior, reward_module)actor_simulator, actor_realworld = _dreamer_make_actors(obs_encoder, rssm_prior, rssm_posterior, mlp_num_units, action_key, proof_environment)value_model = _dreamer_make_value_model(mlp_num_units, value_key)model_based_env = _dreamer_make_mbenv(reward_module, rssm_prior, obs_decoder, proof_environment, use_decoder_in_env, state_dim, rssm_hidden_dim)return world_model, model_based_env, actor_simulator, value_model, actor_realworld", "ground_truth": "    model_based_env = DreamerEnv(\n        world_model=WorldModelWrapper(\n            transition_model,\n            reward_model,\n        ),\n        prior_shape=torch.Size([state_dim]),\n        belief_shape=torch.Size([rssm_hidden_dim]),\n        obs_decoder=mb_env_obs_decoder,\n    )"}
{"completion": "", "ground_truth": "        if self.check_buffer(self.state, min_received_num, check_eval_result):"}
{"completion": "def eval(self, save_ckpt_fn: Callable = None, train_iter: int = -1, envstep: int = -1, n_episode: Optional[int] = None) -> Tuple[bool, float]: ''' Overview: Evaluate policy and store the best policy based on whether it reaches the highest historical reward. Arguments: - save_ckpt_fn (:obj:`Callable`): Saving ckpt function, which will be triggered by getting the best reward. - train_iter (:obj:`int`): Current training iteration. - envstep (:obj:`int`): Current env interaction step. - n_episode (:obj:`int`): Number of evaluation episodes. Returns: - stop_flag (:obj:`bool`): Whether this training program can be ended. - eval_reward (:obj:`float`): Current eval_reward. '''", "ground_truth": "                        eval_monitor.update_reward(env_id, reward)"}
{"completion": "", "ground_truth": "            self._q_network = DRQN(obs_shape, action_shape, hidden_size_list, lstm_type=lstm_type, dueling=dueling)"}
{"completion": "def transform_observation_spec(self, observation_spec: CompositeSpec) -> CompositeSpec: if not isinstance(observation_spec, CompositeSpec): raise ValueError(f\"observation_spec was expected to be of type CompositeSpec. Got {type(observation_spec)} instead.\") [...] return updated_observation_spec``````pythondef normalize_reward(self, tensordict: TensorDictBase) -> TensorDictBase: tensordict = tensordict.to_tensordict() reward = tensordict.get(\"reward\") if reward.device is not None: reward = reward - self._reward_stats[\"mean\"].to(reward.device) reward = reward / self._reward_stats[\"std\"].to(reward.device) else: reward = reward - self._reward_stats[\"mean\"] reward = reward / self._reward_stats[\"std\"] tensordict.set(\"reward\", reward * self.scale) self._normalize_has_been_called = True return tensordict", "ground_truth": "        tensordict.set(\"reward\", reward.to(torch.get_default_dtype()))"}
