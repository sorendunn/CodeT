{"completion": "GymEnv(env_name, frame_skip=frame_skip, device=device), FrameStack(4)))", "ground_truth": "                    GymEnv(env_name, frame_skip=frame_skip, device=device),"}
{"completion": "", "ground_truth": "        module_names, _, _ = self.extract_init_dict(dict(self.config))"}
{"completion": "_meta_data", "ground_truth": "            else [meta_data.to(device) for meta_data in self.meta_data]"}
{"completion": "class TestLimitedSpaceContainer: def test_container(self): container = LimitedSpaceContainer(3) assert len(container) == 0 container.add(1) container.add(2) container.add(3) assert len(container) == 3 assert container.is_full() == True container.add(4) assert len(container) == 3 assert container.is_full() == True assert 1 not in container container.remove(2) assert 2 not in container assert len(container) == 2 container.clear() assert len(container) == 0 container.add(5) assert len(container) == 1 assert container.is_full() == False assert 5 in container def test_iterator(self): container = LimitedSpaceContainer(3) container.add(1) container.add(2) container.add(3) assert list(container) == [1, 2, 3] def test_contains(self): container = LimitedSpaceContainer(3) container.add(1) container.add(2) container.add(3) assert 1", "ground_truth": "        container = LimitedSpaceContainer(0, 5)"}
{"completion": "checkpointer=CalibCheckpointer(save_checkpoint_dir=directory, dump_state=True),) def test_regression_approximations(self): \"\"\"Test approximation methods for regression models.\"\"\" gaussian_approx = MAPPosteriorApproximator(self.reg_train_data_loader) reg_mlp_model = MLP(input_shape=self.reg_input_shape, output_dim=self.reg_output_dim, hidden_units=(10,),) regression_model = ProbRegressor(model=reg_mlp_model, posterior_approximator=gaussian_approx, prior=IsotropicGaussianPrior(),) approximate_model = RegressionTemperatureScaler(regression_model) self.reg_model_fit(approximate_model, self.reg_fit_config_nodir_nodump, self.reg_train_data_loader, self.reg_val_data_loader,) self.reg_model_eval(approximate_model, self.reg_val_data_loader, self.reg_fit_config_nodir_nodump.monitor.metrics, \"Approximation\", self.reg_fit_config_nodir_nodump.device,) def test_classification_approximations(self): \"\"\"Test approximation", "ground_truth": "            checkpointer=CalibCheckpointer(\n                save_checkpoint_dir=directory, dump_state=True\n            ),"}
{"completion": "prob_class.train(train_data_loader=self.class_train_data_loader, calib_data_loader=self.class_val_data_loader, val_data_loader=self.class_val_data_loader, fit_config=self.class_fit_config_restore(tmp_dir), calib_config=self.class_calib_config_nodir_nodump,) prob_class.load_state(checkpoint_path=tmp_dir) prob_class.save_state(checkpoint_path=tmp_dir)", "ground_truth": "            status = prob_class.train(\n                train_data_loader=self.class_train_data_loader,\n                calib_data_loader=self.class_val_data_loader,\n                val_data_loader=self.class_val_data_loader,\n                fit_config=self.class_fit_config_restore(tmp_dir),\n                calib_config=self.class_calib_config_nodir_nodump,\n            )"}
{"completion": "improved = trainer.early_stopping_update(validation_metrics_step3) self.assertTrue(improved) improved = trainer.early_stopping_update(validation_metrics_step4) self.assertFalse(improved) improved = trainer.early_stopping_update(validation_metrics_step5) self.assertFalse(improved) def test_early_stopping_update_ok_max(self): validation_metrics_step1 = {\"metric1\": 1, \"metric2\": 2} validation_metrics_step2 = {\"metric1\": 1.2, \"metric2\": 2} validation_metrics_step3 = {\"metric1\": 1.4, \"metric2\": 2} validation_metrics_step4 = {\"metric1\": 1.1, \"metric2\": 2} validation_metrics_step5 = {\"metric1\": 1.3, \"metric2\": 2} trainer = FakeTrainerWithEarlyStopping(early_stopping_monitor=\"metric1\", early_stopping_min_delta=0, early_stopping_patience=1, early_stopping_mode=\"max\",) improved = trainer.early_stopping_update(validation_metrics_step1) self.assertTrue(improved) improved =", "ground_truth": "        improved = trainer.early_stopping_update(validation_metrics_step3)"}
{"completion": "", "ground_truth": "        inputs = self.get_dummy_inputs(device)"}
{"completion": "pd.kl(None) with pytest.raises(NotImplementedError): pd.entropy() with pytest.raises(NotImplementedError): pd.sample() with pytest.raises(NotImplementedError): pd.mode_logits() with pytest.raises(NotImplementedError): pd.logits() with pytest.raises(NotImplementedError): pd.loc_scale_logits() with pytest.raises(NotImplementedError): pd.postprocess() with pytest.raises(NotImplementedError): pd.arraypd() with pytest.raises(NotImplementedError): pd.add_x() def test_CategoricalPd(self): logits = torch.randn(3, 2) pd = CategoricalPd(logits=logits) assert pd.logits == logits assert pd.sample().shape == (3,) assert pd.mode().shape == (3,) assert pd.kl(None).shape == (3,) assert pd.entropy().shape == (3,) assert pd.neglogp(torch.tensor([0, 1, 1])).shape == (3,) assert pd.mode_logits().shape == (3, 2) assert pd.logits().shape == (3, 2)", "ground_truth": "            pd.sample()"}
{"completion": "tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name, revision=args.revision, use_fast=False)def main(args): if args.tokenizer_name: tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name, revision=args.revision, use_fast=False)", "ground_truth": "        tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name, revision=args.revision, use_fast=False)"}
{"completion": "observation_spec = gs.transform_observation_spec(observation_spec) assert observation_spec.shape[-3] == 1 else: observation_spec = CompositeSpec({ key: BoundedTensorSpec(-1, 1, (nchannels, 16, 16)) for key in keys }) observation_spec = gs.transform_observation_spec(observation_spec) for key in keys: assert observation_spec[key].shape[-3] == 1", "ground_truth": "            observation_spec = gs.transform_observation_spec(observation_spec)"}
{"completion": "def step(self, actions: Dict[int, Any]) -> Dict[int, namedtuple]: self._check_closed() env_ids = list(actions.keys()) assert all([self._env_states[env_id] == EnvState.RUN for env_id in env_ids]), 'current env state are: {}, please check whether the requested env is in reset or done'.format({env_id: self._env_states[env_id] for env_id in env_ids}) for env_id, act in actions.items(): self._pipe_parents[env_id].send(['step', [act], {}]) timesteps = {} step_args = self._async_args['step'] wait_num, timeout = min(step_args['wait_num'], len(env_ids)), step_args['timeout'] rest_env_ids = list(set(env_ids).union(self._waiting_env['step'])) ready_env_ids = [] cur_rest_env_ids = copy.deepcopy(rest_env_ids) while True: rest_conn = [self._pipe_parents[env_id] for env_id in cur_rest_env_ids] ready_conn, ready_ids = AsyncSubprocessEnvManager.wait(rest_conn, min(wait_num, len(rest", "ground_truth": "                env.close()"}
{"completion": "encoded_inputs", "ground_truth": "                                    NewsQAResult(unique_id, start_logits,\n                                                 end_logits))"}
{"completion": "torch.device(_device) for _device in passing_devices] else: raise ValueError(\"passing_devices should be either None, a torch.device \" \"or equivalent or an iterable of devices. \" f\"Found {type(passing_devices)} instead.\") self.passing_devices = self.passing_devices if pin_memory else None (self._policies, self._devices, self._get_weights_fn,) = zip(*sorted(self._policy_dict.items(), key=lambda x: x[0], reverse=True)) self._passing_devices = zip(*sorted(self.passing_devices.items(), key=lambda x: x[0], reverse=True)) self.reset_at_each_iter = reset_at_each_iter self.init_random_frames = init_random_frames self.postproc = postproc self.split_trajs = split_trajs self.postproc = PostprocSelect(env_specs=observation_spec) if postproc is None else postproc self.reset_when_done = reset_when_done self.total_frames = total_frames self.init_with_lag = init_with_lag self.update_at_each", "ground_truth": "                    torch.device(_passing_device) for _passing_device in passing_devices"}
{"completion": "observation_spec = CompositeSpec(pixels=ImageSpec(shape=torch.Size([*batch_size, size, size, 3]),), observation=UnboundedContinuousTensorSpec(shape=torch.Size([*batch_size, size])), observation_orig=UnboundedContinuousTensorSpec(shape=torch.Size([*batch_size, size])), shape=batch_size,)if action_spec is None: action_spec = BoundedTensorSpec(-1, 1, (*batch_size, 7,),)if reward_spec is None: reward_spec = UnboundedContinuousTensorSpec(shape=(*batch_size, 1))", "ground_truth": "            observation_spec = CompositeSpec(\n                pixels=UnboundedContinuousTensorSpec(\n                    shape=torch.Size([*batch_size, 1, 7, 7])\n                ),\n                pixels_orig=UnboundedContinuousTensorSpec(\n                    shape=torch.Size([*batch_size, 1, 7, 7])\n                ),\n                shape=batch_size,\n            )"}
{"completion": "actual: list[pyvizier.ParameterValue] = converter.to_parameter_values(scaled) self.assertGreaterEqual(actual[0].value, 1e-4) self.assertLessEqual(actual[1].value, 0.5) self.assertLessEqual(actual[1].value, 1e2)", "ground_truth": "    actual: list[pyvizier.ParameterValue] = converter.to_parameter_values(  # pytype:disable=annotation-type-mismatch\n        scaled\n    )"}
{"completion": "== spec.dtype assert (spec2.zero() == spec.zero()).all() assert spec2.rand().shape == spec2.shape assert spec2.zero().shape == spec2.shape @pytest.mark.parametrize(\"shape1\", [ None, (), (5,), ],) @pytest.mark.parametrize(\"shape2\", [(), (10,)]) def test_unbounded_continuous(self, shape1, shape2): spec = UnboundedContinuousTensorSpec(shape=shape1, device=\"cpu\", dtype=torch.float64) if shape1 is not None: shape2_real = (*shape2, *shape1) else: shape2_real = shape2 spec2 = spec.expand(shape2_real) assert spec2 is not spec assert spec2.shape == torch.Size([*shape2_real]) assert spec2.dtype == spec.dtype assert spec2.sample().shape == spec2.shape assert spec2.zeros().shape == spec2.shape assert spec2.ones().shape == spec2.shape assert spec2.zeros_like().shape == spec2.shape assert spec2.ones_like", "ground_truth": "        assert (spec2.zero() == spec.zero()).all()"}
{"completion": "transforms.append(normalize) resize = Resize(size, size, in_keys=in_keys) transforms.append(resize) if out_keys is None: out_keys = [\"vip_vec\"] vip_instance = R3M(in_keys=in_keys, out_keys=out_keys, stack_images=stack_images,) vip_instance.load_weights(self.download_path, self.download) transforms.append(vip_instance) super().__init__(*transforms)", "ground_truth": "        transforms.append(normalize)"}
{"completion": "env.set_seed(seed + 10) env.reset() rollout3 = env.rollout(max_steps=100) with pytest.raises(AssertionError): assert_allclose_td(rollout1, rollout3) env.close()@pytest.mark.parametrize(\"device\", get_available_devices())def test_rollout_predictability(device): env = MockSerialEnv(device=device) env.set_seed(100) first = 100 % 17 policy = Actor(torch.nn.Linear(1, 1, bias=False)).to(device) for p in policy.parameters(): p.data.fill_(1.0) td_out = env.rollout(policy=policy, max_steps=200) assert (torch.arange(first, first + 100, device=device) == td_out.get(\"observation\").squeeze()).all() assert (torch.arange(first + 1, first + 101, device=device) == td_out.get((\"next\", \"observation\")).squeeze()).all() assert (torch.arange(first + 1, first + 101, device=device) == td_out.get(\"reward\").squeeze()).all() assert (torch.arange", "ground_truth": "        td_expanded = td.expand(2)"}
{"completion": "assert self.class_prob_output_layer.predict(outputs).shape == (self.n_inputs, self.dim_outputs,) def test_class_prob_output_layer_sample(self): outputs = random.normal(self.rng_outputs, shape=(self.n_inputs, 2 * self.dim_outputs)) assert self.class_prob_output_layer.sample(self.n_samples, outputs).shape == (self.n_samples, self.n_inputs, self.dim_outputs,)", "ground_truth": "        assert self.class_prob_output_layer.predict(outputs).shape == (self.n_inputs,)"}
{"completion": "manager.base import ModelManagerfrom fortuna.output_calibrator.output_calib_manager.base import \\ OutputCalibManagerfrom fortuna.prob_output_layer.base import ProbOutputLayerfrom fortuna.typing import (Array, Batch, CalibMutable, CalibParams, Mutable, Params)from fortuna.utils.random import WithRNGclass Likelihood(WithRNG): def __init__(self, model_manager: ModelManager, prob_output_layer: ProbOutputLayer, output_calib_manager: OutputCalibManager,): \"\"\" A likelihood function abstract class. In this class, the likelihood function is additionally assumed to be a probability density function, i.e. positive and integrating to 1. The likelihood is formed by three objects applied in sequence: the model manager, the output calibrator and the probabilistic output layer. The output maker maps parameters and inputs to outputs. The output calibration takes outputs and returns some calibrated version of them. The probabilistic output layer describes the probability distribution of the calibrated outputs. Parameters ---------- model_manager : ModelManager An model manager. This objects orchestrates the evaluation of the", "ground_truth": "        outs = self.output_calib_manager.apply(\n            params=calib_params[\"output_calibrator\"]\n            if calib_params is not None\n            else None,\n            mutable=calib_mutable[\"output_calibrator\"]\n            if calib_mutable is not None\n            else None,\n            outputs=outputs,\n            calib=\"calib_mutable\" in return_aux,\n        )"}
{"completion": "", "ground_truth": "            vat_loss = VATLoss()  # xi, and eps"}
{"completion": "def test_cross_entropy_uniform_unmasked(): torch.manual_seed(0) C, N, D = 4, 8, 10 x_logits = torch.randn(C, N, D) x_logits -= x_logits.max(dim=-1, keepdim=True).values x = x_logits.softmax(dim=-1) y_logits = torch.randn(C, N, D) y_logits -= y_logits.max(dim=-1, keepdim=True).values y = y_logits.softmax(dim=-1) x_mask = torch.ones(C, N, dtype=torch.bool) y_mask = torch.ones(C, N, dtype=torch.bool) ce = F.cross_entropy(x_logits, y.max(dim=-1).indices, reduction=\"none\") ce_unmasked = F.cross_entropy(x_logits, y.max(dim=-1).indices, reduction=\"none\", ignore_index=-1) wasserstein = F.cross_entropy(x_logits, y.max(dim=-1).indices, weight=torch.ones_like(ce), reduction=\"none\") wasserstein_unmasked = F.cross_entropy(x_logits, y.max(dim=-1).indices, weight", "ground_truth": "    gsde_lazy = LazygSDEModule(sigma_init=sigma_init, learn_sigma=learn_sigma).to("}
{"completion": "self.register_buffer(\"max_log_alpha\", torch.tensor(max_alpha, device=device).log())", "ground_truth": "        self.register_buffer(\n            \"max_log_alpha\", torch.tensor(max_alpha, device=device).log()\n        )"}
{"completion": "def _hook_on_fit_start_init(self): self.ctx.model.train() for param in self.ctx.model.parameters(): param.requires_grad = True self.ctx.optimizer = get_optimizer(self.ctx.model.parameters(), self.cfg.general_trainer.optimizer) self.ctx.scheduler = get_scheduler(self.ctx.optimizer, self.cfg.general_trainer.scheduler)def _hook_on_fit_start_calculate_model_size(self): num_parameters = sum(p.numel() for p in self.ctx.model.parameters() if p.requires_grad) logger.info(f\"Number of trainable model parameters: {num_parameters}\") self.ctx.monitor.update(\"model_parameters\", num_parameters) self.ctx.monitor.initialize_metrics(self.ctx.eval_metrics) self.ctx.monitor.set_model_size(num_parameters)", "ground_truth": "        self.register_hook_in_train(self._hook_on_epoch_start,\n                                    \"on_epoch_start\")"}
{"completion": "_data_loader=self.class_val_data_loader, val_data_loader=self.class_val_data_loader, map_fit_config=self.class_fit_config_nodir_nodump, fit_config=self.class_fit_config_dir_nodump(tmp_dir), calib_config=self.class_calib_config_nodir_nodump,) sample = prob_class.posterior.sample() prob_class.posterior.load_state(tmp_dir) status = prob_class.train(train_data_loader=self.class_train_data_loader, calib_data_loader=self.class_val_data_loader, val_data_loader=self.class_val_data_loader, map_fit_config=self.class_fit_config_nodir_nodump, fit_config=self.class_fit_config_dir_dump(tmp_dir), calib_config=self.class_calib_config_nodir_nodump,) sample = prob_class.posterior.sample() prob_class.posterior.load_state(tmp_dir) status = prob_class.train(train_data_loader=self.class_train_data_loader, calib_data_loader=self.class_val_data_loader, val_data_loader=self.class_val_data_loader, map_fit_config=self.class_fit_config_nodir_n", "ground_truth": "            status = prob_reg.train(\n                train_data_loader=self.reg_train_data_loader,\n                calib_data_loader=self.reg_val_data_loader,\n                val_data_loader=self.reg_val_data_loader,\n                map_fit_config=self.reg_fit_config_nodir_nodump,\n                fit_config=self.reg_fit_config_restore(tmp_dir),\n                calib_config=self.reg_calib_config_nodir_nodump,\n            )"}
{"completion": "timeout = 1 watchdog = WatchDog(timeout) def func(): time.sleep(0.5) watchdog.watch(func) with pytest.raises(Exception): watchdog.watch(lambda: 1/0) assert watchdog.get_elapsed_time() > 0", "ground_truth": "        watchdog = WatchDog(5)"}
{"completion": "flatten = FlattenObservation(-2, -1, out_keys) transforms = [*transforms, cattensors, network, flatten] else: network = _VIPNet(in_keys=in_keys, out_keys=out_keys, model_name=model_name, del_keys=False,) transforms.append(network) self.transforms = transforms @_init_first def forward(self, obs_dict: TensorDict) -> Union[TensorDict, torch.Tensor]: return super().forward(obs_dict)", "ground_truth": "            flatten = FlattenObservation(-2, -1, out_keys)"}
{"completion": "can_sample_thruput, thruput_info = self._thruput_controller.can_sample(size) can_sample = can_sample_stalenss and can_sample_thruput if not can_sample: return None with self._lock: indices = self._get_indices(size, sample_range) sampled_data = self._sample_with_indices(indices, cur_learner_iter) self._periodic_thruput_monitor.sample_data_count += len(sampled_data) if self._enable_track_used_data: self._used_data_remover.update(sampled_data) return sampled_data", "ground_truth": "            can_sample_thruput, thruput_info = self._thruput_controller.can_sample(size)"}
{"completion": "trial4.complete(vz.Measurement({'m1': .7, 'm2': -.7, 's1': 2., 's2': .0})) population = nsga2.Population(evaluations=[ vz.Evaluation(count=1, value=trial2.final_measurement.metrics['m1']), vz.Evaluation(count=2, value=trial0.final_measurement.metrics['m1']), vz.Evaluation(count=3, value=trial3.final_measurement.metrics['m1']), vz.Evaluation(count=4, value=trial1.final_measurement.metrics['m1']), vz.Evaluation(count=5, value=trial4.final_measurement.metrics['m1']) ]) new_population = algorithm.survive(population) self.assertLen(new_population.evaluations, 3) self.assertEqual(new_population.evaluations[0].count, 2) self.assertAlmostEqual(new_population.evaluations[0].value, 1.0) self.assertEqual(new_population.evaluations[1].count, 1) self.assertAlmostEqual(new_population.evaluations[1].value, 0.5) self.assertEqual(new_population.evaluations[2].count", "ground_truth": "    trial4.complete(vz.Measurement({'m1': .3, 'm2': -.3, 's1': 2., 's2': .0}))"}
{"completion": "self._check_output_dim(train_data_loader) return super().train(train_data_loader, val_data_loader, calib_data_loader, fit_config, calib_config, **fit_kwargs,)", "ground_truth": "        return super().train(\n            train_data_loader,\n            val_data_loader,\n            calib_data_loader,\n            fit_config,\n            calib_config,\n            **fit_kwargs,\n        )"}
{"completion": "trial_metrics(trial_proto, include_all_metrics=True) self.assertEqual({'objective': 77.7, 'loss': 56.8}, metrics) def testPyTrialToDictWithFinalMetricsSingleObjective(self): py_study_config = vz.StudyConfig(metric_information=[ vz.MetricInformation(name='objective', goal=vz.ObjectiveMetricGoal.MAXIMIZE) ]) root = py_study_config.search_space.root root.add_float_param('learning_rate', 0.01, 3.0) pytrial = vz.Trial(id=1, completion_time=datetime.datetime(year=2021, month=12, day=2, hour=7, minute=31), parameters={'learning_rate': vz.ParameterValue(0.5)}, final_measurement=vz.Measurement(metrics={ 'loss': vz.Metric(value=56.8), 'objective': vz.Metric(value=77.7), }, elapsed_secs=67, steps=101,),) parameters = py_study_config._pytrial_parameters(pytrial) self.assertEqual({'learning", "ground_truth": "    parameters = py_study_config.trial_parameters(trial_proto)"}
{"completion": "use_register = False) assert ts != ts_other ts_other = OneHotDiscreteTensorSpec(n=n, device=device, dtype=torch.float64, use_register=use_register) assert ts != ts_other ts_other = OneHotDiscreteTensorSpec(n=n, device=device, dtype=dtype, use_register=not use_register) assert ts != ts_other ts_other = TestEquality._ts_make_all_fields_equal(UnboundedContinuousTensorSpec(device=device, dtype=dtype), ts) assert ts != ts_other def test_equality_unbounded(self): device = \"cpu\" dtype = torch.float16 ts = UnboundedContinuousTensorSpec(device=device, dtype=dtype) ts_same = UnboundedContinuousTensorSpec(device=device, dtype=dtype) assert ts == ts_same ts_other = UnboundedContinuousTensorSpec(device=\"cpu:0\", dtype=dtype) assert ts != ts_other ts_other = UnboundedContinuousTensorSpec(device=device, dtype=torch.float64) assert ts != ts_other ts_other = TestEquality._ts_make_all_fields_equal(", "ground_truth": "        ts_other = MultiOneHotDiscreteTensorSpec(nvec=nvec, device=\"cpu:0\", dtype=dtype)"}
{"completion": "improved = trainer.early_stopping_update(validation_metrics_step4)self.assertFalse(improved)self.assertFalse(trainer._early_stopping.should_stop)improved = trainer.early_stopping_update(validation_metrics_step5)self.assertFalse(improved)self.assertTrue(trainer._early_stopping.should_stop)trainer = FakeTrainerWithEarlyStopping(early_stopping_monitor=\"metric1\", early_stopping_min_delta=0, early_stopping_patience=1, early_stopping_mode=\"max\",)improved = trainer.early_stopping_update(validation_metrics_step1)self.assertTrue(improved)improved = trainer.early_stopping_update(validation_metrics_step2)self.assertFalse(improved)self.assertFalse(trainer._early_stopping.should_stop)improved = trainer.early_stopping_update(validation_metrics_step3)self.assertFalse(improved)self.assertTrue(trainer._early_stopping.should_stop)trainer = FakeTrainerWithEarlyStopping(early_stopping_monitor=\"metric1\", early_stopping_min_delta=0, early_stopping_patience=2, early_stopping_mode=\"max\",)improved = trainer.early_stopping_update(validation_metrics_step1)self.assertTrue(improved)improved", "ground_truth": "        improved = trainer.early_stopping_update(validation_metrics_step4)"}
{"completion": "storage = ListStorage(max_size=1_000) sampler = PrioritizedSampler(storage.max_size, alpha, beta, eps, dtype) super(TensorDictPrioritizedReplayBuffer, self).__init__(storage=storage, sampler=sampler, collate_fn=collate_fn, pin_memory=pin_memory, prefetch=prefetch, transform=transform,)", "ground_truth": "            storage = ListStorage(max_size=1_000)"}
{"completion": "self._policy.reset() eval_results = [] with self._timer: self._logger.info(\"Evaluation begin...\")", "ground_truth": "        eval_monitor = VectorEvalMonitor(self._env.env_num, n_episode)"}
{"completion": "vae.to(accelerator.device, dtype=weight_dtype)unet.to(accelerator.device, dtype=weight_dtype)", "ground_truth": "    unet.to(accelerator.device, dtype=weight_dtype)"}
{"completion": "deal_with_resource()`: Returns the resource info needed to start the learner.", "ground_truth": "        self._learner_thread.start()"}
{"completion": "If the code is supposed to be completed, there seems to be some cut-off where the last else part of the if statement is missing. Could you provide the specific part you would like me to fill in?", "ground_truth": "            value = float(params.get_value(variable_name) == category)"}
{"completion": "", "ground_truth": "        metric.add_batch(predictions=preds, references=refs)"}
{"completion": "new_scheduler = scheduler_class.from_pretrained(tmpdirname) new_scheduler.set_timesteps(num_inference_steps) new_scheduler.ets = dummy_past_residuals[:]```In this code fragment, a new instance of the scheduler class is created using the `from_pretrained` method and the configuration is loaded from the temporary directory `tmpdirname`. Then, the number of inference steps is set using the `set_timesteps` method, and the dummy past residuals are copied over to the new scheduler instance.```python output = scheduler.step_prk(residual, time_step, sample, **kwargs).prev_sample```In this line of code, the `step_prk` method of the scheduler instance is called with the `residual`, `time_step`, and `sample` as arguments, along with any additional keyword arguments. The `prev_sample` attribute of the output of the `step_prk` method is then assigned to the `output` variable.```python new_output = new_scheduler.step_prk(residual, time_step, sample, **kwargs).prev_sample", "ground_truth": "                new_scheduler = scheduler_class.from_pretrained(tmpdirname)"}
{"completion": "status = prob_reg.train(train_data_loader=self.reg_train_data_loader, calib_data_loader=self.reg_val_data_loader, val_data_loader=self.reg_val_data_loader, map_fit_config=self.reg_fit_config_nodir_nodump, fit_config=self.reg_fit_config_dir_nodump(tmp_dir), calib_config=self.reg_calib_config_nodir_nodump,) sample = prob_reg.posterior.sample() prob_reg.posterior.load_state(tmp_dir) status = prob_reg.train(train_data_loader=self.reg_train_data_loader, calib_data_loader=self.reg_val_data_loader, val_data_loader=self.reg_val_data_loader, map_fit_config=self.reg_fit_config_nodir_nodump, fit_config=self.reg_fit_config_dir_dump(tmp_dir), calib_config=self.reg_calib_config_nodir_nodump,) sample = prob_reg.posterior.sample() prob_reg.posterior.load_state(tmp_dir) status = prob_reg.train(train_data_loader=self.reg_train_data_loader, calib_data_loader=self.reg_val_data_loader, val_data", "ground_truth": "            status = prob_reg.train(\n                train_data_loader=self.reg_train_data_loader,\n                calib_data_loader=self.reg_val_data_loader,\n                val_data_loader=self.reg_val_data_loader,\n                map_fit_config=self.reg_fit_config_nodir_nodump,\n                fit_config=self.reg_fit_config_dir_nodump(tmp_dir),\n                calib_config=self.reg_calib_config_nodir_nodump,\n            )"}
{"completion": "self._obs_pool.update(obs) if self._transform_obs: obs = to_tensor(obs, dtype=torch.float32) policy_output = self._policy.forward(obs, **policy_kwargs) self._policy_output_pool.update(policy_output) actions = {env_id: output['action'] for env_id, output in policy_output.items()} actions = to_ndarray(actions) envstep_count = 0 with self._timer: self._env.step(actions) for env_id in ready_env_id: env_id = int(env_id) traj_key = self._traj_buffer[env_id][0].add(obs=self._obs_pool.get(env_id), rew=self._env.reward[env_id], done=self._env.done[env_id]) if traj_key is not None and self._env_info[env_id]['step'] >= self._unroll_len: return", "ground_truth": "                self._obs_pool.update(obs)"}
{"completion": "_1 = DummyMetric(experiment_id=\"test_dummy_metric\")preds, refs = DummyMetric.predictions_and_references()expected_results = DummyMetric.expected_results()_1.compute(predictions=preds, references=refs)_2 = DummyMetric(experiment_id=\"test_dummy_metric\")_2.add_batch(predictions=preds, references=refs)_2.compute()_3 = DummyMetric(experiment_id=\"test_dummy_metric\")for pred, ref in zip(preds, refs): _3.add(prediction=pred, reference=ref)_3.compute()_4 = DummyMetric(keep_in_memory=True, experiment_id=\"test_dummy_metric\")for pred, ref in zip(preds, refs): _4.add(prediction=pred, reference=ref)_4.compute()_5 = DummyMetric(keep_in_memory=True, experiment_id=\"test_dummy_metric\")_5.compute(predictions=[], references=[])_6 = DummyMetric(keep_in_memory=True, experiment_id=\"test_dummy_metric\")_6.add_batch(predictions=[1, 2, 3], references=[1, 2, 3, 4])metric_add_batch_and_compute = dummymetric.metric_add_batch_and_computemetric_add_and_compute = dummymetric", "ground_truth": "        metric.compute(predictions=[[\"a\"]], references=[[\"a\"]])"}
{"completion": "@pytest.mark.parametrize(\"safe\", [True, False])@pytest.mark.parametrize(\"spec_type\", [None, \"bounded\", \"unbounded\"])def test_stateful(safe, spec_type): torch.manual_seed(0) param_multiplier = 1 if spec_type is None: spec = None elif spec_type == \"bounded\": spec = BoundedTensorSpec(-0.1, 0.1, 4) elif spec_type == \"unbounded\": spec = UnboundedContinuousTensorSpec(4) if safe and spec is None: with pytest.raises(RuntimeError, match=\"is not a valid configuration as the tensor specs are not specified\"): assert ((td.get(\"out\") < 0.1) | (td.get(\"out\") > -0.1)).all()@pytest.mark.parametrize(\"safe\", [True, False])@pytest.mark.parametrize(\"spec_type\", [None, \"bounded\", \"unbounded\"])def test_functional_with_buffer(safe, spec_type): torch.manual_seed(0) param_multiplier = 1 net = nn.BatchNorm1d(32 * param_multiplier) params = make_functional(net) if spec_type", "ground_truth": "        assert (ts.encode(ts.to_numpy(r)) == r).all()"}
{"completion": "metric = self.prepare_metric(metric) predictions = self.pipeline_predict(pipeline=pipe, inputs=pipe_inputs, metric=metric, metric_inputs=metric_inputs,) processed_preds = self.predictions_processor(predictions=predictions, words=pipe_inputs[DatasetColumn.PIPELINE_INPUT].to_pandas().tolist(), join_by=join_by,) metric_score = metric.compute(predictions=processed_preds[\"predictions\"], references=metric_inputs[\"references\"]) result[self.default_metric_name] = float(metric_score) return result, processed_preds", "ground_truth": "        metric = self.prepare_metric(metric)"}
{"completion": "", "ground_truth": "    assert_allclose_td(td1[s.get(\"_idx\").squeeze()].select(\"a\"), s.select(\"a\"))"}
{"completion": "module = evaluate.load(\"bleu\")launch_gradio_widget(module)", "ground_truth": "module = evaluate.load(\"text_duplicates\")"}
{"completion": "sys.path", "ground_truth": "module = evaluate.load(\"sacrebleu\")"}
{"completion": "observation_spec = BoundedTensorSpec(-1, 1, (*size, nchannels, 16, 16)) observation_spec = unsqueeze.transform_observation_spec(observation_spec) assert observation_spec.shape[len(size) :] == expected_size else: observation_spec = CompositeSpec({ key: BoundedTensorSpec(-1, 1, (*size, nchannels, 16, 16)) for key in keys }) observation_spec = unsqueeze.transform_observation_spec(observation_spec) for key in keys: assert observation_spec[key].shape[len(size) :] == expected_size", "ground_truth": "            observation_spec = unsqueeze.transform_observation_spec(observation_spec)"}
{"completion": "return cls.init(FrozenDict(d[\"params\"]), FrozenDict(d[\"mutable\"]), optimizer, calib_params=FrozenDict(d[\"calib_params\"]), calib_mutable=FrozenDict(d[\"calib_mutable\"]), **kwargs,)", "ground_truth": "        return cls.init(\n            FrozenDict(d[\"params\"]),\n            FrozenDict(d[\"mutable\"]) if d[\"mutable\"] is not None else None,\n            optimizer,\n            FrozenDict(d.get(\"calib_params\"))\n            if d[\"calib_params\"] is not None\n            else None,\n            FrozenDict(d.get(\"calib_mutable\"))\n            if d[\"calib_mutable\"] is not None\n            else None,\n            **kwargs,\n        )"}
{"completion": "trainer.register_op(\"pre_steps_log\", log_reward, accumulate=True) td = TensorDict({\"reward\": torch.ones(3)}, [3]) trainer._pre_steps_log_hook(td) if _has_tqdm and pbar: assert trainer._pbar_str[logname] == 1 else: assert logname not in trainer._pbar_str assert trainer._log_dict[logname][-1] == 1 assert log_reward.count == 1 @pytest.mark.parametrize(\"logname\", [\"a\", \"b\"]) @pytest.mark.parametrize(\"pbar\", [True, False]) def test_log_reward_reset(self, logname, pbar): trainer = mocking_trainer() trainer.collected_frames = 0 log_reward = LogReward(logname, log_pbar=pbar) trainer.register_op(\"pre_steps_log\", log_reward, accumulate=True) td = TensorDict({\"reward\": torch.ones(3)}, [3]) trainer._pre_steps_log_hook(td) if _has_tqdm and pbar: assert trainer._pbar_str[logname] == 1 else: assert", "ground_truth": "        log_reward.register(trainer)"}
{"completion": "timestep = env_manager.step(action) assert len(timestep) == env_manager.env_num watchdog.stop()", "ground_truth": "        timestep = env_manager.step(action)"}
{"completion": "ValueConverter.to_proto(value, 'aa'), study_pb2.Trial.Parameter(parameter_id='aa', value=struct_pb2.Value(string_value='category')),) def testToIntegerProto(self): value = ParameterValue(True) compare.assertProto2Equal(self, proto_converters.ParameterValueConverter.to_proto(value, 'aa'), study_pb2.Trial.Parameter(parameter_id='aa', value=struct_pb2.Value(number_value=1.0)),)class TrialConverterTest(absltest.TestCase): def testFromProtoCompleted(self): proto = study_pb2.Trial(id=str(1)) proto.state = study_pb2.Trial.State.SUCCEEDED proto.parameters.add(parameter_id='float', value=struct_pb2.Value(number_value=1.0)) proto.parameters.add(parameter_id='int', value=struct_pb2.Value(number_value=2)) proto.parameters.add(parameter_id='str', value=struct_pb2.Value(string_value='3')) proto.final_measurement.metrics.add(metric_id='pr-auc', value=0.8) proto.final_measurement.metrics.add", "ground_truth": "    added2 = proto.metadata.add(key='proto', ns='t')"}
{"completion": "class NodeFullBatchTrainer(GeneralTorchTrainer): def parse_data(self, data): init_dict = dict() if isinstance(data, dict): for mode in [\"train\", \"val\", \"test\"]: init_dict[\"{}_loader\".format(mode)] = data.get(mode) init_dict[\"{}_data\".format(mode)] = None init_dict[\"num_{}_data\".format(mode)] = 1 else: raise TypeError(\"Type of data should be dict.\") return init_dict def _hook_on_batch_forward(self, ctx): batch = ctx.data_batch.to(ctx.device) pred = ctx.model(batch)[batch['{}_mask'.format(ctx.cur_split)]] label = batch.y[batch['{}_mask'.format(ctx.cur_split)]] ctx.batch_size = torch.sum(ctx.data_batch['{}_mask'.format(ctx.cur_split)]).item() ctx.loss_batch = CtxVar(ctx.criterion(pred, label), LIFECYCLE.BATCH)", "ground_truth": "        ctx.y_true = CtxVar(label, LIFECYCLE.BATCH)"}
{"completion": "", "ground_truth": "        components = self.get_dummy_components()"}
{"completion": "predictions = task_evaluator.predictions_processor(predictions, words, join_by) self.assertListEqual(predictions[\"predictions\"][0], [\"B-LOC\", \"O\", \"O\", \"B-LOC\", \"O\"])", "ground_truth": "        predictions = task_evaluator.predictions_processor(predictions, words, join_by)"}
{"completion": "td_error = td_error.to(input_tensordict.device) if \"timestep\" in input_tensordict: self._loss_summary(td_error, pred_val, target_value, input_tensordict[\"timestep\"]) return td_error, loss_value def _loss_value(self, input_tensordict: TensorDictBase) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]: if self.delay_value: self.update_targets(value=True) self.update_targets(actor=True) actor_output = self.actor_network(input_tensordict[:, {\"device\"}].unsqueeze(1)) if self.delay_actor: self.update_targets(actor=False) self.update_targets(value=True) next_val = next_state_value(input_tensordict, critic=self.safe_value_network, actor=self.safe_actor_network, delay_critic=self.delay_value, delay_actor=self.delay_actor,) self.update_targets(actor=False) pred_val = self.safe_value_network(input_tensordict) if \"timestep\" in self.safe_value_network.recurse_configure()[1]: assert", "ground_truth": "            td_error = td_error.to(input_tensordict.device)"}
{"completion": "elif alg == 'avg': configuration_space.add_hyperparameter(CS.CategoricalHyperparameter('step', choices=[1, 2, 3, 4]))else: configuration_space.add_hyperparameter(CS.CategoricalHyperparameter('step', choices=[1])) configuration_space.add_hyperparameter(CS.CategoricalHyperparameter('lrserver', choices=[0.1, 0.5, 1.0])) configuration_space.add_hyperparameter(CS.CategoricalHyperparameter('momentumsserver', choices=[0.0, 0.9]))```", "ground_truth": "    cfg.cost = CN()"}
{"completion": "", "ground_truth": "        pipe = self.prepare_pipeline(\n            model_or_pipeline=model_or_pipeline,\n            tokenizer=tokenizer,\n            feature_extractor=feature_extractor,\n            device=device,\n        )"}
{"completion": "composite = { \"key1\": { \"out\": {}, \"in\": {}, \"subkey1\": { \"subkey2\": {} } }, \"key2\": {}}", "ground_truth": "    assert set(composite.keys()) == set(keys)"}
{"completion": "\"\"\"_seed0 == final_seed1 assert_allclose_td(rollout0, rollout1) base_env = suite.load(env_name, task) if from_pixels: render_kwargs = {\"camera_id\": 0} base_env = pixels.Wrapper(base_env, pixels_only=pixels_only, render_kwargs=render_kwargs) env2 = DMControlWrapper(base_env, frame_skip=frame_skip) torch.manual_seed(0) np.random.seed(0) final_seed2 = env2.set_seed(0) tdreset2 = env2.reset() rollout2 = env2.rollout(max_steps=50) assert_allclose_td(tdreset0, tdreset2) assert final_seed0 == final_seed2 assert_allclose_td(rollout0, rollout2) def test_faketd(self, env_name, task, frame_skip, from_pixels, pixels_only): if from_pixels and (not torch.has_cuda or not torch.cuda.device_count()): raise pytest.skip(\"no cuda device\") env = DMControlEnv(env_name, task, frame_skip=frame_skip, from_pixels=from_pixels,", "ground_truth": "            tdrollout.append(env.rollout(max_steps=50))"}
{"completion": "metric.add(inputs=input, targets=target)", "ground_truth": "        metric.add(inputs=input, targets=target)"}
{"completion": "def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, position_ids=None, start_positions=None, end_positions=None, labels=None, pretrain_task=None, contrast_monitor=None, in_contrast_prepare=None, example_indices=None,): if in_contrast_prepare: self.eval() with torch.no_grad(): example_indices = [ k for k in example_indices if k.item() in contrast_monitor.synth_tokens ] if len(example_indices) == 0: return ModelOutput(example_indices=example_indices) outputs = self.model(input_ids=input_ids.to(self.device), attention_mask=attention_mask.to(self.device), token_type_ids=token_type_ids.to(self.device), position_ids=position_ids.to(self.device), start_positions=start_positions.to(self.device), end_positions=end_positions.to(self.device), labels=labels.to(self.device), pretrain_task=pretrain_task, contrast_monitor=contrast_monitor, in_contrast_prepare=in_contrast_prepare, example_indices=example_indices,) return outputs", "ground_truth": "                    ctx.loss_batch = CtxVar(outputs.loss, LIFECYCLE.BATCH)"}
{"completion": "ddim.to(torch_device) ddim.set_progress_bar_config(disable=None) generator = torch.manual_seed(seed) image_ddpm = ddpm(generator=generator, eta=0.0, output_type=\"numpy\").images generator = torch.manual_seed(seed) image_ddim = ddim(generator=generator, eta=0.0, output_type=\"numpy\").images assert np.abs(image_ddpm - image_ddim).sum() < 1e-5, \"Models don't give the same forward pass\" def test_ddpm_ddim_equality_single(self): seed = 0 model_id = \"google/ddpm-cifar10-32\" unet = UNet2DModel.from_pretrained(model_id) ddpm_scheduler = DDPMScheduler() ddim_scheduler = DDIMScheduler() ddpm = DDPMPipeline(unet=unet, scheduler=ddpm_scheduler) ddpm.to(torch_device) ddpm.set_progress_bar_config(disable=None) ddim = DDIMPipeline(unet=unet, scheduler=ddim_scheduler) ddim.to(torch_device) ddim.set_progress_bar_config(dis", "ground_truth": "        ddim.to(torch_device)"}
{"completion": "block = ChannelShuffle(group_num) output = self.run_model(input, block) assert output.shape == (batch_size, in_channels, H, W) def test_one_hot(self): num_classes = 3 input = torch.randint(low=0, high=num_classes, size=(batch_size,)).requires_grad_(True) output = one_hot(input, num_classes) assert output.shape == (batch_size, num_classes) assert isinstance(output, torch.Tensor) def test_nearest_upsample(self): scale_factor = 2 input = torch.rand(batch_size, in_channels, H, W).requires_grad_(True) block = NearestUpsample(scale_factor) output = self.run_model(input, block) output_H = H * scale_factor output_W = W * scale_factor assert output.shape == (batch_size, in_channels, output_H, output_W) def test_bilinear_upsample(self): scale_factor = 2 input = torch.rand(batch_size, in_channels, H, W).requires_grad_(True) block = BilinearUpsample(scale_factor) output = self.run_model(input, block", "ground_truth": "        channel_shuffle = ChannelShuffle(group_num)"}
{"completion": "expected_keys += [\"scale\", \"loc\", \"_eps_gSDE\"] try: assert set(td.keys()) == set(expected_keys) except AssertionError: proof_environment.close() raise tsf_loc = actor.module[0].module[-1].module.transform(td.get(\"loc\")) if exploration == \"random\": with pytest.raises(AssertionError): torch.testing.assert_close(td.get(\"action\"), tsf_loc) else: torch.testing.assert_close(td.get(\"action\"), tsf_loc) td_clone = td.clone() if UNSQUEEZE_SINGLETON and not td_clone.ndimension(): qvalue(td_clone.unsqueeze(0)) else: qvalue(td_clone) expected_keys = [ \"done\", \"action\", \"state_action_value\", \"loc\", \"scale\", ] if len(gsde): expected_keys += [\"_eps_gSDE\"] if from_pixels: expected_keys += [\"pixels\", \"hidden\", \"pixels_orig\", \"_reset\"] else: expected_keys += [\"observation_vector\", \"observation_orig\"] try:", "ground_truth": "            tsf_loc = actor.module[0].module[-1].module.transform(td.get(\"loc\"))"}
{"completion": "", "ground_truth": "        assert not ts.is_in(torch.tensor(np_r))"}
{"completion": "", "ground_truth": "        ms = MultiStep(gamma=gamma, n_steps_max=n).to(device)"}
{"completion": "trainer.training_step_end(1, state, {\"loss\": 1, \"logging_kwargs\": None}, batch, (), {\"val_loss\": 0.1, \"val_accuracy\": 0.5},) msc.assert_called_once_with(state, \"tmp_dir\", keep=3) def test_training_step_end_ok_training_metrics_computation(self): trainer = FakeTrainer(predict_fn=lambda x: x, disable_training_metrics_computation=False, save_checkpoint_dir=\"tmp_dir\", save_every_n_steps=1, keep_top_n_checkpoints=3,) state = FakeTrainState() batch = [[1, 2, 3], [0, 0, 1]] with unittest.mock.patch.object(trainer, \"save_checkpoint\") as msc: with self.assertLogs(level=\"DEBUG\") as cm: trainer.training_step_end(1, state, {\"loss\": 1, \"logging_kwargs\": None}, batch, (), {\"val_loss\": 0.1, \"val_accuracy\": 0.5},) self.assertIn(\"logging_kwargs\", cm.output", "ground_truth": "            training_losses_and_metrics = trainer.training_step_end(\n                1, state, {\"loss\": 1, \"logging_kwargs\": None}, batch, (), {}\n            )"}
{"completion": "with self.__time_proxy.freeze(): pass", "ground_truth": "        self.__time_proxy.freeze()"}
{"completion": "\"lik_log_var=self.joint.likelihood.model_manager.likelihood_log_variance_model.init(self.rng, jnp.zeros((1,) + self.shape_inputs)),)) def test_lik_batched_log_joint_prob(self): log_joint_prob = self.joint.likelihood.batched_log_joint_prob(inputs=self.data_arr, params=self.params) self.assertEqual(log_joint_prob.shape, (self.n_inputs,))", "ground_truth": "                lik_log_var=self.joint.likelihood.model_manager.likelihood_log_variance_model.init(\n                    self.rng, jnp.zeros((1,) + self.shape_inputs)\n                ),"}
{"completion": "td_out = td assert key1 in td_out.keys() assert key2 not in td_out.keys() state_dict = hook.state_dict() assert isinstance(state_dict, dict) assert len(state_dict) == 1 assert \"keys\" in state_dict assert state_dict[\"keys\"] == [key1]", "ground_truth": "        hook.register(trainer)"}
{"completion": "inputs = self.get_dummy_inputs(device)image = sd_pipe(**inputs).imagesimage_slice = image[0, -3:, -3:, -1]assert image.shape == (1, 64, 64, 3)expected_slice = np.array([0.5643, 0.6017, 0.4799, 0.5267, 0.5584, 0.4641, 0.5159, 0.4963, 0.4791])assert np.abs(image_slice.flatten() - expected_slice).max() < 1e-2", "ground_truth": "        inputs = self.get_dummy_inputs(torch_device)"}
{"completion": "metadata = {}", "ground_truth": "    custom_decisions_str = trial.metadata.ns(constants.METADATA_NAMESPACE).get(\n        constants.TRIAL_METADATA_KEY_CUSTOM_TYPE_DECISIONS, None)"}
{"completion": "tensordict_module = SafeModule(MultiHeadLinear(5, 4, 3), in_keys=[\"input\"], out_keys=[\"_\", \"out_2\"],)", "ground_truth": "                spec=CompositeSpec(**spec_dict),"}
{"completion": "results = self.evaluator.compute(data=self.data, metric=\"word_count\",) self.assertIsInstance(results[\"unique_words\"], int) def test_overwrite_default_metric(self): word_count = load(\"word_count\") results = self.evaluator.compute(model_or_pipeline=self.pipe, data=self.data, metric=word_count,) self.assertIsInstance(results[\"unique_words\"], int) results = self.evaluator.compute(model_or_pipeline=self.pipe, data=self.data, metric=\"word_count\",) self.assertIsInstance(results[\"unique_words\"], int) def test_data_loading(self): data = self.evaluator.load_data(\"evaluate/csas_rank\", split=\"validation[:1]\") self.evaluator.prepare_data(data=data, input_column=\"inputs\", label_column=\"labels\",) self.assertEqual(data[0][\"id\"], \"0\")", "ground_truth": "        results = self.evaluator.compute(data=self.data)"}
{"completion": "", "ground_truth": "        return self.__call__(\n            prompt=prompt,\n            negative_prompt=negative_prompt,\n            height=height,\n            width=width,\n            num_inference_steps=num_inference_steps,\n            guidance_scale=guidance_scale,\n            num_images_per_prompt=num_images_per_prompt,\n            eta=eta,\n            generator=generator,\n            latents=latents,\n            max_embeddings_multiples=max_embeddings_multiples,\n            output_type=output_type,\n            return_dict=return_dict,\n            callback=callback,\n            callback_steps=callback_steps,\n            **kwargs,\n        )"}
{"completion": "_evaluator = QuestionAnsweringEvaluator()data = load_dataset(\"squad_v2\", split=\"validation[:2]\")results = _evaluator.compute(model_or_pipeline=\"mrm8488/bert-tiny-finetuned-squadv2\", data=data, metric=\"squad_v2\", squad_v2_format=True,)", "ground_truth": "        predictions = self.predictions_processor(predictions, squad_v2_format=squad_v2_format, ids=data[id_column])"}
{"completion": "py return {}, DatasetColumn(data, input_column)", "ground_truth": "        return {\"references\": data[label_column]}, DatasetColumn(data, input_column)"}
{"completion": "self.assertTrue(improved)improved = trainer.early_stopping_update(validation_metrics_step2)self.assertTrue(improved)improved = trainer.early_stopping_update(validation_metrics_step3)self.assertTrue(improved)improved = trainer.early_stopping_update(validation_metrics_step4)self.assertFalse(improved)self.assertFalse(trainer._early_stopping.should_stop)improved = trainer.early_stopping_update(validation_metrics_step5)self.assertFalse(improved)self.assertTrue(trainer._early_stopping.should_stop)", "ground_truth": "        improved = trainer.early_stopping_update(validation_metrics_step2)"}
{"completion": "__init__` method of the `SyncDataCollector` class.- `create_env_fn`: A callable that creates an instance of the environment. This can be an instance of `EnvBase` or a `Callable` that returns an instance of `EnvBase`.- `policy`: An optional argument that specifies the policy to be used for data collection. This can be an instance of `TensorDictModule` or a callable that takes a `TensorDictBase` object as input and returns a `TensorDictBase` object.- `total_frames`: An optional argument that specifies the total number of frames (steps) to collect. If not provided, data collection will continue indefinitely.- `create_env_kwargs`: Optional keyword arguments to be passed to the `create_env_fn` callable when creating the environment instance.- `max_frames_per_traj`: Maximum number of steps per trajectory. Once a trajectory reaches", "ground_truth": "                    policy_device = torch.device(\"cpu\")"}
{"completion": "training_losses_and_metrics = trainer.training_step_end(1, state, {\"loss\": 1, \"logging_kwargs\": None}, batch, (), {}) msc.assert_called_once_with(state, \"tmp_dir\", keep=3) self.assertEqual(training_losses_and_metrics, {\"loss\": 1})", "ground_truth": "            training_losses_and_metrics = trainer.training_step_end(\n                1,\n                state,\n                {\"loss\": 1, \"logging_kwargs\": {\"metric1:\": 0.1, \"metrics2\": 0.2}},\n                batch,\n                (),\n                {},\n            )"}
{"completion": "setup_league[0].mutate() assert isinstance(setup_league[0].mutation_model, NoMutation) assert isinstance(setup_league[1], HistoricalPlayer) for _ in range(10): setup_league[1].mutate() assert isinstance(setup_league[1].mutation_model, EnvMutation) for i in range(2, 10): assert isinstance(setup_league[i], HistoricalPlayer) setup_league[i].mutate() assert isinstance(setup_league[i].mutation_model, RandomMutation)", "ground_truth": "            assert setup_league[0].mutate({}) is None"}
{"completion": "class DmControlWrapper(GymWrapper): \"\"\"DeepMind Control Suite environment wrapper. Examples: >>> env = DmControlWrapper(env_name=\"cheetah-run\", pixel_wrapper=True, frame_skip=4) >>> td = env.rand_step() >>> print(td) >>> print(env.available_envs) \"\"\" git_url = \"https://github.com/deepmind/dm_control\" libname = \"dm_control\" def __init__(self, env_name, pixel_wrapper=False, **kwargs): kwargs[\"env_name\"] = env_name self._pixel_wrapper = pixel_wrapper super().__init__(**kwargs) def _check_kwargs(self, kwargs: Dict): if \"env_name\" not in kwargs: raise TypeError(\"Could not find environment key 'env_name' in kwargs.\") def _build_env(self, env_name: str, pixel_wrapper: bool = False, frame_skip: int = 1, **kwargs,) -> \"dm_control_wrapper.DmControlWrapper\": if self._pixel_wrapper or pixel_wrapper: import dmc2gym env = dmc2gym.make(", "ground_truth": "    @implement_for(\"gym\", \"0.24.0\", None)"}
{"completion": "trainer.register_op(\"post_optim\", target_net_updater.update) if policy_exploration is not None: trainer.register_op(\"post_optim\", policy_exploration.update) if recorder is not None: recorder = copy.copy(recorder) recorder.seed(trainer.seed) trainer.register_op(\"post_optim\", recorder.record) trainer.register_op(\"post_optim\", recorder.reset) trainer.register_op(\"post_optim\", lambda info_dict: info_dict[\"frame\"] + 1) return trainer", "ground_truth": "        trainer.register_op(\"post_optim\", target_net_updater.step)"}
{"completion": "spec8 = UnboundedDiscreteTensorSpec(shape=(*batch_size, 9), device=\"cpu\", dtype=torch.long,)spec = CompositeSpec(spec1=spec1, spec2=spec2, spec3=spec3, spec4=spec4, spec5=spec5, spec6=spec6, spec7=spec7, spec8=spec8)", "ground_truth": "        spec8 = UnboundedDiscreteTensorSpec(\n            shape=(*batch_size, 9),\n            device=\"cpu\",\n            dtype=torch.long,\n        )"}
{"completion": "\"\"\" mock_env = TransformedEnv(ContinuousActionConvMockEnv(pixel_shape=[3, 64, 64])) default_dict = { \"state\": UnboundedContinuousTensorSpec(state_dim), \"belief\": UnboundedContinuousTensorSpec(rssm_hidden_dim), } mock_env.append_transform(TensorDictPrimer(random=False, default_value=0, **default_dict)) actor_module = DreamerActor(out_features=mock_env.action_spec.shape[0], depth=4, num_cells=mlp_num_units, activation_class=nn.ELU,) actor_model = SafeProbabilisticSequential(SafeModule(actor_module, in_keys=[\"state\", \"belief\"], out_keys=[\"loc\", \"scale\"],), SafeProbabilisticModule(in_keys=[\"loc\", \"scale\"], out_keys=\"action\", default_interaction_mode=\"random\", distribution_class=TanhNormal,),) with torch.no_grad(): td = TensorDict({ \"state\": torch.randn(1, 2, state_dim), \"belief\": torch.randn(1, 2, rssm", "ground_truth": "        mb_env = self._create_mb_env(10, 5).to(device)"}
{"completion": "input_spec = CompositeSpec(action=action_spec, shape=batch_size)", "ground_truth": "            input_spec = CompositeSpec(\n                action=action_spec,\n                observation=UnboundedContinuousTensorSpec(\n                    (\n                        *batch_size,\n                        1,\n                    )\n                ),\n                shape=batch_size,\n            )"}
{"completion": "scheduler.save_config(tmpdirname) new_scheduler = scheduler_class.from_pretrained(tmpdirname) new_scheduler.ets = dummy_past_residuals[:]``````python output = scheduler.step_prk(residual, time_step, sample, **kwargs).prev_sample new_output = new_scheduler.step_prk(residual, time_step, sample, **kwargs).prev_sample``````python output = scheduler.step_plms(residual, time_step, sample, **kwargs).prev_sample new_output = new_scheduler.step_plms(residual, time_step, sample, **kwargs).prev_sample", "ground_truth": "                scheduler.save_config(tmpdirname)"}
{"completion": "\"\"\" Get the model parameters. Returns: torch.Tensor: model parameters \"\"\"", "ground_truth": "        return self._param_filter(\n            self.ctx.model.state_dict() if self.cfg.federate.\n            share_local_model else self.ctx.model.cpu().state_dict())"}
{"completion": "", "ground_truth": "                        self._resource_manager.delete(\"collector\", collector_id)"}
{"completion": "return torch.device(module._hf_hook.execution_device) return self.device def setup_generator(self): r\"\"\" Prepare the generator for image sampling. Setups the generator model, the schedules and generate a depth estimation from text tokens. \"\"\" step_size = self.unet.config.dim self.setup_depth_estimator() self.setup_generator_vae(self.text_encoder.tokenizer) self.text_encoder.to(self.device) if self.unet.config.readout_type != \"direct\": raise ValueError(\"SampleGenerator supports 'direct' readout type only for Stable Diffusion\") self.unet.to(self.device) self.schedule_device_moving(self.unet) feature_dim = self.unet.config.dim block_dim = self.unet.config.block_dim img_size = self.unet.config.sample_size self.unet_img_dim = img_size w, h = int(step_size * img_size), int(step_size * img_size) self.pth_step = 1.0 / self.unet.schedule.n_steps self.all_probs = self.unet", "ground_truth": "                return torch.device(module._hf_hook.execution_device)"}
{"completion": "pytrial = vz.Trial(id=2)pytrial.parameters = { 'learning_rate[0]': vz.ParameterValue(value=0.5), 'learning_rate[1]': vz.ParameterValue(value=0.1), 'units[0]': vz.ParameterValue(value=50), 'units[1]': vz.ParameterValue(value=200), 'activation[0]': vz.ParameterValue(value='relu'), 'activation[1]': vz.ParameterValue(value='relu'), 'synchronous[0]': vz.ParameterValue(value=True), 'synchronous[1]': vz.ParameterValue(value=False), 'batch_size[0]': vz.ParameterValue(value=32.0), 'batch_size[1]': vz.ParameterValue(value=8.0), 'floating_point_param': vz.ParameterValue(value=16.0)}parameters = py_study_config._pytrial_parameters(pytrial)expected = { 'learning_rate", "ground_truth": "      root.add_discrete_param('batch_size', [8, 16, 32], index=index)"}
{"completion": "module = evaluate.load(\"bleu\")launch_gradio_widget(module)", "ground_truth": "module = evaluate.load(\"word_count\")"}
{"completion": "prob_class.train(train_data_loader=self.class_train_data_loader, calib_data_loader=self.class_val_data_loader, val_data_loader=self.class_val_data_loader, fit_config=self.class_fit_config_restore(tmp_dir), calib_config=self.class_calib_config_nodir_nodump,) prob_class.load_state(checkpoint_path=tmp_dir) prob_class.save_state(checkpoint_path=tmp_dir)", "ground_truth": "            prob_class.train(\n                train_data_loader=self.class_train_data_loader,\n                calib_data_loader=self.class_val_data_loader,\n                val_data_loader=self.class_val_data_loader,\n                fit_config=self.class_fit_config_restore(tmp_dir),\n                calib_config=self.class_calib_config_nodir_nodump,\n            )"}
{"completion": "actor = ProbabilisticActor(module=actor_module, in_keys=out_keys, spec=CompositeSpec(action=env_specs[\"action_spec\"]), safe=True, distribution_class=TanhDelta, distribution_kwargs={ \"min\": env_specs[\"action_spec\"].space.minimum, \"max\": env_specs[\"action_spec\"].space.maximum, },).to(device)q_net_default_kwargs = { \"linear_layer_class\": linear_layer_class, \"activation_class\": ACTIVATIONS[cfg.activation],}q_net_default_kwargs.update(value_net_kwargs)in_keys = out_keys + [\"action\"]q_net = DdpgMlpQNet(**q_net_default_kwargs)critic_module = SafeModule(q_net, in_keys=in_keys, out_keys=[\"state_action_value\"])critic = SafeQOperator(module=critic_module, spec=CompositeSpec(reward=env_specs[\"reward_spec\"]), in_keys=in_keys, safe=True,).to(device)return torch.nn.ModuleList([actor, critic])", "ground_truth": "    actor = ProbabilisticActor(\n        module=actor_module,\n        in_keys=[\"param\"],\n        spec=CompositeSpec(action=env_specs[\"action_spec\"]),\n        safe=True,\n        distribution_class=TanhDelta,\n        distribution_kwargs={\n            \"min\": env_specs[\"action_spec\"].space.minimum,\n            \"max\": env_specs[\"action_spec\"].space.maximum,\n        },\n    )"}
{"completion": "", "ground_truth": "        ret = pd.sample(viz=True)"}
{"completion": "", "ground_truth": "        score, scores = bleu_scorer.compute_score(option='closest', verbose=0)"}
{"completion": "calib_prob_class = ProbClassifier(model=self.model, prior=IsotropicGaussianPrior(), output_calibrator=ClassificationTemperatureScaler(),) state = calib_prob_class.joint.init(self.input_shape) assert \"model\" in state.params assert \"params\" in state.params[\"model\"] assert hasattr(state, \"mutable\")", "ground_truth": "        calib_prob_class = ProbClassifier(\n            model=self.model,\n            prior=IsotropicGaussianPrior(),\n            output_calibrator=ClassificationTemperatureScaler(),\n        )"}
{"completion": "", "ground_truth": "        self.__lock = LockContext(lock_type)"}
{"completion": "metric_module = importlib.import_module(evaluate.loading.evaluation_module_factory(os.path.join(evaluation_module_type + \"s\", evaluation_module_name), module_type=evaluation_module_type).module_path) return metric_module.load(*args, **kwargs)", "ground_truth": "            return load(os.path.join(evaluation_module_type + \"s\", evaluation_module_name), *args, **kwargs)"}
{"completion": "COMML_EARNER_REGISTRY = Registry()", "ground_truth": "COMM_LEARNER_REGISTRY = Registry()"}
{"completion": "", "ground_truth": "        self.close()"}
{"completion": "ts_other = CompositeSpec(ts1=bounded_same, ts2=nd_same) assert ts != ts_other ts = CompositeSpec(ts1=bounded, ts2=nd) ts_other = CompositeSpec(ts1=bounded_same, ts2=nd_same) assert ts != ts_other def test_equality_tuple(self): minimum = np.arange(12).reshape((3, 4)) maximum = minimum + 100 device = \"cpu\" dtype = torch.float16 bounded = BoundedTensorSpec(0, 1, torch.Size((1,)), device, dtype) bounded_same = BoundedTensorSpec(0, 1, torch.Size((1,)), device, dtype) bounded_other = BoundedTensorSpec(0, 2, torch.Size((1,)), device, dtype) nd = BoundedTensorSpec(minimum=minimum, maximum=maximum + 1, device=device, dtype=dtype) nd_same = BoundedTensorSpec(minimum=minimum, maximum=maximum + 1, device=device, dtype=dtype) _ = BoundedTensorSpec(minimum=minimum", "ground_truth": "        ts_other = CompositeSpec(ts1=bounded, ts2=nd, ts3=bounded_other)"}
{"completion": "self.reg_lik.mode(params, self.reg_inputs_arr), self.reg_lik.mean(params, self.reg_inputs_arr)) assert self.reg_lik.variance(params, self.reg_inputs_arr).shape == (self.n_inputs, self.output_dim,) assert self.reg_lik.variance(params, self.reg_inputs_gen_fun).shape == (self.batch_size * self.n_batches, self.output_dim,) def test_class_stats(self): params = FrozenDict(dict(model=self.class_lik.model_manager.model.init(self.rng, jnp.zeros((1,) + self.shape_inputs)),)) assert self.class_lik.prob(params, self.class_inputs_arr).shape == (self.n_inputs, self.output_dim,) assert self.class_lik.prob(params, self.class_inputs_gen_fun).shape == (self.batch_size * self.n_batches, self.output_dim,) assert self.class_lik.mode(params, self.class_inputs_arr).shape == (self.n_inputs,) assert self.class_lik.mode(params, self.class_inputs_gen_fun).shape == (self.batch_size * self.n", "ground_truth": "            self.reg_lik.mode(params, self.reg_inputs_arr),"}
{"completion": "def test_predictions_processor(self): task_evaluator = evaluator(\"token-classification\") join_by = \" \" words = [[\"New\", \"York\", \"a\", \"nice\", \"City\", \".\"]] predictions = [ [ {\"start\": 0, \"entity\": \"B-LOC\"}, {\"start\": 2, \"entity\": \"I-LOC\"}, {\"start\": 4, \"entity\": \"I-LOC\"}, {\"start\": 9, \"entity\": \"O\"}, {\"start\": 11, \"entity\": \"O\"}, {\"start\": 16, \"entity\": \"B-LOC\"}, {\"start\": 21, \"entity\": \"O\"}, ] ] processed_predictions = task_evaluator.predictions_processor(predictions, words, join_by) expected_predictions = [['B-LOC', 'I-LOC', 'I-LOC', 'O', 'O', 'B-LOC', 'O']] self.assertListEqual(processed_predictions, expected_predictions) predictions = [ [ {\"start\": 0, \"entity\": \"B", "ground_truth": "        predictions = task_evaluator.predictions_processor(predictions, words, join_by)"}
{"completion": "if ns == 'study': self.study_config.metadata.abs_ns(ns).update(metadatum.abs_ns(ns)) else: self._trials[tid - 1].assign_namespace(ns, metadatum.abs_ns(ns)) def AddTrial(self, trial: vz.Trial) -> None: \"\"\"Adds a trial and assigns a trial ID.\"\"\" trial.id = len(self._trials) + 1 self._trials.append(trial) def SuggestTrials(self, policy: policy.TrialSuggestionPolicy, count: int = 1, *, worker_id: Optional[str] = None) -> List[vz.TrialSuggestion]: next_trials = [] for _ in range(count): next_trial = policy.next(historical_trials=self._trials) if next_trial is not None: self.AddTrial(next_trial) next_trials.append(next_trial) return next_trialsdef GetDistribution(trial: 'InRamPolicySupporter', metric_key: str, flatten: bool = True, dtype: np.dtype = np.float32) -> np", "ground_truth": "        self._trials[tid - 1].metadata.abs_ns(ns).update(metadatum.abs_ns(ns))"}
{"completion": "processor: CalibProcessor = CalibProcessor(),): \"\"\" Configure the probabilistic model calibration. Parameters ---------- optimizer: CalibOptimizer It defines the optimization specifics. checkpointer: CalibCheckpointer It handles saving and restoring checkpoints. monitor: CalibMonitor It monitors the calibration process. processor: CalibProcessor It processes the calibration data. \"\"\" self.optimizer = optimizer self.checkpointer = checkpointer self.monitor = monitor self.processor = processor", "ground_truth": "        processor: CalibProcessor = CalibProcessor(),"}
{"completion": "", "ground_truth": "    sampled_td = rb.sample(3, include_info=True)"}
{"completion": "if inputs_loader is None: raise ValueError(\"Either `input_shape` or `inputs_loader` or `inputs` must be passed.\") params = state.params if state.mutable: for batch_inputs in inputs_loader: batch_size = batch_inputs.shape[0] inputs_unravel = (np.zeros((batch_size,) + input_shape), batch_inputs,) params = self.joint.likelihood.model_manager.apply(params, inputs_unravel, mutable=params, train=True, rng=rng,)[1][\"mutable\"] z1 = random.normal(rng, shape=(n_params,)) z2 = random.normal(rng, shape=(self.posterior_approximator.rank,)) state = state.replace(params=self.unravel(state.mean + self.posterior_approximator.coeff1 * state.std * z1 + self.posterior_approximator.coeff2 * jnp.matmul(state.dev, z2)))", "ground_truth": "            model_manager_state = self.joint.likelihood.model_manager.init(input_shape)"}
{"completion": "self.feat_engr_public_key, self.worker._cfg.client_addresses) self.msg_buffer['feat_encrypted_splits'] = \\ self.comm_manager.receive('client_feat_encrypted_splits') self.iv_filter() def iv_filter(self): logger.info('Start to merge splits for each feature.') self.merge_splits() logger.info('Start to compute iv for each feature.') iv_values = [] for feature_id in range(self.client_feat_dim[1]): values = self.merged_splits[feature_id][1] labels = self.merged_splits[feature_id][2] iv = self.compute_iv(values, labels) iv_values.append(iv) selected_features = [] for feature_id, iv in enumerate(iv_values): if iv >= self._cfg.federate.iv_threshold: selected_features.append(feature_id) self.comm_manager.send(selected_features, self.worker._cfg.client_addresses, msg_type='selected_features')", "ground_truth": "            Message(msg_type='binning',\n                    sender=self.ID,\n                    receiver=list(self.comm_manager.get_neighbors().keys()),\n                    state=self.state,\n                    content=self._cfg.feat_engr.selec_woe_binning))"}
{"completion": "self._produce_learner_thread.start()", "ground_truth": "        self._assign_collector_thread.start()"}
{"completion": "results = self.evaluator.compute(model_or_pipeline=self.pipe, data=data, input_column=\"text\", label_column=\"label\", label_mapping=self.label_mapping,) self.assertEqual(results[\"accuracy\"], 1.0) data = self.evaluator.load_data(\"evaluate/imdb-ci\") results = self.evaluator.compute(model_or_pipeline=self.pipe, data=data, input_column=\"text\", label_column=\"label\", label_mapping=self.label_mapping,) self.assertEqual(results[\"accuracy\"], 1.0) data = Dataset.from_dict({\"label\": [1, 0], \"text\": [\"great movie\", \"horrible movie\"]}) results = self.evaluator.compute(model_or_pipeline=self.pipe, data=data, input_column=\"text\", label_column=\"label\", label_mapping=self.label_mapping,) self.assertEqual(results[\"accuracy\"], 1.0) data = [{\"label\": 1, \"text\": \"great movie\"}, {\"label\": 0, \"text\":", "ground_truth": "        self.evaluator.prepare_data(data=data, input_column=\"text\", label_column=\"label\", second_input_column=None)"}
{"completion": "class ClassificationPredictive(Predictive): def __init__(self, kernel: Kernel, num_classes: int, weights: Optional[Sequence[jnp.ndarray]] = None, forward: Optional[Callable[[jnp.ndarray, bool, Optional[PRNGKeyArray], bool], jnp.ndarray]] = None, rng: Optional[PRNGKeyArray] = None): self.kernel = kernel self.num_classes = num_classes self.weights = weights self.forward = forward self.rng = RNGSequence(rng) def fit(self, inputs_1: jnp.ndarray, targets: jnp.ndarray, inputs_2: Optional[jnp.ndarray] = None, weights: Optional[Sequence[jnp.ndarray]] = None, **kwargs) -> \"AbstractVariationalPosterior\": if self.weights is None: if weights is None: self.weights = (jnp.ones_like(targets),) else: self.weights = weights else: if weights is not None: raise ValueError(\"weights should not be passed if already specified during an earlier call to the function\") kernel = deepcopy(self.kernel) kernel.variance.prior = Prior(Softplus", "ground_truth": "        ensemble_outputs = self.sample_calibrated_outputs(\n            inputs_loader=inputs_loader,\n            n_output_samples=n_posterior_samples,\n            rng=rng,\n            distribute=distribute,\n        )"}
{"completion": "tsf_loc = actor.module.module.transform(td.get(\"loc\")) if exploration == \"random\": with pytest.raises(AssertionError): torch.testing.assert_close(td.get(\"action\"), tsf_loc) else: torch.testing.assert_close(td.get(\"action\"), tsf_loc)``````python expected_keys = [ \"done\", \"action\", \"param\" ] if from_pixels: expected_keys += [\"pixels\", \"hidden\", \"pixels_orig\", \"_reset\"] else: expected_keys += [\"observation_vector\", \"observation_orig\"] if cfg.gSDE: expected_keys += [\"scale\", \"loc\", \"_eps_gSDE\"] try: assert set(td.keys()) == set(expected_keys) except AssertionError: proof_environment.close() raise```", "ground_truth": "            tsf_loc = actor.module[0].module[-1].module.transform(td.get(\"loc\"))"}
{"completion": "_merge_a_into_b(cfg_list, self, self, []) self.__cfg_check_funcs__.clear() self.__cfg_check_funcs__.extend(cfg_check_funcs) self.assert_cfg(check_cfg) set_help_info(self, self.__help_info__) def assert_cfg(self, check_cfg=True): \"\"\" Assert the correctness of the configuration node. Args: check_cfg: whether enable Raises: AssertionError: if the configuration is invalid. \"\"\" if not check_cfg: return for cfg_check_fun in self.__cfg_check_funcs__: cfg_check_fun(self) def new_allowed(self, is_new_allowed): \"\"\" Set this config (and recursively its subconfigs) to allow merging \\ new keys from other configs. Args: is_new_allowed (bool): if True, merge new keys into the config. \"\"\" self.__dict__[CfgNode.NEW_ALLOWED] = is_new_allowed for v in self.__dict__.values(): if isinstance(v, CfgNode): v.set_new_allowed(is_new_allowed) for v in self.values(): if isinstance(v, CfgNode): v", "ground_truth": "        super().merge_from_list(cfg_list)"}
{"completion": "", "ground_truth": "    metric_name = exptr.problem_statement().metric_information.item().name"}
{"completion": "def close_replay_buffer(self) -> None: \"\"\"Closes the replay buffer.\"\"\" if self.is_closed: raise RuntimeError(\"trying to close a closed replay buffer\") if self._verbose: print(f\"closing {self.__class__.__name__}\") self.empty_buffer() self.is_closed = True def empty_buffer(self) -> None: \"\"\"Removes all data from the replay buffer.\"\"\" self.buffer.clear() def sample(self, batch_size: int) -> TensorDict: \"\"\"Samples a batch of data from the replay buffer. Args: batch_size: The size of the batch to sample. Returns: A batch of data sampled from the replay buffer. \"\"\" if len(self.buffer) < batch_size: raise RuntimeError(\"insufficient data in the replay buffer to sample a batch\") indices = np.random.choice(len(self.buffer), size=batch_size, replace=False) batch = [self.buffer[idx] for idx in indices] return self.collate(batch) def collate(self, batch: List[TensorDict]) -> TensorDict: \"\"\"Collates a list of data samples into a single batch. Args:", "ground_truth": "            self.input_spec = self.input_spec.to(device)"}
{"completion": "generate all combinations of indices (without diagonals) offdProd = np.array(list(itertools.combinations(np.arange(n_vars), ord_i))) x_comb = np.zeros((n_samp, offdProd.shape[0], ord_i)) for j in range(ord_i): x_comb[:, :, j] = X[:, offdProd[:, j]] x_allpairs = np.append(x_allpairs, np.prod(x_comb, axis=2), axis=1)return x_allpairsclass AcquisitionOptimizer(abc.ABC): \"\"\"Base class for BOCS acquisition optimizers.\"\"\" def __init__(self, lin_reg: _GibbsLinearRegressor, lamda: float = 1e-4): self._lin_reg = lin_reg self._num_vars = self._lin_reg.num_vars self._lamda = lamda @abc.abstractmethod def argmin(self) -> np.ndarray: \"\"\"Computes argmin using the regressor.\"\"\" passclass SimulatedAnnealing(AcquisitionOptimizer): \"\"\"Simulated Annealing solver.\"\"\" def __init__(self, lin_reg", "ground_truth": "    for kk in range(self._num_repeats):"}
{"completion": "device = \"cpu\"components = self.get_dummy_components()sd_pipe = StableDiffusionPipeline(**components)sd_pipe = sd_pipe.to(device)sd_pipe.set_progress_bar_config(disable=None)inputs = self.get_dummy_inputs(device)image = sd_pipe(**inputs).imagesimage_slice = image[0, -3:, -3:, -1]assert image.shape == (1, 64, 64, 3)expected_slice = np.array([0.5649, 0.6022, 0.4804, 0.5270, 0.5585, 0.4643, 0.5159, 0.4963, 0.4793])assert np.abs(image_slice.flatten() - expected_slice).max() < 1e-2device = \"cpu\"components = self.get_dummy_components()components[\"scheduler\"] = PNDMScheduler(skip_prk_steps=True)sd_pipe = StableDiffusionPipeline(**components)sd_pipe = sd_pipe.to(device)sd_pipe.set_progress_bar_config(disable=None)inputs", "ground_truth": "        components = self.get_dummy_components()"}
{"completion": "self.assertFalse(trainer.should_perform_validation({}, 9)) self.assertTrue(trainer.should_perform_validation({}, 10)) self.assertTrue(trainer.should_perform_validation({}, 20)) def test_early_stopping_update(self): trainer = FakeTrainer(predict_fn=lambda x: x, disable_training_metrics_computation=False) trainer.val_loss = 0.1 observed = trainer.early_stopping_update({\"val_loss\": 0.05}) self.assertTrue(observed) self.assertEqual(trainer.val_loss, 0.05) trainer.val_loss = 0.1 observed = trainer.early_stopping_update({\"val_loss\": 0.15}) self.assertFalse(observed) self.assertEqual(trainer.val_loss, 0.1)", "ground_truth": "        self.assertTrue(trainer.should_perform_validation({}, 10))"}
{"completion": "\"\"\"_fit_config=self.class_fit_config_nodir_nodump,fit_config=self.class_fit_config_dir_dump(tmp_dir),calib_config=self.class_calib_config_nodir_nodump,)sample = prob_class.posterior.sample()prob_class.posterior.load_state(tmp_dir)status = prob_class.train(train_data_loader=self.class_train_data_loader, calib_data_loader=self.class_val_data_loader, val_data_loader=self.class_val_data_loader, map_fit_config=self.class_fit_config_nodir_nodump, fit_config=self.class_fit_config_restore(tmp_dir), calib_config=self.class_calib_config_nodir_nodump,)prob_class_map = ProbClassifier(model=MyModel(self.class_output_dim), posterior_approximator=MAPPosteriorApproximator(), output_calibrator=ClassificationTemperatureScaler(),)status = prob_class_map.train(train_data_loader=self.class_train_data_loader, calib_data_loader=self.class_val_data_loader, val_data_loader=self.class_val_data_loader, map_fit_config=self.class_fit_config_nodir_nodump, fit_config=self.class_fit_config_dir_dump(tmp", "ground_truth": "            prob_class = ProbClassifier(\n                model=MyModel(self.class_output_dim),\n                posterior_approximator=SWAGPosteriorApproximator(rank=2),\n                output_calibrator=ClassificationTemperatureScaler(),\n            )"}
{"completion": "def new_task(self, data: Optional[Mapping[str, Any]] = None) -> Task: with self.__lock: _uuid = uuid4() self.__tasks[_uuid] = Task(self.__http_engine, _uuid, (self.__my_address or ''), partial(self.__before_new_task, data), partial(self.__fill_new_task, data), partial(self.__after_new_task, data), partial(self.__timeout_new_task, data), partial(self.__cancel_new_task, data), partial(self._set_task_exception, _uuid), partial(self.__request_retries), partial(self.__request_retry_waiting),) return self.__tasks[_uuid]", "ground_truth": "            _task = Task(\n                http_engine=self.__http_engine,\n                data=data,\n                task_id=_uuid,\n                before_task_start=self._before_new_task,\n                after_task_start=self._after_new_task,\n                error_task_start=self._error_new_task,\n            )"}
{"completion": "Message(msg_type='feat_dim', sender=self.ID, receiver=[self.server_id], content=(split_data['x'].shape[1], filtered_col))) def callbacks_funcs_for_feat_dim(self, message: Message): feat_dim, filtered_col = message.content self.feat_dim = feat_dim self.filtered_col = filtered_col if hasattr(self, '_init_data_related_var'): self._init_data_related_var() if hasattr(self, '_init_model_related_var'): self._init_model_related_var() self.msg_buffer.pop('feat_dim') self.trigger_train_func(**self.kwargs_for_trigger_train_func) worker.trigger_for_feat_engr = types.MethodType(trigger_for_feat_engr, worker) worker.callbacks_funcs_for_feat_engr_public_keys = types.MethodType(callback_funcs_for_feat_engr_public_keys, worker) worker.callbacks_funcs_for_ask_for_encrypted_norm_feat = types.MethodType(callback_funcs_for_ask_for_encrypted_norm_feat, worker) worker.callbacks_funcs_for_encrypted_norm_feat = types.MethodType(callback_funcs_for_encrypted_norm_feat, worker) worker.callbacks_funcs_for_feat_corrcoef", "ground_truth": "            Message(msg_type='feat_dim',\n                    sender=self.ID,\n                    receiver=[self.server_id],\n                    content=(split_data['x'].shape[1], filtered_col)))"}
{"completion": "status = prob_reg.train(train_data_loader=self.reg_train_data_loader, calib_data_loader=self.reg_val_data_loader, val_data_loader=self.reg_val_data_loader, fit_config=self.reg_fit_config_dir_dump(tmp_dir), calib_config=self.reg_calib_config_nodir_nodump,) sample = prob_reg.posterior.sample() prob_reg.posterior.load_state(tmp_dir) prob_reg.posterior.load_state(tmp_dir) status = prob_reg.train(train_data_loader=self.reg_train_data_loader, calib_data_loader=self.reg_val_data_loader, val_data_loader=self.reg_val_data_loader, fit_config=self.reg_fit_config_restore(tmp_dir), calib_config=self.reg_calib_config_nodir_nodump,) sample = prob_reg.posterior.sample()", "ground_truth": "            status = prob_reg.train(\n                train_data_loader=self.reg_train_data_loader,\n                calib_data_loader=self.reg_val_data_loader,\n                val_data_loader=self.reg_val_data_loader,\n                fit_config=self.reg_fit_config_dir_dump(tmp_dir),\n                calib_config=self.reg_calib_config_nodir_nodump,\n            )"}
{"completion": "", "ground_truth": "        self.load_expert_data()"}
{"completion": "ctx.y_prob = CtxVar(pred, LIFECYCLE.BATCH)", "ground_truth": "        ctx.y_prob = CtxVar(pred, LIFECYCLE.BATCH)"}
{"completion": "Metric called '_METRIC_NAME' is not defined. The above code will throw NameError: name '_METRIC_NAME' is not defined.", "ground_truth": "  for i in range(len(steps)):"}
{"completion": "cfg.fedopt.optimizer.type = 'sgd'cfg.fedopt.optimizer.lr = 0.01cfg.fedopt.optimizer.momentum = 0.9", "ground_truth": "    cfg.fedopt.optimizer.type = Argument(\n        'SGD', description=\"optimizer type for FedOPT\")"}
{"completion": "", "ground_truth": "    converter = converters.TrialToArrayConverter.from_study_config(problem)"}
{"completion": "metric = DummyMetric(experiment_id=\"test_input_numpy\",) self.assertDictEqual(expected_results, metric.compute(predictions=preds, references=refs)) del metric", "ground_truth": "        self.assertDictEqual(expected_results, metric.compute(predictions=preds, references=refs))"}
{"completion": "scheduler.save_config(tmpdirname)", "ground_truth": "                new_scheduler = scheduler_class.from_pretrained(tmpdirname)"}
{"completion": "Message(msg_type='instance_stats', sender=self.ID, receiver=list(self.comm_manager.get_neighbors().keys()), state=self.state, content={})) def callback_funcs_for_instance_norm(self, message: Message): self.msg_buffer['ss_instance_sum'].append(message.content['ss_instance_sum']) self.msg_buffer['ss_instance_sum_norm_square'].append(message.content['ss_instance_sum_norm_square']) if len(self.msg_buffer['ss_instance_sum']) == self.client_num: self.instance_sum = np.sum(self.msg_buffer['ss_instance_sum'], axis=0) self.instance_sum_norm_square = np.sum(self.msg_buffer['ss_instance_sum_norm_square'], axis=0) self.instance_mean = self.instance_sum / \\ (self.client_num * self.client_feat_num) self.instance_std = np.sqrt((self.instance_sum_norm_square / (self.client_num * self.client_feat_num)) - np.square(self.instance_mean)) self.instance_std = np.maximum(self.instance_std, 1e-5", "ground_truth": "            Message(msg_type='ask_for_instance_sum',\n                    sender=self.ID,\n                    receiver=list(self.comm_manager.neighbors.keys()),\n                    timestamp=self.cur_timestamp))"}
{"completion": "", "ground_truth": "        self.comm_manager.send(\n            Message(msg_type=msg_type,\n                    sender=self.ID,\n                    receiver=list(self.comm_manager.neighbors.keys()),\n                    state=self.state,\n                    timestamp=self.cur_timestamp,\n                    content=model_para))"}
{"completion": "UnboundedContinuousTensorSpec(device=device, dtype=dtype), ts assert ts != ts_other", "ground_truth": "            BoundedTensorSpec(0, 1, torch.Size((1,)), device, dtype), ts"}
{"completion": "return super()._monitor_vars_learn() + [ 'policy_loss', 'critic_loss', 'cur_lr_q', 'cur_lr_p', 'target_q_value', 'q_value_1', 'q_value_2', 'alpha', 'td_error', 'target_value', 'entropy' ] + twin_critic", "ground_truth": "            return super()._monitor_vars_learn() + ["}
{"completion": "spec = BoundedTensorSpec(low=torch.zeros(4), high=torch.ones(4), dtype=torch.float32, device=\"cpu\",)", "ground_truth": "            spec = BoundedTensorSpec(-0.1, 0.1, 4)"}
{"completion": "", "ground_truth": "    config = LDMBertConfig(\n        d_model=bert_params.n_embed,\n        encoder_layers=bert_params.n_layer,\n        encoder_ffn_dim=bert_params.n_embed * 4,\n    )"}
{"completion": "DualTransformer2DModel(attn_num_head_channels, out_channels // attn_num_head_channels, in_channels=out_channels, num_layers=1, cross_attention_dim=cross_attention_dim, norm_num_groups=resnet_groups,", "ground_truth": "                    DualTransformer2DModel(\n                        attn_num_head_channels,\n                        out_channels // attn_num_head_channels,\n                        in_channels=out_channels,\n                        num_layers=1,\n                        cross_attention_dim=cross_attention_dim,\n                        norm_num_groups=resnet_groups,\n                    )"}
{"completion": "env.set_seed(0) tdreset = env.reset() assert tdreset.frame.is_cuda == env.device == env.action.is_cuda assert tdreset.frame.size() == (env.n_agents * env.num_envs,) + env._env.observation_space.shape assert tdreset.mask.size() == (env.n_agents * env.num_envs,) + (max_steps,) assert not tdreset.mask.any(dim=-1)[env.n_agents:].any() assert torch.all(env.extended_obs_index() == torch.arange(env.n_agents * env.num_envs)) assert torch.all(env._get_agent_obs_indexes() == tdreset.agent_obs_indexes) tdrollout = env.rollout(max_steps=n_rollout_samples) assert tdrollout.frame.size() == (env.n_agents * env.num_envs,) + env._env.observation_space.shape + (n_rollout_samples,) assert tdrollout.mask.size() == (env.n_agents * env.num_envs,) + (n_rollout_samples,) assert tdrollout.frame.is_cuda == env.device == env.action.is_cuda assert torch.all(env._get_agent_obs_indexes() == tdrollout.agent_obs_indexes) env.close", "ground_truth": "        tensordict = env.rollout(max_steps=n_rollout_samples)"}
{"completion": "super().test_output()", "ground_truth": "        super().test_output()"}
{"completion": "", "ground_truth": "        ts.encode(lb + torch.rand(10) * (ub - lb))"}
{"completion": "", "ground_truth": "        return super().forward(tensordict)"}
{"completion": "flatten = FlattenObservation(-2, -1, out_keys) transforms = [*transforms, cattensors, network, flatten] else: network = _VIPNet(in_keys=in_keys, out_keys=out_keys, model_name=model_name, del_keys=False,) transforms.append(network) self.transforms = transforms @_init_first def forward(self, obs_dict: TensorDict) -> Union[TensorDict, torch.Tensor]: return super().forward(obs_dict)", "ground_truth": "            flatten = FlattenObservation(-2, -1, out_keys)"}
{"completion": "Dataset`]): Specifies the dataset we will run evaluation on. input_column (`str`, defaults to `\"text\"`): The name of the column containing the text feature in the dataset specified by `data`. Returns: `dict`: metric inputs. `list`: pipeline inputs. \"\"\" if isinstance(data, str): ds = load_dataset(data) if not isinstance(ds, Dataset): raise ValueError(\"Please specify a valid `data` object - either a `str` with a name or a `Dataset` object.\") elif isinstance(data, Dataset): ds = data else: raise ValueError(\"Please specify a valid `data` object - either a `str` with a name or a `Dataset", "ground_truth": "        self.check_required_columns(data, {\"input_column\": input_column})"}
{"completion": "", "ground_truth": "        result = metric.compute(**metric_inputs, **self.METRIC_KWARGS)"}
{"completion": "actor = ProbabilisticActor(module=module, in_keys=[\"loc\", \"scale\"], distribution_class=TanhNormal, spec=action_spec,)qvalue = ValueOperator(module=module, in_keys=[\"observation\"],)ppo_loss = PPOLoss(actor_network=deepcopy(actor), value_network=deepcopy(qvalue), num_epochs=10, num_batches=64, clip_range=0.2, entropy_weight=0.01,)ppo_loss(train_data)", "ground_truth": "        actor = ProbabilisticActor(\n            module=module,\n            distribution_class=TanhNormal,\n            in_keys=[\"loc\", \"scale\"],\n            spec=action_spec,\n        )"}
{"completion": "dnn_trial.parameters.add(parameter_id='units[1]', value=struct_pb2.Value(number_value=70)) dnn_trial.parameters.add(parameter_id='activation', value=struct_pb2.Value(string_value='relu')) parameters = py_study_config.trial_parameters(dnn_trial) expected = { 'model_type': 'dnn', 'learning_rate': 2.1, 'units': [49, 70], 'activation': 'relu', } self.assertEqual(expected, parameters) linear_trial = study_pb2.Trial() linear_trial.id = str(2) linear_trial.parameters.add(parameter_id='model_type', value=struct_pb2.Value(string_value='linear')) linear_trial.parameters.add(parameter_id='learning_rate', value=struct_pb2.Value(number_value=0.5)) parameters = py_study_config.trial_parameters(linear_trial) expected = { 'model_type': 'linear', 'learning_rate': 0.5, } self.assertEqual(expected, parameters)", "ground_truth": "    dnn_trial.parameters.add(\n        parameter_id='unts[1]', value=struct_pb2.Value(number_value=79))"}
{"completion": "from federatedscope.core.configs.config import CNfrom federatedscope.core.configs.yacs_config import Argumentfrom federatedscope.register import register_configdef extend_fedprox_cfg(cfg): cfg.fedprox = CN", "ground_truth": "    cfg.fedprox = CN()"}
{"completion": "model", "ground_truth": "        outputs = self.model_manager.apply(\n            params=s.params, inputs=np.zeros((1,) + input_shape), mutable=s.mutable\n        )"}
{"completion": "{ 'state': self.state, 'model_para': model_para, 'total_round_num': self.total_round_num, 'history_results': self.history_results, 'early_stopper': self.early_stopper, 'msg_buffer': self.msg_buffer, 'cutoff': self._cutoff, 'trace': self._trace, 'stop_exploration': self._stop_exploration, 'theta': self._theta, 'z': self._z}", "ground_truth": "                Message(msg_type='finish',\n                        sender=self.ID,\n                        receiver=list(self.comm_manager.neighbors.keys()),\n                        state=self.state,\n                        content=model_para))"}
{"completion": "", "ground_truth": "        spec2 = spec.expand(shape2_real)"}
{"completion": "collector_env_cfg['num_envs'] = collector_env_num collector_env_cfg['manager'] = { 'type': 'subprocess', } buffer_cfg = deep_merge_dicts(NaiveReplayBuffer.default_config(), cfg.policy.collect.replay_buffer) buffer_cfg['policy'] = deep_merge_dicts(buffer_cfg.policy, cfg.policy) set_pkg_seed(seed, use_cuda=cfg.policy.cuda) collector = SampleSerialCollector(cfg.policy.collect.collector) collector.reset_env() stop_flag = False while len(duration_list) < 50: tt = time.time() data = collector.collect(train_iter=start_iter) train_iter += 1 duration = time.time() - tt duration_list.append(duration) if FAST_MODE: break if not FAST_MODE: tt = time.time() result = env_sum(data) duration = time.time() - tt logger.info(\"[Iter: %d] time: %.6f\\tresult: %s\\tmax_reward: %d\", i, duration, out_str(result), int(result.max())) if result.max() >= test_config[cfg.env.env_name]['max_reward", "ground_truth": "        policy = FakePolicy(cfg.policy)"}
{"completion": "super().test_outputs_equivalence()", "ground_truth": "        super().test_outputs_equivalence()"}
{"completion": "with torch.no_grad(), set_exploration_mode(\"random\"): tensordict = model_based_env.rollout(4) tensordict = tensordict.to(device) tensordict = actor_simulator(tensordict) value_model(tensordict) actor_realworld = actor_realworld.to(device) if proof_env_is_none: proof_environment.close() torch.cuda.empty_cache() del proof_environment del tensordict return world_model, model_based_env, actor_simulator, value_model, actor_realworlddef _dreamer_make_world_model(obs_encoder, obs_decoder, rssm_prior, rssm_posterior, reward_module): rssm_rollout = RSSMRollout(SafeModule(rssm_prior, in_keys=[\"state\", \"belief\", \"action\"], out_keys=[ (\"next\", \"prior_mean\"), (\"next\", \"prior_std\"), \"_\", (\"next\", \"belief\"), ],), SafeModule(rssm_posterior, in_keys=[(\"next\", \"belief\"), (\"next\", \"", "ground_truth": "    model_based_env = DreamerEnv(\n        world_model=WorldModelWrapper(\n            transition_model,\n            reward_model,\n        ),\n        prior_shape=torch.Size([state_dim]),\n        belief_shape=torch.Size([rssm_hidden_dim]),\n        obs_decoder=mb_env_obs_decoder,\n    )"}
{"completion": "round or finishing the evaluationif self.check_buffer(self.state, min_received_num, check_eval_result): if not check_eval_result: aggregated_num = self._perform_federated_aggregation() self.state += 1 if self.state % self._cfg.eval.freq == 0 and self.state != \\ self.total_round_num:if self.check_buffer(self.state, minimal_number, check_eval_result=False):train_msg_buffer = self.msg_buffer['train'][self.state]for model_idx in range(self.model_num): model = self.models[model_idx] msg_list = list() for client_id in train_msg_buffer: if self.model_num == 1: pred_embedding = train_msg_buffer[client_id] self.seqs_embedding[client_id] = pred_embedding else: raise ValueError('GlobalContrastFL server not support multi-model.')global_loss_fn = global_NT_xentloss(device=self.device)for client_id in train_msg_buffer: z1 = self.seqs_embedding[client_id][0] z2 = self.seqs_embedding[client_id", "ground_truth": "        if self.check_buffer(self.state, min_received_num, check_eval_result):"}
{"completion": "", "ground_truth": "                        eval_monitor.update_reward(env_id, reward)"}
{"completion": "self._q_network = DRQN(obs_shape, action_shape, hidden_size_list, lstm_type=lstm_type, dueling=dueling) embedding_size = hidden_size_list[-1] self.mixer = mixer if self.mixer: self._mixer = Mixer(agent_num, global_obs_shape, embedding_size) self._global_state_encoder = nn.Identity() def forward(self, data: dict, single_step: bool = True) -> dict: q, lstm_state = self._q_network.forward(data['obs'], data['lstm_state']) if self._need_detach: q = q.detach() residual = q if self.mixer: q = self._mixer.forward(q, data['global_obs']) q = q + residual q = self._act(q) return {'q_value': q, 'lstm_state': lstm_state}", "ground_truth": "            self._q_network = DRQN(obs_shape, action_shape, hidden_size_list, lstm_type=lstm_type, dueling=dueling)"}
{"completion": "", "ground_truth": "        tensordict.set(\"reward\", reward.to(torch.get_default_dtype()))"}
