{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Here are some relevant code fragments from other files of the repo:\n",
      "# --------------------------------------------------\n",
      "# the below code fragment can be found in:\n",
      "# src/diffusers/pipelines/pipeline_flax_utils.py\n",
      "# --------------------------------------------------\n",
      "#             for library_name, library_classes in LOADABLE_CLASSES.items():\n",
      "#                 library = importlib.import_module(library_name)\n",
      "#                 for base_class, save_load_methods in library_classes.items():\n",
      "#                     class_candidate = getattr(library, base_class, None)\n",
      "#                     if class_candidate is not None and issubclass(model_cls, class_candidate):\n",
      "#                         # if we found a suitable base class in LOADABLE_CLASSES then grab its save method\n",
      "#                         save_method_name = save_load_methods[0]\n",
      "#                         break\n",
      "#                 if save_method_name is not None:\n",
      "#                     break\n",
      "# \n",
      "#             save_method = getattr(sub_model, save_method_name)\n",
      "#             expects_params = \"params\" in set(inspect.signature(save_method).parameters.keys())\n",
      "# \n",
      "#             if expects_params:\n",
      "#                 save_method(\n",
      "#                     os.path.join(save_directory, pipeline_component_name), params=params[pipeline_component_name]\n",
      "#                 )\n",
      "#             else:\n",
      "#                 save_method(os.path.join(save_directory, pipeline_component_name))\n",
      "# --------------------------------------------------\n",
      "# the below code fragment can be found in:\n",
      "# src/diffusers/pipelines/pipeline_flax_utils.py\n",
      "# --------------------------------------------------\n",
      "# \n",
      "#             save_method = getattr(sub_model, save_method_name)\n",
      "#             expects_params = \"params\" in set(inspect.signature(save_method).parameters.keys())\n",
      "# \n",
      "#             if expects_params:\n",
      "#                 save_method(\n",
      "#                     os.path.join(save_directory, pipeline_component_name), params=params[pipeline_component_name]\n",
      "#                 )\n",
      "#             else:\n",
      "#                 save_method(os.path.join(save_directory, pipeline_component_name))\n",
      "# \n",
      "#     @classmethod\n",
      "#     def from_pretrained(cls, pretrained_model_name_or_path: Optional[Union[str, os.PathLike]], **kwargs):\n",
      "#         r\"\"\"\n",
      "#         Instantiate a Flax diffusion pipeline from pre-trained pipeline weights.\n",
      "# \n",
      "#         The pipeline is set in evaluation mode by default using `model.eval()` (Dropout modules are deactivated).\n",
      "# \n",
      "#         The warning *Weights from XXX not initialized from pretrained model* means that the weights of XXX do not come\n",
      "#         pretrained with the rest of the model. It is up to you to train those weights with a downstream fine-tuning\n",
      "# --------------------------------------------------\n",
      "# the below code fragment can be found in:\n",
      "# src/diffusers/pipelines/pipeline_flax_utils.py\n",
      "# --------------------------------------------------\n",
      "#                 for base_class, save_load_methods in library_classes.items():\n",
      "#                     class_candidate = getattr(library, base_class, None)\n",
      "#                     if class_candidate is not None and issubclass(model_cls, class_candidate):\n",
      "#                         # if we found a suitable base class in LOADABLE_CLASSES then grab its save method\n",
      "#                         save_method_name = save_load_methods[0]\n",
      "#                         break\n",
      "#                 if save_method_name is not None:\n",
      "#                     break\n",
      "# \n",
      "#             save_method = getattr(sub_model, save_method_name)\n",
      "#             expects_params = \"params\" in set(inspect.signature(save_method).parameters.keys())\n",
      "# \n",
      "#             if expects_params:\n",
      "#                 save_method(\n",
      "#                     os.path.join(save_directory, pipeline_component_name), params=params[pipeline_component_name]\n",
      "#                 )\n",
      "#             else:\n",
      "#                 save_method(os.path.join(save_directory, pipeline_component_name))\n",
      "# \n",
      "#     @classmethod\n",
      "# --------------------------------------------------\n",
      "# the below code fragment can be found in:\n",
      "# src/diffusers/pipelines/pipeline_flax_utils.py\n",
      "# --------------------------------------------------\n",
      "#                     if class_candidate is not None and issubclass(model_cls, class_candidate):\n",
      "#                         # if we found a suitable base class in LOADABLE_CLASSES then grab its save method\n",
      "#                         save_method_name = save_load_methods[0]\n",
      "#                         break\n",
      "#                 if save_method_name is not None:\n",
      "#                     break\n",
      "# \n",
      "#             save_method = getattr(sub_model, save_method_name)\n",
      "#             expects_params = \"params\" in set(inspect.signature(save_method).parameters.keys())\n",
      "# \n",
      "#             if expects_params:\n",
      "#                 save_method(\n",
      "#                     os.path.join(save_directory, pipeline_component_name), params=params[pipeline_component_name]\n",
      "#                 )\n",
      "#             else:\n",
      "#                 save_method(os.path.join(save_directory, pipeline_component_name))\n",
      "# \n",
      "#     @classmethod\n",
      "#     def from_pretrained(cls, pretrained_model_name_or_path: Optional[Union[str, os.PathLike]], **kwargs):\n",
      "#         r\"\"\"\n",
      "# --------------------------------------------------\n",
      "# the below code fragment can be found in:\n",
      "# src/diffusers/pipelines/pipeline_flax_utils.py\n",
      "# --------------------------------------------------\n",
      "#                         save_method_name = save_load_methods[0]\n",
      "#                         break\n",
      "#                 if save_method_name is not None:\n",
      "#                     break\n",
      "# \n",
      "#             save_method = getattr(sub_model, save_method_name)\n",
      "#             expects_params = \"params\" in set(inspect.signature(save_method).parameters.keys())\n",
      "# \n",
      "#             if expects_params:\n",
      "#                 save_method(\n",
      "#                     os.path.join(save_directory, pipeline_component_name), params=params[pipeline_component_name]\n",
      "#                 )\n",
      "#             else:\n",
      "#                 save_method(os.path.join(save_directory, pipeline_component_name))\n",
      "# \n",
      "#     @classmethod\n",
      "#     def from_pretrained(cls, pretrained_model_name_or_path: Optional[Union[str, os.PathLike]], **kwargs):\n",
      "#         r\"\"\"\n",
      "#         Instantiate a Flax diffusion pipeline from pre-trained pipeline weights.\n",
      "# \n",
      "# --------------------------------------------------\n",
      "# the below code fragment can be found in:\n",
      "# src/diffusers/pipelines/pipeline_flax_utils.py\n",
      "# --------------------------------------------------\n",
      "#                 if save_method_name is not None:\n",
      "#                     break\n",
      "# \n",
      "#             save_method = getattr(sub_model, save_method_name)\n",
      "#             expects_params = \"params\" in set(inspect.signature(save_method).parameters.keys())\n",
      "# \n",
      "#             if expects_params:\n",
      "#                 save_method(\n",
      "#                     os.path.join(save_directory, pipeline_component_name), params=params[pipeline_component_name]\n",
      "#                 )\n",
      "#             else:\n",
      "#                 save_method(os.path.join(save_directory, pipeline_component_name))\n",
      "# \n",
      "#     @classmethod\n",
      "#     def from_pretrained(cls, pretrained_model_name_or_path: Optional[Union[str, os.PathLike]], **kwargs):\n",
      "#         r\"\"\"\n",
      "#         Instantiate a Flax diffusion pipeline from pre-trained pipeline weights.\n",
      "# \n",
      "#         The pipeline is set in evaluation mode by default using `model.eval()` (Dropout modules are deactivated).\n",
      "# \n",
      "# --------------------------------------------------\n",
      "\"\"\"Based on the above, complete the following code:\"\"\"\n",
      "\n",
      "rained\"],\n",
      "        \"ProcessorMixin\": [\"save_pretrained\", \"from_pretrained\"],\n",
      "        \"ImageProcessingMixin\": [\"save_pretrained\", \"from_pretrained\"],\n",
      "    },\n",
      "    \"onnxruntime.training\": {\n",
      "        \"ORTModule\": [\"save_pretrained\", \"from_pretrained\"],\n",
      "    },\n",
      "}\n",
      "\n",
      "ALL_IMPORTABLE_CLASSES = {}\n",
      "for library in LOADABLE_CLASSES:\n",
      "    ALL_IMPORTABLE_CLASSES.update(LOADABLE_CLASSES[library])\n",
      "\n",
      "\n",
      "@dataclass\n",
      "class ImagePipelineOutput(BaseOutput):\n",
      "    \"\"\"\n",
      "    Output class for image pipelines.\n",
      "\n",
      "    Args:\n",
      "        images (`List[PIL.Image.Image]` or `np.ndarray`)\n",
      "            List of denoised PIL images of length `batch_size` or numpy array of shape `(batch_size, height, width,\n",
      "            num_channels)`. PIL images or numpy array present the denoised images of the diffusion pipeline.\n",
      "    \"\"\"\n",
      "\n",
      "    images: Union[List[PIL.Image.Image], np.ndarray]\n",
      "\n",
      "\n",
      "@dataclass\n",
      "class AudioPipelineOutput(BaseOutput):\n",
      "    \"\"\"\n",
      "    Output class for audio pipelines.\n",
      "\n",
      "    Args:\n",
      "        audios (`np.ndarray`)\n",
      "            List of denoised samples of shape `(batch_size, num_channels, sample_rate)`. Numpy array present the\n",
      "            denoised audio samples of the diffusion pipeline.\n",
      "    \"\"\"\n",
      "\n",
      "    audios: np.ndarray\n",
      "\n",
      "\n",
      "def is_safetensors_compatible(info) -> bool:\n",
      "    filenames = set(sibling.rfilename for sibling in info.siblings)\n",
      "    pt_filenames = set(filename for filename in filenames if filename.endswith(\".bin\"))\n",
      "    is_safetensors_compatible = any(file.endswith(\".safetensors\") for file in filenames)\n",
      "    for pt_filename in pt_filenames:\n",
      "        prefix, raw = os.path.split(pt_filename)\n",
      "        if raw == \"pytorch_model.bin\":\n",
      "            # transformers specific\n",
      "            sf_filename = os.path.join(prefix, \"model.safetensors\")\n",
      "        else:\n",
      "            sf_filename = pt_filename[: -len(\".bin\")] + \".safetensors\"\n",
      "        if is_safetensors_compatible and sf_filename not in filenames:\n",
      "            logger.warning(f\"{sf_filename} not found\")\n",
      "            is_safetensors_compatible = False\n",
      "    return is_safetensors_compatible\n",
      "\n",
      "\n",
      "class DiffusionPipeline(ConfigMixin):\n",
      "    r\"\"\"\n",
      "    Base class for all models.\n",
      "\n",
      "    [`DiffusionPipeline`] takes care of storing all components (models, schedulers, processors) for diffusion pipelines\n",
      "    and handles methods for loading, downloading and saving models as well as a few methods common to all pipelines to:\n",
      "\n",
      "        - move all PyTorch modules to the device of your choice\n",
      "        - enabling/disabling the progress bar for the denoising iteration\n",
      "\n",
      "    Class attributes:\n",
      "\n",
      "        - **config_name** (`str`) -- name of the config file that will store the class and module names of all\n",
      "          components of the diffusion pipeline.\n",
      "        - **_optional_components** (List[`str`]) -- list of all components that are optional so they don't have to be\n",
      "          passed for the pipeline to function (should be overridden by subclasses).\n",
      "    \"\"\"\n",
      "    config_name = \"model_index.json\"\n",
      "    _optional_components = []\n",
      "\n",
      "    def register_modules(self, **kwargs):\n",
      "        # import it here to avoid circular import\n",
      "        from diffusers import pipelines\n",
      "\n",
      "        for name, module in kwargs.items():\n",
      "            # retrieve library\n",
      "            if module is None:\n",
      "                register_dict = {name: (None, None)}\n",
      "            else:\n",
      "                library = module.__module__.split(\".\")[0]\n",
      "\n",
      "                # check if the module is a pipeline module\n",
      "                pipeline_dir = module.__module__.split(\".\")[-2] if len(module.__module__.split(\".\")) > 2 else None\n",
      "                path = module.__module__.split(\".\")\n",
      "                is_pipeline_module = pipeline_dir in path and hasattr(pipelines, pipeline_dir)\n",
      "\n",
      "                # if library is not in LOADABLE_CLASSES, then it is a custom module.\n",
      "                # Or if it's a pipeline module, then the module is inside the pipeline\n",
      "                # folder so we set the library to module name.\n",
      "                if library not in LOADABLE_CLASSES or is_pipeline_module:\n",
      "                    library = pipeline_dir\n",
      "\n",
      "                # retrieve class_name\n",
      "                class_name = module.__class__.__name__\n",
      "\n",
      "                register_dict = {name: (library, class_name)}\n",
      "\n",
      "            # save model index config\n",
      "            self.register_to_config(**register_dict)\n",
      "\n",
      "            # set models\n",
      "            setattr(self, name, module)\n",
      "\n",
      "    def save_pretrained(\n",
      "        self,\n",
      "        save_directory: Union[str, os.PathLike],\n",
      "        safe_serialization: bool = False,\n",
      "    ):\n",
      "        \"\"\"\n",
      "        Save all variables of the pipeline that can be saved and loaded as well as the pipelines configuration file to\n",
      "        a directory. A pipeline variable can be saved and loaded if its class implements both a save and loading\n",
      "        method. The pipeline can easily be re-loaded using the `[`~DiffusionPipeline.from_pretrained`]` class method.\n",
      "\n",
      "        Arguments:\n",
      "            save_directory (`str` or `os.PathLike`):\n",
      "                Directory to which to save. Will be created if it doesn't exist.\n",
      "            safe_serialization (`bool`, *optional*, defaults to `False`):\n",
      "                Whether to save the model using `safetensors` or the traditional PyTorch way (that uses `pickle`).\n",
      "        \"\"\"\n",
      "        self.save_config(save_directory)\n",
      "\n",
      "        model_index_dict = dict(self.config)\n",
      "        model_index_dict.pop(\"_class_name\")\n",
      "        model_index_dict.pop(\"_diffusers_version\")\n",
      "        model_index_dict.pop(\"_module\", None)\n",
      "\n",
      "        expected_modules, optional_kwargs = self._get_signature_keys(self)\n",
      "\n",
      "        def is_saveable_module(name, value):\n",
      "            if name not in expected_modules:\n",
      "                return False\n",
      "            if name in self._optional_components and value[0] is None:\n",
      "                return False\n",
      "            return True\n",
      "\n",
      "        model_index_dict = {k: v for k, v in model_index_dict.items() if is_saveable_module(k, v)}\n",
      "\n",
      "        for pipeline_component_name in model_index_dict.keys():\n",
      "            sub_model = getattr(self, pipeline_component_name)\n",
      "            model_cls = sub_model.__class__\n",
      "\n",
      "            save_method_name = None\n",
      "            # search for the model's base class in LOADABLE_CLASSES\n",
      "            for library_name, library_classes in LOADABLE_CLASSES.items():\n",
      "                library = importlib.import_module(library_name)\n",
      "                for base_class, save_load_methods in library_classes.items():\n",
      "                    class_candidate = getattr(library, base_class, None)\n",
      "                    if class_candidate is not None and issubclass(model_cls, class_candidate):\n",
      "                        # if we found a suitable base class in LOADABLE_CLASSES then grab its save method\n",
      "                        save_method_name = save_load_methods[0]\n",
      "                        break\n",
      "                if save_method_name is not None:\n",
      "                    break\n",
      "\n",
      "            save_method = getattr(sub_model, save_method_name)\n",
      "\n",
      "            # Call the save method with the argument safe_serialization only if it's supported\n",
      "            save_method_signature = inspect.signature(save_method)\n",
      "            save_method_accept_safe = \"safe_serialization\" in save_method_signature.parameters\n",
      "            if save_method_accept_safe:\n",
      "                save_method(\n",
      "                    os.path.join(save_directory, pipeline_component_name), safe_serialization=safe_serialization\n",
      "                )\n",
      "            else:\n",
      "                save_method(os.path.join(save_directory, pipeline_component_name))\n",
      "\n",
      "    def to(self, torch_device: Optional[Union[str, torch.device]] = None):\n",
      "        if torch_device is None:\n",
      "            return self\n"
     ]
    }
   ],
   "source": [
    "## View Prompt Examples\n",
    "import json\n",
    "\n",
    "# Function to read a JSON Lines file and return the prompt of a desired line\n",
    "def get_prompt_at_line(jsonl_file_path, desired_line_no):\n",
    "    with open(jsonl_file_path, 'r') as file:\n",
    "        for line_no, line in enumerate(file, start=1):\n",
    "            if line_no == desired_line_no:\n",
    "                json_object = json.loads(line)\n",
    "                prompt = json_object.get('prompt', None)\n",
    "                return prompt\n",
    "    return None  # Return None if the desired line was not found\n",
    "\n",
    "# Specify the .jsonl file path and the desired line number\n",
    "jsonl_file_path = 'subsets/rg-one-gram-ws-20-ss-2_0.1_paper.jsonl'\n",
    "desired_line_no = 2 # For example, we want the prompt at line 10\n",
    "\n",
    "# Get the prompt at the desired line\n",
    "prompt_at_desired_line = get_prompt_at_line(jsonl_file_path, desired_line_no)\n",
    "\n",
    "if prompt_at_desired_line:\n",
    "    print(prompt_at_desired_line)\n",
    "else:\n",
    "    print(f\"No prompt found at line {desired_line_no}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
