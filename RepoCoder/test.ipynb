{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Here are some relevant code fragments from other files of the repo:\n",
      "# --------------------------------------------------\n",
      "# the below code fragment can be found in:\n",
      "# src/diffusers/pipelines/pipeline_flax_utils.py\n",
      "# --------------------------------------------------\n",
      "#             for library_name, library_classes in LOADABLE_CLASSES.items():\n",
      "#                 library = importlib.import_module(library_name)\n",
      "#                 for base_class, save_load_methods in library_classes.items():\n",
      "#                     class_candidate = getattr(library, base_class, None)\n",
      "#                     if class_candidate is not None and issubclass(model_cls, class_candidate):\n",
      "#                         # if we found a suitable base class in LOADABLE_CLASSES then grab its save method\n",
      "#                         save_method_name = save_load_methods[0]\n",
      "#                         break\n",
      "#                 if save_method_name is not None:\n",
      "#                     break\n",
      "# \n",
      "#             save_method = getattr(sub_model, save_method_name)\n",
      "#             expects_params = \"params\" in set(inspect.signature(save_method).parameters.keys())\n",
      "# \n",
      "#             if expects_params:\n",
      "#                 save_method(\n",
      "#                     os.path.join(save_directory, pipeline_component_name), params=params[pipeline_component_name]\n",
      "#                 )\n",
      "#             else:\n",
      "#                 save_method(os.path.join(save_directory, pipeline_component_name))\n",
      "# --------------------------------------------------\n",
      "# the below code fragment can be found in:\n",
      "# src/diffusers/pipelines/pipeline_flax_utils.py\n",
      "# --------------------------------------------------\n",
      "# \n",
      "#             save_method = getattr(sub_model, save_method_name)\n",
      "#             expects_params = \"params\" in set(inspect.signature(save_method).parameters.keys())\n",
      "# \n",
      "#             if expects_params:\n",
      "#                 save_method(\n",
      "#                     os.path.join(save_directory, pipeline_component_name), params=params[pipeline_component_name]\n",
      "#                 )\n",
      "#             else:\n",
      "#                 save_method(os.path.join(save_directory, pipeline_component_name))\n",
      "# \n",
      "#     @classmethod\n",
      "#     def from_pretrained(cls, pretrained_model_name_or_path: Optional[Union[str, os.PathLike]], **kwargs):\n",
      "#         r\"\"\"\n",
      "#         Instantiate a Flax diffusion pipeline from pre-trained pipeline weights.\n",
      "# \n",
      "#         The pipeline is set in evaluation mode by default using `model.eval()` (Dropout modules are deactivated).\n",
      "# \n",
      "#         The warning *Weights from XXX not initialized from pretrained model* means that the weights of XXX do not come\n",
      "#         pretrained with the rest of the model. It is up to you to train those weights with a downstream fine-tuning\n",
      "# --------------------------------------------------\n",
      "# the below code fragment can be found in:\n",
      "# src/diffusers/pipelines/pipeline_flax_utils.py\n",
      "# --------------------------------------------------\n",
      "#                 for base_class, save_load_methods in library_classes.items():\n",
      "#                     class_candidate = getattr(library, base_class, None)\n",
      "#                     if class_candidate is not None and issubclass(model_cls, class_candidate):\n",
      "#                         # if we found a suitable base class in LOADABLE_CLASSES then grab its save method\n",
      "#                         save_method_name = save_load_methods[0]\n",
      "#                         break\n",
      "#                 if save_method_name is not None:\n",
      "#                     break\n",
      "# \n",
      "#             save_method = getattr(sub_model, save_method_name)\n",
      "#             expects_params = \"params\" in set(inspect.signature(save_method).parameters.keys())\n",
      "# \n",
      "#             if expects_params:\n",
      "#                 save_method(\n",
      "#                     os.path.join(save_directory, pipeline_component_name), params=params[pipeline_component_name]\n",
      "#                 )\n",
      "#             else:\n",
      "#                 save_method(os.path.join(save_directory, pipeline_component_name))\n",
      "# \n",
      "#     @classmethod\n",
      "# --------------------------------------------------\n",
      "# the below code fragment can be found in:\n",
      "# src/diffusers/pipelines/pipeline_flax_utils.py\n",
      "# --------------------------------------------------\n",
      "#                     if class_candidate is not None and issubclass(model_cls, class_candidate):\n",
      "#                         # if we found a suitable base class in LOADABLE_CLASSES then grab its save method\n",
      "#                         save_method_name = save_load_methods[0]\n",
      "#                         break\n",
      "#                 if save_method_name is not None:\n",
      "#                     break\n",
      "# \n",
      "#             save_method = getattr(sub_model, save_method_name)\n",
      "#             expects_params = \"params\" in set(inspect.signature(save_method).parameters.keys())\n",
      "# \n",
      "#             if expects_params:\n",
      "#                 save_method(\n",
      "#                     os.path.join(save_directory, pipeline_component_name), params=params[pipeline_component_name]\n",
      "#                 )\n",
      "#             else:\n",
      "#                 save_method(os.path.join(save_directory, pipeline_component_name))\n",
      "# \n",
      "#     @classmethod\n",
      "#     def from_pretrained(cls, pretrained_model_name_or_path: Optional[Union[str, os.PathLike]], **kwargs):\n",
      "#         r\"\"\"\n",
      "# --------------------------------------------------\n",
      "# the below code fragment can be found in:\n",
      "# src/diffusers/pipelines/pipeline_flax_utils.py\n",
      "# --------------------------------------------------\n",
      "#                         save_method_name = save_load_methods[0]\n",
      "#                         break\n",
      "#                 if save_method_name is not None:\n",
      "#                     break\n",
      "# \n",
      "#             save_method = getattr(sub_model, save_method_name)\n",
      "#             expects_params = \"params\" in set(inspect.signature(save_method).parameters.keys())\n",
      "# \n",
      "#             if expects_params:\n",
      "#                 save_method(\n",
      "#                     os.path.join(save_directory, pipeline_component_name), params=params[pipeline_component_name]\n",
      "#                 )\n",
      "#             else:\n",
      "#                 save_method(os.path.join(save_directory, pipeline_component_name))\n",
      "# \n",
      "#     @classmethod\n",
      "#     def from_pretrained(cls, pretrained_model_name_or_path: Optional[Union[str, os.PathLike]], **kwargs):\n",
      "#         r\"\"\"\n",
      "#         Instantiate a Flax diffusion pipeline from pre-trained pipeline weights.\n",
      "# \n",
      "# --------------------------------------------------\n",
      "# the below code fragment can be found in:\n",
      "# src/diffusers/pipelines/pipeline_flax_utils.py\n",
      "# --------------------------------------------------\n",
      "#                 if save_method_name is not None:\n",
      "#                     break\n",
      "# \n",
      "#             save_method = getattr(sub_model, save_method_name)\n",
      "#             expects_params = \"params\" in set(inspect.signature(save_method).parameters.keys())\n",
      "# \n",
      "#             if expects_params:\n",
      "#                 save_method(\n",
      "#                     os.path.join(save_directory, pipeline_component_name), params=params[pipeline_component_name]\n",
      "#                 )\n",
      "#             else:\n",
      "#                 save_method(os.path.join(save_directory, pipeline_component_name))\n",
      "# \n",
      "#     @classmethod\n",
      "#     def from_pretrained(cls, pretrained_model_name_or_path: Optional[Union[str, os.PathLike]], **kwargs):\n",
      "#         r\"\"\"\n",
      "#         Instantiate a Flax diffusion pipeline from pre-trained pipeline weights.\n",
      "# \n",
      "#         The pipeline is set in evaluation mode by default using `model.eval()` (Dropout modules are deactivated).\n",
      "# \n",
      "# --------------------------------------------------\n",
      "\"\"\"Based on the above, complete the following code:\"\"\"\n",
      "\n",
      "rained\"],\n",
      "        \"ProcessorMixin\": [\"save_pretrained\", \"from_pretrained\"],\n",
      "        \"ImageProcessingMixin\": [\"save_pretrained\", \"from_pretrained\"],\n",
      "    },\n",
      "    \"onnxruntime.training\": {\n",
      "        \"ORTModule\": [\"save_pretrained\", \"from_pretrained\"],\n",
      "    },\n",
      "}\n",
      "\n",
      "ALL_IMPORTABLE_CLASSES = {}\n",
      "for library in LOADABLE_CLASSES:\n",
      "    ALL_IMPORTABLE_CLASSES.update(LOADABLE_CLASSES[library])\n",
      "\n",
      "\n",
      "@dataclass\n",
      "class ImagePipelineOutput(BaseOutput):\n",
      "    \"\"\"\n",
      "    Output class for image pipelines.\n",
      "\n",
      "    Args:\n",
      "        images (`List[PIL.Image.Image]` or `np.ndarray`)\n",
      "            List of denoised PIL images of length `batch_size` or numpy array of shape `(batch_size, height, width,\n",
      "            num_channels)`. PIL images or numpy array present the denoised images of the diffusion pipeline.\n",
      "    \"\"\"\n",
      "\n",
      "    images: Union[List[PIL.Image.Image], np.ndarray]\n",
      "\n",
      "\n",
      "@dataclass\n",
      "class AudioPipelineOutput(BaseOutput):\n",
      "    \"\"\"\n",
      "    Output class for audio pipelines.\n",
      "\n",
      "    Args:\n",
      "        audios (`np.ndarray`)\n",
      "            List of denoised samples of shape `(batch_size, num_channels, sample_rate)`. Numpy array present the\n",
      "            denoised audio samples of the diffusion pipeline.\n",
      "    \"\"\"\n",
      "\n",
      "    audios: np.ndarray\n",
      "\n",
      "\n",
      "def is_safetensors_compatible(info) -> bool:\n",
      "    filenames = set(sibling.rfilename for sibling in info.siblings)\n",
      "    pt_filenames = set(filename for filename in filenames if filename.endswith(\".bin\"))\n",
      "    is_safetensors_compatible = any(file.endswith(\".safetensors\") for file in filenames)\n",
      "    for pt_filename in pt_filenames:\n",
      "        prefix, raw = os.path.split(pt_filename)\n",
      "        if raw == \"pytorch_model.bin\":\n",
      "            # transformers specific\n",
      "            sf_filename = os.path.join(prefix, \"model.safetensors\")\n",
      "        else:\n",
      "            sf_filename = pt_filename[: -len(\".bin\")] + \".safetensors\"\n",
      "        if is_safetensors_compatible and sf_filename not in filenames:\n",
      "            logger.warning(f\"{sf_filename} not found\")\n",
      "            is_safetensors_compatible = False\n",
      "    return is_safetensors_compatible\n",
      "\n",
      "\n",
      "class DiffusionPipeline(ConfigMixin):\n",
      "    r\"\"\"\n",
      "    Base class for all models.\n",
      "\n",
      "    [`DiffusionPipeline`] takes care of storing all components (models, schedulers, processors) for diffusion pipelines\n",
      "    and handles methods for loading, downloading and saving models as well as a few methods common to all pipelines to:\n",
      "\n",
      "        - move all PyTorch modules to the device of your choice\n",
      "        - enabling/disabling the progress bar for the denoising iteration\n",
      "\n",
      "    Class attributes:\n",
      "\n",
      "        - **config_name** (`str`) -- name of the config file that will store the class and module names of all\n",
      "          components of the diffusion pipeline.\n",
      "        - **_optional_components** (List[`str`]) -- list of all components that are optional so they don't have to be\n",
      "          passed for the pipeline to function (should be overridden by subclasses).\n",
      "    \"\"\"\n",
      "    config_name = \"model_index.json\"\n",
      "    _optional_components = []\n",
      "\n",
      "    def register_modules(self, **kwargs):\n",
      "        # import it here to avoid circular import\n",
      "        from diffusers import pipelines\n",
      "\n",
      "        for name, module in kwargs.items():\n",
      "            # retrieve library\n",
      "            if module is None:\n",
      "                register_dict = {name: (None, None)}\n",
      "            else:\n",
      "                library = module.__module__.split(\".\")[0]\n",
      "\n",
      "                # check if the module is a pipeline module\n",
      "                pipeline_dir = module.__module__.split(\".\")[-2] if len(module.__module__.split(\".\")) > 2 else None\n",
      "                path = module.__module__.split(\".\")\n",
      "                is_pipeline_module = pipeline_dir in path and hasattr(pipelines, pipeline_dir)\n",
      "\n",
      "                # if library is not in LOADABLE_CLASSES, then it is a custom module.\n",
      "                # Or if it's a pipeline module, then the module is inside the pipeline\n",
      "                # folder so we set the library to module name.\n",
      "                if library not in LOADABLE_CLASSES or is_pipeline_module:\n",
      "                    library = pipeline_dir\n",
      "\n",
      "                # retrieve class_name\n",
      "                class_name = module.__class__.__name__\n",
      "\n",
      "                register_dict = {name: (library, class_name)}\n",
      "\n",
      "            # save model index config\n",
      "            self.register_to_config(**register_dict)\n",
      "\n",
      "            # set models\n",
      "            setattr(self, name, module)\n",
      "\n",
      "    def save_pretrained(\n",
      "        self,\n",
      "        save_directory: Union[str, os.PathLike],\n",
      "        safe_serialization: bool = False,\n",
      "    ):\n",
      "        \"\"\"\n",
      "        Save all variables of the pipeline that can be saved and loaded as well as the pipelines configuration file to\n",
      "        a directory. A pipeline variable can be saved and loaded if its class implements both a save and loading\n",
      "        method. The pipeline can easily be re-loaded using the `[`~DiffusionPipeline.from_pretrained`]` class method.\n",
      "\n",
      "        Arguments:\n",
      "            save_directory (`str` or `os.PathLike`):\n",
      "                Directory to which to save. Will be created if it doesn't exist.\n",
      "            safe_serialization (`bool`, *optional*, defaults to `False`):\n",
      "                Whether to save the model using `safetensors` or the traditional PyTorch way (that uses `pickle`).\n",
      "        \"\"\"\n",
      "        self.save_config(save_directory)\n",
      "\n",
      "        model_index_dict = dict(self.config)\n",
      "        model_index_dict.pop(\"_class_name\")\n",
      "        model_index_dict.pop(\"_diffusers_version\")\n",
      "        model_index_dict.pop(\"_module\", None)\n",
      "\n",
      "        expected_modules, optional_kwargs = self._get_signature_keys(self)\n",
      "\n",
      "        def is_saveable_module(name, value):\n",
      "            if name not in expected_modules:\n",
      "                return False\n",
      "            if name in self._optional_components and value[0] is None:\n",
      "                return False\n",
      "            return True\n",
      "\n",
      "        model_index_dict = {k: v for k, v in model_index_dict.items() if is_saveable_module(k, v)}\n",
      "\n",
      "        for pipeline_component_name in model_index_dict.keys():\n",
      "            sub_model = getattr(self, pipeline_component_name)\n",
      "            model_cls = sub_model.__class__\n",
      "\n",
      "            save_method_name = None\n",
      "            # search for the model's base class in LOADABLE_CLASSES\n",
      "            for library_name, library_classes in LOADABLE_CLASSES.items():\n",
      "                library = importlib.import_module(library_name)\n",
      "                for base_class, save_load_methods in library_classes.items():\n",
      "                    class_candidate = getattr(library, base_class, None)\n",
      "                    if class_candidate is not None and issubclass(model_cls, class_candidate):\n",
      "                        # if we found a suitable base class in LOADABLE_CLASSES then grab its save method\n",
      "                        save_method_name = save_load_methods[0]\n",
      "                        break\n",
      "                if save_method_name is not None:\n",
      "                    break\n",
      "\n",
      "            save_method = getattr(sub_model, save_method_name)\n",
      "\n",
      "            # Call the save method with the argument safe_serialization only if it's supported\n",
      "            save_method_signature = inspect.signature(save_method)\n",
      "            save_method_accept_safe = \"safe_serialization\" in save_method_signature.parameters\n",
      "            if save_method_accept_safe:\n",
      "                save_method(\n",
      "                    os.path.join(save_directory, pipeline_component_name), safe_serialization=safe_serialization\n",
      "                )\n",
      "            else:\n",
      "                save_method(os.path.join(save_directory, pipeline_component_name))\n",
      "\n",
      "    def to(self, torch_device: Optional[Union[str, torch.device]] = None):\n",
      "        if torch_device is None:\n",
      "            return self\n"
     ]
    }
   ],
   "source": [
    "## View Prompt Examples\n",
    "import json\n",
    "\n",
    "# Function to read a JSON Lines file and return the prompt of a desired line\n",
    "def get_prompt_at_line(jsonl_file_path, desired_line_no):\n",
    "    with open(jsonl_file_path, 'r') as file:\n",
    "        for line_no, line in enumerate(file, start=1):\n",
    "            if line_no == desired_line_no:\n",
    "                json_object = json.loads(line)\n",
    "                prompt = json_object.get('prompt', None)\n",
    "                return prompt\n",
    "    return None  # Return None if the desired line was not found\n",
    "\n",
    "# Specify the .jsonl file path and the desired line number\n",
    "jsonl_file_path = 'subsets/rg-one-gram-ws-20-ss-2_0.1_paper.jsonl'\n",
    "desired_line_no = 2 # For example, we want the prompt at line 10\n",
    "\n",
    "# Get the prompt at the desired line\n",
    "prompt_at_desired_line = get_prompt_at_line(jsonl_file_path, desired_line_no)\n",
    "\n",
    "if prompt_at_desired_line:\n",
    "    print(prompt_at_desired_line)\n",
    "else:\n",
    "    print(f\"No prompt found at line {desired_line_no}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai==0.28\n",
      "  Downloading openai-0.28.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting requests>=2.20 (from openai==0.28)\n",
      "  Using cached requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting tqdm (from openai==0.28)\n",
      "  Using cached tqdm-4.66.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting aiohttp (from openai==0.28)\n",
      "  Downloading aiohttp-3.9.1-cp311-cp311-win_amd64.whl.metadata (7.6 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests>=2.20->openai==0.28)\n",
      "  Downloading charset_normalizer-3.3.2-cp311-cp311-win_amd64.whl.metadata (34 kB)\n",
      "Collecting idna<4,>=2.5 (from requests>=2.20->openai==0.28)\n",
      "  Using cached idna-3.6-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests>=2.20->openai==0.28)\n",
      "  Using cached urllib3-2.1.0-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests>=2.20->openai==0.28)\n",
      "  Using cached certifi-2023.11.17-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp->openai==0.28)\n",
      "  Downloading attrs-23.2.0-py3-none-any.whl.metadata (9.5 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->openai==0.28)\n",
      "  Downloading multidict-6.0.4-cp311-cp311-win_amd64.whl (28 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp->openai==0.28)\n",
      "  Downloading yarl-1.9.4-cp311-cp311-win_amd64.whl.metadata (32 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->openai==0.28)\n",
      "  Downloading frozenlist-1.4.1-cp311-cp311-win_amd64.whl.metadata (12 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->openai==0.28)\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\soren\\documents\\github\\codet\\repocoder\\.conda\\lib\\site-packages (from tqdm->openai==0.28) (0.4.6)\n",
      "Downloading openai-0.28.0-py3-none-any.whl (76 kB)\n",
      "   ---------------------------------------- 0.0/76.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 76.5/76.5 kB 1.4 MB/s eta 0:00:00\n",
      "Using cached requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "Downloading aiohttp-3.9.1-cp311-cp311-win_amd64.whl (364 kB)\n",
      "   ---------------------------------------- 0.0/364.8 kB ? eta -:--:--\n",
      "   ------------------------ --------------- 225.3/364.8 kB 4.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  358.4/364.8 kB 3.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 364.8/364.8 kB 3.3 MB/s eta 0:00:00\n",
      "Using cached tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
      "Downloading attrs-23.2.0-py3-none-any.whl (60 kB)\n",
      "   ---------------------------------------- 0.0/60.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 60.8/60.8 kB 3.2 MB/s eta 0:00:00\n",
      "Using cached certifi-2023.11.17-py3-none-any.whl (162 kB)\n",
      "Downloading charset_normalizer-3.3.2-cp311-cp311-win_amd64.whl (99 kB)\n",
      "   ---------------------------------------- 0.0/99.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 99.9/99.9 kB 2.9 MB/s eta 0:00:00\n",
      "Downloading frozenlist-1.4.1-cp311-cp311-win_amd64.whl (50 kB)\n",
      "   ---------------------------------------- 0.0/50.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 50.5/50.5 kB 1.3 MB/s eta 0:00:00\n",
      "Using cached idna-3.6-py3-none-any.whl (61 kB)\n",
      "Using cached urllib3-2.1.0-py3-none-any.whl (104 kB)\n",
      "Downloading yarl-1.9.4-cp311-cp311-win_amd64.whl (76 kB)\n",
      "   ---------------------------------------- 0.0/76.7 kB ? eta -:--:--\n",
      "   ---------------------------------------- 76.7/76.7 kB 2.1 MB/s eta 0:00:00\n",
      "Installing collected packages: urllib3, tqdm, multidict, idna, frozenlist, charset-normalizer, certifi, attrs, yarl, requests, aiosignal, aiohttp, openai\n",
      "Successfully installed aiohttp-3.9.1 aiosignal-1.3.1 attrs-23.2.0 certifi-2023.11.17 charset-normalizer-3.3.2 frozenlist-1.4.1 idna-3.6 multidict-6.0.4 openai-0.28.0 requests-2.31.0 tqdm-4.66.1 urllib3-2.1.0 yarl-1.9.4\n"
     ]
    }
   ],
   "source": [
    "!pip install openai==0.28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = '''\n",
    "# Here are some relevant code fragments from other files of the repo:\n",
    "# --------------------------------------------------\n",
    "# the below code fragment can be found in:\n",
    "# src/diffusers/configuration_utils.py\n",
    "# --------------------------------------------------\n",
    "# \n",
    "#         if cls.has_compatibles:\n",
    "#             compatible_classes = [c for c in cls._get_compatibles() if not isinstance(c, DummyObject)]\n",
    "#         else:\n",
    "#             compatible_classes = []\n",
    "# \n",
    "#         expected_keys_comp_cls = set()\n",
    "#         for c in compatible_classes:\n",
    "#             expected_keys_c = cls._get_init_keys(c)\n",
    "#             expected_keys_comp_cls = expected_keys_comp_cls.union(expected_keys_c)\n",
    "#         expected_keys_comp_cls = expected_keys_comp_cls - cls._get_init_keys(cls)\n",
    "#         config_dict = {k: v for k, v in config_dict.items() if k not in expected_keys_comp_cls}\n",
    "# \n",
    "#         # remove attributes from orig class that cannot be expected\n",
    "#         orig_cls_name = config_dict.pop(\"_class_name\", cls.__name__)\n",
    "#         if orig_cls_name != cls.__name__ and hasattr(diffusers_library, orig_cls_name):\n",
    "#             orig_cls = getattr(diffusers_library, orig_cls_name)\n",
    "#             unexpected_keys_from_orig = cls._get_init_keys(orig_cls) - expected_keys\n",
    "#             config_dict = {k: v for k, v in config_dict.items() if k not in unexpected_keys_from_orig}\n",
    "#         # load diffusers library to import compatible and original scheduler\n",
    "#         diffusers_library = importlib.import_module(__name__.split(\".\")[0])\n",
    "# \n",
    "#         if cls.has_compatibles:\n",
    "#             compatible_classes = [c for c in cls._get_compatibles() if not isinstance(c, DummyObject)]\n",
    "#         else:\n",
    "#             compatible_classes = []\n",
    "# \n",
    "#         expected_keys_comp_cls = set()\n",
    "#         for c in compatible_classes:\n",
    "#             expected_keys_c = cls._get_init_keys(c)\n",
    "#             expected_keys_comp_cls = expected_keys_comp_cls.union(expected_keys_c)\n",
    "#         expected_keys_comp_cls = expected_keys_comp_cls - cls._get_init_keys(cls)\n",
    "#         config_dict = {k: v for k, v in config_dict.items() if k not in expected_keys_comp_cls}\n",
    "# \n",
    "#         # remove attributes from orig class that cannot be expected\n",
    "#         orig_cls_name = config_dict.pop(\"_class_name\", cls.__name__)\n",
    "#         if orig_cls_name != cls.__name__ and hasattr(diffusers_library, orig_cls_name):\n",
    "#             orig_cls = getattr(diffusers_library, orig_cls_name)\n",
    "#             unexpected_keys_from_orig = cls._get_init_keys(orig_cls) - expected_keys\n",
    "#             config_dict = {k: v for k, v in config_dict.items() if k not in unexpected_keys_from_orig}\n",
    "# \n",
    "#         # remove private attributes\n",
    "#         config_dict = {k: v for k, v in config_dict.items() if not k.startswith(\"_\")}\n",
    "# \n",
    "#         # 3. Create keyword arguments that will be passed to __init__ from expected keyword arguments\n",
    "# --------------------------------------------------\n",
    "# the below code fragment can be found in:\n",
    "# src/diffusers/configuration_utils.py\n",
    "# --------------------------------------------------\n",
    "# \n",
    "#         return config_dict\n",
    "# \n",
    "#     @staticmethod\n",
    "#     def _get_init_keys(cls):\n",
    "#         return set(dict(inspect.signature(cls.__init__).parameters).keys())\n",
    "# \n",
    "#     @classmethod\n",
    "#     def extract_init_dict(cls, config_dict, **kwargs):\n",
    "#         # 0. Copy origin config dict\n",
    "#         original_dict = {k: v for k, v in config_dict.items()}\n",
    "# \n",
    "#         # 1. Retrieve expected config attributes from __init__ signature\n",
    "#         expected_keys = cls._get_init_keys(cls)\n",
    "#         expected_keys.remove(\"self\")\n",
    "#         # remove general kwargs if present in dict\n",
    "#         if \"kwargs\" in expected_keys:\n",
    "#             expected_keys.remove(\"kwargs\")\n",
    "#         # remove flax internal keys\n",
    "#         if hasattr(cls, \"_flax_internal_args\"):\n",
    "# --------------------------------------------------\n",
    "\"\"\"Based on the above, complete the following code:\"\"\"'\n",
    "\n",
    "\n",
    "            new_scheduler_config = {k: v for k, v in new_scheduler.config.items() if k in scheduler.config}\n",
    "            scheduler_diff = {k: v for k, v in new_scheduler.config.items() if k not in scheduler.config}\n",
    "\n",
    "            # make sure that configs are essentially identical\n",
    "            assert new_scheduler_config == dict(scheduler.config)\n",
    "\n",
    "            # make sure that only differences are for configs that are not in init\n",
    "            init_keys = inspect.signature(scheduler_class.__init__).parameters.keys()\n",
    "            assert set(scheduler_diff.keys()).intersection(set(init_keys)) == set()\n",
    "\n",
    "    def test_from_pretrained(self):\n",
    "        for scheduler_class in self.scheduler_classes:\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "openai.api_base = \"http://3.22.120.71:8888/v1\"\n",
    "\n",
    "completion = openai.Completion.create(model=\"CodeLlama-13b-hf\", prompt=prompt, max_tokens=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'            dict_1 = flaml.save_state(PRETRAINED_SCHEDULE)\\n            scheduled = ScheduledAdaptation(scheduler_from_pretrained=PRETRAINED_SCHEDULE)\\n            dict_2 = flaml.save_state(scheduled.scheduler)\\n            # check that all config attributes of the two schedulers are equivalent\\n            assert dict_1 == dict_'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completion[\"choices\"][0][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import json\n",
    "\n",
    "client = openai.OpenAI()\n",
    "\n",
    "# Set OpenAI API key\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Input and output JSONL file paths\n",
    "base_jsonl_name =  'rg-one-gram-ws-20-ss-2-one-line_0.1_instruct_pass_at_10'\n",
    "\n",
    "use_system_message = True\n",
    "\n",
    "if use_system_message:\n",
    "    ending = \".jsonl\"\n",
    "else:\n",
    "    ending = \"_no_system.jsonl\"\n",
    "\n",
    "# Original and preprocessed JSONL file paths\n",
    "original_responses_path = 'temp_raw_generations/' + base_jsonl_name + '_raw_generations' + ending\n",
    "input_jsonl_file_path = 'temp_subsets/' + base_jsonl_name + '.jsonl'\n",
    "output_jsonl_file_path = \"temp_processed_generations/\" + base_jsonl_name + \"_generations\" + ending\n",
    "\n",
    "#Predefined system message\n",
    "system_message = '''Respond with only the next line completion'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = '''\n",
    "# Here are some relevant code fragments from other files of the repo:\n",
    "# --------------------------------------------------\n",
    "# the below code fragment can be found in:\n",
    "# torchrl/envs/vec_env.py\n",
    "# --------------------------------------------------\n",
    "#         ).clone()\n",
    "# \n",
    "#     @_check_start\n",
    "#     def _shutdown_workers(self) -> None:\n",
    "#         if self.is_closed:\n",
    "#             raise RuntimeError(\n",
    "#                 \"calling {self.__class__.__name__}._shutdown_workers only allowed when env.is_closed = False\"\n",
    "#             )\n",
    "#         for i, channel in enumerate(self.parent_channels):\n",
    "#             if self._verbose:\n",
    "#                 print(f\"closing {i}\")\n",
    "#             # try:\n",
    "#             channel.send((\"close\", None))\n",
    "#             # except:\n",
    "#             #     raise RuntimeError(f\"closing {channel} number {i} failed\")\n",
    "#             msg, _ = channel.recv()\n",
    "#             if msg != \"closing\":\n",
    "#                 raise RuntimeError(\n",
    "#                     f\"Expected 'closing' but received {msg} from worker {i}\"\n",
    "#                 )\n",
    "# \n",
    "#         del self.shared_tensordicts, self.shared_tensordict_parent\n",
    "# --------------------------------------------------\n",
    "# the below code fragment can be found in:\n",
    "# build_tools/setup_helpers/extension.py\n",
    "# --------------------------------------------------\n",
    "#     and \"ATen parallel backend: OpenMP\" in torch.__config__.parallel_info()\n",
    "# )\n",
    "# _TORCH_CUDA_ARCH_LIST = os.environ.get(\"TORCH_CUDA_ARCH_LIST\", None)\n",
    "# \n",
    "# \n",
    "# def get_ext_modules():\n",
    "#     return [\n",
    "#         Extension(name=\"torchrl._torchrl\", sources=[]),\n",
    "#     ]\n",
    "# \n",
    "# \n",
    "# # Based off of\n",
    "# # https://github.com/pybind/cmake_example/blob/580c5fd29d4651db99d8874714b07c0c49a53f8a/setup.py\n",
    "# class CMakeBuild(build_ext):\n",
    "#     def run(self):\n",
    "#         try:\n",
    "#             subprocess.check_output([\"cmake\", \"--version\"])\n",
    "#         except OSError:\n",
    "#             raise RuntimeError(\"CMake is not available.\") from None\n",
    "#         super().run()\n",
    "# \n",
    "#     def build_extension(self, ext):\n",
    "#         # Since two library files (libtorchrl and _torchrl) need to be\n",
    "#         # recognized by setuptools, we instantiate `Extension` twice. (see `get_ext_modules`)\n",
    "#         # This leads to the situation where this `build_extension` method is called twice.\n",
    "#         # However, the following `cmake` command will build all of them at the same time,\n",
    "# --------------------------------------------------\n",
    "# the below code fragment can be found in:\n",
    "# build_tools/setup_helpers/extension.py\n",
    "# --------------------------------------------------\n",
    "#                 [\"cmake\", str(_ROOT_DIR)] + cmake_args,\n",
    "#                 cwd=self.build_temp,\n",
    "#                 stderr=STDOUT,\n",
    "#             )\n",
    "#         except CalledProcessError as exc:\n",
    "#             print(exc.output)\n",
    "# \n",
    "#         try:\n",
    "#             check_output(\n",
    "#                 [\"cmake\", \"--build\", \".\"] + build_args,\n",
    "#                 cwd=self.build_temp,\n",
    "#                 stderr=STDOUT,\n",
    "#             )\n",
    "#         except CalledProcessError as exc:\n",
    "#             print(exc.output)\n",
    "# \n",
    "#     def get_ext_filename(self, fullname):\n",
    "#         ext_filename = super().get_ext_filename(fullname)\n",
    "#         ext_filename_parts = ext_filename.split(\".\")\n",
    "#         without_abi = ext_filename_parts[:-2] + ext_filename_parts[-1:]\n",
    "#         ext_filename = \".\".join(without_abi)\n",
    "#         return ext_filename\n",
    "# --------------------------------------------------\n",
    "\"\"\"Based on the above, complete the following code:\"\"\"'\n",
    "\n",
    "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
    "#\n",
    "# This source code is licensed under the MIT license found in the\n",
    "# LICENSE file in the root directory of this source tree.\n",
    "import argparse\n",
    "import distutils.command.clean\n",
    "import glob\n",
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "import sys\n",
    "from datetime import date\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "from setuptools import find_packages, setup\n",
    "from torch.utils.cpp_extension import BuildExtension, CppExtension\n",
    "\n",
    "cwd = os.path.dirname(os.path.abspath(__file__))\n",
    "try:\n",
    "    sha = (\n",
    "        subprocess.check_output([\"git\", \"rev-parse\", \"HEAD\"], cwd=cwd)\n",
    "        .decode(\"ascii\")\n",
    "        .strip()\n",
    "    )\n",
    "except Exception:\n",
    "    sha = \"Unknown\"\n",
    "\n",
    "\n",
    "def get_version():\n",
    "    version_txt = os.path.join(cwd, \"version.txt\")\n",
    "    with open(version_txt, \"r\") as f:\n",
    "        version = f.readline().strip()\n",
    "    if os.getenv(\"BUILD_VERSION\"):\n",
    "        version = os.getenv(\"BUILD_VERSION\")\n",
    "    elif sha != \"Unknown\":\n",
    "        version += \"+\" + sha[:7]\n",
    "    return version\n",
    "\n",
    "\n",
    "ROOT_DIR = Path(__file__).parent.resolve()\n",
    "\n",
    "\n",
    "package_name = \"torchrl\"\n",
    "\n",
    "\n",
    "def get_nightly_version():\n",
    "    today = date.today()\n",
    "    return f\"{today.year}.{today.month}.{today.day}\"\n",
    "\n",
    "\n",
    "def parse_args(argv: List[str]) -> argparse.Namespace:\n",
    "    parser = argparse.ArgumentParser(description=\"torchrl setup\")\n",
    "    parser.add_argument(\n",
    "        \"--package_name\",\n",
    "        type=str,\n",
    "        default=\"torchrl\",\n",
    "        help=\"the name of this output wheel\",\n",
    "    )\n",
    "    return parser.parse_known_args(argv)\n",
    "\n",
    "\n",
    "def write_version_file(version):\n",
    "    version_path = os.path.join(cwd, \"torchrl\", \"version.py\")\n",
    "    with open(version_path, \"w\") as f:\n",
    "        f.write(\"__version__ = '{}'\n",
    "\".format(version))\n",
    "        f.write(\"git_version = {}\n",
    "\".format(repr(sha)))\n",
    "\n",
    "\n",
    "def _get_pytorch_version():\n",
    "    # if \"PYTORCH_VERSION\" in os.environ:\n",
    "    #     return f\"torch=={os.environ['PYTORCH_VERSION']}\"\n",
    "    return \"torch\"\n",
    "\n",
    "\n",
    "def _get_packages():\n",
    "    exclude = [\n",
    "        \"build*\",\n",
    "        \"test*\",\n",
    "        \"torchrl.csrc*\",\n",
    "        \"third_party*\",\n",
    "        \"tools*\",\n",
    "    ]\n",
    "    return find_packages(exclude=exclude)\n",
    "\n",
    "\n",
    "ROOT_DIR = Path(__file__).parent.resolve()\n",
    "\n",
    "\n",
    "class clean(distutils.command.clean.clean):\n",
    "    def run(self):\n",
    "        # Run default behavior first\n",
    "        distutils.command.clean.clean.run(self)\n",
    "\n",
    "        # Remove torchrl extension\n",
    "        for path in (ROOT_DIR / \"torchrl\").glob(\"**/*.so\"):\n",
    "            print(f\"removing '{path}'\")\n",
    "            path.unlink()\n",
    "        # Remove build directory\n",
    "        build_dirs = [\n",
    "            ROOT_DIR / \"build\",\n",
    "        ]\n",
    "        for path in build_dirs:\n",
    "            if path.exists():\n",
    "                print(f\"removing '{path}' (and everything under it)\")\n",
    "                shutil.rmtree(str(path), ignore_errors=True)\n",
    "\n",
    "\n",
    "# def _run_cmd(cmd):\n",
    "#     try:\n",
    "#         return subprocess.check_output(cmd, cwd=ROOT_DIR).decode(\"ascii\").strip()\n",
    "#     except Exception:\n",
    "#         return None\n",
    "\n",
    "\n",
    "def get_extensions():\n",
    "    extension = CppExtension\n",
    "\n",
    "    extra_link_args = []\n",
    "    extra_compile_args = {\n",
    "        \"cxx\": [\n",
    "            \"-O3\",\n",
    "            \"-std=c++14\",\n",
    "            \"-fdiagnostics-color=always\",\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_system_message:\n",
    "    messages = [{\"role\": \"system\", \"content\": system_message}, {\"role\": \"user\", \"content\": prompt}]\n",
    "else:\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    \n",
    "response = client.chat.completions.create(model=\"gpt-3.5-turbo-1106\",\n",
    "                                            messages=messages,\n",
    "                                            max_tokens=10,\n",
    "                                            temperature=0.5,\n",
    "                                            seed=1,\n",
    "                                            n=3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "            \"-Wall\",\n",
      "            \"-Wextra\",\n",
      "           \n",
      "1\n",
      "        ]\n",
      "    }\n",
      "    return [\n",
      "        extension(\n",
      "\n",
      "2\n",
      "        return extension(\n",
      "            name=\"torchrl._\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(i)\n",
    "    print(response.choices[i].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
