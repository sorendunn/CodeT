{"prompt": "buffer(\"loc\", loc)\n        self.register_buffer(\"scale\", scale)\n\n    def init_stats(\n        self,\n        num_iter: int,\n        reduce_dim: Union[int, Tuple[int]] = 0,\n        cat_dim: Optional[int] = None,\n        key: Optional[str] = None,\n        keep_dims: Optional[Tuple[int]] = None,\n    ) -> None:\n        \"\"\"Initializes the loc and scale stats of the parent environment.\n\n        Normalization constant should ideally make the observation statistics approach\n        those of a standard Gaussian distribution. This method computes a location\n        and scale tensor that will empirically compute the mean and standard\n        deviation of a Gaussian distribution fitted on data generated randomly with\n        the parent environment for a given number of steps.\n\n        Args:\n            num_iter (int): number of random iterations to run in the environment.\n            reduce_dim (int or tuple of int, optional): dimension to compute the mean and std over.\n                Defaults to 0.\n            cat_dim (int, optional): dimension along which the batches collected will be concatenated.\n                It must be part equal to reduce_dim (if integer) or part of the reduce_dim tuple.\n                Defaults to the same value as reduce_dim.\n            key (str, optional): if provided, the summary statistics will be\n                retrieved from that key in the resulting tensordicts.\n                Otherwise, the first key in :obj:`ObservationNorm.in_keys` will be used.\n            keep_dims (tuple of int, optional): the dimensions to keep in the loc and scale.\n                For instance, one may want the location and scale to have shape [C, 1, 1]\n                when normalizing a 3D tensor over the last two dimensions, but not the\n                third. Defaults to None.\n\n        \"\"\"\n        if cat_dim is None:\n            cat_dim = reduce_dim\n            if not isinstance(cat_dim, int):\n                raise ValueError(\n                    \"cat_dim must be specified if reduce_dim is not an integer.\"\n                )\n        if (isinstance(reduce_dim, tuple) and cat_dim not in reduce_dim) or (\n            isinstance(reduce_dim, int) and cat_dim!= reduce_dim\n        ):\n            raise ValueError(\"cat_dim must be part of or equal to reduce_dim.\")\n        if self.loc is not None or self.scale is not None:\n            raise RuntimeError(\n                f\"Loc/Scale are already initialized: ({self.loc}, {self.scale})\"\n            )\n\n        if len(self.in_keys) > 1 and key is None:\n            raise RuntimeError(\n                \"Transform has multiple in_keys but no specific key was passed as an argument\"\n            )\n        key = self.in_keys[0] if key is None else key\n\n        def raise_initialization_exception(module):\n            if (\n                isinstance(module, ObservationNorm)\n                and module.scale is None\n                and module.loc is None\n            ):\n                raise RuntimeError(\n                    \"ObservationNorms need to be initialized in the right order.\"\n                    \"Trying to initialize an ObservationNorm \"\n                    \"while a parent ObservationNorm transform is still uninitialized\"\n                )\n\n        parent = self.parent\n        parent.apply(raise_initialization_exception)\n\n        collected_frames = 0\n        data = []\n        while collected_frames < num_iter:\n            tensordict = parent.rollout(max_steps=num_iter)\n            collected_frames += tensordict.numel()\n            data.append(tensordict.get(key))\n\n        data = torch.cat(data, cat_dim)\n        if isinstance(reduce_dim, int):\n            reduce_dim = [reduce_dim]\n        if keep_dims is not None:\n            if not all(k in reduce_dim for k in keep_dims):\n                raise ValueError(\"keep_dim elements must be part of reduce_dim list.\")\n        else:\n            keep_dims = []\n        loc = data.mean(reduce_dim, keepdim=True)\n        scale = data.std(reduce_dim, keepdim=True)\n        for r in sorted(reduce_dim, reverse=True):\n            if r not in keep_dims:\n                loc = loc.squeeze(r)\n                scale = scale.squeeze(r)\n\n        if not self.standard_normal:\n            scale = 1 / scale.clamp_min(self.eps)\n            loc = -loc * scale\n\n        if not torch.isfinite(loc).all():\n            raise RuntimeError(\"Non-finite values found in loc\")\n        if not torch.isfinite(scale).all():\n            raise RuntimeError(\"Non-finite values found in scale\")\n        self.register_buffer(\"loc\", loc)\n        self.register_buffer(\"scale\", scale.clamp_min(self.eps))\n\n    def _apply_transform(self, obs: torch.Tensor) -> torch.Tensor:\n        if self.loc is None or self.scale is None:\n            raise RuntimeError(\n                \"Loc/Scale have not been initialized. Either pass in values in the constructor \"\n                \"or call the init_stats method\"\n            )\n        if self.standard_normal:\n            loc = self.loc\n            scale = self.scale\n            return (obs - loc) / scale\n        else:\n            scale = self.scale\n            loc = self.loc\n            return obs * scale + loc\n\n    @_apply_to_composite\n    def transform_observation_spec(self, observation_spec: TensorSpec) -> TensorSpec:\n        space = observation_spec.space\n        if isinstance(space, ContinuousBox):\n            space.minimum = self._apply_transform(space.minimum)\n            space.maximum = self._apply_transform(space.maximum)\n        return observation_spec\n\n    def __repr__(self) -> str:\n        if self.loc.numel() == 1 and self.scale.numel() == 1:\n            return (\n                f\"{self.__class__.__name__}(\"\n                f\"loc={float(self.loc):4.4f}, scale\"\n                f\"={float(self.scale):4.4f}, keys={self.in_keys})\"\n            )\n        else:\n            return super().__repr__()\n\n\nclass CatFrames(ObservationTransform):\n    \"\"\"Concatenates successive observation frames into a single tensor.\n\n    This can, for instance, account for movement/velocity of the observed\n    feature. Proposed in \"Playing Atari with Deep Reinforcement Learning\" (\n    https://arxiv.org/pdf/1312.5602.pdf).\n\n    CatFrames is a stateful class and it can be reset to its native state by\n    calling the `reset()` method.\n\n    Args:\n        N (int): number of observation to concatenate.\n        dim (int): dimension along which concatenate the\n            observations. Should be negative, to ensure that it is compatible\n            with environments of different batch_size.\n        in_keys (list of int, optional): keys pointing to the frames that have\n            to be concatenated. Defaults to [\"pixels\"].\n        out_keys (list of int, optional): keys pointing to where the output\n            has to be written. Defaults to the value of `in_keys`.\n\n    \"\"\"\n\n    inplace = False\n    _CAT_DIM_ERR = (\n        \"dim must be > 0 to accomodate for tensordict of \"\n        \"different batch-sizes (since negative dims are batch invariant).\"\n    )\n\n    def __init__(\n        self,\n        N: int,\n        dim: int,\n        in_keys: Optional[Sequence[str]] = None,\n        out_keys: Optional[Sequence[str]] = None,\n    ):\n        if in_keys is None:\n            in_keys = IMAGE_KEYS", "metadata": {"task_id": "pytorch_rl/109", "ground_truth": "        super().__init__(in_keys=in_keys, out_keys=out_keys)", "fpath_tuple": ["pytorch_rl", "torchrl", "envs", "transforms", "transforms.py"], "context_start_lineno": 1347, "line_no": 1522}}
{"prompt": ":Stitched>True</GSpherical:Stitched>\n<GSpherical:StitchingSoftware>nerfstudio</GSpherical:StitchingSoftware>\n</rdf:SphericalVideo>\"\"\",\n        \"utf-8\",\n    )\n    insert_size = len(spherical_metadata) + 8 + 16\n    with open(output_filename, mode=\"r+b\") as mp4file:\n        try:\n            # get file size\n            mp4file_size = os.stat(output_filename).st_size\n\n            # find moov container (probably after ftyp, free, mdat)\n            while True:\n                pos = mp4file.tell()\n                size, tag = struct.unpack(\">I4s\", mp4file.read(8))\n                if tag == b\"moov\":\n                    break\n                mp4file.seek(pos + size)\n            # if moov isn't at end, bail\n            if pos + size!= mp4file_size:\n                # TODO: to support faststart, rewrite all stco offsets\n                raise Exception(\"moov container not at end of file\")\n            # go back and write inserted size\n            mp4file.seek(pos)\n            mp4file.write(struct.pack(\">I\", size + insert_size))\n            # go inside moov\n            mp4file.seek(pos + 8)\n            # find trak container (probably after mvhd)\n            while True:\n                pos = mp4file.tell()\n                size, tag = struct.unpack(\">I4s\", mp4file.read(8))\n                if tag == b\"trak\":\n                    break\n                mp4file.seek(pos + size)\n            # go back and write inserted size\n            mp4file.seek(pos)\n            mp4file.write(struct.pack(\">I\", size + insert_size))\n            # we need to read everything from end of trak to end of file in order to insert\n            # TODO: to support faststart, make more efficient (may load nearly all data)\n            mp4file.seek(pos + size)\n            rest_of_file = mp4file.read(mp4file_size - pos - size)\n            # go to end of trak (again)\n            mp4file.seek(pos + size)\n            # insert our uuid atom with spherical metadata\n            mp4file.write(struct.pack(\">I4s16s\", insert_size, b\"uuid\", spherical_uuid))\n            mp4file.write(spherical_metadata)\n            # write rest of file\n            mp4file.write(rest_of_file)\n        finally:\n            mp4file.close()\n\n\n@dataclass\nclass CropData:\n    \"\"\"Data for cropping an image.\"\"\"\n\n    background_color: TensorType[3] = torch.Tensor([0.0, 0.0, 0.0])\n    \"\"\"background color\"\"\"\n    center: TensorType[3] = torch.Tensor([0.0, 0.0, 0.0])\n    \"\"\"center of the crop\"\"\"\n    scale: TensorType[3] = torch.Tensor([2.0, 2.0, 2.0])\n    \"\"\"scale of the crop\"\"\"\n\n\ndef get_crop_from_json(camera_json: Dict[str, Any]) -> Optional[CropData]:\n    \"\"\"Load crop data from a camera path JSON\n\n    args:\n        camera_json: camera path data\n    returns:\n        Crop data\n    \"\"\"\n    if \"crop\" not in camera_json or camera_json[\"crop\"] is None:\n        return None\n\n    bg_color = camera_json[\"crop\"][\"crop_bg_color\"]\n\n    return CropData(\n        background_color=torch.Tensor([bg_color[\"r\"] / 255.0, bg_color[\"g\"] / 255.0, bg_color[\"b\"] / 255.0]),\n        center=torch.Tensor(camera_json[\"crop\"][\"crop_center\"]),\n        scale=torch.Tensor(camera_json[\"crop\"][\"crop_scale\"]),\n    )\n\n\n@dataclass\nclass RenderTrajectory:\n    \"\"\"Load a checkpoint, render a trajectory, and save to a video file.\"\"\"\n\n    load_config: Path\n    \"\"\"Path to config YAML file.\"\"\"\n    rendered_output_names: List[str] = field(default_factory=lambda: [\"rgb\"])\n    \"\"\"Name of the renderer outputs to use. rgb, depth, etc. concatenates them along y axis\"\"\"\n    traj: Literal[\"spiral\", \"filename\"] = \"spiral\"\n    \"\"\"Trajectory to render.\"\"\"\n    downscale_factor: int = 1\n    \"\"\"Scaling factor to apply to the camera image resolution.\"\"\"\n    camera_path_filename: Path = Path(\"camera_path.json\")\n    \"\"\"Filename of the camera path to render.\"\"\"\n    output_path: Path = Path(\"renders/output.mp4\")\n    \"\"\"Name of the output file.\"\"\"\n    seconds: float = 5.0\n    \"\"\"How long the video should be.\"\"\"\n    output_format: Literal[\"images\", \"video\"] = \"video\"\n    \"\"\"How to save output data.\"\"\"\n    eval_num_rays_per_chunk: Optional[int] = None\n    \"\"\"Specifies number of rays per chunk during eval.\"\"\"\n\n    def main(self) -> None:\n        \"\"\"Main function.\"\"\"\n        _, pipeline, _ = eval_setup(\n            self.load_config,\n            eval_num_rays_per_chunk=self.eval_num_rays_per_chunk,\n            test_mode=\"test\" if self.traj == \"spiral\" else \"inference\",\n        )\n\n        install_checks.check_ffmpeg_installed()\n\n        seconds = self.seconds\n        crop_data = None\n\n        # TODO(ethan): use camera information from parsing args\n        if self.traj == \"spiral\":\n            camera_start = pipeline.datamanager.eval_dataloader.get_camera(image_idx=0).flatten()\n            # TODO(ethan): pass in the up direction of the camera\n            camera_type = CameraType.PERSPECTIVE\n            camera_path = get_spiral_path(camera_start, steps=30, radius=0.1)\n        elif self.traj == \"filename\":\n            with open(self.camera_path_filename, \"r\", encoding=\"utf-8\") as f:\n                camera_path = json.load(f)\n            seconds = camera_path[\"seconds\"]\n            if \"camera_type\" not in camera_path:\n                camera_type = CameraType.PERSPECTIVE\n            elif camera_path[\"camera_type\"] == \"fisheye\":\n                camera_type = CameraType.FISHEYE\n            elif camera_path[\"camera_type\"] == \"equirectangular\":\n                camera_type = CameraType.EQUIRECTANGULAR\n            else:\n                camera_type = CameraType.PERSPECTIVE\n            crop_data = get_crop_from_json(camera_path)\n            camera_path = get_path_from_json(camera_path)\n        else:\n            assert_never(self.traj)\n\n        _render_trajectory_video(\n            pipeline,\n            camera_path,\n            output_filename=self.output_path,\n            rendered_output_names=self.rendered_output_names,\n            rendered_resolution_scaling_factor=1.0 / self.downscale_factor,\n            crop_data=crop_data,\n            seconds=seconds,\n            output_format=self.output_format,\n            camera_type=camera_type,\n        )\n\n\ndef entrypoint():\n    \"\"\"Entrypoint for use with pyproject scripts.\"\"\"\n    tyro.extras.set_accent_color(\"bright_yellow\")\n    tyro.cli(RenderTrajectory).main()\n\n\nif __name__ == \"__main__\":", "metadata": {"task_id": "nerfstudio-project_nerfstudio/28", "ground_truth": "    entrypoint()", "fpath_tuple": ["nerfstudio-project_nerfstudio", "scripts", "render.py"], "context_start_lineno": 165, "line_no": 328}}
{"prompt": ".item() - 201.9864) < 1e-2\n        assert abs(result_mean.item() - 0.2630) < 1e-3\n\n\nclass DDIMSchedulerTest(SchedulerCommonTest):\n    scheduler_classes = (DDIMScheduler,)\n    forward_default_kwargs = ((\"eta\", 0.0), (\"num_inference_steps\", 50))\n\n    def get_scheduler_config(self, **kwargs):\n        config = {\n            \"num_train_timesteps\": 1000,\n            \"beta_start\": 0.0001,\n            \"beta_end\": 0.02,\n            \"beta_schedule\": \"linear\",\n            \"clip_sample\": True,\n        }\n\n        config.update(**kwargs)\n        return config\n\n    def full_loop(self, **config):\n        scheduler_class = self.scheduler_classes[0]\n        scheduler_config = self.get_scheduler_config(**config)\n        scheduler = scheduler_class(**scheduler_config)\n\n        num_inference_steps, eta = 10, 0.0\n\n        model = self.dummy_model()\n        sample = self.dummy_sample_deter\n\n        scheduler.set_timesteps(num_inference_steps)\n\n        for t in scheduler.timesteps:\n            residual = model(sample, t)\n            sample = scheduler.step(residual, t, sample, eta).prev_sample\n\n        return sample\n\n    def test_timesteps(self):\n        for timesteps in [100, 500, 1000]:\n            self.check_over_configs(num_train_timesteps=timesteps)\n\n    def test_steps_offset(self):\n        for steps_offset in [0, 1]:\n            self.check_over_configs(steps_offset=steps_offset)\n\n        scheduler_class = self.scheduler_classes[0]\n        scheduler_config = self.get_scheduler_config(steps_offset=1)\n        scheduler = scheduler_class(**scheduler_config)\n        scheduler.set_timesteps(5)\n        assert torch.equal(scheduler.timesteps, torch.LongTensor([801, 601, 401, 201, 1]))\n\n    def test_betas(self):\n        for beta_start, beta_end in zip([0.0001, 0.001, 0.01, 0.1], [0.002, 0.02, 0.2, 2]):\n            self.check_over_configs(beta_start=beta_start, beta_end=beta_end)\n\n    def test_schedules(self):\n        for schedule in [\"linear\", \"squaredcos_cap_v2\"]:\n            self.check_over_configs(beta_schedule=schedule)\n\n    def test_prediction_type(self):\n        for prediction_type in [\"epsilon\", \"v_prediction\"]:\n            self.check_over_configs(prediction_type=prediction_type)\n\n    def test_clip_sample(self):\n        for clip_sample in [True, False]:\n            self.check_over_configs(clip_sample=clip_sample)\n\n    def test_time_indices(self):\n        for t in [1, 10, 49]:\n            self.check_over_forward(time_step=t)\n\n    def test_inference_steps(self):\n        for t, num_inference_steps in zip([1, 10, 50], [10, 50, 500]):\n            self.check_over_forward(time_step=t, num_inference_steps=num_inference_steps)\n\n    def test_eta(self):\n        for t, eta in zip([1, 10, 49], [0.0, 0.5, 1.0]):\n            self.check_over_forward(time_step=t, eta=eta)\n\n    def test_variance(self):\n        scheduler_class = self.scheduler_classes[0]\n        scheduler_config = self.get_scheduler_config()\n        scheduler = scheduler_class(**scheduler_config)\n\n        assert torch.sum(torch.abs(scheduler._get_variance(0, 0) - 0.0)) < 1e-5\n        assert torch.sum(torch.abs(scheduler._get_variance(420, 400) - 0.14771)) < 1e-5\n        assert torch.sum(torch.abs(scheduler._get_variance(980, 960) - 0.32460)) < 1e-5\n        assert torch.sum(torch.abs(scheduler._get_variance(0, 0) - 0.0)) < 1e-5\n        assert torch.sum(torch.abs(scheduler._get_variance(487, 486) - 0.00979)) < 1e-5\n        assert torch.sum(torch.abs(scheduler._get_variance(999, 998) - 0.02)) < 1e-5\n\n    def test_full_loop_no_noise(self):\n        sample = self.full_loop()\n\n        result_sum = torch.sum(torch.abs(sample))\n        result_mean = torch.mean(torch.abs(sample))\n\n        assert abs(result_sum.item() - 172.0067) < 1e-2\n        assert abs(result_mean.item() - 0.223967) < 1e-3\n\n    def test_full_loop_with_v_prediction(self):\n        sample = self.full_loop(prediction_type=\"v_prediction\")\n\n        result_sum = torch.sum(torch.abs(sample))\n        result_mean = torch.mean(torch.abs(sample))\n\n        assert abs(result_sum.item() - 52.5302) < 1e-2\n        assert abs(result_mean.item() - 0.0684) < 1e-3\n\n    def test_full_loop_with_set_alpha_to_one(self):\n        # We specify different beta, so that the first alpha is 0.99\n        sample = self.full_loop(set_alpha_to_one=True, beta_start=0.01)\n        result_sum = torch.sum(torch.abs(sample))\n        result_mean = torch.mean(torch.abs(sample))\n\n        assert abs(result_sum.item() - 149.8295) < 1e-2\n        assert abs(result_mean.item() - 0.1951) < 1e-3\n\n    def test_full_loop_with_no_set_alpha_to_one(self):\n        # We specify different beta, so that the first alpha is 0.99\n        sample = self.full_loop(set_alpha_to_one=False, beta_start=0.01)\n        result_sum = torch.sum(torch.abs(sample))\n        result_mean = torch.mean(torch.abs(sample))\n\n        assert abs(result_sum.item() - 149.0784) < 1e-2\n        assert abs(result_mean.item() - 0.1941) < 1e-3\n\n\nclass DPMSolverSinglestepSchedulerTest(SchedulerCommonTest):\n    scheduler_classes = (DPMSolverSinglestepScheduler,)\n    forward_default_kwargs = ((\"num_inference_steps\", 25),)\n\n    def get_scheduler_config(self, **kwargs):\n        config = {\n            \"num_train_timesteps\": 1000,\n            \"beta_start\": 0.0001,\n            \"beta_end\": 0.02,\n            \"beta_schedule\": \"linear\",\n            \"solver_order\": 2,\n            \"prediction_type\": \"epsilon\",", "metadata": {"task_id": "huggingface_diffusers/51", "ground_truth": "            \"thresholding\": False,", "fpath_tuple": ["huggingface_diffusers", "tests", "test_scheduler.py"], "context_start_lineno": 719, "line_no": 860}}
{"prompt": "import pytest\nimport numpy as np\nimport torch\nfrom collections import namedtuple\n\nfrom ding.utils.default_helper import lists_to_dicts, dicts_to_lists, squeeze, default_get, override, error_wrapper,\\\n    list_split, LimitedSpaceContainer, set_pkg_seed, deep_merge_dicts, deep_update, flatten_dict\n\n\n@pytest.mark.unittest\nclass TestDefaultHelper():\n\n    def test_lists_to_dicts(self):\n        set_pkg_seed(12)\n        with pytest.raises(ValueError):\n            lists_to_dicts([])\n        with pytest.raises(TypeError):\n            lists_to_dicts([1])\n        assert lists_to_dicts([{1: 1, 10: 3}, {1: 2, 10: 4}]) == {1: [1, 2], 10: [3, 4]}\n        T = namedtuple('T', ['location', 'race'])\n        data = [T({'x': 1, 'y': 2}, 'zerg') for _ in range(3)]\n        output = lists_to_dicts(data)\n        assert isinstance(output, T) and output.__class__ == T\n        assert len(output.location) == 3\n        data = [{'value': torch.randn(1), 'obs': {'scalar': torch.randn(4)}} for _ in range(3)]\n        output = lists_to_dicts(data, recursive=True)\n        assert isinstance(output, dict)\n        assert len(output['value']) == 3\n        assert len(output['obs']['scalar']) == 3\n\n    def test_dicts_to_lists(self):\n        assert dicts_to_lists({1: [1, 2], 10: [3, 4]}) == [{1: 1, 10: 3}, {1: 2, 10: 4}]\n\n    def test_squeeze(self):\n        assert squeeze((4, )) == 4\n        assert squeeze({'a': 4}) == 4\n        assert squeeze([1, 3]) == (1, 3)\n        data = np.random.randn(3)\n        output = squeeze(data)\n        assert (output == data).all()\n\n    def test_default_get(self):\n        assert default_get({}, 'a', default_value=1, judge_fn=lambda x: x < 2) == 1\n        assert default_get({}, 'a', default_fn=lambda: 1, judge_fn=lambda x: x < 2) == 1\n        with pytest.raises(AssertionError):\n            default_get({}, 'a', default_fn=lambda: 1, judge_fn=lambda x: x < 0)\n        assert default_get({'val': 1}, 'val', default_value=2) == 1\n\n    def test_override(self):\n\n        class foo(object):\n\n            def fun(self):\n                raise NotImplementedError\n\n        class foo1(foo):\n\n            @override(foo)\n            def fun(self):\n                return \"a\"\n\n        with pytest.raises(NameError):\n\n            class foo2(foo):\n\n                @override(foo)\n                def func(self):\n                    pass\n\n        with pytest.raises(NotImplementedError):\n            foo().fun()\n        foo1().fun()\n\n    def test_error_wrapper(self):\n\n        def good_ret(a, b=1):\n            return a + b\n\n        wrap_good_ret = error_wrapper(good_ret, 0)\n        assert good_ret(1) == wrap_good_ret(1)\n\n        def bad_ret(a, b=0):\n            return a / b\n\n        wrap_bad_ret = error_wrapper(bad_ret, 0)\n        assert wrap_bad_ret(1) == 0\n\n    def test_list_split(self):\n        data = [i for i in range(10)]\n        output, residual = list_split(data, step=4)\n        assert len(output) == 2\n        assert output[1] == [4, 5, 6, 7]\n        assert residual == [8, 9]\n        output, residual = list_split(data, step=5)\n        assert len(output) == 2\n        assert output[1] == [5, 6, 7, 8, 9]\n        assert residual is None\n\n\n@pytest.mark.unittest\nclass TestLimitedSpaceContainer():\n\n    def test_container(self):\n        container = LimitedSpaceContainer(0, 5)\n        first = container.acquire_space()\n        assert first\n        assert container.cur == 1\n        left = container.get_residual_space()\n        assert left == 4\n        assert container.cur == container.max_val == 5\n        no_space = container.acquire_space()\n        assert not no_space\n        container.increase_space()\n        six = container.acquire_space()\n        assert six\n        for i in range(6):\n            container.release_space()\n            assert container.cur == 5 - i\n        container.decrease_space()\n        assert container.max_val == 5\n\n\n@pytest.mark.unittest\nclass TestDict:\n\n    def test_deep_merge_dicts(self):\n        dict1 = {\n            'a': 3,\n            'b': {\n                'c': 3,\n                'd': {\n                    'e': 6,\n                    'f': 5,\n                }\n            }\n        }\n        dict2 = {\n            'b': {\n                'c': 5,\n                'd': 6,\n                'g': 4,\n            }\n        }\n        new_dict = deep_merge_dicts(dict1, dict2)\n        assert new_dict['a'] == 3\n        assert isinstance(new_dict['b'], dict)\n        assert new_dict['b']['c'] == 5\n        assert new_dict['b']['c'] == 5\n        assert new_dict['b']['g'] == 4\n\n    def test_deep_update(self):\n        dict1 = {\n            'a': 3,\n            'b': {\n                'c': 3,\n                'd': {\n                    'e': 6,\n                    'f': 5,\n                },\n                'z': 4,\n            }\n        }\n        dict2 = {\n            'b': {\n                'c': 5,", "metadata": {"task_id": "opendilab_ACE/118", "ground_truth": "                'd': 6,", "fpath_tuple": ["opendilab_ACE", "ding", "utils", "tests", "test_default_helper.py"], "context_start_lineno": 0, "line_no": 165}}
{"prompt": "Classifier(\n                model=MyModel(self.class_output_dim),\n                posterior_approximator=LaplacePosteriorApproximator(),\n                output_calibrator=ClassificationTemperatureScaler(),\n            )\n            # no save dir, no dump\n            status = prob_class.train(\n                train_data_loader=self.class_train_data_loader,\n                calib_data_loader=self.class_val_data_loader,\n                val_data_loader=self.class_val_data_loader,\n                map_fit_config=self.class_fit_config_nodir_nodump,\n                fit_config=self.class_fit_config_nodir_nodump,\n                calib_config=self.class_calib_config_nodir_nodump,\n            )\n            sample = prob_class.posterior.sample()\n\n            # no save dir but dump\n            with self.assertRaises(ValueError):\n                status = prob_class.train(\n                    train_data_loader=self.class_train_data_loader,\n                    calib_data_loader=self.class_val_data_loader,\n                    val_data_loader=self.class_val_data_loader,\n                    map_fit_config=self.class_fit_config_nodir_nodump,\n                    fit_config=self.class_fit_config_nodir_dump,\n                    calib_config=self.class_calib_config_nodir_nodump,\n                )\n\n            # save dir, no dump\n            status = prob_class.train(\n                train_data_loader=self.class_train_data_loader,\n                calib_data_loader=self.class_val_data_loader,\n                val_data_loader=self.class_val_data_loader,\n                map_fit_config=self.class_fit_config_nodir_nodump,\n                fit_config=self.class_fit_config_dir_nodump(tmp_dir),\n                calib_config=self.class_calib_config_nodir_nodump,\n            )\n            sample = prob_class.posterior.sample()\n            prob_class.posterior.load_state(tmp_dir)\n\n            # save dir and dump\n            status = prob_class.train(\n                train_data_loader=self.class_train_data_loader,\n                calib_data_loader=self.class_val_data_loader,\n                val_data_loader=self.class_val_data_loader,\n                map_fit_config=self.class_fit_config_nodir_nodump,\n                fit_config=self.class_fit_config_dir_dump(tmp_dir),\n                calib_config=self.class_calib_config_nodir_nodump,\n            )\n            sample = prob_class.posterior.sample()\n            prob_class.posterior.load_state(tmp_dir)\n\n            # restore from laplace\n            status = prob_class.train(\n                train_data_loader=self.class_train_data_loader,\n                calib_data_loader=self.class_val_data_loader,\n                val_data_loader=self.class_val_data_loader,\n                map_fit_config=self.class_fit_config_nodir_nodump,\n                fit_config=self.class_fit_config_restore(tmp_dir),\n                calib_config=self.class_calib_config_nodir_nodump,\n            )\n\n            # restore from map\n            prob_class_map = ProbClassifier(\n                model=MyModel(self.class_output_dim),\n                posterior_approximator=MAPPosteriorApproximator(),\n                output_calibrator=ClassificationTemperatureScaler(),\n            )\n            status = prob_class_map.train(\n                train_data_loader=self.class_train_data_loader,\n                calib_data_loader=self.class_val_data_loader,\n                val_data_loader=self.class_val_data_loader,\n                map_fit_config=self.class_fit_config_nodir_nodump,\n                fit_config=self.class_fit_config_dir_dump(tmp_dir),\n                calib_config=self.class_calib_config_nodir_nodump,\n            )\n            status = prob_class.train(\n                train_data_loader=self.class_train_data_loader,\n                calib_data_loader=self.class_val_data_loader,\n                val_data_loader=self.class_val_data_loader,\n                map_fit_config=self.class_fit_config_nodir_nodump,\n                fit_config=self.class_fit_config_restore(tmp_dir),\n                calib_config=self.class_calib_config_nodir_nodump,\n            )\n\n            # load state\n            prob_class.load_state(checkpoint_path=tmp_dir)\n\n            # save state\n            prob_class.save_state(checkpoint_path=tmp_dir)\n\n    def test_dryrun_reg_swag(self):\n        with tempfile.TemporaryDirectory() as tmp_dir:\n            prob_reg = ProbRegressor(\n                model=MyModel(self.reg_output_dim),\n                likelihood_log_variance_model=MyModel(self.reg_output_dim),\n                posterior_approximator=SWAGPosteriorApproximator(rank=2),\n                output_calibrator=RegressionTemperatureScaler(),\n            )\n            # no save dir, no dump\n            status = prob_reg.train(\n                train_data_loader=self.reg_train_data_loader,\n                calib_data_loader=self.reg_val_data_loader,\n                val_data_loader=self.reg_val_data_loader,\n                map_fit_config=self.reg_fit_config_nodir_nodump,\n                fit_config=self.reg_fit_config_nodir_nodump,\n                calib_config=self.reg_calib_config_nodir_nodump,\n            )\n            sample = prob_reg.posterior.sample()\n\n            # no save dir but dump\n            with self.assertRaises(ValueError):\n                status = prob_reg.train(\n                    train_data_loader=self.reg_train_data_loader,\n                    calib_data_loader=self.reg_val_data_loader,\n                    val_data_loader=self.reg_val_data_loader,\n                    map_fit_config=self.reg_fit_config_nodir_nodump,\n                    fit_config=self.reg_fit_config_nodir_dump,\n                    calib_config=self.reg_calib_config_nodir_nodump,\n                )\n\n            # save dir, no dump\n            status = prob_reg.train(\n                train_data_loader=self.reg_train_data_loader,\n                calib_data_loader=self.reg_val_data_loader,\n                val_data_loader=self.reg_val_data_loader,\n                map_fit_config=self.reg_fit_config_nodir_nodump,\n                fit_config=self.reg_fit_config_dir_nodump(tmp_dir),\n                calib_config=self.reg_calib_config_nodir_nodump,\n            )\n            sample = prob_reg.posterior.sample()\n            prob_reg.posterior.load_state(tmp_dir)\n\n            # save dir and dump", "metadata": {"task_id": "awslabs_fortuna/163", "ground_truth": "            status = prob_reg.train(", "fpath_tuple": ["awslabs_fortuna", "tests", "fortuna", "prob_model", "test_train.py"], "context_start_lineno": 674, "line_no": 807}}
{"prompt": "from typing import Optional\n\nimport jax.numpy as jnp\n\nfrom fortuna.typing import Array\n\n\nclass OneDimensionalUncertaintyConformalRegressor:\n    def score(\n        self, val_preds: Array, val_uncertainties: Array, val_targets: Array,\n    ) -> jnp.ndarray:\n        \"\"\"\n        Compute the conformal scores.\n\n        Parameters\n        ----------\n        val_preds: Array\n            A two-dimensional array of predictions over the validation data points.\n        val_uncertainties: Array\n            A two-dimensional array of uncertainty estimates (e.g. the standard deviation). The first\n            dimension is over the validation inputs. The second must have only one component.\n        val_targets: Array\n            A two-dimensional array of validation target variables.\n\n        Returns\n        -------\n        jnp.ndarray\n            Scores.\n        \"\"\"\n        if val_preds.ndim!= 2 or val_preds.shape[1]!= 1:\n            raise ValueError(\n                \"\"\"`val_preds` must be a two-dimensional array. The second dimension must have only one\n            component.\"\"\"\n            )\n        if val_uncertainties.ndim!= 2 or val_uncertainties.shape[1]!= 1:\n            raise ValueError(\n                \"\"\"`val_uncertainties` must be a two-dimensional array. The second dimension must have only\n            one component.\"\"\"\n            )", "metadata": {"task_id": "awslabs_fortuna/101", "ground_truth": "        if (val_uncertainties <= 0).any():", "fpath_tuple": ["awslabs_fortuna", "fortuna", "conformal", "regression", "onedim_uncertainty.py"], "context_start_lineno": 0, "line_no": 39}}
{"prompt": "import unittest\n\nimport jax.numpy as jnp\nfrom flax.core import FrozenDict\nfrom jax import random\n\nfrom fortuna.data.loader import DataLoader\nfrom fortuna.model.mlp import MLP\nfrom fortuna.model.model_manager.regression import RegressionModelManager\nfrom fortuna.output_calibrator.output_calib_manager.base import \\\n    OutputCalibManager\nfrom fortuna.prob_model.joint.base import Joint\nfrom fortuna.prob_model.likelihood.regression import RegressionLikelihood\nfrom fortuna.prob_model.prior import IsotropicGaussianPrior\nfrom fortuna.prob_output_layer.regression import RegressionProbOutputLayer\nfrom tests.make_data import make_array_random_data\n\n\nclass TestJoints(unittest.TestCase):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.shape_inputs = (3,)\n        self.n_inputs = 10\n        self.output_dim = 2\n        self.rng = random.PRNGKey(0)\n        self.joint = Joint(\n            prior=IsotropicGaussianPrior(),\n            likelihood=RegressionLikelihood(\n                model_manager=RegressionModelManager(\n                    model=MLP(output_dim=self.output_dim),\n                    likelihood_log_variance_model=MLP(output_dim=self.output_dim),\n                ),\n                prob_output_layer=RegressionProbOutputLayer(),\n                output_calib_manager=OutputCalibManager(output_calibrator=None),\n            ),\n        )\n\n        self.data_arr = DataLoader.from_array_data(\n            make_array_random_data(\n                n_data=self.n_inputs,\n                shape_inputs=self.shape_inputs,\n                output_dim=self.output_dim,", "metadata": {"task_id": "awslabs_fortuna/57", "ground_truth": "                output_type=\"continuous\",", "fpath_tuple": ["awslabs_fortuna", "tests", "fortuna", "prob_model", "test_joint.py"], "context_start_lineno": 0, "line_no": 42}}
{"prompt": "d;\n    # fy(x, y) = y * d(x, y) + 2 * p2 * x * y + p1 * (r(x, y) + 2 * y^2) - yd;\n    #\n    # We are looking for a solution that satisfies\n    # fx(x, y) = fy(x, y) = 0;\n    fx = d * x + 2 * p1 * x * y + p2 * (r + 2 * x * x) - xd\n    fy = d * y + 2 * p2 * x * y + p1 * (r + 2 * y * y) - yd\n\n    # Compute derivative of d over [x, y]\n    d_r = k1 + r * (2.0 * k2 + r * (3.0 * k3 + r * 4.0 * k4))\n    d_x = 2.0 * x * d_r\n    d_y = 2.0 * y * d_r\n\n    # Compute derivative of fx over x and y.\n    fx_x = d + d_x * x + 2.0 * p1 * y + 6.0 * p2 * x\n    fx_y = d_y * x + 2.0 * p1 * x + 2.0 * p2 * y\n\n    # Compute derivative of fy over x and y.\n    fy_x = d_x * y + 2.0 * p2 * y + 2.0 * p1 * x\n    fy_y = d + d_y * y + 2.0 * p2 * x + 6.0 * p1 * y\n\n    return fx, fy, fx_x, fx_y, fy_x, fy_y\n\n\n@torch.jit.script\ndef radial_and_tangential_undistort(\n    coords: torch.Tensor,\n    distortion_params: torch.Tensor,\n    eps: float = 1e-3,\n    max_iterations: int = 10,\n) -> torch.Tensor:\n    \"\"\"Computes undistorted coords given opencv distortion parameters.\n    Adapted from MultiNeRF\n    https://github.com/google-research/multinerf/blob/b02228160d3179300c7d499dca28cb9ca3677f32/internal/camera_utils.py#L477-L509\n\n    Args:\n        coords: The distorted coordinates.\n        distortion_params: The distortion parameters [k1, k2, k3, k4, p1, p2].\n        eps: The epsilon for the convergence.\n        max_iterations: The maximum number of iterations to perform.\n\n    Returns:\n        The undistorted coordinates.\n    \"\"\"\n\n    # Initialize from the distorted point.\n    x = coords[..., 0]\n    y = coords[..., 1]\n\n    for _ in range(max_iterations):\n        fx, fy, fx_x, fx_y, fy_x, fy_y = _compute_residual_and_jacobian(\n            x=x, y=y, xd=coords[..., 0], yd=coords[..., 1], distortion_params=distortion_params\n        )\n        denominator = fy_x * fx_y - fx_x * fy_y\n        x_numerator = fx * fy_y - fy * fx_y\n        y_numerator = fy * fx_x - fx * fy_x\n        step_x = torch.where(torch.abs(denominator) > eps, x_numerator / denominator, torch.zeros_like(denominator))\n        step_y = torch.where(torch.abs(denominator) > eps, y_numerator / denominator, torch.zeros_like(denominator))\n\n        x = x + step_x\n        y = y + step_y\n\n    return torch.stack([x, y], dim=-1)\n\n\ndef rotation_matrix(a: TensorType[3], b: TensorType[3]) -> TensorType[3, 3]:\n    \"\"\"Compute the rotation matrix that rotates vector a to vector b.\n\n    Args:\n        a: The vector to rotate.\n        b: The vector to rotate to.\n    Returns:\n        The rotation matrix.\n    \"\"\"\n    a = a / torch.linalg.norm(a)\n    b = b / torch.linalg.norm(b)\n    v = torch.cross(a, b)\n    c = torch.dot(a, b)\n    # If vectors are exactly opposite, we add a little noise to one of them\n    if c < -1 + 1e-8:\n        eps = (torch.rand(3) - 0.5) * 0.01\n        return rotation_matrix(a + eps, b)\n    s = torch.linalg.norm(v)\n    skew_sym_mat = torch.Tensor(\n        [\n            [0, -v[2], v[1]],\n            [v[2], 0, -v[0]],\n            [-v[1], v[0], 0],\n        ]\n    )\n    return torch.eye(3) + skew_sym_mat + skew_sym_mat @ skew_sym_mat * ((1 - c) / (s**2 + 1e-8))\n\n\ndef auto_orient_and_center_poses(\n    poses: TensorType[\"num_poses\":..., 4, 4], method: Literal[\"pca\", \"up\", \"none\"] = \"up\", center_poses: bool = True\n) -> Tuple[TensorType[\"num_poses\":..., 3, 4], TensorType[4, 4]]:\n    \"\"\"Orients and centers the poses. We provide two methods for orientation: pca and up.\n\n    pca: Orient the poses so that the principal component of the points is aligned with the axes.\n        This method works well when all of the cameras are in the same plane.\n    up: Orient the poses so that the average up vector is aligned with the z axis.\n        This method works well when images are not at arbitrary angles.\n\n\n    Args:\n        poses: The poses to orient.\n        method: The method to use for orientation.\n        center_poses: If True, the poses are centered around the origin.\n\n    Returns:\n        Tuple of the oriented poses and the transform matrix.\n    \"\"\"\n\n    translation = poses[..., :3, 3]\n\n    mean_translation = torch.mean(translation, dim=0)\n    translation_diff = translation - mean_translation\n\n    if center_poses:\n        translation = mean_translation\n    else:\n        translation = torch.zeros_like(mean_translation)\n\n    if method == \"pca\":\n        _, eigvec = torch.linalg.eigh(translation_diff.T @ translation_diff)\n        eigvec = torch.flip(eigvec, dims=(-1,))\n\n        if torch.linalg.det(eigvec) < 0:\n            eigvec[:, 2] = -eigvec[:, 2]\n\n        transform = torch.cat([eigvec, eigvec @ -translation[..., None]], dim=-1)\n        oriented_poses = transform @ poses\n\n        if oriented_poses.mean(axis=0)[2, 1] < 0:\n            oriented_poses[:, 1:3] = -1 * oriented_poses[:, 1:3]\n    elif method == \"up\":\n        up = torch.mean(poses[:, :3, 1], dim=0)\n        up = up / torch.linalg.norm(up)\n\n        rotation = rotation_matrix(up, torch.Tensor([0, 0, 1]))\n        transform = torch.cat([rotation, rotation @ -translation[..., None]], dim=-1)\n        oriented_poses = transform @ poses\n    elif method == \"none\":", "metadata": {"task_id": "nerfstudio-project_nerfstudio/85", "ground_truth": "        transform = torch.eye(4)", "fpath_tuple": ["nerfstudio-project_nerfstudio", "nerfstudio", "cameras", "camera_utils.py"], "context_start_lineno": 331, "line_no": 474}}
{"prompt": "from typing import Optional, Dict, Union\nimport copy\nimport torch\nimport torch.nn as nn\nfrom ding.utils import SequenceType, MODEL_REGISTRY\nfrom.vac import VAC\n\n\n@MODEL_REGISTRY.register('ppg')\nclass PPG(nn.Module):\n    mode = ['compute_actor', 'compute_critic', 'compute_actor_critic']\n\n    def __init__(\n            self,\n            obs_shape: Union[int, SequenceType],\n            action_shape: Union[int, SequenceType],\n            share_encoder: bool = True,\n            continuous: bool = False,\n            encoder_hidden_size_list: SequenceType = [128, 128, 64],\n            actor_head_hidden_size: int = 64,\n            actor_head_layer_num: int = 2,\n            critic_head_hidden_size: int = 64,\n            critic_head_layer_num: int = 1,\n            activation: Optional[nn.Module] = nn.ReLU(),\n            norm_type: Optional[str] = None,\n    ) -> None:\n        super(PPG, self).__init__()\n        self.actor_critic = VAC(\n            obs_shape, action_shape, share_encoder, continuous, encoder_hidden_size_list, actor_head_hidden_size,\n            actor_head_layer_num, critic_head_hidden_size, critic_head_layer_num, activation, norm_type\n        )\n        self.aux_critic = copy.deepcopy(self.actor_critic.critic)\n\n    def forward(self, inputs: Union[torch.Tensor, Dict], mode: str) -> Dict:\n        assert mode in self.mode, \"not support forward mode: {}/{}\".format(mode, self.mode)\n        return getattr(self, mode)(inputs)\n\n    def compute_actor(self, x: torch.Tensor) -> Dict:\n        \"\"\"\n        ReturnsKeys:\n            - necessary: ``logit``\n        \"\"\"", "metadata": {"task_id": "opendilab_ACE/108", "ground_truth": "        return self.actor_critic(x, mode='compute_actor')", "fpath_tuple": ["opendilab_ACE", "ding", "model", "template", "ppg.py"], "context_start_lineno": 0, "line_no": 42}}
{"prompt": "# Copyright 2022 The Nerfstudio Team. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\nDepth dataset.\n\"\"\"\n\nfrom typing import Dict\n\nfrom nerfstudio.data.dataparsers.base_dataparser import DataparserOutputs\nfrom nerfstudio.data.datasets.base_dataset import InputDataset\nfrom nerfstudio.data.utils.data_utils import get_depth_image_from_path\n\n\nclass DepthDataset(InputDataset):\n    \"\"\"Dataset that returns images and depths.\n\n    Args:\n        dataparser_outputs: description of where and how to read input images.\n        scale_factor: The scaling factor for the dataparser outputs.\n    \"\"\"\n\n    def __init__(self, dataparser_outputs: DataparserOutputs, scale_factor: float = 1.0):\n        super().__init__(dataparser_outputs, scale_factor)\n        assert (\n            \"depth_filenames\" in dataparser_outputs.metadata.keys()\n            and dataparser_outputs.metadata[\"depth_filenames\"] is not None\n        )\n        self.depth_filenames = self.metadata[\"depth_filenames\"]", "metadata": {"task_id": "nerfstudio-project_nerfstudio/9", "ground_truth": "        self.depth_unit_scale_factor = self.metadata[\"depth_unit_scale_factor\"]", "fpath_tuple": ["nerfstudio-project_nerfstudio", "nerfstudio", "data", "datasets", "depth_dataset.py"], "context_start_lineno": 0, "line_no": 40}}
{"prompt": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom __future__ import annotations\n\nimport os\nfrom collections import OrderedDict\nfrom copy import deepcopy\nfrom multiprocessing import connection\nfrom multiprocessing.synchronize import Lock as MpLock\nfrom time import sleep\nfrom typing import Any, Callable, Dict, List, Optional, Sequence, Union\nfrom warnings import warn\n\nimport torch\nfrom tensordict import TensorDict\nfrom tensordict.tensordict import LazyStackedTensorDict, TensorDictBase\nfrom torch import multiprocessing as mp\nfrom torchrl._utils import _check_for_faulty_process\nfrom torchrl.data import CompositeSpec, TensorSpec\nfrom torchrl.data.utils import CloudpickleWrapper, DEVICE_TYPING\nfrom torchrl.envs.common import EnvBase\nfrom torchrl.envs.env_creator import get_env_metadata\n\n\ndef _check_start(fun):\n    def decorated_fun(self: _BatchedEnv, *args, **kwargs):\n        if self.is_closed:", "metadata": {"task_id": "pytorch_rl/185", "ground_truth": "            self._create_td()", "fpath_tuple": ["pytorch_rl", "torchrl", "envs", "vec_env.py"], "context_start_lineno": 0, "line_no": 30}}
{"prompt": "from typing import Union, List\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom functools import reduce\nfrom ding.utils import list_split, MODEL_REGISTRY\nfrom ding.torch_utils.network.nn_module import fc_block, MLP\nfrom ding.torch_utils.network.transformer import ScaledDotProductAttention\nfrom.q_learning import DRQN\nfrom ding.model.template.qmix import Mixer\n\n\nclass MixerStar(nn.Module):\n    \"\"\"\n    Overview:\n        mixer network for Q_star in WQMIX, which mix up the independent q_value of\n        each agent to a total q_value and is diffrent from the Qmix's mixer network,\n        here the mixing network is a feedforward network with 3 hidden layers of 256 dim.\n    Interface:\n        __init__, forward\n    \"\"\"\n\n    def __init__(self, agent_num: int, state_dim: int, mixing_embed_dim: int) -> None:\n        \"\"\"\n        Overview:\n            initialize the mixer network of Q_star in WQMIX.\n        Arguments:\n            - agent_num (:obj:`int`): the number of agent\n            - state_dim(:obj:`int`): the dimension of global observation state\n            - mixing_embed_dim (:obj:`int`): the dimension of mixing state emdedding\n        \"\"\"\n        super(MixerStar, self).__init__()\n        self.agent_num = agent_num\n        self.state_dim = state_dim\n        self.embed_dim = mixing_embed_dim\n        self.input_dim = self.agent_num + self.state_dim  # shape N+A\n        non_lin = nn.ReLU()\n        self.net = nn.Sequential(\n            nn.Linear(self.input_dim, self.embed_dim), non_lin, nn.Linear(self.embed_dim, self.embed_dim), non_lin,\n            nn.Linear(self.embed_dim, self.embed_dim), non_lin, nn.Linear(self.embed_dim, 1)\n        )\n\n        # V(s) instead of a bias for the last layers\n        self.V = nn.Sequential(nn.Linear(self.state_dim, self.embed_dim), non_lin, nn.Linear(self.embed_dim, 1))\n\n    def forward(self, agent_qs: torch.FloatTensor, states: torch.FloatTensor) -> torch.FloatTensor:\n        \"\"\"\n        Overview:\n            forward computation graph of the mixer network for Q_star in WQMIX.\n        Arguments:\n            - agent_qs (:obj:`torch.FloatTensor`): the independent q_value of each agent\n            - states (:obj:`torch.FloatTensor`): the emdedding vector of global state\n        Returns:\n            - q_tot (:obj:`torch.FloatTensor`): the total mixed q_value\n        Shapes:\n            - agent_qs (:obj:`torch.FloatTensor`): :math:`(T,B, N)`, where T is timestep,\n              B is batch size, A is agent_num, N is obs_shape\n            - states (:obj:`torch.FloatTensor`): :math:`(T, B, M)`, where M is global_obs_shape\n            - q_tot (:obj:`torch.FloatTensor`): :math:`(T, B, )`\n        \"\"\"\n        # in below annotations about the shape of the variables, T is timestep,\n        # B is batch_size A is agent_num, N is obs_shape\uff0c for example,\n        # in 3s5z, we can set T=10, B=32, A=8, N=216\n        bs = agent_qs.shape[:-1]  # (T*B, A)\n        states = states.reshape(-1, self.state_dim)  # T*B, N),\n        agent_qs = agent_qs.reshape(-1, self.agent_num)  # (T, B, A) -> (T*B, A)\n        inputs = torch.cat([states, agent_qs], dim=1)  # (T*B, N) (T*B, A)-> (T*B, N+A)\n        advs = self.net(inputs)  # (T*B, 1)\n        vs = self.V(states)  # (T*B, 1)\n        y = advs + vs\n        q_tot = y.view(*bs)  # (T*B, 1) -> (T, B)\n\n        return q_tot\n\n\n@MODEL_REGISTRY.register('wqmix')\nclass WQMix(nn.Module):\n    \"\"\"\n    Overview:\n        WQMIX network, which is same as Qmix network\n    Interface:\n        __init__, forward, _setup_global_encoder\n    \"\"\"\n\n    def __init__(\n            self,\n            agent_num: int,\n            obs_shape: int,", "metadata": {"task_id": "opendilab_ACE/116", "ground_truth": "            global_obs_shape: int,", "fpath_tuple": ["opendilab_ACE", "ding", "model", "template", "wqmix.py"], "context_start_lineno": 0, "line_no": 88}}
{"prompt": "                 msg_type=None,\n                 sender=0,\n                 receiver=0,\n                 state=0,\n                 content='None',\n                 timestamp=0,\n                 strategy=None):\n        self._msg_type = msg_type\n        self._sender = sender\n        self._receiver = receiver\n        self._state = state\n        self._content = content\n        self._timestamp = timestamp\n        self._strategy = strategy\n\n    @property\n    def msg_type(self):\n        return self._msg_type\n\n    @msg_type.setter\n    def msg_type(self, value):\n        self._msg_type = value\n\n    @property\n    def sender(self):\n        return self._sender\n\n    @sender.setter\n    def sender(self, value):\n        self._sender = value\n\n    @property\n    def receiver(self):\n        return self._receiver\n\n    @receiver.setter\n    def receiver(self, value):\n        self._receiver = value\n\n    @property\n    def state(self):\n        return self._state\n\n    @state.setter\n    def state(self, value):\n        self._state = value\n\n    @property\n    def content(self):\n        return self._content\n\n    @content.setter\n    def content(self, value):\n        self._content = value\n\n    @property\n    def timestamp(self):\n        return self._timestamp\n\n    @timestamp.setter\n    def timestamp(self, value):\n        assert isinstance(value, int) or isinstance(value, float), \\\n            \"We only support an int or a float value for timestamp\"\n        self._timestamp = value\n\n    @property\n    def strategy(self):\n        return self._strategy\n\n    @strategy.setter\n    def strategy(self, value):\n        self._strategy = value\n\n    def __lt__(self, other):\n        if self.timestamp!= other.timestamp:\n            return self.timestamp < other.timestamp\n        else:\n            return self.state < other.state\n\n    def transform_to_list(self, x):\n        if isinstance(x, list) or isinstance(x, tuple):\n            return [self.transform_to_list(each_x) for each_x in x]\n        elif isinstance(x, dict):\n            for key in x.keys():\n                x[key] = self.transform_to_list(x[key])\n            return x\n        else:\n            if hasattr(x, 'tolist'):\n                return x.tolist()\n            else:\n                return x\n\n    def msg_to_json(self, to_list=False):\n        if to_list:\n            self.content = self.transform_to_list(self.content)\n\n        json_msg = {\n           'msg_type': self.msg_type,\n           'sender': self.sender,\n           'receiver': self.receiver,\n           'state': self.state,\n            'content': self.content,\n            'timestamp': self.timestamp,\n           'strategy': self.strategy,\n        }\n        return json.dumps(json_msg)\n\n    def json_to_msg(self, json_string):\n        json_msg = json.loads(json_string)\n        self.msg_type = json_msg['msg_type']\n        self.sender = json_msg['sender']\n        self.receiver = json_msg['receiver']\n        self.state = json_msg['state']\n        self.content = json_msg['content']\n        self.timestamp = json_msg['timestamp']\n        self.strategy = json_msg['strategy']\n\n    def create_by_type(self, value, nested=False):\n        if isinstance(value, dict):\n            if isinstance(list(value.keys())[0], str):\n                m_dict = gRPC_comm_manager_pb2.mDict_keyIsString()\n                key_type ='string'\n            else:\n                m_dict = gRPC_comm_manager_pb2.mDict_keyIsInt()\n                key_type = 'int'\n\n            for key in value.keys():\n                m_dict.dict_value[key].MergeFrom(\n                    self.create_by_type(value[key], nested=True))\n            if nested:\n                msg_value = gRPC_comm_manager_pb2.MsgValue()\n                if key_type =='string':\n                    msg_value.dict_msg_stringkey.MergeFrom(m_dict)\n                else:\n                    msg_value.dict_msg_intkey.MergeFrom(m_dict)\n                return msg_value\n            else:\n                return m_dict\n        elif isinstance(value, list) or isinstance(value, tuple):\n            m_list = gRPC_comm_manager_pb2.mList()\n            for each in value:\n                m_list.list_value.append(self.create_by_type(each,\n                                                             nested=True))\n            if nested:\n                msg_value = gRPC_comm_manager_pb2.MsgValue()\n                msg_value.list_msg.MergeFrom(m_list)\n                return msg_value\n            else:\n                return m_list\n        else:\n            m_single = gRPC_comm_manager_pb2.mSingle()\n            if type(value) in [int, np.int32]:\n                m_single.int_value = value\n            elif type(value) in [str]:\n                m_single.str_value = value\n            elif type(value) in [float, np.float32]:\n                m_single.float_value = value\n            else:\n                raise ValueError(\n                    'The data type {} has not been supported.'.format(\n                        type(value)))\n\n            if nested:\n                msg_value = gRPC_comm_manager_pb2.MsgValue()\n                msg_value.single_msg.MergeFrom(m_single)\n                return msg_value\n            else:\n                return m_single\n\n    def build_msg_value(self, value):\n        msg_value = gRPC_comm_manager_pb2.MsgValue()\n\n        if isinstance(value, list) or isinstance(value, tuple):\n            msg_value.list_msg.MergeFrom(self.create_by_type(value))\n        elif isinstance(value, dict):\n            if isinstance(list(value.keys())[0], str):\n                msg_value.dict_msg_stringkey.MergeFrom(\n                    self.create_by_type(value))\n            else:\n                msg_value.dict_msg_intkey.MergeFrom(self.create_by_type(value))\n        else:\n            msg_value.single_msg.MergeFrom(self.create_by_type(value))\n\n        return msg_value\n\n    def transform(self, to_list=False):\n        if to_list:\n            self.content = self.transform_to_list(self.content)\n\n        splited_msg = gRPC_comm_manager_pb2.MessageRequest()  # map/dict\n        splited_msg.msg['sender'].MergeFrom(self.build_msg_value(self.sender))\n        splited_msg.msg['receiver'].MergeFrom(\n            self.build_msg_value(self.receiver))\n        splited_msg.msg['state'].MergeFrom(self.build_msg_value(self.state))\n        splited_msg.msg['msg_type'].MergeFrom(\n            self.build_msg_value(self.msg_type))\n        splited_msg.msg['content'].MergeFrom(self.build_msg_value(\n            self.content))\n        splited_msg.msg['timestamp'].MergeFrom(", "metadata": {"task_id": "alibaba_FederatedScope/116", "ground_truth": "            self.build_msg_value(self.timestamp))", "fpath_tuple": ["alibaba_FederatedScope", "federatedscope", "core", "message.py"], "context_start_lineno": 19, "line_no": 218}}
{"prompt": "Object()\n\n        # mock add obj class to `diffusers`\n        setattr(diffusers, \"SchedulerObject\", SchedulerObject)\n        setattr(diffusers, \"SchedulerObject2\", SchedulerObject2)\n        setattr(diffusers, \"SchedulerObject3\", SchedulerObject3)\n        logger = logging.get_logger(\"diffusers.configuration_utils\")\n        logger.setLevel(diffusers.logging.INFO)\n\n        with tempfile.TemporaryDirectory() as tmpdirname:\n            obj.save_config(tmpdirname)\n\n            with CaptureLogger(logger) as cap_logger_1:\n                config = SchedulerObject.load_config(tmpdirname)\n                new_obj_1 = SchedulerObject.from_config(config)\n\n            with CaptureLogger(logger) as cap_logger_2:\n                config = SchedulerObject2.load_config(tmpdirname)\n                new_obj_2 = SchedulerObject2.from_config(config)\n\n            with CaptureLogger(logger) as cap_logger_3:\n                config = SchedulerObject3.load_config(tmpdirname)\n                new_obj_3 = SchedulerObject3.from_config(config)\n\n        assert new_obj_1.__class__ == SchedulerObject\n        assert new_obj_2.__class__ == SchedulerObject2\n        assert new_obj_3.__class__ == SchedulerObject3\n\n        assert cap_logger_1.out == \"\"\n        assert cap_logger_2.out == \"{'f'} was not found in config. Values will be initialized to default values.\\n\"\n        assert cap_logger_3.out == \"{'f'} was not found in config. Values will be initialized to default values.\\n\"\n\n\nclass SchedulerCommonTest(unittest.TestCase):\n    scheduler_classes = ()\n    forward_default_kwargs = ()\n\n    @property\n    def dummy_sample(self):\n        batch_size = 4\n        num_channels = 3\n        height = 8\n        width = 8\n\n        sample = torch.rand((batch_size, num_channels, height, width))\n\n        return sample\n\n    @property\n    def dummy_sample_deter(self):\n        batch_size = 4\n        num_channels = 3\n        height = 8\n        width = 8\n\n        num_elems = batch_size * num_channels * height * width\n        sample = torch.arange(num_elems)\n        sample = sample.reshape(num_channels, height, width, batch_size)\n        sample = sample / num_elems\n        sample = sample.permute(3, 0, 1, 2)\n\n        return sample\n\n    def get_scheduler_config(self):\n        raise NotImplementedError\n\n    def dummy_model(self):\n        def model(sample, t, *args):\n            return sample * t / (t + 1)\n\n        return model\n\n    def check_over_configs(self, time_step=0, **config):\n        kwargs = dict(self.forward_default_kwargs)\n\n        num_inference_steps = kwargs.pop(\"num_inference_steps\", None)\n\n        for scheduler_class in self.scheduler_classes:\n            # TODO(Suraj) - delete the following two lines once DDPM, DDIM, and PNDM have timesteps casted to float by default\n            if scheduler_class in (EulerAncestralDiscreteScheduler, EulerDiscreteScheduler, LMSDiscreteScheduler):\n                time_step = float(time_step)\n\n            scheduler_config = self.get_scheduler_config(**config)\n            scheduler = scheduler_class(**scheduler_config)\n\n            if scheduler_class == VQDiffusionScheduler:\n                num_vec_classes = scheduler_config[\"num_vec_classes\"]\n                sample = self.dummy_sample(num_vec_classes)\n                model = self.dummy_model(num_vec_classes)\n                residual = model(sample, time_step)\n            else:\n                sample = self.dummy_sample\n                residual = 0.1 * sample\n\n            with tempfile.TemporaryDirectory() as tmpdirname:\n                scheduler.save_config(tmpdirname)\n                new_scheduler = scheduler_class.from_pretrained(tmpdirname)\n\n            if num_inference_steps is not None and hasattr(scheduler, \"set_timesteps\"):\n                scheduler.set_timesteps(num_inference_steps)\n                new_scheduler.set_timesteps(num_inference_steps)\n            elif num_inference_steps is not None and not hasattr(scheduler, \"set_timesteps\"):\n                kwargs[\"num_inference_steps\"] = num_inference_steps\n\n            # Set the seed before step() as some schedulers are stochastic like EulerAncestralDiscreteScheduler, EulerDiscreteScheduler\n            if \"generator\" in set(inspect.signature(scheduler.step).parameters.keys()):\n                kwargs[\"generator\"] = torch.manual_seed(0)\n            output = scheduler.step(residual, time_step, sample, **kwargs).prev_sample\n\n            if \"generator\" in set(inspect.signature(scheduler.step).parameters.keys()):\n                kwargs[\"generator\"] = torch.manual_seed(0)\n            new_output = new_scheduler.step(residual, time_step, sample, **kwargs).prev_sample\n\n            assert torch.sum(torch.abs(output - new_output)) < 1e-5, \"Scheduler outputs are not identical\"\n\n    def check_over_forward(self, time_step=0, **forward_kwargs):\n        kwargs = dict(self.forward_default_kwargs)\n        kwargs.update(forward_kwargs)\n\n        num_inference_steps = kwargs.pop(\"num_inference_steps\", None)\n\n        for scheduler_class in self.scheduler_classes:\n            if scheduler_class in (EulerAncestralDiscreteScheduler, EulerDiscreteScheduler, LMSDiscreteScheduler):\n                time_step = float(time_step)\n\n            scheduler_config = self.get_scheduler_config()\n            scheduler = scheduler_class(**scheduler_config)\n\n            if scheduler_class == VQDiffusionScheduler:\n                num_vec_classes = scheduler_config[\"num_vec_classes\"]\n                sample = self.dummy_sample(num_vec_classes)\n                model = self.dummy_model(num_vec_classes)\n                residual = model(sample, time_step)\n            else:\n                sample = self.dummy_sample\n                residual = 0.1 * sample\n\n            with tempfile.TemporaryDirectory() as tmpdirname:\n                scheduler.save_config(tmpdirname)\n                new_scheduler = scheduler_class.from_pretrained(tmpdirname)\n\n            if num_inference_steps is not None and hasattr(scheduler, \"set_timesteps\"):\n                scheduler.set_timesteps(num_inference_steps)\n                new_scheduler.set_timesteps(num_inference_steps)\n            elif num_inference_steps is not None and not hasattr(scheduler, \"set_timesteps\"):\n                kwargs[\"num_inference_steps\"] = num_inference_steps\n\n            if \"generator\" in set(inspect.signature(scheduler.step).parameters.keys()):\n                kwargs[\"generator\"] = torch.manual_seed(0)", "metadata": {"task_id": "huggingface_diffusers/178", "ground_truth": "            output = scheduler.step(residual, time_step, sample, **kwargs).prev_sample", "fpath_tuple": ["huggingface_diffusers", "tests", "test_scheduler.py"], "context_start_lineno": 184, "line_no": 333}}
{"prompt": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\n\"\"\"\nSample latency benchmarking (using RPC)\n======================================\nA rough benchmark of sample latency using different storage types over the network using `torch.rpc`.\nRun this script with --rank=0 and --rank=1 flags set in separate processes - these ranks correspond to the trainer worker and buffer worker respectively, and both need to be initialised.\ne.g. to benchmark LazyMemmapStorage, run the following commands using either two separate shells or multiprocessing.\n    - python3 benchmark_sample_latency_over_rpc.py --rank=0 --storage=LazyMemmapStorage\n    - python3 benchmark_sample_latency_over_rpc.py --rank=1 --storage=LazyMemmapStorage\nThis code is based on examples/distributed/distributed_replay_buffer.py.\n\"\"\"\nimport argparse\nimport os\nimport pickle\nimport sys\nimport time\nimport timeit\nfrom datetime import datetime\n\nimport torch\nimport torch.distributed.rpc as rpc\nfrom tensordict import TensorDict\nfrom torchrl.data.replay_buffers import RemoteTensorDictReplayBuffer\nfrom torchrl.data.replay_buffers.samplers import RandomSampler\nfrom torchrl.data.replay_buffers.storages import (\n    LazyMemmapStorage,\n    LazyTensorStorage,\n    ListStorage,\n)\nfrom torchrl.data.replay_buffers.writers import RoundRobinWriter\n\nRETRY_LIMIT = 2\nRETRY_DELAY_SECS = 3\nREPLAY_BUFFER_NODE = \"ReplayBuffer\"\nTRAINER_NODE = \"Trainer\"\nTENSOR_SIZE = 3 * 86 * 86\nBUFFER_SIZE = 1001\nBATCH_SIZE = 256\nREPEATS = 1000\n\nstorage_options = {\n    \"LazyMemmapStorage\": LazyMemmapStorage,\n    \"LazyTensorStorage\": LazyTensorStorage,\n    \"ListStorage\": ListStorage,\n}\n\nstorage_arg_options = {\n    \"LazyMemmapStorage\": {\"scratch_dir\": \"/tmp/\", \"device\": torch.device(\"cpu\")},\n    \"LazyTensorStorage\": {},\n    \"ListStorage\": {},\n}\nparser = argparse.ArgumentParser(\n    description=\"RPC Replay Buffer Example\",\n    formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n)\n\nparser.add_argument(\n    \"--rank\",\n    type=int,\n    default=-1,\n    help=\"Node Rank [0 = Replay Buffer, 1 = Dummy Trainer, 2+ = Dummy Data Collector]\",\n)\n\nparser.add_argument(\n    \"--storage\",\n    type=str,", "metadata": {"task_id": "pytorch_rl/9", "ground_truth": "    default=\"LazyMemmapStorage\",", "fpath_tuple": ["pytorch_rl", "benchmarks", "storage", "benchmark_sample_latency_over_rpc.py"], "context_start_lineno": 0, "line_no": 70}}
{"prompt": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import annotations\n\n\"\"\"Setup for pip package.\"\"\"\nimport os\nimport sys\nfrom setuptools import find_namespace_packages\nfrom setuptools import setup\nfrom setuptools.command.build import build\n\n\ndef _get_version():\n  with open('vizier/__init__.py') as fp:\n    for line in fp:\n      if line.startswith('__version__'):\n        g = {}\n        exec(line, g)  # pylint: disable=exec-used\n        return g['__version__']\n    raise ValueError('`__version__` not defined in `vizier/__init__.py`')\n\n\ndef _strip_comments_from_line(s: str) -> str:\n  \"\"\"Parses a line of a requirements.txt file.\"\"\"\n  requirement, *_ = s.split('#')\n  return requirement.strip()\n\n\ndef _parse_requirements(requirements_txt_path: str) -> list[str]:\n  \"\"\"Returns a list of dependencies for setup() from requirements.txt.\"\"\"\n\n  # Currently a requirements.txt is being used to specify dependencies. In order\n  # to avoid specifying it in two places, we're going to use that file as the\n  # source of truth.\n  with open(requirements_txt_path) as fp:\n    # Parse comments.\n    lines = [_strip_comments_from_line(line) for line in fp.read().splitlines()]\n    # Remove empty lines and direct github repos (not allowed in PyPI setups)\n    return [l for l in lines if (l and 'github.com' not in l)]\n\n\nclass BuildCmd(build):\n  \"\"\"Custom installation script to build the protos.\"\"\"\n\n  def run(self):\n    current_path = os.path.dirname(os.path.realpath(__file__))\n    sys.stdout.write('current_path: {}'.format(current_path))\n    with os.scandir('.') as it:\n      for entry in it:\n        if entry.name.startswith('build_protos.sh'):\n          sys.stdout.write('{}'.format(entry))\n    if os.system('bash build_protos.sh'):\n      raise OSError('Failed to run build_protos.sh')\n    build.run(self)\n\n\n_VERSION = _get_version()\n\nsetup(\n    name='google-vizier',\n    version=_VERSION,\n    url='https://github.com/google/vizier',\n    license='Apache License 2.0',\n    author='Vizier Team',\n    description=(\n        'Open Source Vizier: Distributed service framework for blackbox'\n       'optimization and research.'\n    ),\n    long_description=open('README.md').read(),\n    long_description_content_type='text/markdown',\n    author_email='oss-vizier-dev@google.com',\n    # Contained modules and scripts.\n    packages=find_namespace_packages(\n        include=['vizier*'], exclude=['*_test.py', 'examples']\n    ),\n    install_requires=_parse_requirements('requirements.txt'),\n    extras_require={", "metadata": {"task_id": "google_vizier/64", "ground_truth": "        'jax': _parse_requirements('requirements-jax.txt'),", "fpath_tuple": ["google_vizier", "setup.py"], "context_start_lineno": 0, "line_no": 89}}
{"prompt": "import argparse\nimport hashlib\nimport math\nimport os\nfrom pathlib import Path\nfrom typing import Optional\n\nimport torch\nimport torch.nn.functional as F\nimport torch.utils.checkpoint\nfrom torch.utils.data import Dataset\n\nimport colossalai\nfrom colossalai.context.parallel_mode import ParallelMode\nfrom colossalai.core import global_context as gpc\nfrom colossalai.logging import disable_existing_loggers, get_dist_logger\nfrom colossalai.nn.optimizer.gemini_optimizer import GeminiAdamOptimizer\nfrom colossalai.nn.parallel.utils import get_static_torch_model\nfrom colossalai.utils import get_current_device\nfrom colossalai.utils.model.colo_init_context import ColoInitContext\nfrom diffusers import AutoencoderKL, DDPMScheduler, DiffusionPipeline, UNet2DConditionModel\nfrom diffusers.optimization import get_scheduler\nfrom huggingface_hub import HfFolder, Repository, create_repo, whoami\nfrom PIL import Image\nfrom torchvision import transforms\nfrom tqdm.auto import tqdm\nfrom transformers import AutoTokenizer, PretrainedConfig\n\n\ndisable_existing_loggers()\nlogger = get_dist_logger()\n\n\ndef import_model_class_from_model_name_or_path(pretrained_model_name_or_path: str):\n    text_encoder_config = PretrainedConfig.from_pretrained(\n        pretrained_model_name_or_path,\n        subfolder=\"text_encoder\",\n        revision=args.revision,\n    )\n    model_class = text_encoder_config.architectures[0]\n\n    if model_class == \"CLIPTextModel\":\n        from transformers import CLIPTextModel\n\n        return CLIPTextModel\n    elif model_class == \"RobertaSeriesModelWithTransformation\":\n        from diffusers.pipelines.alt_diffusion.modeling_roberta_series import RobertaSeriesModelWithTransformation\n\n        return RobertaSeriesModelWithTransformation\n    else:\n        raise ValueError(f\"{model_class} is not supported.\")\n\n\ndef parse_args(input_args=None):\n    parser = argparse.ArgumentParser(description=\"Simple example of a training script.\")\n    parser.add_argument(\n        \"--pretrained_model_name_or_path\",\n        type=str,\n        default=None,\n        required=True,\n        help=\"Path to pretrained model or model identifier from huggingface.co/models.\",\n    )\n    parser.add_argument(\n        \"--revision\",\n        type=str,\n        default=None,\n        required=False,\n        help=\"Revision of pretrained model identifier from huggingface.co/models.\",\n    )\n    parser.add_argument(\n        \"--tokenizer_name\",\n        type=str,\n        default=None,\n        help=\"Pretrained tokenizer name or path if not the same as model_name\",\n    )\n    parser.add_argument(\n        \"--instance_data_dir\",\n        type=str,\n        default=None,\n        required=True,\n        help=\"A folder containing the training data of instance images.\",\n    )\n    parser.add_argument(\n        \"--class_data_dir\",\n        type=str,\n        default=None,\n        required=False,\n        help=\"A folder containing the training data of class images.\",\n    )\n    parser.add_argument(\n        \"--instance_prompt\",\n        type=str,\n        default=\"a photo of sks dog\",\n        required=False,\n        help=\"The prompt with identifier specifying the instance\",\n    )\n    parser.add_argument(\n        \"--class_prompt\",\n        type=str,\n        default=None,\n        help=\"The prompt to specify images in the same class as provided instance images.\",\n    )\n    parser.add_argument(\n        \"--with_prior_preservation\",\n        default=False,\n        action=\"store_true\",\n        help=\"Flag to add prior preservation loss.\",\n    )", "metadata": {"task_id": "huggingface_diffusers/65", "ground_truth": "    parser.add_argument(\"--prior_loss_weight\", type=float, default=1.0, help=\"The weight of prior preservation loss.\")", "fpath_tuple": ["huggingface_diffusers", "examples", "research_projects", "colossalai", "train_dreambooth_colossalai.py"], "context_start_lineno": 0, "line_no": 108}}
{"prompt": "# coding=utf-8\n# Copyright 2022 HuggingFace Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport inspect\nimport tempfile\nimport unittest\nfrom typing import Dict, List, Tuple\n\nimport numpy as np\nimport torch\n\nfrom diffusers.models import ModelMixin\nfrom diffusers.training_utils import EMAModel\nfrom diffusers.utils import torch_device\n\n\nclass ModelTesterMixin:\n    def test_from_save_pretrained(self):\n        init_dict, inputs_dict = self.prepare_init_args_and_inputs_for_common()\n\n        model = self.model_class(**init_dict)\n        model.to(torch_device)\n        model.eval()\n\n        with tempfile.TemporaryDirectory() as tmpdirname:\n            model.save_pretrained(tmpdirname)\n            new_model = self.model_class.from_pretrained(tmpdirname)\n            new_model.to(torch_device)\n\n        with torch.no_grad():\n            # Warmup pass when using mps (see #372)\n            if torch_device == \"mps\" and isinstance(model, ModelMixin):\n                _ = model(**self.dummy_input)\n                _ = new_model(**self.dummy_input)\n\n            image = model(**inputs_dict)\n            if isinstance(image, dict):\n                image = image.sample\n\n            new_image = new_model(**inputs_dict)\n\n            if isinstance(new_image, dict):\n                new_image = new_image.sample\n\n        max_diff = (image - new_image).abs().sum().item()\n        self.assertLessEqual(max_diff, 5e-5, \"Models give different forward passes\")\n\n    def test_from_save_pretrained_dtype(self):\n        init_dict, inputs_dict = self.prepare_init_args_and_inputs_for_common()\n\n        model = self.model_class(**init_dict)\n        model.to(torch_device)\n        model.eval()\n\n        for dtype in [torch.float32, torch.float16, torch.bfloat16]:\n            if torch_device == \"mps\" and dtype == torch.bfloat16:\n                continue\n            with tempfile.TemporaryDirectory() as tmpdirname:\n                model.to(dtype)\n                model.save_pretrained(tmpdirname)\n                new_model = self.model_class.from_pretrained(tmpdirname, low_cpu_mem_usage=True, torch_dtype=dtype)\n                assert new_model.dtype == dtype\n                new_model = self.model_class.from_pretrained(tmpdirname, low_cpu_mem_usage=False, torch_dtype=dtype)\n                assert new_model.dtype == dtype\n\n    def test_determinism(self):\n        init_dict, inputs_dict = self.prepare_init_args_and_inputs_for_common()\n        model = self.model_class(**init_dict)\n        model.to(torch_device)\n        model.eval()\n\n        with torch.no_grad():\n            # Warmup pass when using mps (see #372)\n            if torch_device == \"mps\" and isinstance(model, ModelMixin):\n                model(**self.dummy_input)\n\n            first = model(**inputs_dict)", "metadata": {"task_id": "huggingface_diffusers/61", "ground_truth": "            if isinstance(first, dict):", "fpath_tuple": ["huggingface_diffusers", "tests", "test_modeling_common.py"], "context_start_lineno": 0, "line_no": 89}}
{"prompt": "# This file is autogenerated by the command `make fix-copies`, do not edit.\n# flake8: noqa\n\nfrom..utils import DummyObject, requires_backends\n\n\nclass FlaxModelMixin(metaclass=DummyObject):\n    _backends = [\"flax\"]\n\n    def __init__(self, *args, **kwargs):\n        requires_backends(self, [\"flax\"])\n\n    @classmethod\n    def from_config(cls, *args, **kwargs):\n        requires_backends(cls, [\"flax\"])\n\n    @classmethod\n    def from_pretrained(cls, *args, **kwargs):\n        requires_backends(cls, [\"flax\"])\n\n\nclass FlaxUNet2DConditionModel(metaclass=DummyObject):\n    _backends = [\"flax\"]\n\n    def __init__(self, *args, **kwargs):\n        requires_backends(self, [\"flax\"])\n\n    @classmethod\n    def from_config(cls, *args, **kwargs):\n        requires_backends(cls, [\"flax\"])\n\n    @classmethod\n    def from_pretrained(cls, *args, **kwargs):\n        requires_backends(cls, [\"flax\"])\n\n\nclass FlaxAutoencoderKL(metaclass=DummyObject):\n    _backends = [\"flax\"]\n\n    def __init__(self, *args, **kwargs):\n        requires_backends(self, [\"flax\"])\n\n    @classmethod\n    def from_config(cls, *args, **kwargs):\n        requires_backends(cls, [\"flax\"])\n\n    @classmethod\n    def from_pretrained(cls, *args, **kwargs):\n        requires_backends(cls, [\"flax\"])\n\n\nclass FlaxDiffusionPipeline(metaclass=DummyObject):\n    _backends = [\"flax\"]\n\n    def __init__(self, *args, **kwargs):\n        requires_backends(self, [\"flax\"])\n\n    @classmethod\n    def from_config(cls, *args, **kwargs):\n        requires_backends(cls, [\"flax\"])\n\n    @classmethod\n    def from_pretrained(cls, *args, **kwargs):\n        requires_backends(cls, [\"flax\"])\n\n\nclass FlaxDDIMScheduler(metaclass=DummyObject):\n    _backends = [\"flax\"]\n\n    def __init__(self, *args, **kwargs):\n        requires_backends(self, [\"flax\"])\n\n    @classmethod\n    def from_config(cls, *args, **kwargs):\n        requires_backends(cls, [\"flax\"])\n\n    @classmethod\n    def from_pretrained(cls, *args, **kwargs):\n        requires_backends(cls, [\"flax\"])\n\n\nclass FlaxDDPMScheduler(metaclass=DummyObject):\n    _backends = [\"flax\"]\n\n    def __init__(self, *args, **kwargs):\n        requires_backends(self, [\"flax\"])\n\n    @classmethod\n    def from_config(cls, *args, **kwargs):\n        requires_backends(cls, [\"flax\"])\n\n    @classmethod\n    def from_pretrained(cls, *args, **kwargs):\n        requires_backends(cls, [\"flax\"])\n\n\nclass FlaxDPMSolverMultistepScheduler(metaclass=DummyObject):\n    _backends = [\"flax\"]\n\n    def __init__(self, *args, **kwargs):\n        requires_backends(self, [\"flax\"])\n\n    @classmethod\n    def from_config(cls, *args, **kwargs):\n        requires_backends(cls, [\"flax\"])\n\n    @classmethod\n    def from_pretrained(cls, *args, **kwargs):\n        requires_backends(cls, [\"flax\"])\n\n\nclass FlaxKarrasVeScheduler(metaclass=DummyObject):\n    _backends = [\"flax\"]\n\n    def __init__(self, *args, **kwargs):\n        requires_backends(self, [\"flax\"])\n\n    @classmethod\n    def from_config(cls, *args, **kwargs):\n        requires_backends(cls, [\"flax\"])\n\n    @classmethod\n    def from_pretrained(cls, *args, **kwargs):\n        requires_backends(cls, [\"flax\"])\n\n\nclass FlaxLMSDiscreteScheduler(metaclass=DummyObject):\n    _backends = [\"flax\"]\n\n    def __init__(self, *args, **kwargs):\n        requires_backends(self, [\"flax\"])\n\n    @classmethod\n    def from_config(cls, *args, **kwargs):\n        requires_backends(cls, [\"flax\"])\n\n    @classmethod\n    def from_pretrained(cls, *args, **kwargs):\n        requires_backends(cls, [\"flax\"])\n\n\nclass FlaxPNDMScheduler(metaclass=DummyObject):", "metadata": {"task_id": "huggingface_diffusers/191", "ground_truth": "    _backends = [\"flax\"]", "fpath_tuple": ["huggingface_diffusers", "src", "diffusers", "utils", "dummy_flax_objects.py"], "context_start_lineno": 0, "line_no": 142}}
{"prompt": "\n        distribute: bool = True,\n    ) -> jnp.ndarray:\n        r\"\"\"\n        Estimate the predictive mean of the target variable, that is\n\n       .. math::\n            \\mathbb{E}_{Y|x, \\mathcal{D}}[Y],\n\n        where:\n         - :math:`x` is an observed input variable;\n         - :math:`Y` is a random target variable;\n         - :math:`\\mathcal{D}` is the observed training data set;\n         - :math:`W` denotes the random model parameters.\n\n        Parameters\n        ----------\n        inputs_loader : InputsLoader\n            A loader of input data points.\n        n_posterior_samples : int\n            Number of samples to draw from the posterior distribution for each input.\n        rng: Optional[PRNGKeyArray]\n            A random number generator. If not passed, this will be taken from the attributes of this class.\n        distribute: bool\n            Whether to distribute computation over multiple devices, if available.\n\n        Returns\n        -------\n        jnp.ndarray\n            An estimate of the predictive mean for each input.\n        \"\"\"\n        if rng is None:\n            rng = self.rng.get()\n\n        return self._loop_fun_through_inputs_loader(\n            self._batched_mean, inputs_loader, n_posterior_samples, rng, distribute\n        )\n\n    def _batched_mean(\n        self,\n        inputs: Array,\n        n_posterior_samples: int = 30,\n        rng: Optional[PRNGKeyArray] = None,\n    ) -> jnp.ndarray:\n        if rng is None:\n            rng = self.rng.get()\n        keys = random.split(rng, n_posterior_samples)\n\n        def fun(i, _curr_sum):\n            _sample = self.posterior.sample(inputs=inputs, rng=keys[i])\n            _curr_sum += self.likelihood._batched_mean(\n                _sample.params,\n                inputs,\n                _sample.mutable,\n                calib_params=_sample.calib_params,\n                calib_mutable=_sample.calib_mutable,\n            )\n            return _curr_sum\n\n        curr_sum = fun(0, 0.0)\n        curr_sum = lax.fori_loop(1, n_posterior_samples, fun, curr_sum)\n        return curr_sum / n_posterior_samples\n\n    @abc.abstractmethod\n    def mode(\n        self,\n        inputs_loader: InputsLoader,\n        n_posterior_samples: int = 30,\n        means: Optional[jnp.ndarray] = None,\n        rng: Optional[PRNGKeyArray] = None,\n        distribute: bool = True,\n    ) -> jnp.ndarray:\n        r\"\"\"\n        Estimate the predictive mode of the target variable, that is\n\n       .. math::\n            \\text{argmax}_y\\ p(y|x, \\mathcal{D}),\n\n        where:\n         - :math:`x` is an observed input variable;\n         - :math:`\\mathcal{D}` is the observed training data set;\n         - :math:`y` is the target variable to optimize upon.\n\n        Parameters\n        ----------\n        inputs_loader : InputsLoader\n            A loader of input data points.\n        n_posterior_samples : int\n            Number of samples to draw from the posterior distribution for each input.\n        means : Optional[jnp.ndarray] = None\n            An estimate of the predictive mean.\n        rng : Optional[PRNGKeyArray]\n            A random number generator. If not passed, this will be taken from the attributes of this class.\n        distribute: bool\n            Whether to distribute computation over multiple devices, if available.\n\n        Returns\n        -------\n        jnp.ndarray\n            An estimate of the predictive mode for each input.\n        \"\"\"\n        pass\n\n    def aleatoric_variance(\n        self,\n        inputs_loader: InputsLoader,\n        n_posterior_samples: int = 30,\n        rng: Optional[PRNGKeyArray] = None,\n        distribute: bool = True,\n    ) -> jnp.ndarray:\n        r\"\"\"\n        Estimate the predictive aleatoric variance of the target variable, that is\n\n       .. math::\n            \\text{Var}_{W|\\mathcal{D}}[\\mathbb{E}_{Y|W, x}[Y]],\n\n        where:\n         - :math:`x` is an observed input variable;\n         - :math:`Y` is a random target variable;\n         - :math:`\\mathcal{D}` is the observed training data set;\n         - :math:`W` denotes the random model parameters.\n\n        Parameters\n        ----------\n        inputs_loader : InputsLoader\n            A loader of input data points.\n        n_posterior_samples : int\n            Number of samples to draw from the posterior distribution for each input.\n        rng : Optional[PRNGKeyArray]\n            A random number generator. If not passed, this will be taken from the attributes of this class.\n        distribute: bool\n            Whether to distribute computation over multiple devices, if available.\n\n        Returns\n        -------\n        jnp.ndarray\n            An estimate of the predictive aleatoric variance for each input.\n        \"\"\"\n        if rng is None:\n            rng = self.rng.get()\n\n        return self._loop_fun_through_inputs_loader(\n            self._batched_aleatoric_variance,\n            inputs_loader,\n            n_posterior_samples,\n            rng,\n            distribute,\n        )\n\n    def _batched_aleatoric_variance(\n        self,\n        inputs: Array,\n        n_posterior_samples: int = 30,\n        rng: Optional[PRNGKeyArray] = None,\n    ) -> jnp.ndarray:\n        if rng is None:\n            rng = self.rng.get()\n        keys = random.split(rng, n_posterior_samples)\n\n        def fun(i, _curr_sum):\n            _sample = self.posterior.sample(inputs=inputs, rng=keys[i])\n            _curr_sum += self.likelihood._batched_variance(\n                _sample.params,\n                inputs,\n                _sample.mutable,\n                calib_params=_sample.calib_params,\n                calib_mutable=_sample.calib_mutable,\n            )\n            return _curr_sum\n\n        curr_sum = fun(0, 0.0)\n        curr_sum = lax.fori_loop(1, n_posterior_samples, fun, curr_sum)\n        return curr_sum / n_posterior_samples\n\n    def epistemic_variance(\n        self,\n        inputs_loader: InputsLoader,\n        n_posterior_samples: int = 30,\n        rng: Optional[PRNGKeyArray] = None,\n        distribute: bool = True,\n    ) -> jnp.ndarray:\n        r\"\"\"\n        Estimate the predictive epistemic variance of the one-hot encoded target variable, that is\n\n       .. math::\n            \\mathbb{E}_{W|D}[\\text{Var}_{Y|W, x}[Y]],\n\n        where:\n         - :math:`x` is an observed input variable;\n         - :math:`Y` is a random target variable;\n         - :math:`\\mathcal{D}` is the observed training data set;\n         - :math:`W` denotes the random model parameters.\n\n        Parameters\n        ----------\n        inputs_loader : InputsLoader", "metadata": {"task_id": "awslabs_fortuna/47", "ground_truth": "            A loader of input data points.", "fpath_tuple": ["awslabs_fortuna", "fortuna", "prob_model", "predictive", "base.py"], "context_start_lineno": 455, "line_no": 651}}
{"prompt": "_config=self.reg_calib_config_nodir_nodump,\n            )\n            sample = prob_reg.posterior.sample()\n\n            # no save dir but dump\n            with self.assertRaises(ValueError):\n                status = prob_reg.train(\n                    train_data_loader=self.reg_train_data_loader,\n                    calib_data_loader=self.reg_val_data_loader,\n                    val_data_loader=self.reg_val_data_loader,\n                    fit_config=self.reg_fit_config_nodir_dump,\n                    calib_config=self.reg_calib_config_nodir_nodump,\n                )\n\n            # save dir, no dump\n            status = prob_reg.train(\n                train_data_loader=self.reg_train_data_loader,\n                calib_data_loader=self.reg_val_data_loader,\n                val_data_loader=self.reg_val_data_loader,\n                fit_config=self.reg_fit_config_dir_nodump(tmp_dir),\n                calib_config=self.reg_calib_config_nodir_nodump,\n            )\n            sample = prob_reg.posterior.sample()\n            prob_reg.posterior.load_state(tmp_dir)\n\n            # save dir and dump\n            status = prob_reg.train(\n                train_data_loader=self.reg_train_data_loader,\n                calib_data_loader=self.reg_val_data_loader,\n                val_data_loader=self.reg_val_data_loader,\n                fit_config=self.reg_fit_config_dir_nodump(tmp_dir),\n                calib_config=self.reg_calib_config_nodir_nodump,\n            )\n            sample = prob_reg.posterior.sample()\n            prob_reg.posterior.load_state(tmp_dir)\n\n            # restore\n            status = prob_reg.train(\n                train_data_loader=self.reg_train_data_loader,\n                calib_data_loader=self.reg_val_data_loader,\n                val_data_loader=self.reg_val_data_loader,\n                fit_config=self.reg_fit_config_restore(tmp_dir),\n                calib_config=self.reg_calib_config_nodir_nodump,\n            )\n\n            # load state\n            prob_reg.load_state(checkpoint_path=tmp_dir)\n\n            # save state\n            prob_reg.save_state(checkpoint_path=tmp_dir)\n\n    def test_dryrun_class_deep_ensemble(self):\n        with tempfile.TemporaryDirectory() as tmp_dir:\n            prob_class = ProbClassifier(\n                model=MyModel(self.class_output_dim),\n                posterior_approximator=DeepEnsemblePosteriorApproximator(),\n                output_calibrator=ClassificationTemperatureScaler(),\n            )\n            # no save dir, no dump\n            status = prob_class.train(\n                train_data_loader=self.class_train_data_loader,\n                calib_data_loader=self.class_val_data_loader,\n                val_data_loader=self.class_val_data_loader,\n                fit_config=self.class_fit_config_nodir_nodump,\n                calib_config=self.class_calib_config_nodir_nodump,\n            )\n            sample = prob_class.posterior.sample()\n\n            # no save dir but dump\n            with self.assertRaises(ValueError):\n                status = prob_class.train(\n                    train_data_loader=self.class_train_data_loader,\n                    calib_data_loader=self.class_val_data_loader,\n                    val_data_loader=self.class_val_data_loader,\n                    fit_config=self.class_fit_config_nodir_dump,\n                    calib_config=self.class_calib_config_nodir_nodump,\n                )\n\n            # save dir, no dump\n            status = prob_class.train(\n                train_data_loader=self.class_train_data_loader,\n                calib_data_loader=self.class_val_data_loader,\n                val_data_loader=self.class_val_data_loader,\n                fit_config=self.class_fit_config_dir_nodump(tmp_dir),\n                calib_config=self.class_calib_config_nodir_nodump,\n            )\n            sample = prob_class.posterior.sample()\n            prob_class.posterior.load_state(tmp_dir)\n\n            # save dir and dump\n            status = prob_class.train(\n                train_data_loader=self.class_train_data_loader,\n                calib_data_loader=self.class_val_data_loader,\n                val_data_loader=self.class_val_data_loader,\n                fit_config=self.class_fit_config_dir_dump(tmp_dir),\n                calib_config=self.class_calib_config_nodir_nodump,\n            )\n            sample = prob_class.posterior.sample()\n            prob_class.posterior.load_state(tmp_dir)\n\n            # restore\n            status = prob_class.train(\n                train_data_loader=self.class_train_data_loader,\n                calib_data_loader=self.class_val_data_loader,\n                val_data_loader=self.class_val_data_loader,\n                fit_config=self.class_fit_config_restore(tmp_dir),\n                calib_config=self.class_calib_config_nodir_nodump,\n            )\n\n            # load state\n            prob_class.load_state(checkpoint_path=tmp_dir)\n\n            # save state\n            prob_class.save_state(checkpoint_path=tmp_dir)\n\n    def test_dryrun_reg_laplace(self):\n        with tempfile.TemporaryDirectory() as tmp_dir:\n            prob_reg = ProbRegressor(\n                model=MyModel(self.reg_output_dim),\n                likelihood_log_variance_model=MyModel(self.reg_output_dim),\n                posterior_approximator=LaplacePosteriorApproximator(),\n                output_calibrator=RegressionTemperatureScaler(),\n            )\n            # no save dir, no dump\n            status = prob_reg.train(\n                train_data_loader=self.reg_train_data_loader,\n                calib_data_loader=self.reg_val_data_loader,\n                val_data_loader=self.reg_val_data_loader,\n                map_fit_config=self.reg_fit_config_nodir_nodump,\n                fit_config=self.reg_fit_config_nodir_nodump,\n                calib_config=self.reg_calib_config_nodir_nodump,\n            )\n            sample = prob_reg.posterior.sample()\n\n            # no save dir but dump\n            with self.assertRaises(ValueError):\n                status = prob_reg.train(\n                    train_data_loader=self.reg_train_data_loader,\n                    calib_data_loader=self.reg_val_data_loader,\n                    val_data_loader=self.reg_val_data_loader,\n                    map_fit_config=self.reg_fit_config_nodir_nodump,\n                    fit_config=self.reg_fit_config_nodir_dump,", "metadata": {"task_id": "awslabs_fortuna/76", "ground_truth": "                    calib_config=self.reg_calib_config_nodir_nodump,", "fpath_tuple": ["awslabs_fortuna", "tests", "fortuna", "prob_model", "test_train.py"], "context_start_lineno": 463, "line_no": 605}}
{"prompt": "import copy\nfrom types import FunctionType\nfrom typing import Type\n\nfrom federatedscope.core.auxiliaries.optimizer_builder import get_optimizer\nfrom federatedscope.core.trainers.torch_trainer import GeneralTorchTrainer\n\nimport numpy as np\n\n\nclass GeneralMultiModelTrainer(GeneralTorchTrainer):\n    def __init__(self,\n                 model_nums,\n                 models_interact_mode=\"sequential\",\n                 model=None,\n                 data=None,\n                 device=None,\n                 config=None,\n                 base_trainer: Type[GeneralTorchTrainer] = None):\n        \"\"\"\n            `GeneralMultiModelTrainer` supports train/eval via multiple\n            internal models\n\n            Arguments:\n                model_nums (int): how many internal models and optimizers\n                will be held by the trainer\n                models_interact_mode (str): how the models interact, can be\n                \"sequential\" or \"parallel\".\n                model: training model\n                data: a dict contains train/val/test data\n                device: device to run\n                config: for trainer-related configuration\n                base_trainer: if given, the GeneralMultiModelTrainer init\n                will based on base_trainer copy\n\n                The sequential mode indicates the interaction at\n                run_routine level\n                [one model runs its whole routine, then do sth. for\n                interaction, then next model runs its whole routine]\n               ... -> run_routine_model_i\n                    -> _switch_model_ctx\n                    -> (on_fit_end, _interact_to_other_models)\n                    -> run_routine_model_i+1\n                    ->...\n\n                The parallel mode indicates the interaction\n                at point-in-time level\n                [At a specific point-in-time, one model call hooks (\n                including interaction), then next model call hooks]\n               ... ->  (on_xxx_point, hook_xxx_model_i)\n                    ->  (on_xxx_point, _interact_to_other_models)\n                    ->  (on_xxx_point, _switch_model_ctx)\n                    ->  (on_xxx_point, hook_xxx_model_i+1)\n                    ->...\n\n        \"\"\"\n        # support two initialization methods for the `GeneralMultiModelTrainer`\n        # 1) from another trainer; or 2) standard init manner given (model,\n        # data, device, config)\n        if base_trainer is None:\n            assert model is not None and \\\n                   data is not None and \\\n                   device is not None and \\\n                   config is not None, \"when not copy construction, (model, \" \\\n                                       \"data, device, config) should not be \" \\\n                                       \"None\"\n            super(GeneralMultiModelTrainer,\n                  self).__init__(model, data, device, config)\n        else:\n            assert isinstance(base_trainer, GeneralMultiModelTrainer) or \\\n                   issubclass(type(base_trainer), GeneralMultiModelTrainer) \\\n                   or isinstance(base_trainer, GeneralTorchTrainer) or \\\n                   issubclass(type(base_trainer), GeneralTorchTrainer) or \\\n                   \"can only copy instances of `GeneralMultiModelTrainer` \" \\\n                   \"and its subclasses, or \" \\\n                   \"`GeneralTorchTrainer` and its subclasses\"\n            self.__dict__ = copy.deepcopy(base_trainer.__dict__)\n\n        assert models_interact_mode in [\"sequential\", \"parallel\"], \\\n            f\"Invalid models_interact_mode, should be `sequential` or \" \\\n            f\"`parallel`, but got {models_interact_mode}\"\n        self.models_interact_mode = models_interact_mode\n\n        if int(model_nums)!= model_nums or model_nums < 1:\n            raise ValueError(\n                f\"model_nums should be integer and >= 1, got {model_nums}.\")\n        self.model_nums = model_nums\n\n        self.ctx.cur_model_idx = 0  # used to mark cur model\n\n        # different internal models can have different hook_set\n        self.hooks_in_train_multiple_models = [self.hooks_in_train]\n        self.hooks_in_eval_multiple_models = [self.hooks_in_eval]\n        self.init_multiple_models()\n        self.init_multiple_model_hooks()\n        assert len(self.ctx.models) == model_nums == \\\n               len(self.hooks_in_train_multiple_models) == len(\n            self.hooks_in_eval_multiple_models),\\\n            \"After init, len(hooks_in_train_multiple_models), \" \\\n            \"len(hooks_in_eval_multiple_models), \" \\\n            \"len(ctx.models) and model_nums should be the same\"\n\n    def init_multiple_models(self):\n        \"\"\"\n            init multiple models and optimizers: the default implementation\n            is copy init manner;\n            ========================= Extension =============================\n            users can override this function according to their own\n            requirements\n        \"\"\"\n\n        additional_models = [\n            copy.deepcopy(self.ctx.model) for _ in range(self.model_nums - 1)\n        ]\n        self.ctx.models = [self.ctx.model] + additional_models\n\n        self.ctx.optimizers = [\n            get_optimizer(self.ctx.models[i], **self.cfg.train.optimizer)", "metadata": {"task_id": "alibaba_FederatedScope/34", "ground_truth": "            for i in range(0, self.model_nums)", "fpath_tuple": ["alibaba_FederatedScope", "federatedscope", "core", "trainers", "trainer_multi_model.py"], "context_start_lineno": 0, "line_no": 118}}
{"prompt": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport functools\nfrom typing import Iterable, Optional, Union\n\nimport torch\nfrom tensordict.tensordict import TensorDict, TensorDictBase\nfrom torch import nn, Tensor\nfrom torch.nn import functional as F\n\nfrom torchrl.envs.utils import step_mdp\nfrom torchrl.modules import SafeModule\n\n\nclass _context_manager:\n    def __init__(self, value=True):\n        self.value = value\n        self.prev = []\n\n    def __call__(self, func):\n        @functools.wraps(func)\n        def decorate_context(*args, **kwargs):\n            with self:\n                return func(*args, **kwargs)\n\n        return decorate_context\n\n\ndef distance_loss(\n    v1: torch.Tensor,\n    v2: torch.Tensor,\n    loss_function: str,\n    strict_shape: bool = True,\n) -> torch.Tensor:\n    \"\"\"Computes a distance loss between two tensors.\n\n    Args:\n        v1 (Tensor): a tensor with a shape compatible with v2\n        v2 (Tensor): a tensor with a shape compatible with v1\n        loss_function (str): One of \"l2\", \"l1\" or \"smooth_l1\" representing which loss function is to be used.\n        strict_shape (bool): if False, v1 and v2 are allowed to have a different shape.\n            Default is :obj:`True`.\n\n    Returns:\n         A tensor of the shape v1.view_as(v2) or v2.view_as(v1) with values equal to the distance loss between the\n        two.\n\n    \"\"\"\n    if v1.shape!= v2.shape and strict_shape:\n        raise RuntimeError(\n            f\"The input tensors have shapes {v1.shape} and {v2.shape} which are incompatible.\"\n        )\n\n    if loss_function == \"l2\":\n        value_loss = F.mse_loss(\n            v1,\n            v2,\n            reduction=\"none\",\n        )\n\n    elif loss_function == \"l1\":\n        value_loss = F.l1_loss(\n            v1,\n            v2,\n            reduction=\"none\",\n        )\n\n    elif loss_function == \"smooth_l1\":\n        value_loss = F.smooth_l1_loss(\n            v1,\n            v2,\n            reduction=\"none\",\n        )\n    else:\n        raise NotImplementedError(f\"Unknown loss {loss_function}\")\n    return value_loss\n\n\nclass TargetNetUpdater:\n    \"\"\"An abstract class for target network update in Double DQN/DDPG.\n\n    Args:\n        loss_module (DQNLoss or DDPGLoss): loss module where the target network should be updated.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        loss_module: Union[\"DQNLoss\", \"DDPGLoss\", \"SACLoss\", \"TD3Loss\"],  # noqa: F821\n    ):\n\n        _target_names = []\n        # for properties\n        for name in loss_module.__class__.__dict__:\n            if (\n                name.startswith(\"target_\")\n                and (name.endswith(\"params\") or name.endswith(\"buffers\"))\n                and (getattr(loss_module, name) is not None)\n            ):\n                _target_names.append(name)\n\n        # for regular lists: raise an exception\n        for name in loss_module.__dict__:\n            if (\n                name.startswith(\"target_\")\n                and (name.endswith(\"params\") or name.endswith(\"buffers\"))\n                and (getattr(loss_module, name) is not None)\n            ):\n                raise RuntimeError(\n                    \"Your module seems to have a target tensor list contained \"\n                    \"in a non-dynamic structure (such as a list). If the \"\n                    \"module is cast onto a device, the reference to these \"\n                    \"tensors will be lost.\"\n                )\n\n        if len(_target_names) == 0:\n            raise RuntimeError(\n                \"Did not find any target parameters or buffers in the loss module.\"\n            )\n\n        _source_names = [\"\".join(name.split(\"target_\")) for name in _target_names]\n\n        for _source in _source_names:\n            try:\n                getattr(loss_module, _source)\n            except AttributeError:\n                raise RuntimeError(\n                    f\"Incongruent target and source parameter lists: \"\n                    f\"{_source} is not an attribute of the loss_module\"\n                )\n\n        self._target_names = _target_names\n        self._source_names = _source_names\n        self.loss_module = loss_module\n        self.initialized = False\n\n    @property\n    def _targets(self):\n        return TensorDict(\n            {name: getattr(self.loss_module, name) for name in self._target_names},\n            [],\n        )\n\n    @property\n    def _sources(self):\n        return TensorDict(\n            {name: getattr(self.loss_module, name) for name in self._source_names},\n            [],\n        )\n\n    def init_(self) -> None:\n        for key, source in self._sources.items(True, True):\n            if not isinstance(key, tuple):\n                key = (key,)", "metadata": {"task_id": "pytorch_rl/32", "ground_truth": "            key = (\"target_\" + key[0], *key[1:])", "fpath_tuple": ["pytorch_rl", "torchrl", "objectives", "utils.py"], "context_start_lineno": 0, "line_no": 157}}
{"prompt": "import argparse\nimport hashlib\nimport itertools\nimport logging\nimport math\nimport os\nimport warnings\nfrom pathlib import Path\nfrom typing import Optional\n\nimport torch\nimport torch.nn.functional as F\nimport torch.utils.checkpoint\nfrom torch.utils.data import Dataset\n\nimport datasets\nimport diffusers\nimport transformers\nfrom accelerate import Accelerator\nfrom accelerate.logging import get_logger\nfrom accelerate.utils import set_seed\nfrom diffusers import AutoencoderKL, DDPMScheduler, DiffusionPipeline, UNet2DConditionModel\nfrom diffusers.optimization import get_scheduler\nfrom diffusers.utils import check_min_version\nfrom diffusers.utils.import_utils import is_xformers_available\nfrom huggingface_hub import HfFolder, Repository, create_repo, whoami\nfrom PIL import Image\nfrom torchvision import transforms\nfrom tqdm.auto import tqdm\nfrom transformers import AutoTokenizer, PretrainedConfig\n\n\n# Will error if the minimal version of diffusers is not installed. Remove at your own risks.\ncheck_min_version(\"0.13.0.dev0\")\n\nlogger = get_logger(__name__)\n\n\ndef import_model_class_from_model_name_or_path(pretrained_model_name_or_path: str, revision: str):\n    text_encoder_config = PretrainedConfig.from_pretrained(\n        pretrained_model_name_or_path,\n        subfolder=\"text_encoder\",\n        revision=revision,\n    )\n    model_class = text_encoder_config.architectures[0]\n\n    if model_class == \"CLIPTextModel\":\n        from transformers import CLIPTextModel\n\n        return CLIPTextModel\n    elif model_class == \"RobertaSeriesModelWithTransformation\":\n        from diffusers.pipelines.alt_diffusion.modeling_roberta_series import RobertaSeriesModelWithTransformation\n\n        return RobertaSeriesModelWithTransformation\n    else:\n        raise ValueError(f\"{model_class} is not supported.\")\n\n\ndef parse_args(input_args=None):\n    parser = argparse.ArgumentParser(description=\"Simple example of a training script.\")\n    parser.add_argument(\n        \"--pretrained_model_name_or_path\",\n        type=str,\n        default=None,\n        required=True,\n        help=\"Path to pretrained model or model identifier from huggingface.co/models.\",\n    )\n    parser.add_argument(\n        \"--revision\",\n        type=str,\n        default=None,\n        required=False,\n        help=\"Revision of pretrained model identifier from huggingface.co/models.\",\n    )\n    parser.add_argument(\n        \"--tokenizer_name\",\n        type=str,\n        default=None,\n        help=\"Pretrained tokenizer name or path if not the same as model_name\",\n    )\n    parser.add_argument(\n        \"--instance_data_dir\",\n        type=str,\n        default=None,\n        required=True,\n        help=\"A folder containing the training data of instance images.\",\n    )\n    parser.add_argument(\n        \"--class_data_dir\",\n        type=str,\n        default=None,\n        required=False,\n        help=\"A folder containing the training data of class images.\",\n    )\n    parser.add_argument(\n        \"--instance_prompt\",\n        type=str,\n        default=None,\n        required=True,\n        help=\"The prompt with identifier specifying the instance\",\n    )\n    parser.add_argument(\n        \"--class_prompt\",\n        type=str,\n        default=None,\n        help=\"The prompt to specify images in the same class as provided instance images.\",\n    )\n    parser.add_argument(\n        \"--with_prior_preservation\",\n        default=False,\n        action=\"store_true\",\n        help=\"Flag to add prior preservation loss.\",\n    )\n    parser.add_argument(\"--prior_loss_weight\", type=float, default=1.0, help=\"The weight of prior preservation loss.\")\n    parser.add_argument(\n        \"--num_class_images\",\n        type=int,\n        default=100,\n        help=(\n            \"Minimal class images for prior preservation loss. If there are not enough images already present in\"\n            \" class_data_dir, additional images will be sampled with class_prompt.\"\n        ),\n    )\n    parser.add_argument(\n        \"--output_dir\",\n        type=str,\n        default=\"text-inversion-model\",\n        help=\"The output directory where the model predictions and checkpoints will be written.\",\n    )\n    parser.add_argument(\"--seed\", type=int, default=None, help=\"A seed for reproducible training.\")\n    parser.add_argument(\n        \"--resolution\",\n        type=int,\n        default=512,\n        help=(\n            \"The resolution for input images, all the images in the train/validation dataset will be resized to this\"\n            \" resolution\"\n        ),\n    )\n    parser.add_argument(\n        \"--center_crop\",\n        default=False,\n        action=\"store_true\",\n        help=(\n            \"Whether to center crop the input images to the resolution. If not set, the images will be randomly\"\n            \" cropped. The images will be resized to the resolution first before cropping.\"\n        ),\n    )\n    parser.add_argument(\"--train_text_encoder\", action=\"store_true\", help=\"Whether to train the text encoder\")\n    parser.add_argument(\n        \"--train_batch_size\", type=int, default=4, help=\"Batch size (per device) for the training dataloader.\"\n    )\n    parser.add_argument(\n        \"--sample_batch_size\", type=int, default=4, help=\"Batch size (per device) for sampling images.\"\n    )\n    parser.add_argument(\"--num_train_epochs\", type=int, default=1)\n    parser.add_argument(\n        \"--max_train_steps\",\n        type=int,\n        default=None,\n        help=\"Total number of training steps to perform.  If provided, overrides num_train_epochs.\",\n    )\n    parser.add_argument(\n        \"--checkpointing_steps\",\n        type=int,\n        default=500,\n        help=(\n            \"Save a checkpoint of the training state every X updates. These checkpoints can be used both as final\"", "metadata": {"task_id": "huggingface_diffusers/54", "ground_truth": "            \" checkpoints in case they are better than the last checkpoint, and are also suitable for resuming\"", "fpath_tuple": ["huggingface_diffusers", "examples", "research_projects", "multi_subject_dreambooth", "train_multi_subject_dreambooth.py"], "context_start_lineno": 0, "line_no": 168}}
{"prompt": " currently, only consider centralized topology\n                self._run_simulation_online()\n\n            else:\n                self._run_simulation()\n\n            self.server._monitor.finish_fed_runner(fl_mode=self.mode)\n\n            return self.server.best_results\n\n        elif self.mode == 'distributed':\n            if self.cfg.distribute.role =='server':\n                self.server.run()\n                return self.server.best_results\n            elif self.cfg.distribute.role == 'client':\n                self.client.join_in()\n                self.client.run()\n\n    def _run_simulation_online(self):\n        def is_broadcast(msg):\n            return len(msg.receiver) >= 1 and msg.sender == 0\n\n        cached_bc_msgs = []\n        cur_idx = 0\n        while True:\n            if len(self.shared_comm_queue) > 0:\n                msg = self.shared_comm_queue.popleft()\n                if is_broadcast(msg):\n                    cached_bc_msgs.append(msg)\n                    # assume there is at least one client\n                    msg = cached_bc_msgs[0]\n                    self._handle_msg(msg, rcv=msg.receiver[cur_idx])\n                    cur_idx += 1\n                    if cur_idx >= len(msg.receiver):\n                        del cached_bc_msgs[0]\n                        cur_idx = 0\n                else:\n                    self._handle_msg(msg)\n            elif len(cached_bc_msgs) > 0:\n                msg = cached_bc_msgs[0]\n                self._handle_msg(msg, rcv=msg.receiver[cur_idx])\n                cur_idx += 1\n                if cur_idx >= len(msg.receiver):\n                    del cached_bc_msgs[0]\n                    cur_idx = 0\n            else:\n                # finished\n                break\n\n    def _run_simulation(self):\n        server_msg_cache = list()\n        while True:\n            if len(self.shared_comm_queue) > 0:\n                msg = self.shared_comm_queue.popleft()\n                if msg.receiver == [self.server_id]:\n                    # For the server, move the received message to a\n                    # cache for reordering the messages according to\n                    # the timestamps\n                    heapq.heappush(server_msg_cache, msg)\n                else:\n                    self._handle_msg(msg)\n            elif len(server_msg_cache) > 0:\n                msg = heapq.heappop(server_msg_cache)\n                if self.cfg.asyn.use and self.cfg.asyn.aggregator \\\n                        == 'time_up':\n                    # When the timestamp of the received message beyond\n                    # the deadline for the currency round, trigger the\n                    # time up event first and push the message back to\n                    # the cache\n                    if self.server.trigger_for_time_up(msg.timestamp):\n                        heapq.heappush(server_msg_cache, msg)\n                    else:\n                        self._handle_msg(msg)\n                else:\n                    self._handle_msg(msg)\n            else:\n                if self.cfg.asyn.use and self.cfg.asyn.aggregator \\\n                        == 'time_up':\n                    self.server.trigger_for_time_up()\n                    if len(self.shared_comm_queue) == 0 and \\\n                            len(server_msg_cache) == 0:\n                        break\n                else:\n                    # terminate when shared_comm_queue and\n                    # server_msg_cache are all empty\n                    break\n\n    def _setup_server(self, resource_info=None, client_resource_info=None):\n        \"\"\"\n        Set up the server\n        \"\"\"\n        self.server_id = 0\n        if self.mode =='standalone':\n            if self.server_id in self.data:\n                server_data = self.data[self.server_id]\n                model = get_model(self.cfg.model,\n                                  server_data,\n                                  backend=self.cfg.backend)\n            else:\n                server_data = None\n                data_representative = self.data[1]\n                model = get_model(\n                    self.cfg.model,\n                    data_representative,\n                    backend=self.cfg.backend\n                )  # get the model according to client's data if the server\n                # does not own data\n            kw = {\n               'shared_comm_queue': self.shared_comm_queue,\n               'resource_info': resource_info,\n                'client_resource_info': client_resource_info\n            }\n        elif self.mode == 'distributed':\n            server_data = self.data\n            model = get_model(self.cfg.model,\n                              server_data,\n                              backend=self.cfg.backend)\n            kw = self.server_address\n            kw.update({'resource_info': resource_info})\n        else:\n            raise ValueError('Mode {} is not provided'.format(\n                self.cfg.mode.type))\n\n        if self.server_class:\n            self._server_device = self.gpu_manager.auto_choice()\n            server = self.server_class(\n                ID=self.server_id,\n                config=self.cfg,\n                data=server_data,\n                model=model,\n                client_num=self.cfg.federate.client_num,\n                total_round_num=self.cfg.federate.total_round_num,\n                device=self._server_device,\n                unseen_clients_id=self.unseen_clients_id,\n                **kw)\n\n            if self.cfg.nbafl.use:\n                from federatedscope.core.trainers.trainer_nbafl import \\\n                    wrap_nbafl_server\n                wrap_nbafl_server(server)\n\n        else:\n            raise ValueError\n\n        logger.info('Server has been set up... ')\n\n        return server\n\n    def _setup_client(self,\n                      client_id=-1,\n                      client_model=None,\n                      resource_info=None):\n        \"\"\"\n        Set up the client\n        \"\"\"\n        self.server_id = 0\n        if self.mode =='standalone':\n            client_data = self.data[client_id]\n            kw = {\n               'shared_comm_queue': self.shared_comm_queue,\n               'resource_info': resource_info\n            }\n        elif self.mode == 'distributed':\n            client_data = self.data\n            kw = self.client_address\n            kw['server_host'] = self.server_address['host']\n            kw['server_port'] = self.server_address['port']\n            kw['resource_info'] = resource_info\n        else:\n            raise ValueError('Mode {} is not provided'.format(\n                self.cfg.mode.type))\n\n        if self.client_class:\n            client_specific_config = self.cfg.clone()\n            if self.client_cfgs and \\\n                    self.client_cfgs.get('client_{}'.format(client_id)):\n                client_specific_config.defrost()\n                client_specific_config.merge_from_other_cfg(\n                    self.client_cfgs.get('client_{}'.format(client_id)))\n                client_specific_config.freeze()\n            client_device = self._server_device if \\\n                self.cfg.federate.share_local_model else \\\n                self.gpu_manager.auto_choice()\n            client = self.client_class(\n                ID=client_id,\n                server_id=self.server_id,\n                config=client_specific_config,\n                data=client_data,", "metadata": {"task_id": "alibaba_FederatedScope/149", "ground_truth": "                model=client_model or get_model(client_specific_config.model,", "fpath_tuple": ["alibaba_FederatedScope", "federatedscope", "core", "fed_runner.py"], "context_start_lineno": 725, "line_no": 913}}
{"prompt": "reg_fit_config_nodir_nodump,\n                fit_config=self.reg_fit_config_nodir_nodump,\n                calib_config=self.reg_calib_config_nodir_nodump,\n            )\n            sample = prob_reg.posterior.sample()\n\n            # no save dir but dump\n            with self.assertRaises(ValueError):\n                status = prob_reg.train(\n                    train_data_loader=self.reg_train_data_loader,\n                    calib_data_loader=self.reg_val_data_loader,\n                    val_data_loader=self.reg_val_data_loader,\n                    map_fit_config=self.reg_fit_config_nodir_nodump,\n                    fit_config=self.reg_fit_config_nodir_dump,\n                    calib_config=self.reg_calib_config_nodir_nodump,\n                )\n\n            # save dir, no dump\n            status = prob_reg.train(\n                train_data_loader=self.reg_train_data_loader,\n                calib_data_loader=self.reg_val_data_loader,\n                val_data_loader=self.reg_val_data_loader,\n                map_fit_config=self.reg_fit_config_nodir_nodump,\n                fit_config=self.reg_fit_config_dir_nodump(tmp_dir),\n                calib_config=self.reg_calib_config_nodir_nodump,\n            )\n            sample = prob_reg.posterior.sample()\n            prob_reg.posterior.load_state(tmp_dir)\n\n            # save dir and dump\n            status = prob_reg.train(\n                train_data_loader=self.reg_train_data_loader,\n                calib_data_loader=self.reg_val_data_loader,\n                val_data_loader=self.reg_val_data_loader,\n                map_fit_config=self.reg_fit_config_nodir_nodump,\n                fit_config=self.reg_fit_config_dir_nodump(tmp_dir),\n                calib_config=self.reg_calib_config_nodir_nodump,\n            )\n            sample = prob_reg.posterior.sample()\n            prob_reg.posterior.load_state(tmp_dir)\n\n            # restore from swag\n            status = prob_reg.train(\n                train_data_loader=self.reg_train_data_loader,\n                calib_data_loader=self.reg_val_data_loader,\n                val_data_loader=self.reg_val_data_loader,\n                map_fit_config=self.reg_fit_config_nodir_nodump,\n                fit_config=self.reg_fit_config_restore(tmp_dir),\n                calib_config=self.reg_calib_config_nodir_nodump,\n            )\n\n            # restore from map\n            prob_reg_map = ProbRegressor(\n                model=MyModel(self.reg_output_dim),\n                likelihood_log_variance_model=MyModel(self.reg_output_dim),\n                posterior_approximator=MAPPosteriorApproximator(),\n                output_calibrator=RegressionTemperatureScaler(),\n            )\n            status = prob_reg_map.train(\n                train_data_loader=self.reg_train_data_loader,\n                calib_data_loader=self.reg_val_data_loader,\n                val_data_loader=self.reg_val_data_loader,\n                map_fit_config=self.reg_fit_config_nodir_nodump,\n                fit_config=self.reg_fit_config_dir_dump(tmp_dir),\n                calib_config=self.reg_calib_config_nodir_nodump,\n            )\n            status = prob_reg.train(\n                train_data_loader=self.reg_train_data_loader,\n                calib_data_loader=self.reg_val_data_loader,\n                val_data_loader=self.reg_val_data_loader,\n                map_fit_config=self.reg_fit_config_nodir_nodump,\n                fit_config=self.reg_fit_config_restore(tmp_dir),\n                calib_config=self.reg_calib_config_nodir_nodump,\n            )\n\n            # load state\n            prob_reg.load_state(checkpoint_path=tmp_dir)\n\n            # save state\n            prob_reg.save_state(checkpoint_path=tmp_dir)\n\n    def test_dryrun_class_swag(self):\n        with tempfile.TemporaryDirectory() as tmp_dir:\n            prob_class = ProbClassifier(\n                model=MyModel(self.class_output_dim),\n                posterior_approximator=SWAGPosteriorApproximator(rank=2),\n                output_calibrator=ClassificationTemperatureScaler(),\n            )\n            # no save dir, no dump\n            status = prob_class.train(\n                train_data_loader=self.class_train_data_loader,\n                calib_data_loader=self.class_val_data_loader,\n                val_data_loader=self.class_val_data_loader,\n                map_fit_config=self.class_fit_config_nodir_nodump,\n                fit_config=self.class_fit_config_nodir_nodump,\n                calib_config=self.class_calib_config_nodir_nodump,\n            )\n            sample = prob_class.posterior.sample()\n\n            # no save dir but dump\n            with self.assertRaises(ValueError):\n                status = prob_class.train(\n                    train_data_loader=self.class_train_data_loader,\n                    calib_data_loader=self.class_val_data_loader,\n                    val_data_loader=self.class_val_data_loader,\n                    map_fit_config=self.class_fit_config_nodir_nodump,\n                    fit_config=self.class_fit_config_nodir_dump,\n                    calib_config=self.class_calib_config_nodir_nodump,\n                )\n\n            # save dir, no dump\n            status = prob_class.train(\n                train_data_loader=self.class_train_data_loader,\n                calib_data_loader=self.class_val_data_loader,\n                val_data_loader=self.class_val_data_loader,\n                map_fit_config=self.class_fit_config_nodir_nodump,\n                fit_config=self.class_fit_config_dir_nodump(tmp_dir),\n                calib_config=self.class_calib_config_nodir_nodump,\n            )\n            sample = prob_class.posterior.sample()\n            prob_class.posterior.load_state(tmp_dir)\n\n            # save dir and dump\n            status = prob_class.train(\n                train_data_loader=self.class_train_data_loader,\n                calib_data_loader=self.class_val_data_loader,\n                val_data_loader=self.class_val_data_loader,\n                map_fit_config=self.class_fit_config_nodir_nodump,\n                fit_config=self.class_fit_config_dir_dump(tmp_dir),\n                calib_config=self.class_calib_config_nodir_nodump,\n            )", "metadata": {"task_id": "awslabs_fortuna/7", "ground_truth": "            sample = prob_class.posterior.sample()", "fpath_tuple": ["awslabs_fortuna", "tests", "fortuna", "prob_model", "test_train.py"], "context_start_lineno": 777, "line_no": 908}}
{"prompt": "            Connect to slave end.\n        \"\"\"\n        raise NotImplementedError  # pragma: no cover\n\n    @abstractmethod\n    def disconnect(self):\n        \"\"\"\n        Overview:\n            Disconnect from slave end.\n        \"\"\"\n        raise NotImplementedError  # pragma: no cover\n\n    @abstractmethod\n    def new_task(self, data: Optional[Mapping[str, Any]] = None):\n        \"\"\"\n        Overview:\n            Send new task to slave end and receive task result from it.\n        Arguments:\n            - data (:obj:`Optional[Mapping[str, Any]]`): Data of the new task\n        Returns:\n            - result (:obj:`Mapping[str, Any]`): Result of the task processed by slave end\n        \"\"\"\n        raise NotImplementedError  # pragma: no cover\n\n    def start(self):\n        \"\"\"\n        Overview:\n            Alias for `connect`, for supporting context manager.\n        \"\"\"\n        self.connect()\n\n    def close(self):\n        \"\"\"\n        Overview:\n            Alias for `disconnect`, for support context manager.\n        \"\"\"\n        self.disconnect()\n\n\nclass SlaveConnection(_ISlaveConnection, metaclass=ABCMeta):\n    \"\"\"\n    Overview:\n        Slave connection object, which need to directly interact with slave end.\n    \"\"\"\n\n    def __init__(\n        self,\n        host: str,\n        port: Optional[int] = None,\n        https: bool = False,\n        channel: Optional[int] = None,\n        my_address: Optional[str] = None,\n        token: Optional[str] = None,\n        request_retries: Optional[int] = None,\n        request_retry_waiting: Optional[float] = None,\n    ):\n        \"\"\"\n        Overview:\n            Constructor of `SlaveConnection`\n        Arguments:\n            - host (:obj:`str`): Host of the slave server\n            - port (:obj:`Optional[int]`): Port of the slave server (None means `7236`)\n            - https (:obj:`bool`): Use https or not\n            - channel (:obj:`Optional[int]`): Channel id for the slave client.\n            - my_address (:obj:`Optional[str]`): The address of current server (None will grep local ip automatically, \\\n                this address will be used when connect to slave, the slave's request will be send to this address, \\\n                **so please make sure the address can be achieved by slave**)\n            - token (:obj:`Optional[str]`): Token of this connection, it is a token for authenticate to the \\\n                connection (`None` means this token would be randomly generated)\n            - request_retries (:obj:`Optional[int]`): Max times for request retries (None means `5`)\n            - request_retry_waiting (:obj:`Optional[float]`): Sleep time before requests' retrying (None means `1.0`, \\\n                unit: second)\n        \"\"\"\n        # meta info part\n        self.__channel = channel or DEFAULT_CHANNEL\n        self.__my_address = my_address\n        self.__token = token or random_token()\n\n        # request part\n        self.__http_engine = get_http_engine_class(\n            headers={\n                'Channel': lambda: str(self.__channel),\n                'Token': lambda: self.__token,\n            },\n            http_error_gene=get_slave_exception_by_error,\n        )()(host, port or DEFAULT_SLAVE_PORT, https)\n        self.__request_retries = max(request_retries or DEFAULT_REQUEST_RETRIES, 0)\n        self.__request_retry_waiting = max(request_retry_waiting or DEFAULT_REQUEST_RETRY_WAITING, 0.0)\n\n        # threading part\n        self.__lock = Lock()\n        self.__is_connected = False\n\n        # task part\n        self.__tasks = {}\n\n        self.__init_triggers()\n\n    def __request(self, method: str, path: str, data: Optional[Mapping[str, Any]] = None) -> requests.Response:\n        return self.__http_engine.request(\n            method,\n            path,\n            data,\n            retries=self.__request_retries,\n            retry_waiting=self.__request_retry_waiting,\n        )\n\n    @property\n    def is_connected(self) -> bool:\n        \"\"\"\n        Overview:\n            Check connection status\n        Returns:\n            - connected (:obj:`bool`): Whether this connection is still alive\n        \"\"\"\n        with self.__lock:\n            return self.__is_connected\n\n    def _before_connect(self) -> Mapping[str, Any]:\n        pass  # pragma: no cover\n\n    def _after_connect(\n            self, status_code: int, success: bool, code: int, message: Optional[str], data: Optional[Mapping[str, Any]]\n    ) -> Any:\n        pass  # pragma: no cover\n\n    def _error_connect(self, error: RequestException) -> Any:\n        raise error  # pragma: no cover\n\n    def __connect(self):\n        try:\n            response = self.__request(\n                'POST', '/connect', {\n                   'master': {\n                        'address': self.__my_address,\n                    },\n                    'data': (self._before_connect() or {})\n                }\n            )\n        except RequestException as err:\n            return self._error_connect(err)\n        else:\n            self.__is_connected = True\n            return self._after_connect(*get_values_from_response(response))\n\n    def connect(self):\n        with self.__lock:\n            return self.__connect()\n\n    def _before_disconnect(self) -> Mapping[str, Any]:\n        pass  # pragma: no cover\n\n    def _after_disconnect(\n            self, status_code: int, success: bool, code: int, message: Optional[str], data: Optional[Mapping[str, Any]]\n    ) -> Any:\n        pass  # pragma: no cover\n\n    def _error_disconnect(self, error: RequestException) -> Any:\n        raise error  # pragma: no cover\n\n    def __disconnect(self):\n        try:\n            response = self.__request('DELETE', '/disconnect', {\n                'data': self._before_disconnect() or {},\n            })\n        except RequestException as err:\n            return self._error_disconnect(err)\n        else:\n            self.__is_connected = False\n            return self._after_disconnect(*get_values_from_response(response))\n\n    def disconnect(self):\n        with self.__lock:\n            return self.__disconnect()\n\n    def _before_new_task(self, data: Optional[Mapping[str, Any]] = None) -> Mapping[str, Any]:\n        return data  # pragma: no cover\n\n    def _after_new_task(\n            self, status_code: int, success: bool, code: int, message: Optional[str], data: Optional[Mapping[str, Any]]\n    ) -> Any:\n        pass  # pragma: no cover\n\n    def _error_new_task(self, error: RequestException) -> Any:\n        raise error  # pragma: no cover\n\n    def new_task(self, data: Optional[Mapping[str, Any]] = None) -> Task:\n        with self.__lock:\n            _uuid = uuid4()\n            _task = Task(\n                http_engine=self.__http_engine,\n                data=data,\n                task_id=_uuid,\n                before_task_start=self._before_new_task,\n                after_task_start=self._after_new_task,", "metadata": {"task_id": "opendilab_ACE/66", "ground_truth": "                error_task_start=self._error_new_task,", "fpath_tuple": ["opendilab_ACE", "ding", "interaction", "master", "connection.py"], "context_start_lineno": 42, "line_no": 237}}
{"prompt": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport distutils.sysconfig\nimport os\nimport platform\nimport subprocess\nfrom pathlib import Path\nfrom subprocess import CalledProcessError, check_output, STDOUT\n\nimport torch\nfrom setuptools import Extension\nfrom setuptools.command.build_ext import build_ext\n\n\n_THIS_DIR = Path(__file__).parent.resolve()\n_ROOT_DIR = _THIS_DIR.parent.parent.resolve()\n_TORCHRL_DIR = _ROOT_DIR / \"torchrl\"\n\n\ndef _get_build(var, default=False):\n    if var not in os.environ:\n        return default\n\n    val = os.environ.get(var, \"0\")\n    trues = [\"1\", \"true\", \"TRUE\", \"on\", \"ON\", \"yes\", \"YES\"]\n    falses = [\"0\", \"false\", \"FALSE\", \"off\", \"OFF\", \"no\", \"NO\"]\n    if val in trues:\n        return True\n    if val not in falses:\n        print(\n            f\"WARNING: Unexpected environment variable value `{var}={val}`. \"\n            f\"Expected one of {trues + falses}\"\n        )\n    return False\n\n\n_BUILD_SOX = False if platform.system() == \"Windows\" else _get_build(\"BUILD_SOX\", True)\n_BUILD_KALDI = (\n    False if platform.system() == \"Windows\" else _get_build(\"BUILD_KALDI\", True)\n)\n_BUILD_RNNT = _get_build(\"BUILD_RNNT\", True)\n_USE_ROCM = False  # _get_build(\n# \"USE_ROCM\", torch.cuda.is_available() and torch.version.hip is not None\n# )\n_USE_CUDA = False  # _get_build(\n# \"USE_CUDA\", torch.cuda.is_available() and torch.version.hip is None\n# )\n_USE_OPENMP = (\n    _get_build(\"USE_OPENMP\", True)\n    and \"ATen parallel backend: OpenMP\" in torch.__config__.parallel_info()\n)\n_TORCH_CUDA_ARCH_LIST = os.environ.get(\"TORCH_CUDA_ARCH_LIST\", None)\n\n\ndef get_ext_modules():\n    return [\n        Extension(name=\"torchrl._torchrl\", sources=[]),\n    ]\n\n\n# Based off of\n# https://github.com/pybind/cmake_example/blob/580c5fd29d4651db99d8874714b07c0c49a53f8a/setup.py\nclass CMakeBuild(build_ext):\n    def run(self):\n        try:\n            subprocess.check_output([\"cmake\", \"--version\"])\n        except OSError:\n            raise RuntimeError(\"CMake is not available.\") from None\n        super().run()\n\n    def build_extension(self, ext):\n        # Since two library files (libtorchrl and _torchrl) need to be\n        # recognized by setuptools, we instantiate `Extension` twice. (see `get_ext_modules`)\n        # This leads to the situation where this `build_extension` method is called twice.\n        # However, the following `cmake` command will build all of them at the same time,\n        # so, we do not need to perform `cmake` twice.\n        # Therefore we call `cmake` only for `torchrl._torchrl`.\n        if ext.name!= \"torchrl._torchrl\":\n            return\n\n        extdir = os.path.abspath(os.path.dirname(self.get_ext_fullpath(ext.name)))\n\n        # required for auto-detection of auxiliary \"native\" libs\n        if not extdir.endswith(os.path.sep):\n            extdir += os.path.sep\n\n        cfg = \"Debug\" if self.debug else \"Release\"\n\n        cmake_args = [\n            f\"-DCMAKE_BUILD_TYPE={cfg}\",\n            f\"-DCMAKE_PREFIX_PATH={torch.utils.cmake_prefix_path}\",\n            f\"-DCMAKE_INSTALL_PREFIX={extdir}\",\n            \"-DCMAKE_VERBOSE_MAKEFILE=ON\",\n            f\"-DPython_INCLUDE_DIR={distutils.sysconfig.get_python_inc()}\",\n            \"-DBUILD_TORCHRL_PYTHON_EXTENSION:BOOL=ON\",\n            f\"-DUSE_CUDA:BOOL={'ON' if _USE_CUDA else 'OFF'}\",\n        ]\n        build_args = [\"--target\", \"install\"]\n        # Pass CUDA architecture to cmake\n        if _TORCH_CUDA_ARCH_LIST is not None:\n            # Convert MAJOR.MINOR[+PTX] list to new style one\n            # defined at https://cmake.org/cmake/help/latest/prop_tgt/CUDA_ARCHITECTURES.html\n            _arches = _TORCH_CUDA_ARCH_LIST.replace(\".\", \"\").split(\";\")\n            _arches = [\n                arch[:-4] if arch.endswith(\"+PTX\") else f\"{arch}-real\"\n                for arch in _arches\n            ]\n            cmake_args += [f\"-DCMAKE_CUDA_ARCHITECTURES={';'.join(_arches)}\"]\n\n        # Default to Ninja\n        if \"CMAKE_GENERATOR\" not in os.environ or platform.system() == \"Windows\":\n            cmake_args += [\"-GNinja\"]\n        if platform.system() == \"Windows\":\n            import sys\n\n            python_version = sys.version_info\n            cmake_args += [\n                \"-DCMAKE_C_COMPILER=cl\",\n                \"-DCMAKE_CXX_COMPILER=cl\",\n                f\"-DPYTHON_VERSION={python_version.major}.{python_version.minor}\",\n            ]\n\n        # Set CMAKE_BUILD_PARALLEL_LEVEL to control the parallel build level\n        # across all generators.\n        if \"CMAKE_BUILD_PARALLEL_LEVEL\" not in os.environ:\n            # self.parallel is a Python 3 only way to set parallel jobs by hand\n            # using -j in the build_ext call, not supported by pip or PyPA-build.\n            if hasattr(self, \"parallel\") and self.parallel:\n                # CMake 3.12+ only.\n                build_args += [\"-j{}\".format(self.parallel)]\n\n        if not os.path.exists(self.build_temp):", "metadata": {"task_id": "pytorch_rl/130", "ground_truth": "            os.makedirs(self.build_temp)", "fpath_tuple": ["pytorch_rl", "build_tools", "setup_helpers", "extension.py"], "context_start_lineno": 0, "line_no": 135}}
{"prompt": "from collections.abc import Sequence, Mapping\nfrom typing import List, Dict, Union, Any\n\nimport torch\nimport re\nfrom torch._six import string_classes\nimport collections.abc as container_abcs\n\nint_classes = int\nnp_str_obj_array_pattern = re.compile(r'[SaUO]')\n\ndefault_collate_err_msg_format = (\n    \"default_collate: batch must contain tensors, numpy arrays, numbers, \"\n    \"dicts or lists; found {}\"\n)\n\n\ndef default_collate(batch: Sequence, cat_1dim: bool = True) -> Union[torch.Tensor, Mapping, Sequence]:\n    \"\"\"\n    Overview:\n        Put each data field into a tensor with outer dimension batch size.\n    Example:\n        >>> # a list with B tensors shaped (m, n) -->> a tensor shaped (B, m, n)\n        >>> a = [torch.zeros(2,3) for _ in range(4)]\n        >>> default_collate(a).shape\n        torch.Size([4, 2, 3])\n        >>>\n        >>> # a list with B lists, each list contains m elements -->> a list of m tensors, each with shape (B, )\n        >>> a = [[0 for __ in range(3)] for _ in range(4)]\n        >>> default_collate(a)\n        [tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0]), tensor([0, 0, 0, 0])]\n        >>>\n        >>> # a list with B dicts, whose values are tensors shaped :math:`(m, n)` -->>\n        >>> # a dict whose values are tensors with shape :math:`(B, m, n)`\n        >>> a = [{i: torch.zeros(i,i+1) for i in range(2, 4)} for _ in range(4)]\n        >>> print(a[0][2].shape, a[0][3].shape)\n        torch.Size([2, 3]) torch.Size([3, 4])\n        >>> b = default_collate(a)\n        >>> print(b[2].shape, b[3].shape)\n        torch.Size([4, 2, 3]) torch.Size([4, 3, 4])\n    Arguments:\n        - batch (:obj:`Sequence`): a data sequence, whose length is batch size, whose element is one piece of data\n    Returns:\n        - ret (:obj:`Union[torch.Tensor, Mapping, Sequence]`): the collated data, with batch size into each data field.\\\n            the return dtype depends on the original element dtype, can be [torch.Tensor, Mapping, Sequence].\n    \"\"\"\n    elem = batch[0]\n    elem_type = type(elem)\n    if isinstance(elem, torch.Tensor):\n        out = None\n        if torch.utils.data.get_worker_info() is not None:\n            # If we're in a background process, directly concatenate into a\n            # shared memory tensor to avoid an extra copy\n            numel = sum([x.numel() for x in batch])\n            storage = elem.storage()._new_shared(numel)\n            out = elem.new(storage)\n        if elem.shape == (1, ) and cat_1dim:\n            # reshape (B, 1) -> (B)\n            return torch.cat(batch, 0, out=out)\n            # return torch.stack(batch, 0, out=out)\n        else:\n            return torch.stack(batch, 0, out=out)\n    elif elem_type.__module__ == 'numpy' and elem_type.__name__!='str_' \\\n            and elem_type.__name__!='string_':\n        if elem_type.__name__ == 'ndarray':\n            # array of string classes and object\n            if np_str_obj_array_pattern.search(elem.dtype.str) is not None:\n                raise TypeError(default_collate_err_msg_format.format(elem.dtype))\n            return default_collate([torch.as_tensor(b) for b in batch], cat_1dim=cat_1dim)\n        elif elem.shape == ():  # scalars\n            return torch.as_tensor(batch)\n    elif isinstance(elem, float):\n        return torch.tensor(batch, dtype=torch.float32)", "metadata": {"task_id": "opendilab_ACE/36", "ground_truth": "    elif isinstance(elem, int_classes):", "fpath_tuple": ["opendilab_ACE", "ding", "utils", "data", "collate_fn.py"], "context_start_lineno": 0, "line_no": 73}}
{"prompt": ".\n      index: Specifies the multi-dimensional index for this parameter. E.g. if\n        name='id' and index=0, then a single ParameterConfig with name 'id[0]'\n        is added. `index` should be >= 0.\n\n    Returns:\n      ParameterConfigSelector for the newly added parameter(s).\n\n    Raises:\n      ValueError: If `index` is invalid (e.g. negative).\n    \"\"\"\n    param_names = self._get_parameter_names_to_create(name=name, index=index)\n\n    new_params = []\n    for param_name in param_names:\n      new_pc = ParameterConfig.factory(\n          name=param_name,\n          feasible_values=sorted(feasible_values),\n          scale_type=scale_type,\n          default_value=default_value)\n      new_params.append(new_pc)\n    return self._add_parameters(new_params)\n\n  def add_bool_param(self,\n                     name: str,\n                     feasible_values: Optional[Sequence[bool]] = None,\n                     *,\n                     default_value: Optional[bool] = None,\n                     scale_type: Optional[ScaleType] = None,\n                     index: Optional[int] = None) -> 'ParameterConfigSelector':\n    \"\"\"Adds boolean-valued parameter config(s) to the selected search space(s).\n\n    Args:\n      name: The parameter's name. Cannot be empty.\n      feasible_values: An optional list of feasible boolean values, i.e. one of\n        the following: [True], [False], [True, False], [False, True].\n      default_value: A default value for the Parameter.\n      scale_type: Scaling to be applied. NOT VALIDATED.\n      index: Specifies the multi-dimensional index for this parameter. E.g. if\n        name='match' and index=0, then a single ParameterConfig with name\n       'match[0]' is added. `index` should be >= 0.\n\n    Returns:\n      ParameterConfigSelector for the newly added parameter(s).\n\n    Raises:\n      ValueError: If `feasible_values` has invalid values.\n      ValueError: If `index` is invalid (e.g. negative).\n    \"\"\"\n    allowed_values = (None, (True, False), (False, True), (True,), (False,))\n    if feasible_values is not None and tuple(\n        feasible_values) not in allowed_values:\n      raise ValueError('feasible_values must be one of %s; got: %s.' %\n                       (allowed_values, feasible_values))\n    # Boolean parameters are represented as categorical parameters internally.\n    bool_to_string = lambda x: trial.TRUE_VALUE if x else trial.FALSE_VALUE\n    if feasible_values is None:\n      categories = (trial.TRUE_VALUE, trial.FALSE_VALUE)\n    else:\n      categories = [bool_to_string(x) for x in feasible_values]\n    feasible_values = sorted(categories, reverse=True)\n\n    if default_value is not None:\n      default_value = bool_to_string(default_value)\n\n    param_names = self._get_parameter_names_to_create(name=name, index=index)\n\n    new_params = []\n    for param_name in param_names:\n      new_pc = ParameterConfig.factory(\n          name=param_name,\n          feasible_values=sorted(feasible_values),\n          scale_type=scale_type,\n          default_value=default_value,\n          external_type=ExternalType.BOOLEAN)\n      new_params.append(new_pc)\n    return self._add_parameters(new_params)\n\n  @overload\n  def select(\n      self,\n      parameter_name: str,\n      parameter_values: None,\n  ) -> ParameterConfigSelector:\n   ...\n\n  @overload\n  def select(\n      self, parameter_name: str,\n      parameter_values: MonotypeParameterSequence) -> 'SearchSpaceSelector':\n   ...\n\n  def select(self,\n             parameter_name,\n             parameter_values: Optional[MonotypeParameterSequence] = None):\n    \"\"\"Selects a parameter config or its subspace.\n\n    This method is for constructing a _conditional_ search space.\n\n    EXAMPLE: Suppose we have a selector to the root of the search space with one\n    categorical parameter.\n    root = pyvizier.SearchSpace().root\n    root.add_categorical_param('model_type', ['dnn', 'linear'])\n\n    1) Select a `ParameterConfig`:\n      model = root.select('model_type')\n\n    2) Select a subspace conditioned on `model_type == 'dnn'` and add\n    a child parameter `hidden_units`:\n      dnn_subspace = root.select('model_type', ['dnn'])\n      dnn_subspace.add_int_param('hidden_layers',...)\n\n    or equivalently,\n      dnn_subspace = root.select('model_type').select_values(['dnn'])\n      dnn_subspace.add_int_param('hidden_layers',...)\n\n    3) Traverse your search space by chaining select() calls:\n      root.select('model_type', ['dnn']).select('hidden_layers', [1, 2])\n\n    4) Select more than one search space simultaneously:\n      selected = root.select('model_type', ['dnn', 'linear'])\n       .add_categorical_param('optimizer', ['adam', 'adagrad'])\n      assert len(selected) == 4  # {dnn, linear} x {adam, adagard}\n\n    Args:\n      parameter_name:\n      parameter_values: Optional parameter values for this selector, which will\n        be used to add child parameters, or traverse a conditional tree.\n\n    Returns:\n      ParameterConfigSelector for `ParameterConfig`(s) if the values are not\n        specified.\n      SearchSpaceSelector for subspace(s) if parameter_values are specified.\n    \"\"\"\n    if parameter_values is None:\n      selected_configs = []\n      for space in self._selected:\n        selected_configs.append(space.get(parameter_name))\n      return ParameterConfigSelector(selected_configs)\n    else:\n      selected_spaces = []\n      for space in self._selected:\n        selected_parameter = space.get(parameter_name)\n        for value in parameter_values:\n          selected_spaces.append(selected_parameter.subspace(value))\n      return SearchSpaceSelector(selected_spaces)\n\n  @classmethod\n  def _get_parameter_names_to_create(cls,\n                                     *,\n                                     name: str,\n                                     length: Optional[int] = None,\n                                     index: Optional[int] = None) -> List[str]:\n    \"\"\"Returns the names of all parameters which should be created.\n\n    Args:\n      name: The base parameter name.\n      length: Specifies the length of a multi-dimensional parameters. If larger\n        than 1, then multiple ParameterConfigs are added. E.g. if name='rate'\n        and length=2, then two ParameterConfigs with names 'rate[0]', 'rate[1]'\n        are added. Cannot be specified together with `index`.\n      index: Specifies the multi-dimensional index for this parameter. Cannot be\n        specified together with `length`. E.g. if name='rate' and index=1, then\n        a single ParameterConfig with name 'rate[1]' is added.\n\n    Returns:\n      List of parameter names to create.\n\n    Raises:\n      ValueError: If `length` or `index` are invalid.\n    \"\"\"\n    if length is not None and index is not None:", "metadata": {"task_id": "google_vizier/59", "ground_truth": "      raise ValueError('Only one of `length` and `index` can be specified. Got'", "fpath_tuple": ["google_vizier", "vizier", "_src", "pyvizier", "shared", "parameter_config.py"], "context_start_lineno": 865, "line_no": 1037}}
{"prompt": "fit_config=self.reg_fit_config_dir_nodump(tmp_dir),\n                calib_config=self.reg_calib_config_nodir_nodump,\n            )\n            sample = prob_reg.posterior.sample()\n            prob_reg.posterior.load_state(tmp_dir)\n\n            # save dir and dump\n            status = prob_reg.train(\n                train_data_loader=self.reg_train_data_loader,\n                calib_data_loader=self.reg_val_data_loader,\n                val_data_loader=self.reg_val_data_loader,\n                map_fit_config=self.reg_fit_config_nodir_nodump,\n                fit_config=self.reg_fit_config_dir_dump(tmp_dir),\n                calib_config=self.reg_calib_config_nodir_nodump,\n            )\n            sample = prob_reg.posterior.sample()\n            prob_reg.posterior.load_state(tmp_dir)\n\n            # restore from laplace\n            prob_reg.train(\n                train_data_loader=self.reg_train_data_loader,\n                calib_data_loader=self.reg_val_data_loader,\n                val_data_loader=self.reg_val_data_loader,\n                map_fit_config=self.reg_fit_config_nodir_nodump,\n                fit_config=self.reg_fit_config_restore(tmp_dir),\n                calib_config=self.reg_calib_config_nodir_nodump,\n            )\n\n            # restore from map\n            prob_reg_map = ProbRegressor(\n                model=MyModel(self.reg_output_dim),\n                likelihood_log_variance_model=MyModel(self.reg_output_dim),\n                posterior_approximator=MAPPosteriorApproximator(),\n                output_calibrator=RegressionTemperatureScaler(),\n            )\n            status = prob_reg_map.train(\n                train_data_loader=self.reg_train_data_loader,\n                calib_data_loader=self.reg_val_data_loader,\n                val_data_loader=self.reg_val_data_loader,\n                map_fit_config=self.reg_fit_config_nodir_nodump,\n                fit_config=self.reg_fit_config_dir_dump(tmp_dir),\n                calib_config=self.reg_calib_config_nodir_nodump,\n            )\n            status = prob_reg.train(\n                train_data_loader=self.reg_train_data_loader,\n                calib_data_loader=self.reg_val_data_loader,\n                val_data_loader=self.reg_val_data_loader,\n                map_fit_config=self.reg_fit_config_nodir_nodump,\n                fit_config=self.reg_fit_config_restore(tmp_dir),\n                calib_config=self.reg_calib_config_nodir_nodump,\n            )\n\n            # load state\n            prob_reg.load_state(checkpoint_path=tmp_dir)\n\n            # save state\n            prob_reg.save_state(checkpoint_path=tmp_dir)\n\n    def test_dryrun_class_laplace(self):\n        with tempfile.TemporaryDirectory() as tmp_dir:\n            prob_class = ProbClassifier(\n                model=MyModel(self.class_output_dim),\n                posterior_approximator=LaplacePosteriorApproximator(),\n                output_calibrator=ClassificationTemperatureScaler(),\n            )\n            # no save dir, no dump\n            status = prob_class.train(\n                train_data_loader=self.class_train_data_loader,\n                calib_data_loader=self.class_val_data_loader,\n                val_data_loader=self.class_val_data_loader,\n                map_fit_config=self.class_fit_config_nodir_nodump,\n                fit_config=self.class_fit_config_nodir_nodump,\n                calib_config=self.class_calib_config_nodir_nodump,\n            )\n            sample = prob_class.posterior.sample()\n\n            # no save dir but dump\n            with self.assertRaises(ValueError):\n                status = prob_class.train(\n                    train_data_loader=self.class_train_data_loader,\n                    calib_data_loader=self.class_val_data_loader,\n                    val_data_loader=self.class_val_data_loader,\n                    map_fit_config=self.class_fit_config_nodir_nodump,\n                    fit_config=self.class_fit_config_nodir_dump,\n                    calib_config=self.class_calib_config_nodir_nodump,\n                )\n\n            # save dir, no dump\n            status = prob_class.train(\n                train_data_loader=self.class_train_data_loader,\n                calib_data_loader=self.class_val_data_loader,\n                val_data_loader=self.class_val_data_loader,\n                map_fit_config=self.class_fit_config_nodir_nodump,\n                fit_config=self.class_fit_config_dir_nodump(tmp_dir),\n                calib_config=self.class_calib_config_nodir_nodump,\n            )\n            sample = prob_class.posterior.sample()\n            prob_class.posterior.load_state(tmp_dir)\n\n            # save dir and dump\n            status = prob_class.train(\n                train_data_loader=self.class_train_data_loader,\n                calib_data_loader=self.class_val_data_loader,\n                val_data_loader=self.class_val_data_loader,\n                map_fit_config=self.class_fit_config_nodir_nodump,\n                fit_config=self.class_fit_config_dir_dump(tmp_dir),\n                calib_config=self.class_calib_config_nodir_nodump,\n            )\n            sample = prob_class.posterior.sample()\n            prob_class.posterior.load_state(tmp_dir)\n\n            # restore from laplace\n            status = prob_class.train(\n                train_data_loader=self.class_train_data_loader,\n                calib_data_loader=self.class_val_data_loader,\n                val_data_loader=self.class_val_data_loader,\n                map_fit_config=self.class_fit_config_nodir_nodump,\n                fit_config=self.class_fit_config_restore(tmp_dir),\n                calib_config=self.class_calib_config_nodir_nodump,\n            )\n\n            # restore from map\n            prob_class_map = ProbClassifier(\n                model=MyModel(self.class_output_dim),\n                posterior_approximator=MAPPosteriorApproximator(),\n                output_calibrator=ClassificationTemperatureScaler(),\n            )\n            status = prob_class_map.train(\n                train_data_loader=self.class_train_data_loader,\n                calib_data_loader=self.class_val_data_loader,\n                val_data_loader=self.class_val_data_loader,\n                map_fit_config=self.class_fit_config_nodir_nodump,", "metadata": {"task_id": "awslabs_fortuna/51", "ground_truth": "                fit_config=self.class_fit_config_dir_dump(tmp_dir),", "fpath_tuple": ["awslabs_fortuna", "tests", "fortuna", "prob_model", "test_train.py"], "context_start_lineno": 614, "line_no": 746}}
{"prompt": " removed. This Shouldn't happen.\"\n              \"Pool Features:\\n%sPool rewards:\\n%s\"\n          ),\n          self._features,\n          self._rewards,\n      )\n      return np.zeros((self.batch_size, self._n_features))\n\n    # Ignore fireflies that were removed from the pool.\n    features_diffs = features_diffs[:, ~inf_indx, :]\n    scaled_force = scaled_force[:, ~inf_indx, :]\n\n    # Separate forces to pull and push so to normalize them separately.\n    scaled_pulls = np.where(scaled_force > 0, scaled_force, 0)\n    scaled_push = np.where(scaled_force < 0, scaled_force, 0)\n\n    if self.config.mutate_normalization_type == \"mean\":\n      # Divide the push and pull forces by the number of flies participating.\n      # Also multiply by normalization_scale.\n      norm_scaled_pulls = self.config.normalization_scale * np.nan_to_num(\n          scaled_pulls / np.sum(scaled_pulls > 0, axis=1, keepdims=True), 0\n      )\n      norm_scaled_push = self.config.normalization_scale * np.nan_to_num(\n          scaled_push / np.sum(scaled_push < 0, axis=1, keepdims=True), 0\n      )\n    elif self.config.mutate_normalization_type == \"random\":\n      # Create random matrices and normalize each row, s.t. the sum is 1.\n      pull_rand_matrix = self._rng.uniform(\n          0, 1, size=scaled_pulls.shape\n      ) * np.int_(scaled_pulls > 0)\n      pull_weight_matrix = pull_rand_matrix / np.sum(\n          pull_rand_matrix, axis=1, keepdims=True\n      )\n      push_rand_matrix = self._rng.uniform(\n          0, 1, size=scaled_pulls.shape\n      ) * np.int_(scaled_pulls > 0)\n      push_weight_matrix = push_rand_matrix / np.sum(\n          push_rand_matrix, axis=1, keepdims=True\n      )\n      # Normalize pulls/pulls by the weight matrices.\n      # Also multiply by normalization_scale.\n      norm_scaled_pulls = (\n          self.config.normalization_scale * scaled_pulls * pull_weight_matrix\n      )\n      norm_scaled_push = (\n          self.config.normalization_scale * scaled_push * push_weight_matrix\n      )\n    elif self.config.mutate_normalization_type == \"unnormalized\":\n      # Doesn't normalize the forces. Use this option with caution.\n      norm_scaled_pulls = scaled_pulls\n      norm_scaled_push = scaled_push\n\n    # Sums normalized forces (pull/push) of all fireflies.\n    return np.sum(\n        features_diffs * (norm_scaled_pulls + norm_scaled_push), axis=1\n    )\n\n  def _create_perturbations(self) -> np.ndarray:\n    \"\"\"Create random perturbations for the newly creatd batch.\n\n    Returns:\n      batched perturbations: (base_size, n_features)\n    \"\"\"\n    # Generate normalized noise for each batch.\n    batch_noise = self._rng.laplace(size=(self.batch_size, self._n_features))\n    batch_noise /= np.max(np.abs(batch_noise), axis=1, keepdims=True)\n    # Scale the noise by the each fly current perturbation.\n    return (\n        batch_noise\n        * self._perturbations[self._batch_slice][:, np.newaxis]\n        * self._perturbation_factors\n    )\n\n  def update(self, batch_rewards: np.ndarray) -> None:\n    \"\"\"Update the firefly pool based on the new batch of results.\n\n    Arguments:\n      batch_rewards: (batch_size, )\n    \"\"\"\n    self._update_best_reward(batch_rewards)\n    if self._iterations < self.pool_size // self.batch_size:\n      # The strategy is still initializing. Assign rewards.\n      self._features[self._batch_slice] = self._last_suggested_features\n      self._rewards[self._batch_slice] = batch_rewards\n    else:\n      # Pass the new batch rewards and the associated last suggested features.\n      self._update_pool_features_and_rewards(batch_rewards)\n      self._trim_pool()\n    self._increment_batch()\n    self._iterations += 1\n\n  def _update_best_reward(self, batch_rewards: np.ndarray) -> None:\n    \"\"\"Store the best result seen thus far to be used in pool trimming.\"\"\"\n    self._best_reward = np.max([self._best_reward, np.max(batch_rewards)])\n\n  def _update_pool_features_and_rewards(\n      self,\n      batch_rewards: np.ndarray,\n  ):\n    \"\"\"Update the features and rewards for flies with improved rewards.\n\n    Arguments:\n      batch_rewards: (batch_size, )\n    \"\"\"\n    sliced_features = self._features[self._batch_slice]\n    sliced_rewards = self._rewards[self._batch_slice]\n    sliced_perturbations = self._perturbations[self._batch_slice]\n    # Find indices of flies that their generated features made an improvement.\n    improve_indx = batch_rewards > sliced_rewards\n    # Update successful flies' with the associated last features and rewards.\n    sliced_features[improve_indx] = self._last_suggested_features[improve_indx]\n    sliced_rewards[improve_indx] = batch_rewards[improve_indx]\n    # Penalize unsuccessful flies.\n    sliced_perturbations[~improve_indx] *= self.config.penalize_factor\n\n  def _trim_pool(self) -> None:\n    \"\"\"Trim the pool by replacing unsuccessful fireflies with new random ones.\n\n    A firefly is considered unsuccessful if its current perturbation is below\n    'perturbation_lower_bound' and it's not the best fly seen thus far.\n    Random features are created to replace the existing ones, and rewards\n    are set to -np.inf to indicate that we don't have values for those feaures\n    yet and we shouldn't use them during suggest.\n    \"\"\"\n    sliced_perturbations = self._perturbations[self._batch_slice]\n    indx = sliced_perturbations < self.config.perturbation_lower_bound\n    n_remove = np.sum(indx)\n    if n_remove > 0:\n      sliced_features = self._features[self._batch_slice]\n      sliced_rewards = self._rewards[self._batch_slice]\n      # Ensure the best firefly is never removed. For optimization purposes,\n      # this logic is inside the if statement to be peformed only if needed.\n      indx = indx & (sliced_rewards!= self._best_reward)\n      n_remove = np.sum(indx)\n      if n_remove == 0:\n        return\n      # Replace fireflies with random features and evaluate rewards.\n      sliced_features[indx] = self._param_handler.random_features(\n          n_remove, self._n_features\n      )\n      sliced_perturbations[indx] = (\n          np.ones(\n              n_remove,\n          )", "metadata": {"task_id": "google_vizier/119", "ground_truth": "          * self.config.perturbation", "fpath_tuple": ["google_vizier", "vizier", "_src", "algorithms", "optimizers", "eagle_strategy.py"], "context_start_lineno": 467, "line_no": 611}}
{"prompt": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport dataclasses\nimport uuid\nfrom datetime import datetime\n\nimport hydra\nimport torch.cuda\nfrom hydra.core.config_store import ConfigStore\nfrom torchrl.envs import EnvCreator, ParallelEnv\nfrom torchrl.envs.transforms import RewardScaling, TransformedEnv\nfrom torchrl.envs.utils import set_exploration_mode\nfrom torchrl.modules import OrnsteinUhlenbeckProcessWrapper\nfrom torchrl.record import VideoRecorder\nfrom torchrl.record.loggers import generate_exp_name, get_logger\nfrom torchrl.trainers.helpers.collectors import (\n    make_collector_offpolicy,\n    OffPolicyCollectorConfig,\n)\nfrom torchrl.trainers.helpers.envs import (\n    correct_for_frame_skip,\n    EnvConfig,\n    initialize_observation_norm_transforms,\n    parallel_env_constructor,\n    retrieve_observation_norms_state_dict,\n    transformed_env_constructor,\n)\nfrom torchrl.trainers.helpers.logger import LoggerConfig\nfrom torchrl.trainers.helpers.losses import LossConfig, make_sac_loss\nfrom torchrl.trainers.helpers.models import make_sac_model, SACModelConfig\nfrom torchrl.trainers.helpers.replay_buffer import make_replay_buffer, ReplayArgsConfig\nfrom torchrl.trainers.helpers.trainers import make_trainer, TrainerConfig\n\nconfig_fields = [\n    (config_field.name, config_field.type, config_field)\n    for config_cls in (\n        TrainerConfig,\n        OffPolicyCollectorConfig,\n        EnvConfig,\n        LossConfig,\n        SACModelConfig,\n        LoggerConfig,\n        ReplayArgsConfig,\n    )\n    for config_field in dataclasses.fields(config_cls)\n]\n\nConfig = dataclasses.make_dataclass(cls_name=\"Config\", fields=config_fields)\ncs = ConfigStore.instance()\ncs.store(name=\"config\", node=Config)\n\nDEFAULT_REWARD_SCALING = {\n    \"Hopper-v1\": 5,\n    \"Walker2d-v1\": 5,\n    \"HalfCheetah-v1\": 5,\n    \"cheetah\": 5,\n    \"Ant-v2\": 5,\n    \"Humanoid-v2\": 20,\n    \"humanoid\": 100,\n}\n\n\n@hydra.main(version_base=None, config_path=\".\", config_name=\"config\")\ndef main(cfg: \"DictConfig\"):  # noqa: F821\n\n    cfg = correct_for_frame_skip(cfg)\n\n    if not isinstance(cfg.reward_scaling, float):\n        cfg.reward_scaling = DEFAULT_REWARD_SCALING.get(cfg.env_name, 5.0)\n\n    device = (\n        torch.device(\"cpu\")\n        if torch.cuda.device_count() == 0\n        else torch.device(\"cuda:0\")\n    )\n\n    exp_name = \"_\".join(\n        [\n            \"SAC\",\n            cfg.exp_name,\n            str(uuid.uuid4())[:8],\n            datetime.now().strftime(\"%y_%m_%d-%H_%M_%S\"),\n        ]\n    )\n\n    exp_name = generate_exp_name(\"SAC\", cfg.exp_name)\n    logger = get_logger(\n        logger_type=cfg.logger, logger_name=\"sac_logging\", experiment_name=exp_name\n    )\n    video_tag = exp_name if cfg.record_video else \"\"\n\n    key, init_env_steps, stats = None, None, None\n    if not cfg.vecnorm and cfg.norm_stats:\n        if not hasattr(cfg, \"init_env_steps\"):\n            raise AttributeError(\"init_env_steps missing from arguments.\")\n        key = (\"next\", \"pixels\") if cfg.from_pixels else (\"next\", \"observation_vector\")\n        init_env_steps = cfg.init_env_steps\n        stats = {\"loc\": None, \"scale\": None}\n    elif cfg.from_pixels:\n        stats = {\"loc\": 0.5, \"scale\": 0.5}\n\n    proof_env = transformed_env_constructor(\n        cfg=cfg,\n        use_env_creator=False,\n        stats=stats,\n    )()\n    initialize_observation_norm_transforms(\n        proof_environment=proof_env, num_iter=init_env_steps, key=key\n    )\n    _, obs_norm_state_dict = retrieve_observation_norms_state_dict(proof_env)[0]\n\n    model = make_sac_model(\n        proof_env,\n        cfg=cfg,\n        device=device,\n    )\n    loss_module, target_net_updater = make_sac_loss(model, cfg)\n\n    actor_model_explore = model[0]\n    if cfg.ou_exploration:\n        actor_model_explore = OrnsteinUhlenbeckProcessWrapper(\n            actor_model_explore,\n            annealing_num_steps=cfg.annealing_frames,\n            sigma=cfg.ou_sigma,\n            theta=cfg.ou_theta,\n        ).to(device)\n    if device == torch.device(\"cpu\"):\n        # mostly for debugging\n        actor_model_explore.share_memory()\n\n    if cfg.gSDE:\n        with torch.no_grad(), set_exploration_mode(\"random\"):\n            # get dimensions to build the parallel env\n            proof_td = actor_model_explore(proof_env.reset().to(device))\n        action_dim_gsde, state_dim_gsde = proof_td.get(\"_eps_gSDE\").shape[-2:]\n        del proof_td\n    else:", "metadata": {"task_id": "pytorch_rl/6", "ground_truth": "        action_dim_gsde, state_dim_gsde = None, None", "fpath_tuple": ["pytorch_rl", "examples", "sac", "sac.py"], "context_start_lineno": 0, "line_no": 140}}
{"prompt": "import jax.numpy as jnp", "metadata": {"task_id": "awslabs_fortuna/169", "ground_truth": "from fortuna.typing import Array", "fpath_tuple": ["awslabs_fortuna", "fortuna", "conformal", "regression", "cvplus.py"], "context_start_lineno": 0, "line_no": 1}}
{"prompt": "annels,\n                        out_channels // attn_num_head_channels,\n                        in_channels=out_channels,\n                        num_layers=1,\n                        cross_attention_dim=cross_attention_dim,\n                        norm_num_groups=resnet_groups,\n                    )\n                )\n        self.attentions = nn.ModuleList(attentions)\n        self.resnets = nn.ModuleList(resnets)\n\n        if add_downsample:\n            self.downsamplers = nn.ModuleList(\n                [\n                    Downsample2D(\n                        out_channels, use_conv=True, out_channels=out_channels, padding=downsample_padding, name=\"op\"\n                    )\n                ]\n            )\n        else:\n            self.downsamplers = None\n\n        self.gradient_checkpointing = False\n\n    def forward(\n        self, hidden_states, temb=None, encoder_hidden_states=None, attention_mask=None, cross_attention_kwargs=None\n    ):\n        # TODO(Patrick, William) - attention mask is not used\n        output_states = ()\n\n        for resnet, attn in zip(self.resnets, self.attentions):\n            if self.training and self.gradient_checkpointing:\n\n                def create_custom_forward(module, return_dict=None):\n                    def custom_forward(*inputs):\n                        if return_dict is not None:\n                            return module(*inputs, return_dict=return_dict)\n                        else:\n                            return module(*inputs)\n\n                    return custom_forward\n\n                hidden_states = torch.utils.checkpoint.checkpoint(create_custom_forward(resnet), hidden_states, temb)\n                hidden_states = torch.utils.checkpoint.checkpoint(\n                    create_custom_forward(attn, return_dict=False),\n                    hidden_states,\n                    encoder_hidden_states,\n                    cross_attention_kwargs,\n                )[0]\n            else:\n                hidden_states = resnet(hidden_states, temb)\n                hidden_states = attn(\n                    hidden_states,\n                    encoder_hidden_states=encoder_hidden_states,\n                    cross_attention_kwargs=cross_attention_kwargs,\n                ).sample\n\n            output_states += (hidden_states,)\n\n        if self.downsamplers is not None:\n            for downsampler in self.downsamplers:\n                hidden_states = downsampler(hidden_states)\n\n            output_states += (hidden_states,)\n\n        return hidden_states, output_states\n\n\nclass DownBlock2D(nn.Module):\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        temb_channels: int,\n        dropout: float = 0.0,\n        num_layers: int = 1,\n        resnet_eps: float = 1e-6,\n        resnet_time_scale_shift: str = \"default\",\n        resnet_act_fn: str = \"swish\",\n        resnet_groups: int = 32,\n        resnet_pre_norm: bool = True,\n        output_scale_factor=1.0,\n        add_downsample=True,\n        downsample_padding=1,\n    ):\n        super().__init__()\n        resnets = []\n\n        for i in range(num_layers):\n            in_channels = in_channels if i == 0 else out_channels\n            resnets.append(\n                ResnetBlock2D(\n                    in_channels=in_channels,\n                    out_channels=out_channels,\n                    temb_channels=temb_channels,\n                    eps=resnet_eps,\n                    groups=resnet_groups,\n                    dropout=dropout,\n                    time_embedding_norm=resnet_time_scale_shift,\n                    non_linearity=resnet_act_fn,\n                    output_scale_factor=output_scale_factor,\n                    pre_norm=resnet_pre_norm,\n                )\n            )\n\n        self.resnets = nn.ModuleList(resnets)\n\n        if add_downsample:\n            self.downsamplers = nn.ModuleList(\n                [\n                    Downsample2D(\n                        out_channels, use_conv=True, out_channels=out_channels, padding=downsample_padding, name=\"op\"\n                    )\n                ]\n            )\n        else:\n            self.downsamplers = None\n\n        self.gradient_checkpointing = False\n\n    def forward(self, hidden_states, temb=None):\n        output_states = ()\n\n        for resnet in self.resnets:\n            if self.training and self.gradient_checkpointing:\n\n                def create_custom_forward(module):\n                    def custom_forward(*inputs):\n                        return module(*inputs)\n\n                    return custom_forward\n\n                hidden_states = torch.utils.checkpoint.checkpoint(create_custom_forward(resnet), hidden_states, temb)\n            else:\n                hidden_states = resnet(hidden_states, temb)\n\n            output_states += (hidden_states,)\n\n        if self.downsamplers is not None:\n            for downsampler in self.downsamplers:\n                hidden_states = downsampler(hidden_states)\n\n            output_states += (hidden_states,)\n\n        return hidden_states, output_states\n\n\nclass DownEncoderBlock2D(nn.Module):\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        dropout: float = 0.0,\n        num_layers: int = 1,\n        resnet_eps: float = 1e-6,\n        resnet_time_scale_shift: str = \"default\",\n        resnet_act_fn: str = \"swish\",\n        resnet_groups: int = 32,\n        resnet_pre_norm: bool = True,\n        output_scale_factor=1.0,\n        add_downsample=True,\n        downsample_padding=1,\n    ):\n        super().__init__()\n        resnets = []\n\n        for i in range(num_layers):\n            in_channels = in_channels if i == 0 else out_channels\n            resnets.append(\n                ResnetBlock2D(\n                    in_channels=in_channels,\n                    out_channels=out_channels,\n                    temb_channels=None,\n                    eps=resnet_eps,\n                    groups=resnet_groups,\n                    dropout=dropout,\n                    time_embedding_norm=resnet_time_scale_shift,\n                    non_linearity=resnet_act_fn,\n                    output_scale_factor=output_scale_factor,\n                    pre_norm=resnet_pre_norm,\n                )\n            )\n\n        self.resnets = nn.ModuleList(resnets)\n\n        if add_downsample:\n            self.downsamplers = nn.ModuleList(\n                [\n                    Downsample2D(\n                        out_channels, use_conv=True, out_channels=out_channels, padding=downsample_padding, name=\"op\"\n                    )\n                ]\n            )\n        else:\n            self.downsamplers = None\n\n    def forward(self, hidden_states):\n        for resnet in self.resnets:\n            hidden_states = resnet(hidden_states, temb=None)\n\n        if self.downsamplers is not None:\n            for downsampler in self.downsamplers:\n                hidden_states = downsampler(hidden_states)\n\n        return hidden_states\n\n\nclass AttnDownEncoderBlock2D(nn.Module):\n    def __init__(\n        self,\n        in_channels: int,", "metadata": {"task_id": "huggingface_diffusers/13", "ground_truth": "        out_channels: int,", "fpath_tuple": ["huggingface_diffusers", "src", "diffusers", "models", "unet_2d_blocks.py"], "context_start_lineno": 737, "line_no": 948}}
{"prompt": " = self._get_q_value({'obs': obs_repeat, 'action': random_actions_tensor})\n        # q2_rand = self._get_q_value(obs, random_actions_tensor, network=self.qf2)\n        q_curr_actions = self._get_q_value({'obs': obs_repeat, 'action': curr_actions_tensor})\n        # q2_curr_actions = self._get_tensor_values(obs, curr_actions_tensor, network=self.qf2)\n        q_next_actions = self._get_q_value({'obs': obs_repeat, 'action': new_curr_actions_tensor})\n        # q2_next_actions = self._get_tensor_values(obs, new_curr_actions_tensor, network=self.qf2)\n\n        cat_q1 = torch.stack([q_rand[0], q_pred[0], q_next_actions[0], q_curr_actions[0]], 1)\n        cat_q2 = torch.stack([q_rand[1], q_pred[1], q_next_actions[1], q_curr_actions[1]], 1)\n        std_q1 = torch.std(cat_q1, dim=1)\n        std_q2 = torch.std(cat_q2, dim=1)\n        if self._min_q_version == 3:\n            # importance sammpled version\n            random_density = np.log(0.5 ** curr_actions_tensor.shape[-1])\n            cat_q1 = torch.stack(\n                [\n                    q_rand[0] - random_density, q_next_actions[0] - new_log_pis.detach(),\n                    q_curr_actions[0] - curr_log_pis.detach()\n                ], 1\n            )\n            cat_q2 = torch.stack(\n                [\n                    q_rand[1] - random_density, q_next_actions[1] - new_log_pis.detach(),\n                    q_curr_actions[1] - curr_log_pis.detach()\n                ], 1\n            )\n\n        min_qf1_loss = torch.logsumexp(cat_q1, dim=1).mean() * self._min_q_weight\n        min_qf2_loss = torch.logsumexp(cat_q2, dim=1).mean() * self._min_q_weight\n        \"\"\"Subtract the log likelihood of data\"\"\"\n        min_qf1_loss = min_qf1_loss - q_pred[0].mean() * self._min_q_weight\n        min_qf2_loss = min_qf2_loss - q_pred[1].mean() * self._min_q_weight\n\n        if self._with_lagrange:\n            alpha_prime = torch.clamp(self.log_alpha_prime.exp(), min=0.0, max=1000000.0)\n            min_qf1_loss = alpha_prime * (min_qf1_loss - self.target_action_gap)\n            min_qf2_loss = alpha_prime * (min_qf2_loss - self.target_action_gap)\n\n            self.alpha_prime_optimizer.zero_grad()\n            alpha_prime_loss = (-min_qf1_loss - min_qf2_loss) * 0.5\n            alpha_prime_loss.backward(retain_graph=True)\n            self.alpha_prime_optimizer.step()\n\n        loss_dict['critic_loss'] += min_qf1_loss\n        if self._twin_critic:\n            loss_dict['twin_critic_loss'] += min_qf2_loss\n\n        # update q network\n        self._optimizer_q.zero_grad()\n        loss_dict['critic_loss'].backward(retain_graph=True)\n        if self._twin_critic:\n            loss_dict['twin_critic_loss'].backward()\n        self._optimizer_q.step()\n\n        # evaluate to get action distribution\n        (mu, sigma) = self._learn_model.forward(data['obs'], mode='compute_actor')['logit']\n        dist = Independent(Normal(mu, sigma), 1)\n        pred = dist.rsample()\n        action = torch.tanh(pred)\n        y = 1 - action.pow(2) + 1e-6\n        log_prob = dist.log_prob(pred).unsqueeze(-1)\n        log_prob = log_prob - torch.log(y).sum(-1, keepdim=True)\n\n        eval_data = {'obs': obs, 'action': action}\n        new_q_value = self._learn_model.forward(eval_data, mode='compute_critic')['q_value']\n        if self._twin_critic:\n            new_q_value = torch.min(new_q_value[0], new_q_value[1])\n\n        # =================\n        # value network\n        # =================\n        # compute value loss\n        if self._value_network:\n            # new_q_value: (bs, ), log_prob: (bs, act_shape) -> target_v_value: (bs, )\n            target_v_value = (new_q_value.unsqueeze(-1) - self._alpha * log_prob).mean(dim=-1)\n            loss_dict['value_loss'] = F.mse_loss(v_value, target_v_value.detach())\n\n            # update value network\n            self._optimizer_value.zero_grad()\n            loss_dict['value_loss'].backward()\n            self._optimizer_value.step()\n\n        # =================\n        # policy network\n        # =================\n        # compute policy loss\n        policy_loss = (self._alpha * log_prob - new_q_value.unsqueeze(-1)).mean()\n\n        loss_dict['policy_loss'] = policy_loss\n\n        # update policy network\n        self._optimizer_policy.zero_grad()\n        loss_dict['policy_loss'].backward()\n        self._optimizer_policy.step()\n\n        # compute alpha loss\n        if self._auto_alpha:\n            if self._log_space:\n                log_prob = log_prob + self._target_entropy\n                loss_dict['alpha_loss'] = -(self._log_alpha * log_prob.detach()).mean()\n\n                self._alpha_optim.zero_grad()\n                loss_dict['alpha_loss'].backward()\n                self._alpha_optim.step()\n                self._alpha = self._log_alpha.detach().exp()\n            else:\n                log_prob = log_prob + self._target_entropy\n                loss_dict['alpha_loss'] = -(self._alpha * log_prob.detach()).mean()\n\n                self._alpha_optim.zero_grad()\n                loss_dict['alpha_loss'].backward()\n                self._alpha_optim.step()\n                self._alpha = max(0, self._alpha)\n\n        loss_dict['total_loss'] = sum(loss_dict.values())\n\n        info_dict = {}\n        if self._value_network:\n            info_dict['cur_lr_v'] = self._optimizer_value.defaults['lr']\n\n        # =============\n        # after update\n        # =============\n        self._forward_learn_cnt += 1\n        # target update\n        self._target_model.update(self._learn_model.state_dict())\n        return {\n            'cur_lr_q': self._optimizer_q.defaults['lr'],\n            'cur_lr_p': self._optimizer_policy.defaults['lr'],", "metadata": {"task_id": "opendilab_ACE/154", "ground_truth": "            'priority': td_error_per_sample.abs().tolist(),", "fpath_tuple": ["opendilab_ACE", "ding", "policy", "cql.py"], "context_start_lineno": 377, "line_no": 506}}
{"prompt": "import os\nfrom dataclasses import dataclass\nfrom pathlib import Path\nfrom typing import Tuple\n\nimport cv2\nimport numpy as np\nimport tyro\nfrom nuscenes.nuscenes import NuScenes as NuScenesDatabase", "metadata": {"task_id": "nerfstudio-project_nerfstudio/126", "ground_truth": "from nuscenes.utils.data_classes import Box", "fpath_tuple": ["nerfstudio-project_nerfstudio", "scripts", "datasets", "process_nuscenes_masks.py"], "context_start_lineno": 0, "line_no": 9}}
{"prompt": "import os\nimport json", "metadata": {"task_id": "opendilab_ACE/29", "ground_truth": "from typing import Tuple", "fpath_tuple": ["opendilab_ACE", "ding", "utils", "k8s_helper.py"], "context_start_lineno": 0, "line_no": 2}}
{"prompt": "Var('_T')\n\n\nclass DesignerFactory(Protocol[_T]):\n  \"\"\"Protocol (PEP-544) for a designer factory.\"\"\"\n\n  def __call__(self, problem: vz.ProblemStatement) -> _T:\n    pass\n\n\nclass DesignerPolicy(pythia.Policy):\n  \"\"\"Wraps a Designer into a pythia Policy.\n\n  > IMPORTANT: If your Designer class is (partially) serializable, use\n  > (Partially)SerializableDesignerPolicy instead.\n\n  When a Designer cannot be (partially) serialized, we create a new Designer\n  instance at the start of each `suggest()` call. The most recently used\n  Designer instance can be saved in self._designer. However, it is for\n  interactive analysis/debugging purposes only and never used in\n  future `suggest()` calls.\n  \"\"\"\n\n  def __init__(\n      self,\n      supporter: pythia.PolicySupporter,\n      designer_factory: DesignerFactory[vza.Designer],\n  ):\n    \"\"\"Init.\n\n    Args:\n      supporter:\n      designer_factory:\n    \"\"\"\n    self._supporter = supporter\n    self._designer_factory = designer_factory\n\n  def suggest(self, request: pythia.SuggestRequest) -> pythia.SuggestDecision:\n    designer = self._designer_factory(request.study_config)\n    new_trials = self._supporter.GetTrials(\n        status_matches=vz.TrialStatus.COMPLETED)\n    designer.update(vza.CompletedTrials(new_trials))\n    self._designer = designer  # saved for debugging purposes only.\n    return pythia.SuggestDecision(\n        designer.suggest(request.count), metadata=vz.MetadataDelta())\n\n  def early_stop(self,\n                 request: pythia.EarlyStopRequest) -> pythia.EarlyStopDecisions:\n    raise NotImplementedError(\n        'DesignerPolicy does not support the early_stop() method.')\n\n\nclass _SerializableDesignerPolicyBase(pythia.Policy,\n                                      serializable.PartiallySerializable,\n                                      Generic[_T], abc.ABC):\n  \"\"\"Partially implemented class for wrapping a (Partially)SerializableDesigner.\n\n  Inherited by (Partially)SerializableDesignerPolicy which is fully implemented.\n\n  (Partially)SerializableDesignerPolicy maintains a synchronized state between\n  the set of trial ids that were passed to Designer via `update()` call, and\n  the Designer instance that was used for during last `suggest()` call.\n\n  The policy's `dump()` contains the trial ids passed to update() and the\n  result of `dump()` called on the wrapped Designer.\n\n  Unlike in the basic DesignerPolicy class, this class tries to minimize\n  the computation by following these steps in order:\n    * Re-use the saved Designer instance from the last `suggest()` call.\n    * `load()` the Designer state from the study-level metadata.\n    * If either of the first two steps succeedes, then we update the Designer\n    with newly completed trials only.\n    * Otherwise, we update the Designer with all trials.\n\n  > NOTE: This Policy itself is PartiallySerializable.\n  \"\"\"\n\n  _ns_designer = 'designer'\n\n  def __init__(\n      self,\n      problem_statement: vz.ProblemStatement,\n      supporter: pythia.PolicySupporter,\n      designer_factory: Callable[[vz.ProblemStatement], _T],\n      *,\n      ns_root: str = 'designer_policy_v0',\n      verbose: int = 0,\n  ):\n    \"\"\"Init.\n\n    Args:\n      problem_statement:\n      supporter:\n      designer_factory:\n      ns_root: Root of the namespace where policy state is stored.\n      verbose: Logging verbosity.\n    \"\"\"\n    self._supporter = supporter\n    self._designer_factory = designer_factory\n    self._ns_root = ns_root\n    self._incorporated_trial_ids = set()\n    self._problem_statement = problem_statement\n    self._verbose = verbose\n    self._designer = None\n\n  def suggest(self, request: pythia.SuggestRequest) -> pythia.SuggestDecision:\n    \"\"\"Perform a suggest operation.\n\n    The order of operations is:\n    1. Initialize the designer and load its state from metadata.\n    2. Update the designer with newly completed trials.\n    3. Generate suggestions from the designer.\n    4. Dump the state of the designer and store it in metadata.\n\n    Arguments:\n      request: Pythia suggestion request objects.\n\n    Returns:\n      The suggestions from the designer.\n    \"\"\"\n    # Note that we can avoid O(Num trials) dependency in the standard scenario,\n    # by storing only the last element in a consecutive sequence, e.g.,\n    # instead of storing [1,2,3,4,11,12,13,21], store: [4,13,21], but\n    # we keep things simple in this pseudocode.\n    self._initialize_designer(request.study_config)\n    new_trials = self._get_new_trials(request.max_trial_id)\n    self.designer.update(vza.CompletedTrials(new_trials))\n    self._incorporated_trial_ids |= set(t.id for t in new_trials)\n\n    logging.info(\n        'Updated with %s trials. Designer has seen a total of %s trials.',\n        len(new_trials),\n        len(self._incorporated_trial_ids),\n    )\n    metadata_delta = vz.MetadataDelta()\n    # During the'suggest' call the designer's state could be changed, therefore\n    # the state is dumped and stored only after'suggest' was called.\n    suggestions = pythia.SuggestDecision(\n        self.designer.suggest(request.count), metadata=metadata_delta)\n    metadata_delta.on_study.ns(self._ns_root).attach(self.dump())\n    return suggestions\n\n  def early_stop(self,\n                 request: pythia.EarlyStopRequest) -> pythia.EarlyStopDecisions:\n    raise NotImplementedError(\n        'PartiallySerializableDesignerPolicy does not implement early_stop().')\n\n  @property\n  def designer(self) -> _T:\n    if self._designer is None:\n      raise ValueError('`self._designer` has not been initialized!'\n                       'Use self._restore_designer(..) to initialize it.')\n    return self._designer\n\n  @abc.abstractmethod\n  def _restore_designer(self, designer_metadata: vz.Metadata) -> _T:\n    \"\"\"Creates a new Designer by restoring the state from `designer_metadata`.\n\n    Args:\n      designer_metadata:\n\n    Returns:\n      New Designer object.\n\n    Raises:\n      DecodeError: `designer_metadata` does not contain valid information\n        to restore a Designer state.\n    \"\"\"\n\n  # TODO: Use timestamps to avoid metadata blowup.\n  def load(self, md: vz.Metadata) -> None:\n    if 'incorporated_trial_ids' in md:\n      try:\n        self._incorporated_trial_ids = set(\n            json.loads(md['incorporated_trial_ids']))\n      except json.JSONDecodeError as e:\n        raise serializable.HarmlessDecodeError from e\n    else:\n      raise serializable.HarmlessDecodeError()\n    logging.info(\n        'Successfully recovered the policy state, which incorporated %s trials',\n        len(self._incorporated_trial_ids),\n    )\n    self._designer = self._restore_designer(md.ns(self._ns_designer))\n\n  def _initialize_designer(self,", "metadata": {"task_id": "google_vizier/65", "ground_truth": "                           problem_statement: vz.ProblemStatement) -> None:", "fpath_tuple": ["google_vizier", "vizier", "_src", "algorithms", "policies", "designer_policy.py"], "context_start_lineno": 27, "line_no": 213}}
{"prompt": "# Copyright 2020 The HuggingFace Evaluate Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\" Regard measurement. \"\"\"\n\nfrom collections import defaultdict\nfrom operator import itemgetter\nfrom statistics import mean\n\nimport datasets\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer, pipeline\n\nimport evaluate\n\n\nlogger = evaluate.logging.get_logger(__name__)\n\n\n_CITATION = \"\"\"\n@article{https://doi.org/10.48550/arxiv.1909.01326,\n  doi = {10.48550/ARXIV.1909.01326},\n  url = {https://arxiv.org/abs/1909.01326},\n  author = {Sheng, Emily and Chang, Kai-Wei and Natarajan, Premkumar and Peng, Nanyun},", "metadata": {"task_id": "huggingface_evaluate/96", "ground_truth": "  title = {The Woman Worked as a Babysitter: On Biases in Language Generation},", "fpath_tuple": ["huggingface_evaluate", "measurements", "regard", "regard.py"], "context_start_lineno": 0, "line_no": 34}}
{"prompt": "ism\": \"Uses the Phototourism data.\",\n}\n\nmethod_configs[\"nerfacto\"] = TrainerConfig(\n    method_name=\"nerfacto\",\n    steps_per_eval_batch=500,\n    steps_per_save=2000,\n    max_num_iterations=30000,\n    mixed_precision=True,\n    pipeline=VanillaPipelineConfig(\n        datamanager=VanillaDataManagerConfig(\n            dataparser=NerfstudioDataParserConfig(),\n            train_num_rays_per_batch=4096,\n            eval_num_rays_per_batch=4096,\n            camera_optimizer=CameraOptimizerConfig(\n                mode=\"SO3xR3\", optimizer=AdamOptimizerConfig(lr=6e-4, eps=1e-8, weight_decay=1e-2)\n            ),\n        ),\n        model=NerfactoModelConfig(eval_num_rays_per_chunk=1 << 15),\n    ),\n    optimizers={\n        \"proposal_networks\": {\n            \"optimizer\": AdamOptimizerConfig(lr=1e-2, eps=1e-15),\n            \"scheduler\": None,\n        },\n        \"fields\": {\n            \"optimizer\": AdamOptimizerConfig(lr=1e-2, eps=1e-15),\n            \"scheduler\": None,\n        },\n    },\n    viewer=ViewerConfig(num_rays_per_chunk=1 << 15),\n    vis=\"viewer\",\n)\n\nmethod_configs[\"depth-nerfacto\"] = TrainerConfig(\n    method_name=\"depth-nerfacto\",\n    steps_per_eval_batch=500,\n    steps_per_save=2000,\n    max_num_iterations=30000,\n    mixed_precision=True,\n    pipeline=VanillaPipelineConfig(\n        datamanager=DepthDataManagerConfig(\n            dataparser=NerfstudioDataParserConfig(),\n            train_num_rays_per_batch=4096,\n            eval_num_rays_per_batch=4096,\n            camera_optimizer=CameraOptimizerConfig(\n                mode=\"SO3xR3\", optimizer=AdamOptimizerConfig(lr=6e-4, eps=1e-8, weight_decay=1e-2)\n            ),\n        ),\n        model=DepthNerfactoModelConfig(eval_num_rays_per_chunk=1 << 15),\n    ),\n    optimizers={\n        \"proposal_networks\": {\n            \"optimizer\": AdamOptimizerConfig(lr=1e-2, eps=1e-15),\n            \"scheduler\": None,\n        },\n        \"fields\": {\n            \"optimizer\": AdamOptimizerConfig(lr=1e-2, eps=1e-15),\n            \"scheduler\": None,\n        },\n    },\n    viewer=ViewerConfig(num_rays_per_chunk=1 << 15),\n    vis=\"viewer\",\n)\n\nmethod_configs[\"instant-ngp\"] = TrainerConfig(\n    method_name=\"instant-ngp\",\n    steps_per_eval_batch=500,\n    steps_per_save=2000,\n    max_num_iterations=30000,\n    mixed_precision=True,\n    pipeline=DynamicBatchPipelineConfig(\n        datamanager=VanillaDataManagerConfig(dataparser=NerfstudioDataParserConfig(), train_num_rays_per_batch=8192),\n        model=InstantNGPModelConfig(eval_num_rays_per_chunk=8192),\n    ),\n    optimizers={\n        \"fields\": {\n            \"optimizer\": AdamOptimizerConfig(lr=1e-2, eps=1e-15),\n            \"scheduler\": None,\n        }\n    },\n    viewer=ViewerConfig(num_rays_per_chunk=64000),\n    vis=\"viewer\",\n)\n\n\nmethod_configs[\"instant-ngp-bounded\"] = TrainerConfig(\n    method_name=\"instant-ngp-bounded\",\n    steps_per_eval_batch=500,\n    steps_per_save=2000,\n    max_num_iterations=30000,\n    mixed_precision=True,\n    pipeline=DynamicBatchPipelineConfig(\n        datamanager=VanillaDataManagerConfig(dataparser=InstantNGPDataParserConfig(), train_num_rays_per_batch=8192),\n        model=InstantNGPModelConfig(\n            eval_num_rays_per_chunk=8192,\n            contraction_type=ContractionType.AABB,\n            render_step_size=0.001,\n            max_num_samples_per_ray=48,\n            near_plane=0.01,\n            background_color=\"black\",\n        ),\n    ),\n    optimizers={\n        \"fields\": {\n            \"optimizer\": AdamOptimizerConfig(lr=1e-2, eps=1e-15),\n            \"scheduler\": None,\n        }\n    },\n    viewer=ViewerConfig(num_rays_per_chunk=64000),\n    vis=\"viewer\",\n)\n\n\nmethod_configs[\"mipnerf\"] = TrainerConfig(\n    method_name=\"mipnerf\",\n    pipeline=VanillaPipelineConfig(\n        datamanager=VanillaDataManagerConfig(dataparser=NerfstudioDataParserConfig(), train_num_rays_per_batch=1024),\n        model=VanillaModelConfig(\n            _target=MipNerfModel,\n            loss_coefficients={\"rgb_loss_coarse\": 0.1, \"rgb_loss_fine\": 1.0},\n            num_coarse_samples=128,\n            num_importance_samples=128,\n            eval_num_rays_per_chunk=1024,\n        ),\n    ),\n    optimizers={\n        \"fields\": {\n            \"optimizer\": RAdamOptimizerConfig(lr=5e-4, eps=1e-08),\n            \"scheduler\": None,\n        }\n    },\n)\n\nmethod_configs[\"semantic-nerfw\"] = TrainerConfig(\n    method_name=\"semantic-nerfw\",\n    steps_per_eval_batch=500,\n    steps_per_save=2000,\n    max_num_iterations=30000,\n    mixed_precision=True,\n    pipeline=VanillaPipelineConfig(\n        datamanager=SemanticDataManagerConfig(\n            dataparser=FriendsDataParserConfig(), train_num_rays_per_batch=4096, eval_num_rays_per_batch=8192\n        ),\n        model=SemanticNerfWModelConfig(eval_num_rays_per_chunk=1 << 16),\n    ),\n    optimizers={\n        \"proposal_networks\": {\n            \"optimizer\": AdamOptimizerConfig(lr=1e-2, eps=1e-15),\n            \"scheduler\": None,\n        },\n        \"fields\": {\n            \"optimizer\": AdamOptimizerConfig(lr=1e-2, eps=1e-15),\n            \"scheduler\": None,\n        },\n    },\n    viewer=ViewerConfig(num_rays_per_chunk=1 << 16),\n    vis=\"viewer\",\n)\n\nmethod_configs[\"vanilla-nerf\"] = TrainerConfig(\n    method_name=\"vanilla-nerf\",\n    pipeline=VanillaPipelineConfig(\n        datamanager=VanillaDataManagerConfig(\n            dataparser=BlenderDataParserConfig(),\n        ),\n        model=VanillaModelConfig(_target=NeRFModel),", "metadata": {"task_id": "nerfstudio-project_nerfstudio/118", "ground_truth": "    ),", "fpath_tuple": ["nerfstudio-project_nerfstudio", "nerfstudio", "configs", "method_configs.py"], "context_start_lineno": 68, "line_no": 235}}
{"prompt": "loader,\n                fit_config=self.class_fit_config_dir_dump(tmp_dir),\n                calib_config=self.class_calib_config_nodir_nodump,\n            )\n            sample = prob_class.posterior.sample()\n            prob_class.posterior.load_state(tmp_dir)\n\n            # restore\n            prob_class.train(\n                train_data_loader=self.class_train_data_loader,\n                calib_data_loader=self.class_val_data_loader,\n                val_data_loader=self.class_val_data_loader,\n                fit_config=self.class_fit_config_restore(tmp_dir),\n                calib_config=self.class_calib_config_nodir_nodump,\n            )\n\n            # load state\n            prob_class.load_state(checkpoint_path=tmp_dir)\n\n            # save state\n            prob_class.save_state(checkpoint_path=tmp_dir)\n\n    def test_dryrun_reg_advi(self):\n        with tempfile.TemporaryDirectory() as tmp_dir:\n            prob_reg = ProbRegressor(\n                model=MyModel(self.reg_output_dim),\n                likelihood_log_variance_model=MyModel(self.reg_output_dim),\n                posterior_approximator=ADVIPosteriorApproximator(),\n                output_calibrator=RegressionTemperatureScaler(),\n            )\n            # no save dir, no dump\n            status = prob_reg.train(\n                train_data_loader=self.reg_train_data_loader,\n                calib_data_loader=self.reg_val_data_loader,\n                val_data_loader=self.reg_val_data_loader,\n                fit_config=self.reg_fit_config_nodir_nodump,\n                calib_config=self.reg_calib_config_nodir_nodump,\n            )\n            sample = prob_reg.posterior.sample()\n\n            # no save dir but dump\n            with self.assertRaises(ValueError):\n                status = prob_reg.train(\n                    train_data_loader=self.reg_train_data_loader,\n                    calib_data_loader=self.reg_val_data_loader,\n                    val_data_loader=self.reg_val_data_loader,\n                    fit_config=self.reg_fit_config_nodir_dump,\n                    calib_config=self.reg_calib_config_nodir_nodump,\n                )\n\n            # save dir, no dump\n            status = prob_reg.train(\n                train_data_loader=self.reg_train_data_loader,\n                calib_data_loader=self.reg_val_data_loader,\n                val_data_loader=self.reg_val_data_loader,\n                fit_config=self.reg_fit_config_dir_nodump(tmp_dir),\n                calib_config=self.reg_calib_config_nodir_nodump,\n            )\n            sample = prob_reg.posterior.sample()\n            prob_reg.posterior.load_state(tmp_dir)\n\n            # save dir and dump\n            status = prob_reg.train(\n                train_data_loader=self.reg_train_data_loader,\n                calib_data_loader=self.reg_val_data_loader,\n                val_data_loader=self.reg_val_data_loader,\n                fit_config=self.reg_fit_config_dir_nodump(tmp_dir),\n                calib_config=self.reg_calib_config_nodir_nodump,\n            )\n            sample = prob_reg.posterior.sample()\n            prob_reg.posterior.load_state(tmp_dir)\n\n            # restore from advi\n            prob_reg.train(\n                train_data_loader=self.reg_train_data_loader,\n                calib_data_loader=self.reg_val_data_loader,\n                val_data_loader=self.reg_val_data_loader,\n                fit_config=self.reg_fit_config_restore(tmp_dir),\n                calib_config=self.reg_calib_config_nodir_nodump,\n            )\n\n            # restore from map\n            prob_reg_map = ProbRegressor(\n                model=MyModel(self.reg_output_dim),\n                likelihood_log_variance_model=MyModel(self.reg_output_dim),\n                posterior_approximator=MAPPosteriorApproximator(),\n                output_calibrator=RegressionTemperatureScaler(),\n            )\n            status = prob_reg_map.train(\n                train_data_loader=self.reg_train_data_loader,\n                calib_data_loader=self.reg_val_data_loader,\n                val_data_loader=self.reg_val_data_loader,\n                fit_config=self.reg_fit_config_dir_dump(tmp_dir),\n                calib_config=self.reg_calib_config_nodir_nodump,\n            )\n            prob_reg.train(\n                train_data_loader=self.reg_train_data_loader,\n                calib_data_loader=self.reg_val_data_loader,\n                val_data_loader=self.reg_val_data_loader,\n                fit_config=self.reg_fit_config_restore(tmp_dir),\n                calib_config=self.reg_calib_config_nodir_nodump,\n            )\n\n            # load state\n            prob_reg.load_state(checkpoint_path=tmp_dir)\n\n            # save state\n            prob_reg.save_state(checkpoint_path=tmp_dir)\n\n    def test_dryrun_class_advi(self):\n        with tempfile.TemporaryDirectory() as tmp_dir:\n            prob_class = ProbClassifier(\n                model=MyModel(self.class_output_dim),\n                posterior_approximator=ADVIPosteriorApproximator(),\n                output_calibrator=ClassificationTemperatureScaler(),\n            )\n            # no save dir, no dump\n            status = prob_class.train(\n                train_data_loader=self.class_train_data_loader,\n                calib_data_loader=self.class_val_data_loader,\n                val_data_loader=self.class_val_data_loader,\n                fit_config=self.class_fit_config_nodir_nodump,\n                calib_config=self.class_calib_config_nodir_nodump,\n            )\n            sample = prob_class.posterior.sample()\n\n            # no save dir but dump\n            with self.assertRaises(ValueError):\n                status = prob_class.train(\n                    train_data_loader=self.class_train_data_loader,\n                    calib_data_loader=self.class_val_data_loader,\n                    val_data_loader=self.class_val_data_loader,\n                    fit_config=self.class_fit_config_nodir_dump,\n                    calib_config=self.class_calib_config_nodir_nodump,\n                )\n\n            # save dir, no dump\n            status = prob_class.train(\n                train_data_loader=self.class_train_data_loader,\n                calib_data_loader=self.class_val_data_loader,\n                val_data_loader=self.class_val_data_loader,\n                fit_config=self.class_fit_config_dir_nodump(tmp_dir),", "metadata": {"task_id": "awslabs_fortuna/40", "ground_truth": "                calib_config=self.class_calib_config_nodir_nodump,", "fpath_tuple": ["awslabs_fortuna", "tests", "fortuna", "prob_model", "test_train.py"], "context_start_lineno": 255, "line_no": 397}}
{"prompt": "from enum import unique, IntEnum\nfrom threading import Lock\nfrom typing import Mapping, Any, Optional, Callable\nfrom uuid import UUID, uuid4\n\nimport enum_tools\nimport requests\nfrom requests import RequestException\n\nfrom.base import _BEFORE_HOOK_TYPE, _AFTER_HOOK_TYPE, _ERROR_HOOK_TYPE\nfrom..base import HttpEngine, get_values_from_response, default_func\n\n\n@enum_tools.documentation.document_enum\n@unique\nclass TaskResultType(IntEnum):\n    \"\"\"\n    Overview:\n        Types of the task result\n    \"\"\"\n    COMPLETED = 1  # doc: Task complete without error\n    FAILED = 2  # doc: Task end with error\n\n\n@enum_tools.documentation.document_enum\n@unique\nclass TaskStatus(IntEnum):\n    \"\"\"\n    Overview:\n        Status of a task\n    \"\"\"\n    IDLE = 0x00  # doc: Task not started, waiting for awake\n\n    STARTING = 0x11  # doc: Task is starting, but initialization is not completed.\n    STARTED = 0x12  # doc: Task started, initialization is completed.\n    START_FAILED = 0x13  # doc: Task start failed, error occurred when initializing.\n\n    COMPLETED = 0x21  # doc: Task completed without error\n    FAILED = 0x22  # doc: Task ended with error\n\n\n_COMPLETE_TRIGGER_NAME = '__TASK_COMPLETE__'\n_FAIL_TRIGGER_NAME = '__TASK_FAIL__'\n\n\nclass Task:\n    \"\"\"\n    Overview:\n        Task object of the connections.\n        Linking call is fully supported.\n    Example:\n        - A simple and common usage\n        >>> with master.new_connection('cnn1,', '127.0.0.1', 2333) as connection:\n        >>>     task = connection.new_task({'data': 233})\n        >>>     # task is not sent yet\n        >>>\n        >>>     task = task.on_complete(func1).on_fail(func2).on_complete(func3).start().join()\n        >>>     # task is completed or failed after this line\n        >>>     # when task completed : func1(result) --> func3(result)\n        >>>     # when task failed : func2(result)\n    \"\"\"\n\n    def __init__(\n        self,\n        http_engine: HttpEngine,\n        data: Mapping[str, Any],\n        task_id: Optional[UUID] = None,\n        before_task_start: Optional[_BEFORE_HOOK_TYPE] = None,\n        after_task_start: Optional[_AFTER_HOOK_TYPE] = None,\n        error_task_start: Optional[_ERROR_HOOK_TYPE] = None\n    ):\n        \"\"\"\n        Overview:\n            Constructor of `Task`\n        Arguments:\n            - http_engine (:obj:`HttpEngine`): Http engine object used by the task\n            - data (:obj:`Mapping[str, Any]`): Task data of the task\n            - task_id (:obj:`Optional[UUID]`): Id of the task\n            - before_task_start (:obj:`Optional[_BEFORE_HOOK_TYPE]`): Callback to be executed before task start \\\n                (`None` means do nothing)\n            - after_task_start (:obj:`Optional[_AFTER_HOOK_TYPE]`): Callback to be executed after task start \\\n                (`None` means do nothing)\n            - error_task_start (:obj:`Optional[_ERROR_HOOK_TYPE]`): Callback to be executed when task start failed \\\n                (`None` means do nothing)\n        \"\"\"\n        self.__http_engine = http_engine\n        self.__lock = Lock()\n\n        self.__task_id = task_id or uuid4()\n        self.__task_data = data\n        self.__task_result = None\n        self.__task_status = TaskStatus.IDLE\n        self.__task_lock = Lock()\n\n        self.__before_task_start = before_task_start or (lambda d: d)\n        self.__after_task_start = default_func(None)(after_task_start)\n        self.__error_task_start = default_func(None)(error_task_start)\n        self.__after_task_completed_callbacks = []\n        self.__after_task_failed_callbacks = []\n\n        self.__init_triggers()\n\n    def __request(self, method: str, path: str, data: Optional[Mapping[str, Any]] = None) -> requests.Response:\n        return self.__http_engine.request(method, path, data)\n\n    def __task_start(self):\n        try:\n            self.__task_status = TaskStatus.STARTING\n            response = self.__request(\n                'POST', '/task/new', {\n                    'task': {\n                        'id': str(self.__task_id)\n                    },\n                    'data': self.__before_task_start(self.__task_data) or {}\n                }\n            )\n        except RequestException as err:\n            self.__task_status = TaskStatus.START_FAILED\n            return self.__error_task_start(err)\n        else:\n            self.__task_status = TaskStatus.STARTED\n            ret = self.__after_task_start(*get_values_from_response(response))\n            self.__task_lock.acquire()\n            return ret\n\n    def __task_complete(self, result: Mapping[str, Any]):\n        self.__task_status = TaskStatus.COMPLETED\n        self.__task_result = result\n        for _callback in self.__after_task_completed_callbacks:\n            _callback(self.__task_data, result)\n        self.__task_lock.release()\n\n    def __task_fail(self, result: Mapping[str, Any]):\n        self.__task_status = TaskStatus.FAILED\n        self.__task_result = result\n        for _callback in self.__after_task_failed_callbacks:\n            _callback(self.__task_data, result)\n        self.__task_lock.release()\n\n    # trigger methods\n    def __task_complete_trigger(self, result: Mapping[str, Any]):\n        with self.__lock:\n            if self.__task_status == TaskStatus.STARTED:", "metadata": {"task_id": "opendilab_ACE/163", "ground_truth": "                self.__task_complete(result)", "fpath_tuple": ["opendilab_ACE", "ding", "interaction", "master", "task.py"], "context_start_lineno": 0, "line_no": 143}}
{"prompt": "1 += delgramcountergood_rep[delgram] / delgramcounter_rep[delgram]\n        deltmpscore2 += delgramcountergood_rep[delgram] / delgramcounterall_rep[delgram]\n    # Define 0/0=1 instead of 0 to give higher scores for predictions that match\n    # a target exactly.\n    delscore_precision = 1\n    if len(delgramcounter_rep) > 0:\n        delscore_precision = deltmpscore1 / len(delgramcounter_rep)\n\n    # ADDITION\n    addgramcounter = set(cgramcounter) - set(sgramcounter)\n    addgramcountergood = set(addgramcounter) & set(rgramcounter)\n    addgramcounterall = set(rgramcounter) - set(sgramcounter)\n\n    addtmpscore = 0\n    for addgram in addgramcountergood:\n        addtmpscore += 1\n\n    # Define 0/0=1 instead of 0 to give higher scores for predictions that match\n    # a target exactly.\n    addscore_precision = 1\n    addscore_recall = 1\n    if len(addgramcounter) > 0:\n        addscore_precision = addtmpscore / len(addgramcounter)\n    if len(addgramcounterall) > 0:\n        addscore_recall = addtmpscore / len(addgramcounterall)\n    addscore = 0\n    if addscore_precision > 0 or addscore_recall > 0:\n        addscore = 2 * addscore_precision * addscore_recall / (addscore_precision + addscore_recall)\n\n    return (keepscore, delscore_precision, addscore)\n\n\ndef SARIsent(ssent, csent, rsents):\n    numref = len(rsents)\n\n    s1grams = ssent.split(\" \")\n    c1grams = csent.split(\" \")\n    s2grams = []\n    c2grams = []\n    s3grams = []\n    c3grams = []\n    s4grams = []\n    c4grams = []\n\n    r1gramslist = []\n    r2gramslist = []\n    r3gramslist = []\n    r4gramslist = []\n    for rsent in rsents:\n        r1grams = rsent.split(\" \")\n        r2grams = []\n        r3grams = []\n        r4grams = []\n        r1gramslist.append(r1grams)\n        for i in range(0, len(r1grams) - 1):\n            if i < len(r1grams) - 1:\n                r2gram = r1grams[i] + \" \" + r1grams[i + 1]\n                r2grams.append(r2gram)\n            if i < len(r1grams) - 2:\n                r3gram = r1grams[i] + \" \" + r1grams[i + 1] + \" \" + r1grams[i + 2]\n                r3grams.append(r3gram)\n            if i < len(r1grams) - 3:\n                r4gram = r1grams[i] + \" \" + r1grams[i + 1] + \" \" + r1grams[i + 2] + \" \" + r1grams[i + 3]\n                r4grams.append(r4gram)\n        r2gramslist.append(r2grams)\n        r3gramslist.append(r3grams)\n        r4gramslist.append(r4grams)\n\n    for i in range(0, len(s1grams) - 1):\n        if i < len(s1grams) - 1:\n            s2gram = s1grams[i] + \" \" + s1grams[i + 1]\n            s2grams.append(s2gram)\n        if i < len(s1grams) - 2:\n            s3gram = s1grams[i] + \" \" + s1grams[i + 1] + \" \" + s1grams[i + 2]\n            s3grams.append(s3gram)\n        if i < len(s1grams) - 3:\n            s4gram = s1grams[i] + \" \" + s1grams[i + 1] + \" \" + s1grams[i + 2] + \" \" + s1grams[i + 3]\n            s4grams.append(s4gram)\n\n    for i in range(0, len(c1grams) - 1):\n        if i < len(c1grams) - 1:\n            c2gram = c1grams[i] + \" \" + c1grams[i + 1]\n            c2grams.append(c2gram)\n        if i < len(c1grams) - 2:\n            c3gram = c1grams[i] + \" \" + c1grams[i + 1] + \" \" + c1grams[i + 2]\n            c3grams.append(c3gram)\n        if i < len(c1grams) - 3:\n            c4gram = c1grams[i] + \" \" + c1grams[i + 1] + \" \" + c1grams[i + 2] + \" \" + c1grams[i + 3]\n            c4grams.append(c4gram)\n\n    (keep1score, del1score, add1score) = SARIngram(s1grams, c1grams, r1gramslist, numref)\n    (keep2score, del2score, add2score) = SARIngram(s2grams, c2grams, r2gramslist, numref)\n    (keep3score, del3score, add3score) = SARIngram(s3grams, c3grams, r3gramslist, numref)\n    (keep4score, del4score, add4score) = SARIngram(s4grams, c4grams, r4gramslist, numref)\n    avgkeepscore = sum([keep1score, keep2score, keep3score, keep4score]) / 4\n    avgdelscore = sum([del1score, del2score, del3score, del4score]) / 4\n    avgaddscore = sum([add1score, add2score, add3score, add4score]) / 4\n    finalscore = (avgkeepscore + avgdelscore + avgaddscore) / 3\n    return finalscore\n\n\ndef normalize(sentence, lowercase: bool = True, tokenizer: str = \"13a\", return_str: bool = True):\n\n    # Normalization is requried for the ASSET dataset (one of the primary\n    # datasets in sentence simplification) to allow using space\n    # to split the sentence. Even though Wiki-Auto and TURK datasets,\n    # do not require normalization, we do it for consistency.\n    # Code adapted from the EASSE library [1] written by the authors of the ASSET dataset.\n    # [1] https://github.com/feralvam/easse/blob/580bba7e1378fc8289c663f864e0487188fe8067/easse/utils/preprocessing.py#L7\n\n    if lowercase:\n        sentence = sentence.lower()\n\n    if tokenizer in [\"13a\", \"intl\"]:\n        if version.parse(sacrebleu.__version__).major >= 2:\n            normalized_sent = sacrebleu.metrics.bleu._get_tokenizer(tokenizer)()(sentence)\n        else:\n            normalized_sent = sacrebleu.TOKENIZERS[tokenizer]()(sentence)\n    elif tokenizer == \"moses\":\n        normalized_sent = sacremoses.MosesTokenizer().tokenize(sentence, return_str=True, escape=False)", "metadata": {"task_id": "huggingface_evaluate/89", "ground_truth": "    elif tokenizer == \"penn\":", "fpath_tuple": ["huggingface_evaluate", "metrics", "wiki_split", "wiki_split.py"], "context_start_lineno": 155, "line_no": 275}}
{"prompt": " torch.Generator\n        components = self.get_dummy_components()\n        pipe = StableDiffusionDepth2ImgPipeline(**components)\n        pipe = pipe.to(device)\n        pipe.set_progress_bar_config(disable=None)\n\n        inputs = self.get_dummy_inputs(device)\n        inputs[\"prompt\"] = [inputs[\"prompt\"]] * 2\n        inputs[\"image\"] = 2 * [inputs[\"image\"]]\n        image = pipe(**inputs).images\n        image_slice = image[-1, -3:, -3:, -1]\n\n        assert image.shape == (2, 32, 32, 3)\n\n        if torch_device == \"mps\":\n            expected_slice = np.array([0.6501, 0.5150, 0.4939, 0.6688, 0.5437, 0.5758, 0.5115, 0.4406, 0.4551])\n        else:\n            expected_slice = np.array([0.6681, 0.5023, 0.6611, 0.7605, 0.5724, 0.7959, 0.7240, 0.5871, 0.5383])\n\n        assert np.abs(image_slice.flatten() - expected_slice).max() < 1e-3\n\n    def test_stable_diffusion_depth2img_num_images_per_prompt(self):\n        device = \"cpu\"  # ensure determinism for the device-dependent torch.Generator\n        components = self.get_dummy_components()\n        pipe = StableDiffusionDepth2ImgPipeline(**components)\n        pipe = pipe.to(device)\n        pipe.set_progress_bar_config(disable=None)\n\n        # test num_images_per_prompt=1 (default)\n        inputs = self.get_dummy_inputs(device)\n        images = pipe(**inputs).images\n\n        assert images.shape == (1, 32, 32, 3)\n\n        # test num_images_per_prompt=1 (default) for batch of prompts\n        batch_size = 2\n        inputs = self.get_dummy_inputs(device)\n        inputs[\"prompt\"] = [inputs[\"prompt\"]] * batch_size\n        images = pipe(**inputs).images\n\n        assert images.shape == (batch_size, 32, 32, 3)\n\n        # test num_images_per_prompt for single prompt\n        num_images_per_prompt = 2\n        inputs = self.get_dummy_inputs(device)\n        images = pipe(**inputs, num_images_per_prompt=num_images_per_prompt).images\n\n        assert images.shape == (num_images_per_prompt, 32, 32, 3)\n\n        # test num_images_per_prompt for batch of prompts\n        batch_size = 2\n        inputs = self.get_dummy_inputs(device)\n        inputs[\"prompt\"] = [inputs[\"prompt\"]] * batch_size\n        images = pipe(**inputs, num_images_per_prompt=num_images_per_prompt).images\n\n        assert images.shape == (batch_size * num_images_per_prompt, 32, 32, 3)\n\n    def test_stable_diffusion_depth2img_pil(self):\n        device = \"cpu\"  # ensure determinism for the device-dependent torch.Generator\n        components = self.get_dummy_components()\n        pipe = StableDiffusionDepth2ImgPipeline(**components)\n        pipe = pipe.to(device)\n        pipe.set_progress_bar_config(disable=None)\n\n        inputs = self.get_dummy_inputs(device)\n\n        image = pipe(**inputs).images\n        image_slice = image[0, -3:, -3:, -1]\n\n        if torch_device == \"mps\":\n            expected_slice = np.array([0.53232, 0.47015, 0.40868, 0.45651, 0.4891, 0.4668, 0.4287, 0.48822, 0.47439])\n        else:\n            expected_slice = np.array([0.6853, 0.3740, 0.4856, 0.7130, 0.7402, 0.5535, 0.4828, 0.6182, 0.5053])\n\n        assert np.abs(image_slice.flatten() - expected_slice).max() < 1e-3\n\n\n@slow\n@require_torch_gpu\nclass StableDiffusionDepth2ImgPipelineSlowTests(unittest.TestCase):\n    def tearDown(self):\n        super().tearDown()\n        gc.collect()\n        torch.cuda.empty_cache()\n\n    def get_inputs(self, device=\"cpu\", dtype=torch.float32, seed=0):\n        generator = torch.Generator(device=device).manual_seed(seed)\n        init_image = load_image(\n            \"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/depth2img/two_cats.png\"\n        )\n        inputs = {\n            \"prompt\": \"two tigers\",\n            \"image\": init_image,\n            \"generator\": generator,\n            \"num_inference_steps\": 3,\n            \"strength\": 0.75,\n            \"guidance_scale\": 7.5,\n            \"output_type\": \"numpy\",\n        }\n        return inputs\n\n    def test_stable_diffusion_depth2img_pipeline_default(self):\n        pipe = StableDiffusionDepth2ImgPipeline.from_pretrained(\n            \"stabilityai/stable-diffusion-2-depth\", safety_checker=None\n        )\n        pipe.to(torch_device)\n        pipe.set_progress_bar_config(disable=None)\n        pipe.enable_attention_slicing()\n\n        inputs = self.get_inputs()\n        image = pipe(**inputs).images\n        image_slice = image[0, 253:256, 253:256, -1].flatten()\n\n        assert image.shape == (1, 480, 640, 3)\n        expected_slice = np.array([0.9057, 0.9365, 0.9258, 0.8937, 0.8555, 0.8541, 0.8260, 0.7747, 0.7421])\n\n        assert np.abs(expected_slice - image_slice).max() < 1e-4\n\n    def test_stable_diffusion_depth2img_pipeline_k_lms(self):\n        pipe = StableDiffusionDepth2ImgPipeline.from_pretrained(\n            \"stabilityai/stable-diffusion-2-depth\", safety_checker=None\n        )\n        pipe.scheduler = LMSDiscreteScheduler.from_config(pipe.scheduler.config)\n        pipe.to(torch_device)\n        pipe.set_progress_bar_config(disable=None)\n        pipe.enable_attention_slicing()\n\n        inputs = self.get_inputs()\n        image = pipe(**inputs).images\n        image_slice = image[0, 253:256, 253:256, -1].flatten()\n\n        assert image.shape == (1, 480, 640, 3)\n        expected_slice = np.array([0.6363, 0.6274, 0.6309, 0.6370, 0.6226, 0.6286, 0.6213, 0.6453, 0.6306])\n\n        assert np.abs(expected_slice - image_slice).max() < 1e-4\n\n    def test_stable_diffusion_depth2img_pipeline_ddim(self):", "metadata": {"task_id": "huggingface_diffusers/189", "ground_truth": "        pipe = StableDiffusionDepth2ImgPipeline.from_pretrained(", "fpath_tuple": ["huggingface_diffusers", "tests", "pipelines", "stable_diffusion_2", "test_stable_diffusion_depth.py"], "context_start_lineno": 316, "line_no": 453}}
{"prompt": "ies=download_config.proxies,\n            resume_download=download_config.resume_download,\n            user_agent=download_config.user_agent,\n            local_files_only=download_config.local_files_only,\n            use_etag=download_config.use_etag,\n            max_retries=download_config.max_retries,\n            use_auth_token=download_config.use_auth_token,\n            ignore_url_params=download_config.ignore_url_params,\n            download_desc=download_config.download_desc,\n        )\n    elif os.path.exists(url_or_filename):\n        # File, and it exists.\n        output_path = url_or_filename\n    elif is_local_path(url_or_filename):\n        # File, but it doesn't exist.\n        raise FileNotFoundError(f\"Local file {url_or_filename} doesn't exist\")\n    else:\n        # Something unknown\n        raise ValueError(f\"unable to parse {url_or_filename} as a URL or as a local path\")\n\n    if output_path is None:\n        return output_path\n\n    if download_config.extract_compressed_file:\n        output_path = ExtractManager(cache_dir=download_config.cache_dir).extract(\n            output_path, force_extract=download_config.force_extract\n        )\n\n    return output_path\n\n\ndef get_datasets_user_agent(user_agent: Optional[Union[str, dict]] = None) -> str:\n    ua = f\"datasets/{__version__}; python/{config.PY_VERSION}\"\n    ua += f\"; pyarrow/{config.PYARROW_VERSION}\"\n    if config.TORCH_AVAILABLE:\n        ua += f\"; torch/{config.TORCH_VERSION}\"\n    if config.TF_AVAILABLE:\n        ua += f\"; tensorflow/{config.TF_VERSION}\"\n    if config.JAX_AVAILABLE:\n        ua += f\"; jax/{config.JAX_VERSION}\"\n    if isinstance(user_agent, dict):\n        ua += f\"; {'; '.join(f'{k}/{v}' for k, v in user_agent.items())}\"\n    elif isinstance(user_agent, str):\n        ua += \"; \" + user_agent\n    return ua\n\n\ndef get_authentication_headers_for_url(url: str, use_auth_token: Optional[Union[str, bool]] = None) -> dict:\n    \"\"\"Handle the HF authentication\"\"\"\n    headers = {}\n    if url.startswith(config.HF_ENDPOINT):\n        token = None\n        if isinstance(use_auth_token, str):\n            token = use_auth_token\n        elif bool(use_auth_token):\n            from huggingface_hub import hf_api\n\n            token = hf_api.HfFolder.get_token()\n        if token:\n            headers[\"authorization\"] = f\"Bearer {token}\"\n    return headers\n\n\nclass OfflineModeIsEnabled(ConnectionError):\n    pass\n\n\ndef _raise_if_offline_mode_is_enabled(msg: Optional[str] = None):\n    \"\"\"Raise an OfflineModeIsEnabled error (subclass of ConnectionError) if HF_EVALUATE_OFFLINE is True.\"\"\"\n    if config.HF_EVALUATE_OFFLINE:\n        raise OfflineModeIsEnabled(\n            \"Offline mode is enabled.\" if msg is None else \"Offline mode is enabled. \" + str(msg)\n        )\n\n\ndef _retry(\n    func,\n    func_args: Optional[tuple] = None,\n    func_kwargs: Optional[dict] = None,\n    exceptions: Type[requests.exceptions.RequestException] = requests.exceptions.RequestException,\n    status_codes: Optional[List[int]] = None,\n    max_retries: int = 0,\n    base_wait_time: float = 0.5,\n    max_wait_time: float = 2,\n):\n    func_args = func_args or ()\n    func_kwargs = func_kwargs or {}\n    retry = 0\n    while True:\n        try:\n            return func(*func_args, **func_kwargs)\n        except exceptions as err:\n            if retry >= max_retries or (status_codes and err.response.status_code not in status_codes):\n                raise err\n            else:\n                sleep_time = min(max_wait_time, base_wait_time * 2**retry)  # Exponential backoff\n                logger.info(f\"{func} timed out, retrying in {sleep_time}s... [{retry/max_retries}]\")\n                time.sleep(sleep_time)\n                retry += 1\n\n\ndef _request_with_retry(\n    method: str,\n    url: str,\n    max_retries: int = 0,\n    base_wait_time: float = 0.5,\n    max_wait_time: float = 2,\n    timeout: float = 10.0,\n    **params,\n) -> requests.Response:\n    \"\"\"Wrapper around requests to retry in case it fails with a ConnectTimeout, with exponential backoff.\n\n    Note that if the environment variable HF_EVALUATE_OFFLINE is set to 1, then a OfflineModeIsEnabled error is raised.\n\n    Args:\n        method (str): HTTP method, such as 'GET' or 'HEAD'.\n        url (str): The URL of the resource to fetch.\n        max_retries (int): Maximum number of retries, defaults to 0 (no retries).\n        base_wait_time (float): Duration (in seconds) to wait before retrying the first time. Wait time between\n            retries then grows exponentially, capped by max_wait_time.\n        max_wait_time (float): Maximum amount of time between two retries, in seconds.\n        **params: Params to pass to :obj:`requests.request`.\n    \"\"\"\n    _raise_if_offline_mode_is_enabled(f\"Tried to reach {url}\")\n    tries, success = 0, False\n    while not success:\n        tries += 1\n        try:\n            response = requests.request(method=method.upper(), url=url, timeout=timeout, **params)\n            success = True\n        except (requests.exceptions.ConnectTimeout, requests.exceptions.ConnectionError) as err:\n            if tries > max_retries:\n                raise err\n            else:\n                logger.info(f\"{method} request to {url} timed out, retrying... [{tries/max_retries}]\")\n                sleep_time = min(max_wait_time, base_wait_time * 2 ** (tries - 1))  # Exponential backoff\n                time.sleep(sleep_time)\n    return response\n\n\ndef ftp_head(url, timeout=10.0):\n    _raise_if_offline_mode_is_enabled(f\"Tried to reach {url}\")\n    try:\n        with closing(urllib.request.urlopen(url, timeout=timeout)) as r:\n            r.read(1)\n    except Exception:\n        return False\n    return True\n\n\ndef ftp_get(url, temp_file, timeout=10.0):\n    _raise_if_offline_mode_is_enabled(f\"Tried to reach {url}\")\n    try:\n        logger.info(f\"Getting through FTP {url} into {temp_file.name}\")\n        with closing(urllib.request.urlopen(url, timeout=timeout)) as r:\n            shutil.copyfileobj(r, temp_file)\n    except urllib.error.URLError as e:\n        raise ConnectionError(e) from None\n\n\ndef http_get(", "metadata": {"task_id": "huggingface_evaluate/178", "ground_truth": "    url, temp_file, proxies=None, resume_size=0, headers=None, cookies=None, timeout=100.0, max_retries=0, desc=None", "fpath_tuple": ["huggingface_evaluate", "src", "evaluate", "utils", "file_utils.py"], "context_start_lineno": 227, "line_no": 388}}
{"prompt": "import argparse\nimport hashlib\nimport logging\nimport math\nimport os\nfrom pathlib import Path\nfrom typing import Optional\n\nimport numpy as np\nimport torch\nimport torch.utils.checkpoint\nfrom torch.utils.data import Dataset\n\nimport jax\nimport jax.numpy as jnp\nimport optax\nimport transformers\nfrom diffusers import (\n    FlaxAutoencoderKL,\n    FlaxDDPMScheduler,\n    FlaxPNDMScheduler,\n    FlaxStableDiffusionPipeline,\n    FlaxUNet2DConditionModel,\n)\nfrom diffusers.pipelines.stable_diffusion import FlaxStableDiffusionSafetyChecker\nfrom diffusers.utils import check_min_version\nfrom flax import jax_utils\nfrom flax.training import train_state\nfrom flax.training.common_utils import shard\nfrom huggingface_hub import HfFolder, Repository, create_repo, whoami\nfrom jax.experimental.compilation_cache import compilation_cache as cc\nfrom PIL import Image\nfrom torchvision import transforms\nfrom tqdm.auto import tqdm\nfrom transformers import CLIPFeatureExtractor, CLIPTokenizer, FlaxCLIPTextModel, set_seed\n\n\n# Will error if the minimal version of diffusers is not installed. Remove at your own risks.\ncheck_min_version(\"0.13.0.dev0\")\n\n# Cache compiled models across invocations of this script.\ncc.initialize_cache(os.path.expanduser(\"~/.cache/jax/compilation_cache\"))\n\nlogger = logging.getLogger(__name__)\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=\"Simple example of a training script.\")\n    parser.add_argument(\n        \"--pretrained_model_name_or_path\",\n        type=str,\n        default=None,\n        required=True,\n        help=\"Path to pretrained model or model identifier from huggingface.co/models.\",\n    )\n    parser.add_argument(\n        \"--pretrained_vae_name_or_path\",\n        type=str,\n        default=None,\n        help=\"Path to pretrained vae or vae identifier from huggingface.co/models.\",\n    )\n    parser.add_argument(\n        \"--revision\",\n        type=str,\n        default=None,\n        required=False,\n        help=\"Revision of pretrained model identifier from huggingface.co/models.\",\n    )\n    parser.add_argument(\n        \"--tokenizer_name\",\n        type=str,\n        default=None,\n        help=\"Pretrained tokenizer name or path if not the same as model_name\",\n    )\n    parser.add_argument(\n        \"--instance_data_dir\",\n        type=str,\n        default=None,\n        required=True,\n        help=\"A folder containing the training data of instance images.\",\n    )\n    parser.add_argument(\n        \"--class_data_dir\",\n        type=str,\n        default=None,\n        required=False,\n        help=\"A folder containing the training data of class images.\",\n    )\n    parser.add_argument(\n        \"--instance_prompt\",\n        type=str,\n        default=None,\n        help=\"The prompt with identifier specifying the instance\",\n    )\n    parser.add_argument(\n        \"--class_prompt\",\n        type=str,\n        default=None,\n        help=\"The prompt to specify images in the same class as provided instance images.\",\n    )\n    parser.add_argument(\n        \"--with_prior_preservation\",\n        default=False,\n        action=\"store_true\",\n        help=\"Flag to add prior preservation loss.\",\n    )\n    parser.add_argument(\"--prior_loss_weight\", type=float, default=1.0, help=\"The weight of prior preservation loss.\")\n    parser.add_argument(\n        \"--num_class_images\",\n        type=int,", "metadata": {"task_id": "huggingface_diffusers/198", "ground_truth": "        default=100,", "fpath_tuple": ["huggingface_diffusers", "examples", "dreambooth", "train_dreambooth_flax.py"], "context_start_lineno": 0, "line_no": 110}}
{"prompt": " obj]\n        elif isinstance(schema, Sequence):\n            # We allow to reverse list of dict => dict of list for compatiblity with tfds\n            if isinstance(schema.feature, dict):\n                if isinstance(obj, (list, tuple)):\n                    # obj is a list of dict\n                    for k, dict_tuples in zip_dict(schema.feature, *obj):\n                        for sub_obj in dict_tuples[1:]:\n                            if _check_non_null_non_empty_recursive(sub_obj, dict_tuples[0]):\n                                self._enforce_nested_string_type(dict_tuples[0], sub_obj)\n                                break\n                    return None\n                else:\n                    # obj is a single dict\n                    for k, (sub_schema, sub_objs) in zip_dict(schema.feature, obj):\n                        for sub_obj in sub_objs:\n                            if _check_non_null_non_empty_recursive(sub_obj, sub_schema):\n                                self._enforce_nested_string_type(sub_schema, sub_obj)\n                                break\n                    return None\n            # schema.feature is not a dict\n            if isinstance(obj, str):  # don't interpret a string as a list\n                raise ValueError(f\"Got a string but expected a list instead: '{obj}'\")\n            if obj is None:\n                return None\n            else:\n                if len(obj) > 0:\n                    for first_elmt in obj:\n                        if _check_non_null_non_empty_recursive(first_elmt, schema.feature):\n                            break\n                    if not isinstance(first_elmt, list):\n                        return self._enforce_nested_string_type(schema.feature, first_elmt)\n\n        elif isinstance(schema, Value):\n            if pa.types.is_string(schema.pa_type) and not isinstance(obj, str):\n                raise TypeError(f\"Expected type str but got {type(obj)}.\")\n\n\nclass Metric(EvaluationModule):\n    \"\"\"A Metric is the base class and common API for all metrics.\n\n    Args:\n        config_name (`str`):\n            This is used to define a hash specific to a metric computation script and prevents the metric's data\n            to be overridden when the metric loading script is modified.\n        keep_in_memory (`bool`):\n            Keep all predictions and references in memory. Not possible in distributed settings.\n        cache_dir (`str`):\n            Path to a directory in which temporary prediction/references data will be stored.\n            The data directory should be located on a shared file-system in distributed setups.\n        num_process (`int`):\n            Specify the total number of nodes in a distributed settings.\n            This is useful to compute metrics in distributed setups (in particular non-additive metrics like F1).\n        process_id (`int`):\n            Specify the id of the current process in a distributed setup (between 0 and num_process-1)\n            This is useful to compute metrics in distributed setups (in particular non-additive metrics like F1).\n        seed (`int`, *optional*):\n            If specified, this will temporarily set numpy's random seed when [`~evaluate.Metric.compute`] is run.\n        experiment_id (`str`):\n            A specific experiment id. This is used if several distributed evaluations share the same file system.\n            This is useful to compute metrics in distributed setups (in particular non-additive metrics like F1).\n        max_concurrent_cache_files (`int`):\n            Max number of concurrent metric cache files (default `10000`).\n        timeout (`Union[int, float]`):\n            Timeout in second for distributed setting synchronization.\n    \"\"\"\n\n\nclass Comparison(EvaluationModule):\n    \"\"\"A Comparison is the base class and common API for all comparisons.\n\n    Args:\n        config_name (`str`):\n            This is used to define a hash specific to a comparison computation script and prevents the comparison's data\n            to be overridden when the comparison loading script is modified.\n        keep_in_memory (`bool`):\n            Keep all predictions and references in memory. Not possible in distributed settings.\n        cache_dir (`str`):\n            Path to a directory in which temporary prediction/references data will be stored.\n            The data directory should be located on a shared file-system in distributed setups.\n        num_process (`int`):\n            Specify the total number of nodes in a distributed settings.\n            This is useful to compute  comparisons in distributed setups (in particular non-additive comparisons).\n        process_id (`int`):\n            Specify the id of the current process in a distributed setup (between 0 and num_process-1)\n            This is useful to compute  comparisons in distributed setups (in particular non-additive comparisons).\n        seed (`int`, *optional*):\n            If specified, this will temporarily set numpy's random seed when [`~evaluate.Comparison.compute`] is run.\n        experiment_id (`str`):\n            A specific experiment id. This is used if several distributed evaluations share the same file system.\n            This is useful to compute  comparisons in distributed setups (in particular non-additive comparisons).\n        max_concurrent_cache_files (`int`):\n            Max number of concurrent comparison cache files (default `10000`).\n        timeout (`Union[int, float]`):\n            Timeout in second for distributed setting synchronization.\n    \"\"\"\n\n\nclass Measurement(EvaluationModule):\n    \"\"\"A Measurement is the base class and common API for all measurements.\n\n    Args:\n        config_name (`str`):\n            This is used to define a hash specific to a measurement computation script and prevents the measurement's data\n            to be overridden when the measurement loading script is modified.\n        keep_in_memory (`bool`):\n            Keep all predictions and references in memory. Not possible in distributed settings.\n        cache_dir (`str`):\n            Path to a directory in which temporary prediction/references data will be stored.\n            The data directory should be located on a shared file-system in distributed setups.\n        num_process (`int`):\n            Specify the total number of nodes in a distributed settings.\n            This is useful to compute measurements in distributed setups (in particular non-additive measurements).\n        process_id (`int`):\n            Specify the id of the current process in a distributed setup (between 0 and num_process-1)\n            This is useful to compute measurements in distributed setups (in particular non-additive measurements).\n        seed (`int`, *optional*):\n            If specified, this will temporarily set numpy's random seed when [`~evaluate.Measurement.compute`] is run.\n        experiment_id (`str`):\n            A specific experiment id. This is used if several distributed evaluations share the same file system.\n            This is useful to compute measurements in distributed setups (in particular non-additive measurements).\n        max_concurrent_cache_files (`int`):\n            Max number of concurrent measurement cache files (default `10000`).\n        timeout (`Union[int, float]`):\n            Timeout in second for distributed setting synchronization.\n    \"\"\"\n\n\nclass CombinedEvaluations:\n    def __init__(self, evaluation_modules, force_prefix=False):\n        from.loading import load  # avoid circular imports\n\n        self.evaluation_module_names = None\n        if isinstance(evaluation_modules, list):\n            self.evaluation_modules = evaluation_modules\n        elif isinstance(evaluation_modules, dict):\n            self.evaluation_modules = list(evaluation_modules.values())\n            self.evaluation_module_names = list(evaluation_modules.keys())\n        loaded_modules = []\n\n        for module in self.evaluation_modules:\n            if isinstance(module, str):\n                module = load(module)\n            loaded_modules.append(module)\n        self.evaluation_modules = loaded_modules\n\n        if self.evaluation_module_names is None:", "metadata": {"task_id": "huggingface_evaluate/135", "ground_truth": "            self.evaluation_module_names = [module.name for module in self.evaluation_modules]", "fpath_tuple": ["huggingface_evaluate", "src", "evaluate", "module.py"], "context_start_lineno": 739, "line_no": 886}}
{"prompt": "# Copyright 2022 The HuggingFace Datasets Authors and the current dataset script contributor.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Perplexity Metric.\"\"\"\n\nimport datasets\nimport numpy as np\nimport torch\nfrom torch.nn import CrossEntropyLoss\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nimport evaluate\nfrom evaluate import logging\n\n\n_CITATION = \"\"\"\\\n\n\"\"\"\n\n_DESCRIPTION = \"\"\"\nPerplexity (PPL) can be used for evaluating to what extent a dataset is similar to the distribution of text that a given model was trained on.\nIt is defined as the exponentiated average negative log-likelihood of a sequence, calculated with exponent base `e`.\n\nFor more information, see https://huggingface.co/docs/transformers/perplexity\n\"\"\"\n\n_KWARGS_DESCRIPTION = \"\"\"\nArgs:\n    model_id (str): model used for calculating Perplexity\n            NOTE: Perplexity can only be calculated for causal language models.\n                    This includes models such as gpt2, causal variations of bert,\n                    causal versions of t5, and more (the full list can be found\n                    in the AutoModelForCausalLM documentation here:\n                    https://huggingface.co/docs/transformers/master/en/model_doc/auto#transformers.AutoModelForCausalLM )\n\n    data (list of str): input data, each separate text snippet\n        is one list entry.\n    batch_size (int): the batch size to run texts through the model. Defaults to 16.\n    add_start_token (bool): whether to add the start token to the texts,\n        so the perplexity can include the probability of the first word. Defaults to True.\n    device (str): device to run on, defaults to 'cuda' when available\nReturns:\n    perplexity: dictionary containing the perplexity scores for the texts", "metadata": {"task_id": "huggingface_evaluate/104", "ground_truth": "        in the input list, as well as the mean perplexity. If one of the input texts is", "fpath_tuple": ["huggingface_evaluate", "measurements", "perplexity", "perplexity.py"], "context_start_lineno": 0, "line_no": 53}}
{"prompt": "_network(step_td, **kwargs)\n            next_value = step_td.get(self.value_key)\n\n        done = tensordict.get(\"done\")\n        if self.vectorized:\n            adv = vec_td_lambda_advantage_estimate(\n                gamma, lmbda, value, next_value, reward, done\n            )\n        else:\n            adv = td_lambda_advantage_estimate(\n                gamma, lmbda, value, next_value, reward, done\n            )\n\n        tensordict.set(self.advantage_key, adv)\n        tensordict.set(self.value_target_key, adv + value)\n        return tensordict\n\n\nclass GAE(nn.Module):\n    \"\"\"A class wrapper around the generalized advantage estimate functional.\n\n    Refer to \"HIGH-DIMENSIONAL CONTINUOUS CONTROL USING GENERALIZED ADVANTAGE ESTIMATION\"\n    https://arxiv.org/pdf/1506.02438.pdf for more context.\n\n    Args:\n        gamma (scalar): exponential mean discount.\n        lmbda (scalar): trajectory discount.\n        value_network (SafeModule): value operator used to retrieve the value estimates.\n        average_gae (bool): if True, the resulting GAE values will be standardized.\n            Default is :obj:`False`.\n        differentiable (bool, optional): if True, gradients are propagated throught\n            the computation of the value function. Default is :obj:`False`.\n        advantage_key (str or tuple of str, optional): the key of the advantage entry.\n            Defaults to \"advantage\".\n        value_target_key (str or tuple of str, optional): the key of the advantage entry.\n            Defaults to \"value_target\".\n        value_key (str or tuple of str, optional): the value key to read from the input tensordict.\n            Defaults to \"state_value\".\n\n    GAE will return an :obj:`\"advantage\"` entry containing the advange value. It will also\n    return a :obj:`\"value_target\"` entry with the return value that is to be used\n    to train the value network. Finally, if :obj:`gradient_mode` is :obj:`True`,\n    an additional and differentiable :obj:`\"value_error\"` entry will be returned,\n    which simple represents the difference between the return and the value network\n    output (i.e. an additional distance loss should be applied to that signed value).\n\n    \"\"\"\n\n    def __init__(\n        self,\n        gamma: Union[float, torch.Tensor],\n        lmbda: float,\n        value_network: SafeModule,\n        average_gae: bool = False,\n        differentiable: bool = False,\n        advantage_key: Union[str, Tuple] = \"advantage\",\n        value_target_key: Union[str, Tuple] = \"value_target\",\n        value_key: Union[str, Tuple] = \"state_value\",\n    ):\n        super().__init__()\n        try:\n            device = next(value_network.parameters()).device\n        except StopIteration:\n            device = torch.device(\"cpu\")\n        self.register_buffer(\"gamma\", torch.tensor(gamma, device=device))\n        self.register_buffer(\"lmbda\", torch.tensor(lmbda, device=device))\n        self.value_network = value_network\n        self.value_key = value_key\n        if value_key not in value_network.out_keys:\n            raise KeyError(\n                f\"value key '{value_key}' not found in value network out_keys.\"\n            )\n\n        self.average_gae = average_gae\n        self.differentiable = differentiable\n\n        self.advantage_key = advantage_key\n        self.value_target_key = value_target_key\n\n        self.in_keys = (\n            value_network.in_keys\n            + [\"reward\", \"done\"]\n            + [(\"next\", in_key) for in_key in value_network.in_keys]\n        )\n        self.out_keys = [self.advantage_key, self.value_target_key]\n\n    @property\n    def is_functional(self):\n        return (\n            \"_is_stateless\" in self.value_network.__dict__\n            and self.value_network.__dict__[\"_is_stateless\"]\n        )\n\n    @_self_set_grad_enabled\n    @dispatch_kwargs\n    def forward(\n        self,\n        tensordict: TensorDictBase,\n        *unused_args,\n        params: Optional[List[Tensor]] = None,\n        target_params: Optional[List[Tensor]] = None,\n    ) -> TensorDictBase:\n        \"\"\"Computes the GAE given the data in tensordict.\n\n        If a functional module is provided, a nested TensorDict containing the parameters\n        (and if relevant the target parameters) can be passed to the module.\n\n        Args:\n            tensordict (TensorDictBase): A TensorDict containing the data\n                (an observation key, \"action\", \"reward\", \"done\" and \"next\" tensordict state\n                as returned by the environment) necessary to compute the value estimates and the GAE.\n                The data passed to this module should be structured as :obj:`[*B, T, F]` where :obj:`B` are\n                the batch size, :obj:`T` the time dimension and :obj:`F` the feature dimension(s).\n            params (TensorDictBase, optional): A nested TensorDict containing the params\n                to be passed to the functional value network module.\n            target_params (TensorDictBase, optional): A nested TensorDict containing the\n                target params to be passed to the functional value network module.\n\n        Returns:\n            An updated TensorDict with an advantage and a value_error keys as defined in the constructor.\n\n        Examples:\n            >>> from tensordict import TensorDict\n            >>> value_net = SafeModule(\n           ...     nn.Linear(3, 1), in_keys=[\"obs\"], out_keys=[\"state_value\"]\n           ... )\n            >>> module = GAE(\n           ...     gamma=0.98,\n           ...     lmbda=0.94,\n           ...     value_network=value_net,\n           ...     differentiable=False,\n           ... )\n            >>> obs, next_obs = torch.randn(2, 1, 10, 3)\n            >>> reward = torch.randn(1, 10, 1)\n            >>> done = torch.zeros(1, 10, 1, dtype=torch.bool)\n            >>> tensordict = TensorDict({\"obs\": obs, \"next\": {\"obs\": next_obs}, \"done\": done, \"reward\": reward}, [1, 10])\n            >>> _ = module(tensordict)\n            >>> assert \"advantage\" in tensordict.keys()\n\n        The module supports non-tensordict (i.e. unpacked tensordict) inputs too:\n\n        Examples:\n            >>> value_net = SafeModule(\n           ...     nn.Linear(3, 1), in_keys=[\"obs\"], out_keys=[\"state_value\"]\n           ... )\n            >>> module = GAE(\n           ...     gamma=0.98,\n           ...     lmbda=0.94,\n           ...     value_network=value_net,\n           ...     differentiable=False,\n           ... )\n            >>> obs, next_obs = torch.randn(2, 1, 10, 3)\n            >>> reward = torch.randn(1, 10, 1)\n            >>> done = torch.zeros(1, 10, 1, dtype=torch.bool)\n            >>> advantage, value_target = module(obs=obs, reward=reward, done=done, next_obs=next_obs)\n\n        \"\"\"", "metadata": {"task_id": "pytorch_rl/36", "ground_truth": "        if tensordict.batch_dims < 1:", "fpath_tuple": ["pytorch_rl", "torchrl", "objectives", "value", "advantages.py"], "context_start_lineno": 369, "line_no": 526}}
{"prompt": ", jnp.ndarray]:\n        return grad, loss\n\n    def on_train_start(\n        self,\n        state: CalibState,\n        data_loaders: List[DataLoader],\n        outputs_loaders: List[TargetsLoader],\n        rng: PRNGKeyArray,\n    ) -> Tuple[CalibState, List[DataLoader], List[TargetsLoader], PRNGKeyArray]:\n        return state, data_loaders, outputs_loaders, rng\n\n    def on_train_end(self, state: CalibState) -> CalibState:\n        self.save_checkpoint(\n            state,\n            save_checkpoint_dir=self.save_checkpoint_dir,\n            keep=self.keep_top_n_checkpoints,\n            force_save=True,\n        )\n        return state\n\n    def on_val_start(self, state: CalibState) -> CalibState:\n        return state\n\n    def compute_metrics(\n        self,\n        preds: Array,\n        uncertainties: Array,\n        targets: Array,\n        metrics: Optional[\n            Tuple[Callable[[jnp.ndarray, jnp.ndarray, Array], Array],...]\n        ],\n    ) -> Dict[str, Array]:\n        metrics_vals = {}\n        for metric in metrics:\n            metrics_vals[metric.__name__] = metric(preds, uncertainties, targets)\n        return metrics_vals\n\n\nclass JittedMixin:\n    @partial(jax.jit, static_argnums=(0, 4, 6))\n    def training_step(\n        self,\n        state: CalibState,\n        batch: Batch,\n        outputs: Array,\n        fun: Callable,\n        rng: PRNGKeyArray,\n        n_data: int,\n    ) -> Tuple[CalibState, Dict[str, Any]]:\n        return super().training_step(state, batch, outputs, fun, rng, n_data)\n\n    @partial(jax.jit, static_argnums=(0, 4, 6))\n    def val_loss_step(\n        self,\n        state: CalibState,\n        batch: Batch,\n        outputs: Array,\n        fun: Callable,\n        rng: PRNGKeyArray,\n        n_data: int,\n    ) -> Dict[str, jnp.ndarray]:\n        return super().val_loss_step(state, batch, outputs, fun, rng, n_data)\n\n\nclass MultiDeviceMixin:\n    all_reduce_mean = jax.pmap(lambda x: lax.pmean(x, \"x\"), \"x\")\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.multi_device = True\n\n    @staticmethod\n    def _add_device_dim_to_data_loader(data_loader: DataLoader) -> DataLoader:\n        def _reshape_batch(batch):\n            n_devices = jax.local_device_count()\n            if batch.shape[0] % n_devices!= 0:\n                raise ValueError(\n                    f\"The size of all batches must be a multiple of {n_devices}, that is the number of \"\n                    f\"available devices. However, a batch with shape {batch.shape[0]} was found. \"\n                    f\"Please set an appropriate batch size.\"\n                )\n            return batch.reshape((n_devices, -1) + batch.shape[1:])\n\n        class DataLoaderWrapper:\n            def __init__(self, data_loader: DataLoader):\n                self._data_loader = data_loader\n\n            def __iter__(self):\n                data_loader = map(\n                    lambda batch: tree_map(_reshape_batch, batch), self._data_loader\n                )\n                data_loader = jax_utils.prefetch_to_device(data_loader, 2)\n                yield from data_loader\n\n        return (\n            DataLoaderWrapper(data_loader) if data_loader is not None else data_loader\n        )\n\n    @staticmethod\n    def _add_device_dim_to_outputs_loader(\n        outputs_loader: TargetsLoader,\n    ) -> TargetsLoader:\n        def _reshape_batch(batch):\n            n_devices = jax.local_device_count()\n            if batch.shape[0] % n_devices!= 0:\n                raise ValueError(\n                    f\"The size of all output batches must be a multiple of {n_devices}, that is the number of \"\n                    f\"available devices. However, a batch of outputs with shape {batch.shape[0]} was found. \"\n                    f\"Please set an appropriate batch size.\"\n                )\n            return batch.reshape((n_devices, -1) + batch.shape[1:])\n\n        class TargetsLoaderWrapper:\n            def __init__(self, outputs_loader: TargetsLoader):\n                self._outputs_loader = outputs_loader\n\n            def __iter__(self):\n                outputs_loader = map(\n                    lambda batch: tree_map(_reshape_batch, batch), self._outputs_loader\n                )\n                outputs_loader = jax_utils.prefetch_to_device(outputs_loader, 2)\n                yield from outputs_loader\n\n        return (\n            TargetsLoaderWrapper(outputs_loader)\n            if outputs_loader is not None\n            else outputs_loader\n        )\n\n    @staticmethod\n    def sync_mutable(state: CalibState) -> CalibState:\n        return (\n            state.replace(mutable=MultiDeviceMixin.all_reduce_mean(state.mutable))\n            if state.mutable[\"output_calibrator\"] is not None\n            else state\n        )\n\n    @staticmethod\n    def sync_gradients_and_loss(\n        grads: jnp.ndarray, loss: jnp.ndarray\n    ) -> Tuple[jnp.ndarray, jnp.ndarray]:\n        grad = lax.pmean(grads, axis_name=\"batch\")\n        loss = lax.pmean(loss, axis_name=\"batch\")\n        return grad, loss\n\n    def save_checkpoint(\n        self,\n        state: CalibState,\n        save_checkpoint_dir: Path,\n        keep: int = 1,\n        force_save: bool = False,\n        prefix: str = \"checkpoint_\",\n    ) -> None:\n        state = self.sync_mutable(state)\n        state = jax.device_get(tree_map(lambda x: x[0], state))\n        return super(MultiDeviceMixin, self).save_checkpoint(\n            state, save_checkpoint_dir, keep, force_save, prefix\n        )\n\n    def on_train_start(\n        self,\n        state: CalibState,\n        data_loaders: List[DataLoader],\n        outputs_loaders: List[TargetsLoader],\n        rng: PRNGKeyArray,\n    ) -> Tuple[CalibState, List[DataLoader], List[TargetsLoader], PRNGKeyArray]:\n        state, data_loaders, outputs_loaders, rng = super(\n            MultiDeviceMixin, self\n        ).on_train_start(state, data_loaders, outputs_loaders, rng)\n        state = jax_utils.replicate(state)\n        data_loaders = [\n            self._add_device_dim_to_data_loader(dl) if dl is not None else dl\n            for dl in data_loaders\n        ]\n        outputs_loaders = [\n            self._add_device_dim_to_outputs_loader(ol) if ol is not None else ol\n            for ol in outputs_loaders\n        ]\n        model_key = random.split(rng, jax.local_device_count())", "metadata": {"task_id": "awslabs_fortuna/141", "ground_truth": "        return state, data_loaders, outputs_loaders, model_key", "fpath_tuple": ["awslabs_fortuna", "fortuna", "calibration", "calibrator.py"], "context_start_lineno": 424, "line_no": 604}}
{"prompt": "# coding=utf-8\n# Copyright 2022 HuggingFace Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport gc\nimport unittest\n\nimport numpy as np\nimport torch\n\nfrom diffusers import AutoencoderKL, DDIMScheduler, LDMTextToImagePipeline, UNet2DConditionModel\nfrom diffusers.utils.testing_utils import load_numpy, nightly, require_torch_gpu, slow, torch_device\nfrom transformers import CLIPTextConfig, CLIPTextModel, CLIPTokenizer\n\nfrom...test_pipelines_common import PipelineTesterMixin\n\n\ntorch.backends.cuda.matmul.allow_tf32 = False\n\n\nclass LDMTextToImagePipelineFastTests(PipelineTesterMixin, unittest.TestCase):\n    pipeline_class = LDMTextToImagePipeline\n    test_cpu_offload = False\n\n    def get_dummy_components(self):\n        torch.manual_seed(0)\n        unet = UNet2DConditionModel(\n            block_out_channels=(32, 64),\n            layers_per_block=2,\n            sample_size=32,\n            in_channels=4,", "metadata": {"task_id": "huggingface_diffusers/88", "ground_truth": "            out_channels=4,", "fpath_tuple": ["huggingface_diffusers", "tests", "pipelines", "latent_diffusion", "test_latent_diffusion.py"], "context_start_lineno": 0, "line_no": 42}}
{"prompt": "from copy import deepcopy\nfrom ding.entry import serial_pipeline\nfrom easydict import EasyDict\n\nagent_num = 8\ncollector_env_num = 8\nevaluator_env_num = 8\n\nmain_config = dict(\n    env=dict(\n        map_name='8m_vs_9m',\n        difficulty=7,\n        reward_type='original',\n        agent_num=agent_num,\n        collector_env_num=collector_env_num,\n        evaluator_env_num=evaluator_env_num,\n        manager=dict(shared_memory=True, ),\n        stop_value=1.999,\n        n_evaluator_episode=32,\n    ),\n    policy=dict(\n        cuda=True,\n        priority=False,\n        model=dict(\n            agent_num=agent_num,\n            embed_num=6,\n            state_len=33, # 32+ num of unit type\n            relation_len=6,\n            hidden_len=256,\n            local_pred_len=6,\n            global_pred_len=12,\n        ),\n        learn=dict(\n            multi_gpu=False,\n            update_per_collect=50,\n            batch_size=320,\n            learning_rate=0.0003,\n            clip_value=50,\n            double_q=False,\n            target_update_theta=0.008,\n            nstep=3,\n            discount_factor=0.99,\n            aux_loss_weight=dict(\n                begin=10,\n                end=10,\n                T_max=400000,\n            ),\n            aux_label_norm=True,\n            shuffle=True,\n            learning_rate_type='cosine',\n            learning_rate_tmax=90000,\n            learning_rate_eta_min=3e-6,\n            learner=dict(\n                hook=dict(\n                    log_show_after_iter=2000,\n                    save_ckpt_after_iter=10000000000,\n                    save_ckpt_after_run=True,\n                ),\n            ),\n        ),", "metadata": {"task_id": "opendilab_ACE/94", "ground_truth": "        collect=dict(", "fpath_tuple": ["opendilab_ACE", "exp", "smac", "8m9m", "config.py"], "context_start_lineno": 0, "line_no": 60}}
{"prompt": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import annotations\n\n\"\"\"OSS Vizier client.\"\"\"\n\n# TODO: Raise vizier-specific exceptions.\n\nfrom typing import Callable, Iterator, Iterable, Any, Collection, Mapping, Optional, Type\nimport attr\n\nfrom vizier.client import client_abc\nfrom vizier.service import constants\nfrom vizier.service import pyvizier as vz\nfrom vizier.service import resources\nfrom vizier.service import vizier_client\n\n# Redeclared so users do not have to also import client_abc and vizier_client.\nNO_ENDPOINT = constants.NO_ENDPOINT\nResourceNotFoundError = client_abc.ResourceNotFoundError\n\n\n# TODO: Consider if user should set a one-line flag explicitly to\n# denote local NO_ENDPOINT server will be used.\n@attr.define\nclass _EnviromentVariables:\n  server_endpoint: str = attr.field(\n      default=NO_ENDPOINT, validator=attr.validators.instance_of(str)\n  )\n\n\nenvironment_variables = _EnviromentVariables()\n\n\n@attr.define\nclass Trial(client_abc.TrialInterface):\n  \"\"\"Trial class.\n\n  This class owns a Vizier client of the Study that contains the Trial that\n  it is associated with.\n  \"\"\"\n\n  _client: vizier_client.VizierClient = attr.field()\n  _id: int = attr.field(validator=attr.validators.instance_of(int))\n\n  @property\n  def id(self) -> int:", "metadata": {"task_id": "google_vizier/140", "ground_truth": "    return self._id", "fpath_tuple": ["google_vizier", "vizier", "service", "clients.py"], "context_start_lineno": 0, "line_no": 59}}
{"prompt": "\"\"\"This file is part of https://github.com/mit-han-lab/dlg.\nMIT License\nCopyright (c) 2019 Ildoo Kim\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n\"\"\"\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import grad\nimport torchvision\nfrom torchvision import models, datasets, transforms\n\n\ndef weights_init(m):\n    if hasattr(m, \"weight\"):\n        m.weight.data.uniform_(-0.5, 0.5)\n    if hasattr(m, \"bias\"):\n        m.bias.data.uniform_(-0.5, 0.5)\n\n\nclass LeNet(nn.Module):\n    def __init__(self):\n        super(LeNet, self).__init__()\n        act = nn.Sigmoid\n        self.body = nn.Sequential(\n            nn.Conv2d(3, 12, kernel_size=5, padding=5 // 2, stride=2),\n            act(),\n            nn.Conv2d(12, 12, kernel_size=5, padding=5 // 2, stride=2),\n            act(),\n            nn.Conv2d(12, 12, kernel_size=5, padding=5 // 2, stride=1),\n            act(),\n        )\n\n        self.fc = nn.Sequential(nn.Linear(768, 100))\n\n    def forward(self, x):\n        out = self.body(x)\n        out = out.view(out.size(0), -1)\n        # print(out.size())\n        out = self.fc(out)\n        return out\n\n\n'''ResNet in PyTorch.\nFor Pre-activation ResNet, see 'preact_resnet.py'.\nReference:\n[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n'''\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, in_planes, planes, stride=1):\n        super(BasicBlock, self).__init__()\n\n        self.conv1 = nn.Conv2d(in_planes,\n                               planes,\n                               kernel_size=3,\n                               stride=stride,\n                               padding=1,\n                               bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes,", "metadata": {"task_id": "alibaba_FederatedScope/98", "ground_truth": "                               planes,", "fpath_tuple": ["alibaba_FederatedScope", "federatedscope", "attack", "models", "vision.py"], "context_start_lineno": 0, "line_no": 79}}
{"prompt": "# Copyright 2022 The Nerfstudio Team. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\nCollection of Losses.\n\"\"\"\nfrom enum import Enum\n\nimport torch\nfrom torch import nn\nfrom torchtyping import TensorType\n\nfrom nerfstudio.cameras.rays import RaySamples\n\nL1Loss = nn.L1Loss\nMSELoss = nn.MSELoss\n\nLOSSES = {\"L1\": L1Loss, \"MSE\": MSELoss}\n\nEPS = 1.0e-7\n\n# Sigma scale factor from Urban Radiance Fields (Rematas et al., 2022)\nURF_SIGMA_SCALE_FACTOR = 3.0\n\n\nclass DephtLossType(Enum):\n    \"\"\"Types of depth losses for depth supervision.\"\"\"\n\n    DS_NERF = 1\n    URF = 2\n\n\ndef outer(\n    t0_starts: TensorType[..., \"num_samples_0\"],\n    t0_ends: TensorType[..., \"num_samples_0\"],\n    t1_starts: TensorType[..., \"num_samples_1\"],\n    t1_ends: TensorType[..., \"num_samples_1\"],\n    y1: TensorType[..., \"num_samples_1\"],\n) -> TensorType[..., \"num_samples_0\"]:\n    \"\"\"Faster version of\n\n    https://github.com/kakaobrain/NeRF-Factory/blob/f61bb8744a5cb4820a4d968fb3bfbed777550f4a/src/model/mipnerf360/helper.py#L117\n    https://github.com/google-research/multinerf/blob/b02228160d3179300c7d499dca28cb9ca3677f32/internal/stepfun.py#L64\n\n    Args:\n        t0_starts: start of the interval edges\n        t0_ends: end of the interval edges\n        t1_starts: start of the interval edges\n        t1_ends: end of the interval edges\n        y1: weights\n    \"\"\"\n    cy1 = torch.cat([torch.zeros_like(y1[..., :1]), torch.cumsum(y1, dim=-1)], dim=-1)\n\n    idx_lo = torch.searchsorted(t1_starts.contiguous(), t0_starts.contiguous(), side=\"right\") - 1", "metadata": {"task_id": "nerfstudio-project_nerfstudio/55", "ground_truth": "    idx_lo = torch.clamp(idx_lo, min=0, max=y1.shape[-1] - 1)", "fpath_tuple": ["nerfstudio-project_nerfstudio", "nerfstudio", "model_components", "losses.py"], "context_start_lineno": 0, "line_no": 65}}
{"prompt": "import json\nimport os\nimport re\nimport sys\nfrom pathlib import Path\n\nimport numpy as np\nfrom datasets import Value\n\nfrom.logging import get_logger\n\n\nlogger = get_logger(__name__)\n\nREGEX_YAML_BLOCK = re.compile(r\"---[\\n\\r]+([\\S\\s]*?)[\\n\\r]+---[\\n\\r]\")\n\n\ndef infer_gradio_input_types(feature_types):\n    \"\"\"\n    Maps metric feature types to input types for gradio Dataframes:\n        - float/int -> numbers\n        - string -> strings\n        - any other -> json\n    Note that json is not a native gradio type but will be treated as string that\n    is then parsed as a json.\n    \"\"\"\n    input_types = []\n    for feature_type in feature_types:\n        input_type = \"json\"\n        if isinstance(feature_type, Value):\n            if feature_type.dtype.startswith(\"int\") or feature_type.dtype.startswith(\"float\"):\n                input_type = \"number\"\n            elif feature_type.dtype == \"string\":\n                input_type = \"str\"\n        input_types.append(input_type)\n    return input_types\n\n\ndef json_to_string_type(input_types):\n    \"\"\"Maps json input type to str.\"\"\"\n    return [\"str\" if i == \"json\" else i for i in input_types]\n\n\ndef parse_readme(filepath):\n    \"\"\"Parses a repositories README and removes\"\"\"\n    if not os.path.exists(filepath):\n        return \"No README.md found.\"\n    with open(filepath, \"r\") as f:\n        text = f.read()\n        match = REGEX_YAML_BLOCK.search(text)\n        if match:\n            text = text[match.end() :]\n    return text\n\n\ndef parse_gradio_data(data, input_types):\n    \"\"\"Parses data from gradio Dataframe for use in metric.\"\"\"\n    metric_inputs = {}\n    data.replace(\"\", np.nan, inplace=True)\n    data.dropna(inplace=True)\n    for feature_name, input_type in zip(data, input_types):\n        if input_type == \"json\":\n            metric_inputs[feature_name] = [json.loads(d) for d in data[feature_name].to_list()]\n        elif input_type == \"str\":\n            metric_inputs[feature_name] = [d.strip('\"') for d in data[feature_name].to_list()]\n        else:\n            metric_inputs[feature_name] = data[feature_name]\n    return metric_inputs\n\n\ndef parse_test_cases(test_cases, feature_names, input_types):\n    \"\"\"", "metadata": {"task_id": "huggingface_evaluate/175", "ground_truth": "    Parses test cases to be used in gradio Dataframe. Note that an apostrophe is added", "fpath_tuple": ["huggingface_evaluate", "src", "evaluate", "utils", "gradio.py"], "context_start_lineno": 0, "line_no": 72}}
{"prompt": "import PIL.Image\nimport PIL.ImageOps\nfrom packaging import version\n\n\nif version.parse(version.parse(PIL.__version__).base_version) >= version.parse(\"9.1.0\"):\n    PIL_INTERPOLATION = {\n        \"linear\": PIL.Image.Resampling.BILINEAR,\n        \"bilinear\": PIL.Image.Resampling.BILINEAR,\n        \"bicubic\": PIL.Image.Resampling.BICUBIC,\n        \"lanczos\": PIL.Image.Resampling.LANCZOS,", "metadata": {"task_id": "huggingface_diffusers/161", "ground_truth": "        \"nearest\": PIL.Image.Resampling.NEAREST,", "fpath_tuple": ["huggingface_diffusers", "src", "diffusers", "utils", "pil_utils.py"], "context_start_lineno": 0, "line_no": 11}}
{"prompt": "]\n            for i, config in enumerate(configs):\n                available_worker = 0\n                while not flags[available_worker].is_set():\n                    available_worker = (available_worker + 1) % len(threads)\n                if thread_results[available_worker]:\n                    completed_trial_results = thread_results[available_worker]\n                    cfg_idx = completed_trial_results['cfg_idx']\n                    perfs[cfg_idx] = completed_trial_results['perf']\n                    logger.info(\n                        \"Evaluate the {}-th config {} and get performance {}\".\n                        format(cfg_idx, configs[cfg_idx], perfs[cfg_idx]))\n                    thread_results[available_worker].clear()\n\n                trial_cfg = self._cfg.clone()\n                trial_cfg.merge_from_list(config2cmdargs(config))\n                flags[available_worker].clear()\n                trial = TrialExecutor(i, flags[available_worker],\n                                      thread_results[available_worker],\n                                      trial_cfg, self._client_cfgs)\n                trial.start()\n                threads[available_worker] = trial\n\n            for i in range(len(flags)):\n                if not flags[i].is_set():\n                    threads[i].join()\n            for i in range(len(thread_results)):\n                if thread_results[i]:\n                    completed_trial_results = thread_results[i]\n                    cfg_idx = completed_trial_results['cfg_idx']\n                    perfs[cfg_idx] = completed_trial_results['perf']\n                    # TODO: Support num_worker in WandB\n                    logger.info(\n                        \"Evaluate the {}-th config {} and get performance {}\".\n                        format(cfg_idx, configs[cfg_idx], perfs[cfg_idx]))\n                    thread_results[i].clear()\n\n        else:\n            perfs = [None] * len(configs)\n            for i, config in enumerate(configs):\n                trial_cfg = self._cfg.clone()\n                trial_cfg.merge_from_list(config2cmdargs(config))\n                results = make_trial(trial_cfg, self._client_cfgs)\n                key1, key2 = trial_cfg.hpo.metric.split('.')\n                perfs[i] = results[key1][key2]\n                logger.info(\n                    \"Evaluate the {}-th config {} and get performance {}\".\n                    format(i, config, perfs[i]))\n                if self._cfg.wandb.use:\n                    log2wandb(i, config, results, trial_cfg)\n        return perfs\n\n    def optimize(self):\n        perfs = self._evaluate(self._init_configs)\n        results = summarize_hpo_results(self._init_configs,\n                                        perfs,\n                                        white_list=set(\n                                            self._search_space.keys()),\n                                        desc=self._cfg.hpo.larger_better,\n                                        use_wandb=self._cfg.wandb.use)\n        logger.info(\n            \"========================== HPO Final ==========================\")\n        logger.info(\"\\n{}\".format(results))\n        logger.info(\"====================================================\")\n\n        return results\n\n\nclass IterativeScheduler(ModelFreeBase):\n    \"\"\"The base class for HPO algorithms that divide the whole optimization\n    procedure into iterations.\n    \"\"\"\n    def _setup(self):\n        self._stage = 0\n        return super(IterativeScheduler, self)._setup()\n\n    def _stop_criterion(self, configs, last_results):\n        \"\"\"To determine whether the algorithm should be terminated.\n\n        Arguments:\n            configs (list): each element is a trial configuration.\n            last_results (DataFrame): each row corresponds to a specific\n            configuration as well as its latest performance.\n        :returns: whether to terminate.\n        :rtype: bool\n        \"\"\"\n        raise NotImplementedError\n\n    def _iteration(self, configs):\n        \"\"\"To evaluate the given collection of configurations at this stage.\n\n        Arguments:\n            configs (list): each element is a trial configuration.\n        :returns: the performances of the given configurations.\n        :rtype: list\n        \"\"\"\n\n        perfs = self._evaluate(configs)\n        return perfs\n\n    def _generate_next_population(self, configs, perfs):\n        \"\"\"To generate the configurations for the next stage.\n\n        Arguments:\n            configs (list): the configurations of last stage.\n            perfs (list): their corresponding performances.\n        :returns: configuration for the next stage.\n        :rtype: list\n        \"\"\"\n\n        raise NotImplementedError\n\n    def optimize(self):\n        current_configs = deepcopy(self._init_configs)\n        last_results = None\n        while not self._stop_criterion(current_configs, last_results):\n            current_perfs = self._iteration(current_configs)\n            last_results = summarize_hpo_results(\n                current_configs,\n                current_perfs,\n                white_list=set(self._search_space.keys()),\n                desc=self._cfg.hpo.larger_better,\n                use_wandb=self._cfg.wandb.use)\n            self._stage += 1\n            logger.info(\n                \"========================== Stage{} ==========================\"\n               .format(self._stage))\n            logger.info(\"\\n{}\".format(last_results))\n            logger.info(\"====================================================\")\n            current_configs = self._generate_next_population(\n                current_configs, current_perfs)\n\n        return current_configs\n\n\nclass SuccessiveHalvingAlgo(IterativeScheduler):\n    \"\"\"Successive Halving Algorithm (SHA) tailored to FL setting, where,\n    in each iteration, just a limited number of communication rounds are\n    allowed for each trial.\n    \"\"\"\n    def _setup(self):\n        init_configs = super(SuccessiveHalvingAlgo, self)._setup()\n\n        for trial_cfg in init_configs:\n            trial_cfg['federate.save_to'] = os.path.join(\n                self._cfg.hpo.working_folder,\n                \"{}.pth\".format(config2str(trial_cfg)))\n\n        if self._cfg.hpo.sha.budgets:\n            for trial_cfg in init_configs:\n                trial_cfg[\n                    'federate.total_round_num'] = self._cfg.hpo.sha.budgets[\n                        self._stage]\n                trial_cfg['eval.freq'] = self._cfg.hpo.sha.budgets[self._stage]\n\n        return init_configs\n\n    def _stop_criterion(self, configs, last_results):\n        return len(configs) <= 1\n\n    def _generate_next_population(self, configs, perfs):\n        indices = [(i, val) for i, val in enumerate(perfs)]\n        indices.sort(key=lambda x: x[1], reverse=self._cfg.hpo.larger_better)\n        next_population = [\n            configs[tp[0]] for tp in\n            indices[:math.\n                    ceil(float(len(indices)) / self._cfg.hpo.sha.elim_rate)]\n        ]\n\n        for trial_cfg in next_population:\n            if 'federate.restore_from' not in trial_cfg:\n                trial_cfg['federate.restore_from'] = trial_cfg[\n                    'federate.save_to']\n            if self._cfg.hpo.sha.budgets and self._stage < len(", "metadata": {"task_id": "alibaba_FederatedScope/130", "ground_truth": "                    self._cfg.hpo.sha.budgets):", "fpath_tuple": ["alibaba_FederatedScope", "federatedscope", "autotune", "algos.py"], "context_start_lineno": 146, "line_no": 320}}
{"prompt": "# Copyright 2020 HuggingFace Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport doctest\nimport glob\nimport importlib\nimport inspect\nimport os\nimport re\nfrom contextlib import contextmanager\nfrom functools import wraps\nfrom unittest.mock import patch\n\nimport numpy as np\nimport pytest\nfrom absl.testing import parameterized\n\nimport evaluate\nfrom evaluate import load\n\nfrom.utils import _run_slow_tests, for_all_test_methods, local, slow\n\n\nREQUIRE_FAIRSEQ = {\"comet\"}\n_has_fairseq = importlib.util.find_spec(\"fairseq\") is not None\n\nUNSUPPORTED_ON_WINDOWS = {\"code_eval\"}\n_on_windows = os.name == \"nt\"\n\nSLOW_METRIC = {\"perplexity\", \"regard\", \"toxicity\"}\n\n\ndef skip_if_metric_requires_fairseq(test_case):\n    @wraps(test_case)\n    def wrapper(self, evaluation_module_name, evaluation_module_type):\n        if not _has_fairseq and evaluation_module_name in REQUIRE_FAIRSEQ:\n            self.skipTest('\"test requires Fairseq\"')\n        else:\n            test_case(self, evaluation_module_name, evaluation_module_type)\n\n    return wrapper\n\n\ndef skip_on_windows_if_not_windows_compatible(test_case):\n    @wraps(test_case)\n    def wrapper(self, evaluation_module_name, evaluation_module_type):\n        if _on_windows and evaluation_module_name in UNSUPPORTED_ON_WINDOWS:\n            self.skipTest('\"test not supported on Windows\"')\n        else:\n            test_case(self, evaluation_module_name, evaluation_module_type)\n\n    return wrapper\n\n\ndef skip_slow_metrics(test_case):\n    @wraps(test_case)\n    def wrapper(self, evaluation_module_name, evaluation_module_type):\n        if not _run_slow_tests and evaluation_module_name in SLOW_METRIC:\n            self.skipTest('\"test is slow\"')\n        else:\n            test_case(self, evaluation_module_name, evaluation_module_type)\n\n    return wrapper\n\n\ndef get_local_module_names():", "metadata": {"task_id": "huggingface_evaluate/0", "ground_truth": "    metrics = [metric_dir.split(os.sep)[-2] for metric_dir in glob.glob(\"./metrics/*/\")]", "fpath_tuple": ["huggingface_evaluate", "tests", "test_metric_common.py"], "context_start_lineno": 0, "line_no": 77}}
{"prompt": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom __future__ import annotations\n\nimport itertools\nfrom copy import deepcopy\nfrom typing import Iterator, List, Optional, Tuple, Union\n\nimport torch\n\nfrom tensordict.nn import make_functional, repopulate_module\n\nfrom tensordict.tensordict import TensorDictBase\nfrom torch import nn, Tensor\nfrom torch.nn import Parameter\n\nfrom torchrl.modules import SafeModule\nfrom torchrl.modules.utils import Buffer\n\n_has_functorch = False\ntry:\n    import functorch as ft  # noqa\n\n    _has_functorch = True\n    FUNCTORCH_ERR = \"\"\nexcept ImportError:", "metadata": {"task_id": "pytorch_rl/87", "ground_truth": "    print(", "fpath_tuple": ["pytorch_rl", "torchrl", "objectives", "common.py"], "context_start_lineno": 0, "line_no": 29}}
{"prompt": ".data.replay_buffers.utils import accept_remote_rref_invocation\nfrom torchrl.data.replay_buffers.writers import RoundRobinWriter\n\nRETRY_LIMIT = 2\nRETRY_DELAY_SECS = 3\nREPLAY_BUFFER_NODE = \"ReplayBuffer\"\nTRAINER_NODE = \"Trainer\"\n\nparser = argparse.ArgumentParser(\n    description=\"RPC Replay Buffer Example\",\n    formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n)\n\nparser.add_argument(\n    \"--rank\",\n    type=int,\n    default=-1,\n    help=\"Node Rank [0 = Replay Buffer, 1 = Dummy Trainer, 2+ = Dummy Data Collector]\",\n)\n\n\nclass DummyDataCollectorNode:\n    \"\"\"Data collector node responsible for collecting experiences used for learning.\n\n    Args:\n        replay_buffer (rpc.RRef): the RRef associated with the construction of the replay buffer\n    \"\"\"\n\n    def __init__(self, replay_buffer: rpc.RRef) -> None:\n        self.id = rpc.get_worker_info().id\n        self.replay_buffer = replay_buffer\n        print(\"Data Collector Node constructed\")\n\n    def _submit_random_item_async(self) -> rpc.RRef:\n        td = TensorDict({\"a\": torch.randint(100, (1,))}, [])\n        return rpc.remote(\n            self.replay_buffer.owner(),\n            ReplayBufferNode.add,\n            args=(\n                self.replay_buffer,\n                td,\n            ),\n        )\n\n    @accept_remote_rref_invocation\n    def collect(self):\n        \"\"\"Method that begins experience collection (we just generate random TensorDicts in this example). `accept_remote_rref_invocation` enables this method to be invoked remotely provided the class instantiation `rpc.RRef` is provided in place of the object reference.\"\"\"\n        for elem in range(50):\n            time.sleep(random.randint(1, 4))\n            print(\n                f\"Collector [{self.id}] submission {elem}: {self._submit_random_item_async().to_here()}\"\n            )\n\n\nclass DummyTrainerNode:\n    \"\"\"Trainer node responsible for learning from experiences sampled from an experience replay buffer.\"\"\"\n\n    def __init__(self) -> None:\n        print(\"DummyTrainerNode\")\n        self.id = rpc.get_worker_info().id\n        self.replay_buffer = self._create_replay_buffer()\n        self._create_and_launch_data_collectors()\n\n    def train(self, iterations: int) -> None:\n        for iteration in range(iterations):\n            print(f\"[{self.id}] Training Iteration: {iteration}\")\n            time.sleep(3)\n            batch = rpc.rpc_sync(\n                self.replay_buffer.owner(),\n                ReplayBufferNode.sample,\n                args=(self.replay_buffer, 16),\n            )\n            print(f\"[{self.id}] Sample Obtained Iteration: {iteration}\")\n            print(f\"{batch}\")\n\n    def _create_replay_buffer(self) -> rpc.RRef:\n        while True:\n            try:\n                replay_buffer_info = rpc.get_worker_info(REPLAY_BUFFER_NODE)\n                buffer_rref = rpc.remote(\n                    replay_buffer_info, ReplayBufferNode, args=(10000,)\n                )\n                print(f\"Connected to replay buffer {replay_buffer_info}\")\n                return buffer_rref\n            except Exception as e:\n                print(f\"Failed to connect to replay buffer: {e}\")\n                time.sleep(RETRY_DELAY_SECS)\n\n    def _create_and_launch_data_collectors(self) -> None:\n        data_collector_number = 2\n        retries = 0\n        data_collectors = []\n        data_collector_infos = []\n        # discover launched data collector nodes (with retry to allow collectors to dynamically join)\n        while True:\n            try:\n                data_collector_info = rpc.get_worker_info(\n                    f\"DataCollector{data_collector_number}\"\n                )\n                print(f\"Data collector info: {data_collector_info}\")\n                dc_ref = rpc.remote(\n                    data_collector_info,\n                    DummyDataCollectorNode,\n                    args=(self.replay_buffer,),\n                )\n                data_collectors.append(dc_ref)\n                data_collector_infos.append(data_collector_info)\n                data_collector_number += 1\n                retries = 0\n            except Exception:\n                retries += 1\n                print(\n                    f\"Failed to connect to DataCollector{data_collector_number} with {retries} retries\"\n                )\n                if retries >= RETRY_LIMIT:\n                    print(f\"{len(data_collectors)} data collectors\")\n                    for data_collector_info, data_collector in zip(\n                        data_collector_infos, data_collectors\n                    ):\n                        rpc.remote(\n                            data_collector_info,\n                            DummyDataCollectorNode.collect,\n                            args=(data_collector,),\n                        )\n                    break\n                else:\n                    time.sleep(RETRY_DELAY_SECS)\n\n\nclass ReplayBufferNode(RemoteTensorDictReplayBuffer):\n    \"\"\"Experience replay buffer node that is capable of accepting remote connections. Being a `RemoteTensorDictReplayBuffer` means all of it's public methods are remotely invokable using `torch.rpc`.\n    Using a LazyMemmapStorage is highly advised in distributed settings with shared storage due to the lower serialisation cost of MemmapTensors as well as the ability to specify file storage locations which can improve ability to recover from node failures.\n\n    Args:\n        capacity (int): the maximum number of elements that can be stored in the replay buffer.\n    \"\"\"\n\n    def __init__(self, capacity: int):\n        super().__init__(\n            storage=LazyMemmapStorage(\n                max_size=capacity, scratch_dir=\"/tmp/\", device=torch.device(\"cpu\")\n            ),\n            sampler=RandomSampler(),\n            writer=RoundRobinWriter(),\n            collate_fn=lambda x: x,\n        )\n\n\nif __name__ == \"__main__\":\n    args = parser.parse_args()\n    rank = args.rank\n    print(f\"Rank: {rank}\")\n\n    os.environ[\"MASTER_ADDR\"] = \"localhost\"\n    os.environ[\"MASTER_PORT\"] = \"29500\"\n    os.environ[\"TORCH_DISTRIBUTED_DEBUG\"] = \"DETAIL\"\n    str_init_method = \"tcp://localhost:10000\"\n    options = rpc.TensorPipeRpcBackendOptions(\n        num_worker_threads=16, init_method=str_init_method\n    )\n    if rank == 0:\n        # rank 0 is the trainer\n        rpc.init_rpc(\n            TRAINER_NODE,\n            rank=rank,\n            backend=rpc.BackendType.TENSORPIPE,\n            rpc_backend_options=options,\n        )\n        print(f\"Initialised Trainer Node {rank}\")\n        trainer = DummyTrainerNode()\n        trainer.train(100)\n        breakpoint()\n    elif rank == 1:\n        # rank 1 is the replay buffer\n        # replay buffer waits passively for construction instructions from trainer node\n        print(REPLAY_BUFFER_NODE)\n        rpc.init_rpc(\n            REPLAY_BUFFER_NODE,", "metadata": {"task_id": "pytorch_rl/66", "ground_truth": "            rank=rank,", "fpath_tuple": ["pytorch_rl", "examples", "distributed", "distributed_replay_buffer.py"], "context_start_lineno": 21, "line_no": 199}}
{"prompt": "# Copyright 2022 The HuggingFace Datasets Authors and the current dataset script contributor.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"MASE - Mean Absolute Scaled Error Metric\"\"\"\n\nimport datasets\nimport numpy as np\nfrom sklearn.metrics import mean_absolute_error\n\nimport evaluate\n\n\n_CITATION = \"\"\"\\\n@article{HYNDMAN2006679,\n    title = {Another look at measures of forecast accuracy},\n    journal = {International Journal of Forecasting},\n    volume = {22},\n    number = {4},\n    pages = {679--688},\n    year = {2006},\n    issn = {0169-2070},\n    doi = {https://doi.org/10.1016/j.ijforecast.2006.03.001},\n    url = {https://www.sciencedirect.com/science/article/pii/S0169207006000239},\n    author = {Rob J. Hyndman and Anne B. Koehler},\n}\n\"\"\"\n\n_DESCRIPTION = \"\"\"\\\nMean Absolute Scaled Error (MASE) is the mean absolute error of the forecast values, divided by the mean absolute error of the in-sample one-step naive forecast.\n\"\"\"\n\n\n_KWARGS_DESCRIPTION = \"\"\"\nArgs:\n    predictions: array-like of shape (n_samples,) or (n_samples, n_outputs)\n        Estimated target values.", "metadata": {"task_id": "huggingface_evaluate/140", "ground_truth": "    references: array-like of shape (n_samples,) or (n_samples, n_outputs)", "fpath_tuple": ["huggingface_evaluate", "metrics", "mase", "mase.py"], "context_start_lineno": 0, "line_no": 46}}
{"prompt": "import torch\nfrom torch_geometric.loader import GraphSAINTRandomWalkSampler, NeighborSampler\n\nfrom federatedscope.core.trainers.enums import LIFECYCLE\nfrom federatedscope.core.monitors import Monitor\nfrom federatedscope.core.trainers.context import CtxVar\nfrom federatedscope.register import register_trainer\nfrom federatedscope.core.trainers import GeneralTorchTrainer\nfrom federatedscope.core.auxiliaries.ReIterator import ReIterator\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n\nclass NodeFullBatchTrainer(GeneralTorchTrainer):\n    def parse_data(self, data):\n        \"\"\"Populate \"{}_data\", \"{}_loader\" and \"num_{}_data\" for different\n        modes\n        \"\"\"\n        init_dict = dict()\n        if isinstance(data, dict):\n            for mode in [\"train\", \"val\", \"test\"]:\n                init_dict[\"{}_loader\".format(mode)] = data.get(mode)\n                init_dict[\"{}_data\".format(mode)] = None\n                # For node-level task dataloader contains one graph\n                init_dict[\"num_{}_data\".format(mode)] = 1\n        else:\n            raise TypeError(\"Type of data should be dict.\")\n        return init_dict\n\n    def _hook_on_batch_forward(self, ctx):\n        batch = ctx.data_batch.to(ctx.device)\n        pred = ctx.model(batch)[batch['{}_mask'.format(ctx.cur_split)]]\n        label = batch.y[batch['{}_mask'.format(ctx.cur_split)]]\n        ctx.batch_size = torch.sum(ctx.data_batch['{}_mask'.format(\n            ctx.cur_split)]).item()\n\n        ctx.loss_batch = CtxVar(ctx.criterion(pred, label), LIFECYCLE.BATCH)\n        ctx.y_true = CtxVar(label, LIFECYCLE.BATCH)\n        ctx.y_prob = CtxVar(pred, LIFECYCLE.BATCH)\n\n    def _hook_on_batch_forward_flop_count(self, ctx):\n        if not isinstance(self.ctx.monitor, Monitor):\n            logger.warning(\n                f\"The trainer {type(self)} does contain a valid monitor, \"\n                f\"this may be caused by \"\n                f\"initializing trainer subclasses without passing a valid \"\n                f\"monitor instance.\"\n                f\"Plz check whether this is you want.\")\n            return\n\n        if self.cfg.eval.count_flops and self.ctx.monitor.flops_per_sample \\\n                == 0:\n            # calculate the flops_per_sample\n            try:\n                batch = ctx.data_batch.to(ctx.device)\n                from torch_geometric.data import Data\n                if isinstance(batch, Data):\n                    x, edge_index = batch.x, batch.edge_index\n                from fvcore.nn import FlopCountAnalysis\n                flops_one_batch = FlopCountAnalysis(ctx.model,\n                                                    (x, edge_index)).total()\n\n                if self.model_nums > 1 and ctx.mirrored_models:\n                    flops_one_batch *= self.model_nums\n                    logger.warning(\n                        \"the flops_per_batch is multiplied by \"\n                        \"internal model nums as self.mirrored_models=True.\"\n                        \"if this is not the case you want, \"\n                        \"please customize the count hook\")\n                self.ctx.monitor.track_avg_flops(flops_one_batch,\n                                                 ctx.batch_size)\n            except:\n                logger.warning(\n                    \"current flop count implementation is for general \"\n                    \"NodeFullBatchTrainer case: \"\n                    \"1) the ctx.model takes only batch = ctx.data_batch as \"\n                    \"input.\"\n                    \"Please check the forward format or implement your own \"\n                    \"flop_count function\")\n                self.ctx.monitor.flops_per_sample = -1  # warning at the\n                # first failure\n\n        # by default, we assume the data has the same input shape,\n        # thus simply multiply the flops to avoid redundant forward", "metadata": {"task_id": "alibaba_FederatedScope/182", "ground_truth": "        self.ctx.monitor.total_flops += self.ctx.monitor.flops_per_sample * \\", "fpath_tuple": ["alibaba_FederatedScope", "federatedscope", "gfl", "trainer", "nodetrainer.py"], "context_start_lineno": 0, "line_no": 85}}
{"prompt": "# Copyright 2022 The Nerfstudio Team. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\nInstant-NGP field implementations using tiny-cuda-nn, torch,....\n\"\"\"\n\n\nfrom typing import Optional\n\nimport torch\nfrom nerfacc import ContractionType, contract\nfrom torch.nn.parameter import Parameter\nfrom torchtyping import TensorType\n\nfrom nerfstudio.cameras.rays import RaySamples\nfrom nerfstudio.data.scene_box import SceneBox\nfrom nerfstudio.field_components.activations import trunc_exp\nfrom nerfstudio.field_components.embedding import Embedding\nfrom nerfstudio.field_components.field_heads import FieldHeadNames\nfrom nerfstudio.fields.base_field import Field\n\ntry:\n    import tinycudann as tcnn\nexcept ImportError:\n    # tinycudann module doesn't exist\n    pass\n\n\ndef get_normalized_directions(directions: TensorType[\"bs\":..., 3]):\n    \"\"\"SH encoding must be in the range [0, 1]\n\n    Args:\n        directions: batch of directions\n    \"\"\"\n    return (directions + 1.0) / 2.0\n\n\nclass TCNNInstantNGPField(Field):\n    \"\"\"TCNN implementation of the Instant-NGP field.\n\n    Args:\n        aabb: parameters of scene aabb bounds\n        num_layers: number of hidden layers\n        hidden_dim: dimension of hidden layers\n        geo_feat_dim: output geo feat dimensions\n        num_layers_color: number of hidden layers for color network\n        hidden_dim_color: dimension of hidden layers for color network\n        use_appearance_embedding: whether to use appearance embedding\n        num_images: number of images, requried if use_appearance_embedding is True\n        appearance_embedding_dim: dimension of appearance embedding\n        contraction_type: type of contraction\n        num_levels: number of levels of the hashmap for the base mlp\n        log2_hashmap_size: size of the hashmap for the base mlp\n    \"\"\"\n\n    def __init__(\n        self,\n        aabb,\n        num_layers: int = 2,\n        hidden_dim: int = 64,\n        geo_feat_dim: int = 15,\n        num_layers_color: int = 3,\n        hidden_dim_color: int = 64,\n        use_appearance_embedding: bool = False,\n        num_images: Optional[int] = None,\n        appearance_embedding_dim: int = 32,\n        contraction_type: ContractionType = ContractionType.UN_BOUNDED_SPHERE,\n        num_levels: int = 16,\n        log2_hashmap_size: int = 19,\n    ) -> None:\n        super().__init__()\n\n        self.aabb = Parameter(aabb, requires_grad=False)\n        self.geo_feat_dim = geo_feat_dim\n        self.contraction_type = contraction_type\n\n        self.use_appearance_embedding = use_appearance_embedding\n        if use_appearance_embedding:\n            assert num_images is not None\n            self.appearance_embedding_dim = appearance_embedding_dim\n            self.appearance_embedding = Embedding(num_images, appearance_embedding_dim)\n\n        # TODO: set this properly based on the aabb\n        per_level_scale = 1.4472692012786865\n\n        self.direction_encoding = tcnn.Encoding(\n            n_input_dims=3,\n            encoding_config={\n                \"otype\": \"SphericalHarmonics\",\n                \"degree\": 4,\n            },\n        )\n\n        self.mlp_base = tcnn.NetworkWithInputEncoding(\n            n_input_dims=3,\n            n_output_dims=1 + self.geo_feat_dim,\n            encoding_config={\n                \"otype\": \"HashGrid\",\n                \"n_levels\": num_levels,\n                \"n_features_per_level\": 2,\n                \"log2_hashmap_size\": log2_hashmap_size,\n                \"base_resolution\": 16,\n                \"per_level_scale\": per_level_scale,\n            },\n            network_config={", "metadata": {"task_id": "nerfstudio-project_nerfstudio/193", "ground_truth": "                \"otype\": \"FullyFusedMLP\",", "fpath_tuple": ["nerfstudio-project_nerfstudio", "nerfstudio", "fields", "instant_ngp_field.py"], "context_start_lineno": 0, "line_no": 117}}
{"prompt": "from abc import ABC, abstractmethod\nfrom collections import defaultdict\nfrom easydict import EasyDict\nimport copy\n\nfrom ding.utils import import_module, COMMANDER_REGISTRY, LimitedSpaceContainer\n\n\nclass BaseCommander(ABC):\n    r\"\"\"\n    Overview:\n        Base parallel commander abstract class.\n    Interface:\n        get_collector_task\n    \"\"\"\n\n    @classmethod\n    def default_config(cls: type) -> EasyDict:\n        cfg = EasyDict(copy.deepcopy(cls.config))\n        cfg.cfg_type = cls.__name__ + 'Dict'\n        return cfg\n\n    @abstractmethod\n    def get_collector_task(self) -> dict:\n        raise NotImplementedError\n\n    def judge_collector_finish(self, task_id: str, info: dict) -> bool:\n        collector_done = info.get('collector_done', False)\n        if collector_done:\n            return True\n        return False\n\n    def judge_learner_finish(self, task_id: str, info: dict) -> bool:\n        learner_done = info.get('learner_done', False)\n        if learner_done:\n            return True\n        return False\n\n\n@COMMANDER_REGISTRY.register('naive')\nclass NaiveCommander(BaseCommander):\n    r\"\"\"\n    Overview:\n        A naive implementation of parallel commander.\n    Interface:\n        __init__, get_collector_task, get_learner_task, finsh_collector_task, finish_learner_task,\n        notify_fail_collector_task, notify_fail_learner_task, update_learner_info\n    \"\"\"\n    config = dict(\n        collector_task_space=1,\n        learner_task_space=1,\n        eval_interval=60,\n    )\n\n    def __init__(self, cfg: dict) -> None:\n        r\"\"\"\n        Overview:\n            Init the naive commander according to config\n        Arguments:\n            - cfg (:obj:`dict`): The config to init commander. Should include \\\n                \"collector_task_space\" and \"learner_task_space\".\n        \"\"\"\n        self._cfg = cfg\n        self._exp_name = cfg.exp_name\n        commander_cfg = self._cfg.policy.other.commander\n        self._collector_task_space = LimitedSpaceContainer(0, commander_cfg.collector_task_space)\n        self._learner_task_space = LimitedSpaceContainer(0, commander_cfg.learner_task_space)\n\n        self._collector_env_cfg = copy.deepcopy(self._cfg.env)\n        self._collector_env_cfg.pop('collector_episode_num')\n        self._collector_env_cfg.pop('evaluator_episode_num')\n        self._collector_env_cfg.manager.episode_num = self._cfg.env.collector_episode_num\n\n        self._collector_task_count = 0\n        self._learner_task_count = 0\n        self._learner_info = defaultdict(list)\n        self._learner_task_finish_count = 0\n        self._collector_task_finish_count = 0\n\n    def get_collector_task(self) -> dict:\n        r\"\"\"\n        Overview:\n            Get a new collector task when ``collector_task_count`` is smaller than ``collector_task_space``.\n        Return:\n            - task (:obj:`dict`): New collector task.\n        \"\"\"\n        if self._collector_task_space.acquire_space():\n            self._collector_task_count += 1\n            collector_cfg = copy.deepcopy(self._cfg.policy.collect.collector)\n            collector_cfg.collect_setting = {'eps': 0.9}\n            collector_cfg.eval_flag = False\n            collector_cfg.policy = copy.deepcopy(self._cfg.policy)\n            collector_cfg.policy_update_path = 'test.pth'\n            collector_cfg.env = self._collector_env_cfg\n            collector_cfg.exp_name = self._exp_name\n            return {\n                'task_id': 'collector_task_id{}'.format(self._collector_task_count),\n                'buffer_id': 'test',\n                'collector_cfg': collector_cfg,\n            }\n        else:\n            return None\n\n    def get_learner_task(self) -> dict:\n        r\"\"\"\n        Overview:\n            Get the new learner task when task_count is less than task_space\n        Return:\n            - task (:obj:`dict`): the new learner task\n        \"\"\"\n        if self._learner_task_space.acquire_space():\n            self._learner_task_count += 1\n            learner_cfg = copy.deepcopy(self._cfg.policy.learn.learner)\n            learner_cfg.exp_name = self._exp_name\n            return {\n                'task_id': 'learner_task_id{}'.format(self._learner_task_count),\n                'policy_id': 'test.pth',\n                'buffer_id': 'test',\n                'learner_cfg': learner_cfg,\n               'replay_buffer_cfg': copy.deepcopy(self._cfg.policy.other.replay_buffer),\n                'policy': copy.deepcopy(self._cfg.policy),\n            }\n        else:\n            return None\n\n    def finish_collector_task(self, task_id: str, finished_task: dict) -> None:\n        r\"\"\"\n        Overview:\n            finish collector task will add the collector_task_finish_count\n        \"\"\"\n        self._collector_task_space.release_space()\n        self._collector_task_finish_count += 1\n\n    def finish_learner_task(self, task_id: str, finished_task: dict) -> str:\n        r\"\"\"\n        Overview:\n            finish learner task will add the learner_task_finish_count and get the buffer_id of task to close the buffer\n        Return:\n            the finished_task buffer_id\n        \"\"\"\n        self._learner_task_finish_count += 1\n        self._learner_task_space.release_space()\n        return finished_task['buffer_id']\n\n    def notify_fail_collector_task(self, task: dict) -> None:\n        r\"\"\"\n        Overview:\n            naive coordinator will pass when need to notify_fail_collector_task\n        \"\"\"\n        self._collector_task_space.release_space()\n\n    def notify_fail_learner_task(self, task: dict) -> None:\n        r\"\"\"\n        Overview:", "metadata": {"task_id": "opendilab_ACE/42", "ground_truth": "            naive coordinator will pass when need to notify_fail_learner_task", "fpath_tuple": ["opendilab_ACE", "ding", "worker", "coordinator", "base_parallel_commander.py"], "context_start_lineno": 0, "line_no": 154}}
{"prompt": "# coding=utf-8\n# Copyright 2022 HuggingFace Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport gc\nimport random\nimport unittest\n\nimport numpy as np\nimport torch\n\nfrom diffusers import (\n    AutoencoderKL,\n    DPMSolverMultistepScheduler,\n    LMSDiscreteScheduler,\n    PNDMScheduler,\n    StableDiffusionInpaintPipeline,\n    UNet2DConditionModel,\n)\nfrom diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion_inpaint import prepare_mask_and_masked_image\nfrom diffusers.utils import floats_tensor, load_image, load_numpy, nightly, slow, torch_device\nfrom diffusers.utils.testing_utils import require_torch_gpu\nfrom PIL import Image\nfrom transformers import CLIPTextConfig, CLIPTextModel, CLIPTokenizer\n\nfrom...test_pipelines_common import PipelineTesterMixin\n\n\ntorch.backends.cuda.matmul.allow_tf32 = False\n\n\nclass StableDiffusionInpaintPipelineFastTests(PipelineTesterMixin, unittest.TestCase):\n    pipeline_class = StableDiffusionInpaintPipeline\n\n    def get_dummy_components(self):\n        torch.manual_seed(0)\n        unet = UNet2DConditionModel(\n            block_out_channels=(32, 64),\n            layers_per_block=2,\n            sample_size=32,\n            in_channels=9,\n            out_channels=4,\n            down_block_types=(\"DownBlock2D\", \"CrossAttnDownBlock2D\"),\n            up_block_types=(\"CrossAttnUpBlock2D\", \"UpBlock2D\"),\n            cross_attention_dim=32,\n        )\n        scheduler = PNDMScheduler(skip_prk_steps=True)\n        torch.manual_seed(0)\n        vae = AutoencoderKL(\n            block_out_channels=[32, 64],\n            in_channels=3,\n            out_channels=3,\n            down_block_types=[\"DownEncoderBlock2D\", \"DownEncoderBlock2D\"],\n            up_block_types=[\"UpDecoderBlock2D\", \"UpDecoderBlock2D\"],\n            latent_channels=4,\n        )\n        torch.manual_seed(0)\n        text_encoder_config = CLIPTextConfig(\n            bos_token_id=0,\n            eos_token_id=2,\n            hidden_size=32,\n            intermediate_size=37,\n            layer_norm_eps=1e-05,\n            num_attention_heads=4,\n            num_hidden_layers=5,\n            pad_token_id=1,\n            vocab_size=1000,\n        )\n        text_encoder = CLIPTextModel(text_encoder_config)\n        tokenizer = CLIPTokenizer.from_pretrained(\"hf-internal-testing/tiny-random-clip\")\n\n        components = {\n            \"unet\": unet,\n            \"scheduler\": scheduler,\n            \"vae\": vae,\n            \"text_encoder\": text_encoder,\n            \"tokenizer\": tokenizer,\n            \"safety_checker\": None,\n            \"feature_extractor\": None,\n        }", "metadata": {"task_id": "huggingface_diffusers/142", "ground_truth": "        return components", "fpath_tuple": ["huggingface_diffusers", "tests", "pipelines", "stable_diffusion", "test_stable_diffusion_inpaint.py"], "context_start_lineno": 0, "line_no": 91}}
{"prompt": " the device-dependent torch.Generator\n        components = self.get_dummy_components()\n        pipe = StableDiffusionDepth2ImgPipeline(**components)\n        pipe = pipe.to(device)\n        pipe.set_progress_bar_config(disable=None)\n\n        inputs = self.get_dummy_inputs(device)\n\n        image = pipe(**inputs).images\n        image_slice = image[0, -3:, -3:, -1]\n\n        if torch_device == \"mps\":\n            expected_slice = np.array([0.53232, 0.47015, 0.40868, 0.45651, 0.4891, 0.4668, 0.4287, 0.48822, 0.47439])\n        else:\n            expected_slice = np.array([0.6853, 0.3740, 0.4856, 0.7130, 0.7402, 0.5535, 0.4828, 0.6182, 0.5053])\n\n        assert np.abs(image_slice.flatten() - expected_slice).max() < 1e-3\n\n\n@slow\n@require_torch_gpu\nclass StableDiffusionDepth2ImgPipelineSlowTests(unittest.TestCase):\n    def tearDown(self):\n        super().tearDown()\n        gc.collect()\n        torch.cuda.empty_cache()\n\n    def get_inputs(self, device=\"cpu\", dtype=torch.float32, seed=0):\n        generator = torch.Generator(device=device).manual_seed(seed)\n        init_image = load_image(\n            \"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/depth2img/two_cats.png\"\n        )\n        inputs = {\n            \"prompt\": \"two tigers\",\n            \"image\": init_image,\n            \"generator\": generator,\n            \"num_inference_steps\": 3,\n            \"strength\": 0.75,\n            \"guidance_scale\": 7.5,\n            \"output_type\": \"numpy\",\n        }\n        return inputs\n\n    def test_stable_diffusion_depth2img_pipeline_default(self):\n        pipe = StableDiffusionDepth2ImgPipeline.from_pretrained(\n            \"stabilityai/stable-diffusion-2-depth\", safety_checker=None\n        )\n        pipe.to(torch_device)\n        pipe.set_progress_bar_config(disable=None)\n        pipe.enable_attention_slicing()\n\n        inputs = self.get_inputs()\n        image = pipe(**inputs).images\n        image_slice = image[0, 253:256, 253:256, -1].flatten()\n\n        assert image.shape == (1, 480, 640, 3)\n        expected_slice = np.array([0.9057, 0.9365, 0.9258, 0.8937, 0.8555, 0.8541, 0.8260, 0.7747, 0.7421])\n\n        assert np.abs(expected_slice - image_slice).max() < 1e-4\n\n    def test_stable_diffusion_depth2img_pipeline_k_lms(self):\n        pipe = StableDiffusionDepth2ImgPipeline.from_pretrained(\n            \"stabilityai/stable-diffusion-2-depth\", safety_checker=None\n        )\n        pipe.scheduler = LMSDiscreteScheduler.from_config(pipe.scheduler.config)\n        pipe.to(torch_device)\n        pipe.set_progress_bar_config(disable=None)\n        pipe.enable_attention_slicing()\n\n        inputs = self.get_inputs()\n        image = pipe(**inputs).images\n        image_slice = image[0, 253:256, 253:256, -1].flatten()\n\n        assert image.shape == (1, 480, 640, 3)\n        expected_slice = np.array([0.6363, 0.6274, 0.6309, 0.6370, 0.6226, 0.6286, 0.6213, 0.6453, 0.6306])\n\n        assert np.abs(expected_slice - image_slice).max() < 1e-4\n\n    def test_stable_diffusion_depth2img_pipeline_ddim(self):\n        pipe = StableDiffusionDepth2ImgPipeline.from_pretrained(\n            \"stabilityai/stable-diffusion-2-depth\", safety_checker=None\n        )\n        pipe.scheduler = DDIMScheduler.from_config(pipe.scheduler.config)\n        pipe.to(torch_device)\n        pipe.set_progress_bar_config(disable=None)\n        pipe.enable_attention_slicing()\n\n        inputs = self.get_inputs()\n        image = pipe(**inputs).images\n        image_slice = image[0, 253:256, 253:256, -1].flatten()\n\n        assert image.shape == (1, 480, 640, 3)\n        expected_slice = np.array([0.6424, 0.6524, 0.6249, 0.6041, 0.6634, 0.6420, 0.6522, 0.6555, 0.6436])\n\n        assert np.abs(expected_slice - image_slice).max() < 1e-4\n\n    def test_stable_diffusion_depth2img_intermediate_state(self):\n        number_of_steps = 0\n\n        def callback_fn(step: int, timestep: int, latents: torch.FloatTensor) -> None:\n            callback_fn.has_been_called = True\n            nonlocal number_of_steps\n            number_of_steps += 1\n            if step == 1:\n                latents = latents.detach().cpu().numpy()\n                assert latents.shape == (1, 4, 60, 80)\n                latents_slice = latents[0, -3:, -3:, -1]\n                expected_slice = np.array(\n                    [-0.7168, -1.5137, -0.1418, -2.9219, -2.7266, -2.4414, -2.1035, -3.0078, -1.7051]\n                )\n\n                assert np.abs(latents_slice.flatten() - expected_slice).max() < 5e-2\n            elif step == 2:\n                latents = latents.detach().cpu().numpy()\n                assert latents.shape == (1, 4, 60, 80)\n                latents_slice = latents[0, -3:, -3:, -1]\n                expected_slice = np.array(\n                    [-0.7109, -1.5068, -0.1403, -2.9160, -2.7207, -2.4414, -2.1035, -3.0059, -1.7090]\n                )\n\n                assert np.abs(latents_slice.flatten() - expected_slice).max() < 5e-2\n\n        callback_fn.has_been_called = False\n\n        pipe = StableDiffusionDepth2ImgPipeline.from_pretrained(\n            \"stabilityai/stable-diffusion-2-depth\", safety_checker=None, torch_dtype=torch.float16\n        )\n        pipe = pipe.to(torch_device)\n        pipe.set_progress_bar_config(disable=None)\n        pipe.enable_attention_slicing()\n\n        inputs = self.get_inputs(dtype=torch.float16)\n        pipe(**inputs, callback=callback_fn, callback_steps=1)\n        assert callback_fn.has_been_called", "metadata": {"task_id": "huggingface_diffusers/93", "ground_truth": "        assert number_of_steps == 2", "fpath_tuple": ["huggingface_diffusers", "tests", "pipelines", "stable_diffusion_2", "test_stable_diffusion_depth.py"], "context_start_lineno": 374, "line_no": 508}}
{"prompt": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\nimport argparse\nimport time\nimport warnings\n\nimport pytest\nimport torch\nfrom tensordict import TensorDict\nfrom torch import multiprocessing as mp\n\n\nclass TestShared:\n    @staticmethod\n    def remote_process(command_pipe_child, command_pipe_parent, tensordict):\n        command_pipe_parent.close()\n        assert tensordict.is_shared()\n        t0 = time.time()\n        tensordict.zero_()\n        print(f\"zeroing time: {time.time() - t0}\")\n        command_pipe_child.send(\"done\")\n        command_pipe_child.close()\n        del command_pipe_child, command_pipe_parent, tensordict\n\n    @staticmethod\n    def driver_func(subtd, td):\n        assert subtd.is_shared()\n        command_pipe_parent, command_pipe_child = mp.Pipe()\n        proc = mp.Process(\n            target=TestShared.remote_process,\n            args=(command_pipe_child, command_pipe_parent, subtd),\n        )\n        proc.start()\n        command_pipe_child.close()\n        command_pipe_parent.recv()\n        for item in subtd.values():\n            assert (item == 0).all()\n\n        for item in td[0].values():\n            assert (item == 0).all()\n        command_pipe_parent.close()\n        proc.join()\n        del command_pipe_child, command_pipe_parent, proc\n\n    @pytest.mark.parametrize(\"indexing_method\", range(3))\n    def test_shared(self, indexing_method):\n        torch.manual_seed(0)\n        tensordict = TensorDict(\n            source={\n                \"a\": torch.randn(1000, 200),\n                \"b\": torch.randn(1000, 100),\n                \"done\": torch.zeros(1000, 100, dtype=torch.bool).bernoulli_(),\n            },\n            batch_size=[1000],\n        )\n\n        td = tensordict.clone().share_memory_()\n        if indexing_method == 0:\n            subtd = TensorDict(\n                source={key: item[0] for key, item in td.items()},\n                batch_size=[],\n                _is_shared=True,\n            )\n        elif indexing_method == 1:\n            subtd = td.get_sub_tensordict(0)\n        elif indexing_method == 2:\n            subtd = td[0]\n        else:\n            raise NotImplementedError\n\n        assert subtd.is_shared()\n\n        self.driver_func(subtd, td)\n\n\nclass TestStack:\n    @staticmethod\n    def remote_process(command_pipe_child, command_pipe_parent, tensordict):\n        command_pipe_parent.close()\n        assert isinstance(tensordict, TensorDict), f\"td is of type {type(tensordict)}\"\n        assert tensordict.is_shared() or tensordict.is_memmap()\n        new_tensordict = torch.stack(\n            [\n                tensordict[i].contiguous().clone().zero_()\n                for i in range(tensordict.shape[0])\n            ],\n            0,\n        )\n        cmd = command_pipe_child.recv()\n        t0 = time.time()\n        if cmd == \"stack\":\n            tensordict.copy_(new_tensordict)\n        elif cmd == \"serial\":\n            for i, td in enumerate(new_tensordict.tensordicts):\n                tensordict.update_at_(td, i)\n        time_spent = time.time() - t0\n        command_pipe_child.send(time_spent)\n        command_pipe_child.close()\n        del command_pipe_child, command_pipe_parent\n\n    @staticmethod\n    def driver_func(td, stack):\n\n        command_pipe_parent, command_pipe_child = mp.Pipe()\n        proc = mp.Process(\n            target=TestStack.remote_process,\n            args=(command_pipe_child, command_pipe_parent, td),\n        )\n        proc.start()\n        command_pipe_child.close()\n        command_pipe_parent.send(\"stack\" if stack else \"serial\")\n        time_spent = command_pipe_parent.recv()\n        print(f\"stack {stack}: time={time_spent}\")\n        for item in td.values():\n            assert (item == 0).all()\n        proc.join()\n        command_pipe_parent.close()\n        return time_spent\n\n    @pytest.mark.parametrize(\"shared\", [\"shared\", \"memmap\"])\n    def test_shared(self, shared):\n        print(f\"test_shared: shared={shared}\")\n        torch.manual_seed(0)\n        tensordict = TensorDict(\n            source={\n                \"a\": torch.randn(100, 2),", "metadata": {"task_id": "pytorch_rl/154", "ground_truth": "                \"b\": torch.randn(100, 1),", "fpath_tuple": ["pytorch_rl", "test", "test_shared.py"], "context_start_lineno": 0, "line_no": 128}}
{"prompt": ", *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.multi_device = True\n\n    @staticmethod\n    def _add_device_dim_to_data_loader(data_loader: DataLoader) -> DataLoader:\n        def _reshape_batch(batch):\n            n_devices = jax.local_device_count()\n            if batch.shape[0] % n_devices!= 0:\n                raise ValueError(\n                    f\"The size of all batches must be a multiple of {n_devices}, that is the number of \"\n                    f\"available devices. However, a batch with shape {batch.shape[0]} was found. \"\n                    f\"Please set an appropriate batch size.\"\n                )\n            return batch.reshape((n_devices, -1) + batch.shape[1:])\n\n        class DataLoaderWrapper:\n            def __init__(self, data_loader: DataLoader):\n                self._data_loader = data_loader\n\n            def __iter__(self):\n                data_loader = map(\n                    lambda batch: tree_map(_reshape_batch, batch), self._data_loader\n                )\n                data_loader = jax_utils.prefetch_to_device(data_loader, 2)\n                yield from data_loader\n\n        return (\n            DataLoaderWrapper(data_loader) if data_loader is not None else data_loader\n        )\n\n    @staticmethod\n    def _add_device_dim_to_outputs_loader(\n        outputs_loader: TargetsLoader,\n    ) -> TargetsLoader:\n        def _reshape_batch(batch):\n            n_devices = jax.local_device_count()\n            if batch.shape[0] % n_devices!= 0:\n                raise ValueError(\n                    f\"The size of all output batches must be a multiple of {n_devices}, that is the number of \"\n                    f\"available devices. However, a batch of outputs with shape {batch.shape[0]} was found. \"\n                    f\"Please set an appropriate batch size.\"\n                )\n            return batch.reshape((n_devices, -1) + batch.shape[1:])\n\n        class TargetsLoaderWrapper:\n            def __init__(self, outputs_loader: TargetsLoader):\n                self._outputs_loader = outputs_loader\n\n            def __iter__(self):\n                outputs_loader = map(\n                    lambda batch: tree_map(_reshape_batch, batch), self._outputs_loader\n                )\n                outputs_loader = jax_utils.prefetch_to_device(outputs_loader, 2)\n                yield from outputs_loader\n\n        return (\n            TargetsLoaderWrapper(outputs_loader)\n            if outputs_loader is not None\n            else outputs_loader\n        )\n\n    @staticmethod\n    def sync_mutable(state: CalibState) -> CalibState:\n        return (\n            state.replace(mutable=MultiDeviceMixin.all_reduce_mean(state.mutable))\n            if state.mutable[\"output_calibrator\"] is not None\n            else state\n        )\n\n    @staticmethod\n    def sync_gradients_and_loss(\n        grads: jnp.ndarray, loss: jnp.ndarray\n    ) -> Tuple[jnp.ndarray, jnp.ndarray]:\n        grad = lax.pmean(grads, axis_name=\"batch\")\n        loss = lax.pmean(loss, axis_name=\"batch\")\n        return grad, loss\n\n    def save_checkpoint(\n        self,\n        state: CalibState,\n        save_checkpoint_dir: Path,\n        keep: int = 1,\n        force_save: bool = False,\n        prefix: str = \"checkpoint_\",\n    ) -> None:\n        state = self.sync_mutable(state)\n        state = jax.device_get(tree_map(lambda x: x[0], state))\n        return super(MultiDeviceMixin, self).save_checkpoint(\n            state, save_checkpoint_dir, keep, force_save, prefix\n        )\n\n    def on_train_start(\n        self,\n        state: CalibState,\n        data_loaders: List[DataLoader],\n        outputs_loaders: List[TargetsLoader],\n        rng: PRNGKeyArray,\n    ) -> Tuple[CalibState, List[DataLoader], List[TargetsLoader], PRNGKeyArray]:\n        state, data_loaders, outputs_loaders, rng = super(\n            MultiDeviceMixin, self\n        ).on_train_start(state, data_loaders, outputs_loaders, rng)\n        state = jax_utils.replicate(state)\n        data_loaders = [\n            self._add_device_dim_to_data_loader(dl) if dl is not None else dl\n            for dl in data_loaders\n        ]\n        outputs_loaders = [\n            self._add_device_dim_to_outputs_loader(ol) if ol is not None else ol\n            for ol in outputs_loaders\n        ]\n        model_key = random.split(rng, jax.local_device_count())\n        return state, data_loaders, outputs_loaders, model_key\n\n    def on_train_end(self, state: CalibState) -> CalibState:\n        state = super(MultiDeviceMixin, self).on_train_end(state)\n        return jax.device_get(tree_map(lambda x: x[0], state))\n\n    @partial(jax.pmap, axis_name=\"batch\", static_broadcasted_argnums=(0, 4, 6))\n    def training_step(\n        self,\n        state: CalibState,\n        batch: Batch,\n        outputs: Array,\n        fun: Callable,\n        rng: PRNGKeyArray,\n        n_data: int,\n    ) -> Tuple[CalibState, Dict[str, Any]]:\n        return super().training_step(state, batch, outputs, fun, rng, n_data)\n\n    def training_step_end(\n        self,\n        current_epoch: int,\n        state: CalibState,\n        aux: Dict[str, Any],\n        batch: Batch,\n        metrics: Optional[Tuple[Callable[[jnp.ndarray, Array], float],...]],\n    ) -> Dict[str, jnp.ndarray]:\n        training_losses_and_metrics = super(MultiDeviceMixin, self).training_step_end(\n            current_epoch, state, aux, batch, metrics\n        )\n        return tree_map(lambda x: x.mean(), training_losses_and_metrics)\n\n    def on_val_start(self, state: CalibState) -> CalibState:\n        state = super(MultiDeviceMixin, self).on_val_start(state)\n        if state.mutable[\"output_calibrator\"] is not None:\n            state = self.sync_mutable(state)\n        return state\n\n    @partial(jax.pmap, axis_name=\"batch\", static_broadcasted_argnums=(0, 4, 6))\n    def val_loss_step(\n        self,\n        state: CalibState,\n        batch: Batch,\n        outputs: Array,\n        fun: Callable,\n        rng: PRNGKeyArray,\n        n_data: int,\n    ) -> Dict[str, jnp.ndarray]:\n        val_losses = super().val_loss_step(state, batch, outputs, fun, rng, n_data)\n        return lax.pmean(val_losses, axis_name=\"batch\")\n\n    def val_metrics_step(\n        self,\n        aux: Dict[str, jnp.ndarray],\n        batch: Batch,\n        metrics: Optional[", "metadata": {"task_id": "awslabs_fortuna/66", "ground_truth": "            Tuple[Callable[[jnp.ndarray, jnp.ndarray, Array], Array], ...]", "fpath_tuple": ["awslabs_fortuna", "fortuna", "calibration", "calibrator.py"], "context_start_lineno": 492, "line_no": 659}}
{"prompt": "import copy\nfrom collections import defaultdict\nimport numpy as np\nimport pytest\nfrom easydict import EasyDict\nimport os\nimport pickle\nimport time\n\nfrom ding.worker.replay_buffer import AdvancedReplayBuffer\nfrom ding.utils import deep_merge_dicts\nfrom ding.worker.replay_buffer.tests.conftest import generate_data, generate_data_list\n\ndemo_data_path = \"test_demo_data\"\n\n\n@pytest.fixture(scope=\"function\")\ndef setup_demo_buffer_factory():\n    demo_data = {'data': generate_data_list(10)}\n    with open(demo_data_path, \"wb\") as f:\n        pickle.dump(demo_data, f)\n\n    def generator():\n        while True:\n            cfg = copy.deepcopy(AdvancedReplayBuffer.default_config())\n            cfg.replay_buffer_size = 64\n            cfg.max_use = 2\n            cfg.max_staleness = 1000\n            cfg.alpha = 0.6\n            cfg.beta = 0.6\n            cfg.enable_track_used_data = False\n            yield AdvancedReplayBuffer(instance_name=\"demo\", cfg=cfg)\n\n    return generator()\n\n\n@pytest.mark.unittest\nclass TestAdvancedBuffer:\n\n    def test_push(self):\n        buffer_cfg = deep_merge_dicts(AdvancedReplayBuffer.default_config(), EasyDict(dict(replay_buffer_size=64)))\n        advanced_buffer = AdvancedReplayBuffer(buffer_cfg, tb_logger=None, instance_name='test')\n        start_pointer = advanced_buffer._tail\n        start_vaildlen = advanced_buffer.count()\n        start_data_id = advanced_buffer._next_unique_id\n        valid_count = 0\n        for _ in range(100):\n            if advanced_buffer._data[advanced_buffer._tail] is None:\n                valid_count += 1\n            advanced_buffer.push(generate_data(), 0)\n        assert (advanced_buffer.replay_buffer_size == 64)\n        assert (advanced_buffer.count() == 64 == start_vaildlen + valid_count)\n        assert (advanced_buffer.push_count == start_vaildlen + 100)\n        assert (advanced_buffer._tail == (start_pointer + 100) % advanced_buffer.replay_buffer_size)\n        assert (advanced_buffer._next_unique_id == start_data_id + 100)\n        # invalid item append test\n        advanced_buffer.push([], 0)\n        assert (advanced_buffer.count() == 64 == start_vaildlen + valid_count)\n        assert (advanced_buffer.push_count == start_vaildlen + 100)\n        assert (advanced_buffer._tail == (start_pointer + 100) % advanced_buffer.replay_buffer_size)\n        assert (advanced_buffer._next_unique_id == start_data_id + 100)\n\n        buffer_cfg = deep_merge_dicts(AdvancedReplayBuffer.default_config(), EasyDict(dict(replay_buffer_size=64)))\n        advanced_buffer = AdvancedReplayBuffer(buffer_cfg, tb_logger=None, instance_name='test')\n        start_pointer = advanced_buffer._tail\n        start_data_id = advanced_buffer._next_unique_id\n        replay_buffer_size = advanced_buffer.replay_buffer_size\n        extend_num = int(0.6 * replay_buffer_size)\n        for i in range(1, 4):\n            data = generate_data_list(extend_num)\n            advanced_buffer.push(data, 0)\n            assert advanced_buffer._tail == (start_pointer + extend_num * i) % replay_buffer_size\n            assert advanced_buffer._next_unique_id == start_data_id + extend_num * i\n            assert advanced_buffer._valid_count == min(start_data_id + extend_num * i, replay_buffer_size)\n\n    def test_update(self):\n        buffer_cfg = deep_merge_dicts(AdvancedReplayBuffer.default_config(), EasyDict(dict(replay_buffer_size=64)))\n        advanced_buffer = AdvancedReplayBuffer(buffer_cfg, tb_logger=None, instance_name='test')\n        for _ in range(64):\n            advanced_buffer.push(generate_data(), 0)\n            assert advanced_buffer.count() == sum([d is not None for d in advanced_buffer._data])\n        selected_idx = [1, 4, 8, 30, 63]", "metadata": {"task_id": "opendilab_ACE/183", "ground_truth": "        info = {'priority': [], 'replay_unique_id': [], 'replay_buffer_idx': []}", "fpath_tuple": ["opendilab_ACE", "ding", "worker", "replay_buffer", "tests", "test_advanced_buffer.py"], "context_start_lineno": 0, "line_no": 82}}
{"prompt": "import logging\nfrom typing import Optional\n\nfrom jax._src.prng import PRNGKeyArray\n\nfrom fortuna.data.loader import DataLoader\nfrom fortuna.prob_model.fit_config import FitConfig\nfrom fortuna.prob_model.joint.base import Joint\nfrom fortuna.prob_model.joint.state import JointState\nfrom fortuna.prob_model.posterior.base import Posterior\nfrom fortuna.prob_model.posterior.map import MAP_NAME\nfrom fortuna.prob_model.posterior.map.map_approximator import \\\n    MAPPosteriorApproximator\nfrom fortuna.prob_model.posterior.map.map_state import MAPState\nfrom fortuna.prob_model.posterior.map.map_trainer import (\n    JittedMAPTrainer, MAPTrainer, MultiDeviceMAPTrainer)\nfrom fortuna.prob_model.posterior.posterior_state_repository import \\\n    PosteriorStateRepository\nfrom fortuna.typing import Status\nfrom fortuna.utils.device import select_trainer_given_devices\n\nlogger = logging.getLogger(__name__)\n\n\nclass MAPPosterior(Posterior):\n    def __init__(\n        self, joint: Joint, posterior_approximator: MAPPosteriorApproximator,\n    ):\n        \"\"\"\n        Maximum-a-Posteriori (MAP) approximate posterior class.\n\n        Parameters\n        ----------\n        joint: Joint\n            A Joint distribution object.\n        posterior_approximator: MAPPosteriorApproximator\n            A MAP posterior approximator.\n        \"\"\"\n        super().__init__(joint=joint, posterior_approximator=posterior_approximator)\n\n    def __str__(self):\n        return MAP_NAME\n\n    def fit(\n        self,\n        train_data_loader: DataLoader,\n        val_data_loader: Optional[DataLoader] = None,\n        fit_config: FitConfig = FitConfig(),\n        **kwargs\n    ) -> Status:\n        if (\n            fit_config.checkpointer.dump_state is True\n            and not fit_config.checkpointer.save_checkpoint_dir\n        ):\n            raise ValueError(\n                \"`save_checkpoint_dir` must be passed when `dump_state` is set to True.\"\n            )\n        (init_prob_model_state, n_train_data, n_val_data,) = self._init(\n            train_data_loader, val_data_loader\n        )\n\n        trainer_cls = select_trainer_given_devices(\n            devices=fit_config.processor.devices,\n            BaseTrainer=MAPTrainer,\n            JittedTrainer=JittedMAPTrainer,\n            MultiDeviceTrainer=MultiDeviceMAPTrainer,\n            disable_jit=fit_config.processor.disable_jit,\n        )\n\n        trainer = trainer_cls(\n            predict_fn=self.joint.likelihood.prob_output_layer.predict,\n            save_checkpoint_dir=fit_config.checkpointer.save_checkpoint_dir,", "metadata": {"task_id": "awslabs_fortuna/192", "ground_truth": "            save_every_n_steps=fit_config.checkpointer.save_every_n_steps,", "fpath_tuple": ["awslabs_fortuna", "fortuna", "prob_model", "posterior", "map", "map_posterior.py"], "context_start_lineno": 0, "line_no": 72}}
{"prompt": "from typing import Union\n\nimport torch\n\nfrom diffusers import (\n    AutoencoderKL,\n    DDIMScheduler,\n    DiffusionPipeline,\n    LMSDiscreteScheduler,\n    PNDMScheduler,\n    UNet2DConditionModel,\n)\nfrom PIL import Image\nfrom torchvision import transforms as tfms\nfrom tqdm.auto import tqdm\nfrom transformers import CLIPTextModel, CLIPTokenizer\n\n\nclass MagicMixPipeline(DiffusionPipeline):\n    def __init__(\n        self,\n        vae: AutoencoderKL,\n        text_encoder: CLIPTextModel,\n        tokenizer: CLIPTokenizer,\n        unet: UNet2DConditionModel,\n        scheduler: Union[PNDMScheduler, LMSDiscreteScheduler, DDIMScheduler],\n    ):\n        super().__init__()\n\n        self.register_modules(vae=vae, text_encoder=text_encoder, tokenizer=tokenizer, unet=unet, scheduler=scheduler)\n\n    # convert PIL image to latents\n    def encode(self, img):\n        with torch.no_grad():\n            latent = self.vae.encode(tfms.ToTensor()(img).unsqueeze(0).to(self.device) * 2 - 1)\n            latent = 0.18215 * latent.latent_dist.sample()\n        return latent\n\n    # convert latents to PIL image\n    def decode(self, latent):\n        latent = (1 / 0.18215) * latent\n        with torch.no_grad():\n            img = self.vae.decode(latent).sample\n        img = (img / 2 + 0.5).clamp(0, 1)\n        img = img.detach().cpu().permute(0, 2, 3, 1).numpy()\n        img = (img * 255).round().astype(\"uint8\")\n        return Image.fromarray(img[0])\n\n    # convert prompt into text embeddings, also unconditional embeddings\n    def prep_text(self, prompt):\n        text_input = self.tokenizer(\n            prompt,\n            padding=\"max_length\",\n            max_length=self.tokenizer.model_max_length,\n            truncation=True,\n            return_tensors=\"pt\",\n        )\n\n        text_embedding = self.text_encoder(text_input.input_ids.to(self.device))[0]\n\n        uncond_input = self.tokenizer(\n            \"\",\n            padding=\"max_length\",", "metadata": {"task_id": "huggingface_diffusers/163", "ground_truth": "            max_length=self.tokenizer.model_max_length,", "fpath_tuple": ["huggingface_diffusers", "examples", "community", "magic_mix.py"], "context_start_lineno": 0, "line_no": 63}}
{"prompt": "# Copyright 2022 The Nerfstudio Team. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\nDataset.\n\"\"\"\nfrom __future__ import annotations\n\nfrom copy import deepcopy\nfrom pathlib import Path", "metadata": {"task_id": "nerfstudio-project_nerfstudio/6", "ground_truth": "from typing import Dict, List", "fpath_tuple": ["nerfstudio-project_nerfstudio", "nerfstudio", "data", "datasets", "base_dataset.py"], "context_start_lineno": 0, "line_no": 21}}
{"prompt": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import annotations\n\n\"\"\"High-level wrappers for stochastic process hyperparameter optimizers.\"\"\"\n\n# TODO: Add optimizers that are frequently used in the literature.\n\nimport functools\nimport time\nfrom typing import Optional, Protocol\n\nfrom absl import logging\nimport attr\nimport chex\nimport jax\nfrom jax import numpy as jnp\nfrom jax import random\nimport jaxopt\nimport optax\nfrom tensorflow_probability.substrates import jax as tfp\nimport tree\nfrom vizier._src.jax import stochastic_process_model as sp\n\ntfb = tfp.bijectors\n\nPRNGKey = chex.PRNGKey\nParams = chex.ArrayTree\nOptState = chex.ArrayTree\nArray = chex.Array\n\n\nclass Setup(Protocol):\n  \"\"\"Set up the model parameters given RNG key.\"\"\"\n\n  def __call__(self, rng: PRNGKey):\n    \"\"\"Set up the model parameters given RNG key.\"\"\"\n    pass\n\n\nclass LossFunction(Protocol):\n  \"\"\"Evaluates model params and returns (loss, dict of auxiliary metrics).\"\"\"\n\n  def __call__(self, params: Params) -> tuple[Array, dict[str, Array]]:\n    \"\"\"Evaluates model params and returns (loss, dict of auxiliary metrics).\"\"\"\n    pass\n\n\nclass Optimizer(Protocol):\n  \"\"\"Optimizes the LossFunction.\n\n  Example:\n\n  ```python\n  setup: Setup = lambda rng: jax.random.uniform(rng, minval=-5, maxval=5)\n\n  def loss_fn(xs):  # satisfies `LossFunction` Protocol\n    xs = jax.nn.sigmoid(xs)\n    return jnp.cos(xs * 5 * 2 * jnp.pi) * (2 - 5 * (xs - 0.5)**2), dict()\n\n  optimize = optimizers.OptaxTrainWithRandomRestarts(\n      optax.adam(5e-3), epochs=500, verbose=True, random_restarts=50)\n  optimal_params, metrics = optimize(setup, loss_fn, jax.random.PRNGKey(0))\n  ```\n  \"\"\"\n\n  def __call__(\n      self,\n      setup: Setup,\n      loss_fn: LossFunction,\n      rng: PRNGKey,\n      *,\n      constraints: Optional[sp.Constraint] = None,\n  ) -> tuple[Params, dict[str, Array]]:\n    \"\"\"Optimizes a LossFunction expecting Params as input.\n\n    When constraint bijectors are applied, note that the returned parameters are\n    in the constrained space (the parameter domain), not the unconstrained space\n    over which the optimization takes place.\n\n    If the Optimizer uses lower and upper bounds, then it is responsible for\n    converting `None` bounds to `+inf` or `-inf` as necessary.\n\n    Args:\n      setup: Generates initial points.\n      loss_fn: Evaluates a point.\n      rng: JAX PRNGKey.\n      constraints: Parameter constraints.\n\n    Returns:\n      Tuple containing optimal input in the constrained space and optimization\n      metrics.\n    \"\"\"\n    pass\n\n\n@attr.define\nclass OptaxTrainWithRandomRestarts(Optimizer):\n  \"\"\"Wraps an Optax optimizer.\n\n  Attributes:\n    optimizer: Optax optimizer such as `optax.adam(1e-2)`.\n    epochs: Number of train epochs.\n    verbose: If >=1, logs the train progress. If >=2, logs the gradients.\n    random_restarts: Must be a non-negative number. If positive, optimizes from\n      multiple random initializations and returns the best.\n    best_n: Return this many optimal points. Must be less than random_restarts.\n      Set it to None if you want to avoid getting extra dimensions in the\n      result.\n  \"\"\"\n\n  optimizer: optax.GradientTransformation = attr.field()\n  epochs: int = attr.field(kw_only=True)\n  verbose: int = attr.field(kw_only=True, default=0, converter=int)\n  random_restarts: int = attr.field(kw_only=True, default=0)\n  best_n: Optional[int] = attr.field(kw_only=True, default=None)\n\n  def __attrs_post_init__(self):\n    if self.random_restarts < (self.best_n or 0):\n      raise ValueError(\n          f'Cannot generate {self.best_n} results from'\n          f' {self.random_restarts} restarts'\n      )\n\n  def __call__(\n      self,\n      setup: Setup,\n      loss_fn: LossFunction,\n      rng: PRNGKey,\n      *,\n      constraints: Optional[sp.Constraint] = None,\n  ) -> tuple[Params, dict[str, Array]]:\n    if constraints is None or constraints.bijector is None:\n      bijector = None\n      unconstrained_loss_fn = loss_fn\n    else:\n      bijector = constraints.bijector\n      unconstrained_loss_fn = lambda x: loss_fn(bijector(x))\n\n    grad_fn = jax.value_and_grad(unconstrained_loss_fn, has_aux=True)\n\n    def _setup_all(rng: chex.PRNGKey) -> tuple[Params, OptState]:\n      \"\"\"Sets up both model params and optimizer state.\"\"\"\n      params = setup(rng)\n      if bijector is not None:\n        params = bijector.inverse(params)\n      opt_state = self.optimizer.init(params)\n      return params, opt_state\n\n    def _train_step(\n        params: Params, opt_state: OptState\n    ) -> tuple[Params, OptState, dict[str, Array]]:\n      \"\"\"One train step.\"\"\"\n      (loss, metrics), grads = grad_fn(params)", "metadata": {"task_id": "google_vizier/128", "ground_truth": "      logging.log_if(logging.INFO, 'gradients: %s', self.verbose >= 2, grads)", "fpath_tuple": ["google_vizier", "vizier", "_src", "jax", "optimizers", "optimizers.py"], "context_start_lineno": 0, "line_no": 166}}
{"prompt": "ordict)\n        >>> assert \"my_info_key\" in tensordict.keys()\n\n    \"\"\"\n\n    def __init__(\n        self,\n        keys: List[str] = None,\n        spec: Union[Sequence[TensorSpec], Dict[str, TensorSpec]] = None,\n    ):\n        if keys is None:\n            keys = []\n        self.keys = keys\n\n        if isinstance(spec, Sequence):\n            if len(spec)!= len(self.keys):\n                raise ValueError(\n                    \"If specifying specs for info keys with a sequence, the \"\n                    \"length of the sequence must match the number of keys\"\n                )\n            self._info_spec = dict(zip(self.keys, spec))\n        else:\n            if spec is None:\n                spec = {}\n\n            self._info_spec = {\n                key: spec.get(key, UnboundedContinuousTensorSpec()) for key in self.keys\n            }\n\n    def __call__(\n        self, info_dict: Dict[str, Any], tensordict: TensorDictBase\n    ) -> TensorDictBase:\n        if not isinstance(info_dict, dict) and len(self.keys):\n            warnings.warn(\n                f\"Found an info_dict of type {type(info_dict)} \"\n                f\"but expected type or subtype `dict`.\"\n            )\n        for key in self.keys:\n            if key in info_dict:\n                tensordict[key] = info_dict[key]\n        return tensordict\n\n    @property\n    def info_spec(self) -> Dict[str, TensorSpec]:\n        return self._info_spec\n\n\nclass GymLikeEnv(_EnvWrapper):\n    \"\"\"A gym-like env is an environment.\n\n    Its behaviour is similar to gym environments in what common methods (specifically reset and step) are expected to do.\n\n    A :obj:`GymLikeEnv` has a :obj:`.step()` method with the following signature:\n\n        ``env.step(action: np.ndarray) -> Tuple[Union[np.ndarray, dict], double, bool, *info]``\n\n    where the outputs are the observation, reward and done state respectively.\n    In this implementation, the info output is discarded (but specific keys can be read\n    by updating info_dict_reader, see :obj:`set_info_dict_reader` class method).\n\n    By default, the first output is written at the \"observation\" key-value pair in the output tensordict, unless\n    the first output is a dictionary. In that case, each observation output will be put at the corresponding\n    :obj:`f\"{key}\"` location for each :obj:`f\"{key}\"` of the dictionary.\n\n    It is also expected that env.reset() returns an observation similar to the one observed after a step is completed.\n    \"\"\"\n\n    _info_dict_reader: BaseInfoDictReader\n\n    @classmethod\n    def __new__(cls, *args, **kwargs):\n        cls._info_dict_reader = None\n        return super().__new__(cls, *args, _batch_locked=True, **kwargs)\n\n    def read_action(self, action):\n        \"\"\"Reads the action obtained from the input TensorDict and transforms it in the format expected by the contained environment.\n\n        Args:\n            action (Tensor or TensorDict): an action to be taken in the environment\n\n        Returns: an action in a format compatible with the contained environment.\n\n        \"\"\"\n        return self.action_spec.to_numpy(action, safe=False)\n\n    def read_done(self, done):\n        \"\"\"Done state reader.\n\n        Reads a done state and returns a tuple containing:\n        - a done state to be set in the environment\n        - a boolean value indicating whether the frame_skip loop should be broken\n\n        Args:\n            done (np.ndarray, boolean or other format): done state obtained from the environment\n\n        \"\"\"\n        return done, done\n\n    def read_reward(self, total_reward, step_reward):\n        \"\"\"Reads a reward and the total reward so far (in the frame skip loop) and returns a sum of the two.\n\n        Args:\n            total_reward (torch.Tensor or TensorDict): total reward so far in the step\n            step_reward (reward in the format provided by the inner env): reward of this particular step\n\n        \"\"\"\n        return total_reward + self.reward_spec.encode(step_reward)\n\n    def read_obs(\n        self, observations: Union[Dict[str, Any], torch.Tensor, np.ndarray]\n    ) -> Dict[str, Any]:\n        \"\"\"Reads an observation from the environment and returns an observation compatible with the output TensorDict.\n\n        Args:\n            observations (observation under a format dictated by the inner env): observation to be read.\n\n        \"\"\"\n        if isinstance(observations, dict):\n            observations = {key: value for key, value in observations.items()}\n        if not isinstance(observations, (TensorDict, dict)):\n            (key,) = itertools.islice(self.observation_spec.keys(), 1)\n            observations = {key: observations}\n        observations = self.observation_spec.encode(observations)\n        return observations\n\n    def _step(self, tensordict: TensorDictBase) -> TensorDictBase:\n        action = tensordict.get(\"action\")\n        action_np = self.read_action(action)\n\n        reward = self.reward_spec.zero()\n        for _ in range(self.wrapper_frame_skip):\n            obs, _reward, done, *info = self._output_transform(\n                self._env.step(action_np)\n            )\n            if isinstance(obs, list) and len(obs) == 1:\n                # Until gym 0.25.2 we had rendered frames returned in lists of length 1\n                obs = obs[0]\n            if len(info) == 2:\n                # gym 0.26\n                truncation, info = info\n            elif len(info) == 1:\n                info = info[0]\n            elif len(info) == 0:\n                info = None\n            else:\n                raise ValueError(\n                    \"the environment output is expected to be either\"\n                    \"obs, reward, done, truncation, info (gym >= 0.26) or \"\n                    f\"obs, reward, done, info. Got info with types = ({[type(x) for x in info]})\"\n                )\n\n            if _reward is None:\n                _reward = self.reward_spec.zero()\n\n            reward = self.read_reward(reward, _reward)\n\n            if isinstance(done, bool) or (\n                isinstance(done, np.ndarray) and not len(done)\n            ):\n                done = torch.tensor([done], device=self.device)\n\n            done, do_break = self.read_done(done)\n            if do_break:\n                break\n\n        obs_dict = self.read_obs(obs)\n\n        if reward is None:\n            reward = np.nan\n        reward = self._to_tensor(reward, dtype=self.reward_spec.dtype)\n        done = self._to_tensor(done, dtype=torch.bool)\n\n        tensordict_out = TensorDict(\n            obs_dict, batch_size=tensordict.batch_size, device=self.device\n        )\n\n        tensordict_out.set(\"reward\", reward)\n        tensordict_out.set(\"done\", done)\n        if self.info_dict_reader is not None and info is not None:\n            self.info_dict_reader(info, tensordict_out)\n\n        return tensordict_out\n\n    def _reset(", "metadata": {"task_id": "pytorch_rl/101", "ground_truth": "        self, tensordict: Optional[TensorDictBase] = None, **kwargs", "fpath_tuple": ["pytorch_rl", "torchrl", "envs", "gym_like.py"], "context_start_lineno": 50, "line_no": 234}}
{"prompt": "# Copyright 2020 The HuggingFace Datasets Authors and the current dataset script contributor.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Recall metric.\"\"\"\n\nimport datasets\nfrom sklearn.metrics import recall_score\n\nimport evaluate\n\n\n_DESCRIPTION = \"\"\"\nRecall is the fraction of the positive examples that were correctly labeled by the model as positive. It can be computed with the equation:\nRecall = TP / (TP + FN)\nWhere TP is the true positives and FN is the false negatives.\n\"\"\"\n\n\n_KWARGS_DESCRIPTION = \"\"\"\nArgs:\n- **predictions** (`list` of `int`): The predicted labels.\n- **references** (`list` of `int`): The ground truth labels.\n- **labels** (`list` of `int`): The set of labels to include when `average` is not set to `binary`, and their order when average is `None`. Labels present in the data can be excluded in this input, for example to calculate a multiclass average ignoring a majority negative class, while labels not present in the data will result in 0 components in a macro average. For multilabel targets, labels are column indices. By default, all labels in y_true and y_pred are used in sorted order. Defaults to None.\n- **pos_label** (`int`): The class label to use as the 'positive class' when calculating the recall. Defaults to `1`.\n- **average** (`string`): This parameter is required for multiclass/multilabel targets. If None, the scores for each class are returned. Otherwise, this determines the type of averaging performed on the data. Defaults to `'binary'`.\n    - `'binary'`: Only report results for the class specified by `pos_label`. This is applicable only if the target labels and predictions are binary.\n    - `'micro'`: Calculate metrics globally by counting the total true positives, false negatives, and false positives.", "metadata": {"task_id": "huggingface_evaluate/147", "ground_truth": "    - `'macro'`: Calculate metrics for each label, and find their unweighted mean. This does not take label imbalance into account.", "fpath_tuple": ["huggingface_evaluate", "metrics", "recall", "recall.py"], "context_start_lineno": 0, "line_no": 37}}
{"prompt": "# Copyright 2022 The Nerfstudio Team. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\nField for compound nerf model, adds scene contraction and image embeddings to instant ngp\n\"\"\"\n\n\nfrom typing import Dict, Optional, Tuple\n\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.nn.parameter import Parameter\nfrom torchtyping import TensorType\n\nfrom nerfstudio.cameras.rays import RaySamples\nfrom nerfstudio.data.scene_box import SceneBox\nfrom nerfstudio.field_components.activations import trunc_exp\nfrom nerfstudio.field_components.embedding import Embedding\nfrom nerfstudio.field_components.encodings import Encoding, HashEncoding, SHEncoding\nfrom nerfstudio.field_components.field_heads import (\n    DensityFieldHead,\n    FieldHead,\n    FieldHeadNames,\n    PredNormalsFieldHead,\n    RGBFieldHead,\n    SemanticFieldHead,\n    TransientDensityFieldHead,\n    TransientRGBFieldHead,\n    UncertaintyFieldHead,\n)\nfrom nerfstudio.field_components.mlp import MLP\nfrom nerfstudio.field_components.spatial_distortions import (\n    SceneContraction,\n    SpatialDistortion,\n)\nfrom nerfstudio.fields.base_field import Field\n\ntry:\n    import tinycudann as tcnn\nexcept ImportError:\n    # tinycudann module doesn't exist\n    pass\n\n\ndef get_normalized_directions(directions: TensorType[\"bs\":..., 3]):\n    \"\"\"SH encoding must be in the range [0, 1]\n\n    Args:\n        directions: batch of directions\n    \"\"\"\n    return (directions + 1.0) / 2.0\n\n\nclass TCNNNerfactoField(Field):\n    \"\"\"Compound Field that uses TCNN\n\n    Args:\n        aabb: parameters of scene aabb bounds\n        num_images: number of images in the dataset\n        num_layers: number of hidden layers\n        hidden_dim: dimension of hidden layers\n        geo_feat_dim: output geo feat dimensions\n        num_levels: number of levels of the hashmap for the base mlp\n        max_res: maximum resolution of the hashmap for the base mlp\n        log2_hashmap_size: size of the hashmap for the base mlp\n        num_layers_color: number of hidden layers for color network\n        num_layers_transient: number of hidden layers for transient network\n        hidden_dim_color: dimension of hidden layers for color network\n        hidden_dim_transient: dimension of hidden layers for transient network\n        appearance_embedding_dim: dimension of appearance embedding\n        transient_embedding_dim: dimension of transient embedding\n        use_transient_embedding: whether to use transient embedding\n        use_semantics: whether to use semantic segmentation\n        num_semantic_classes: number of semantic classes\n        use_pred_normals: whether to use predicted normals\n        use_average_appearance_embedding: whether to use average appearance embedding or zeros for inference\n        spatial_distortion: spatial distortion to apply to the scene\n    \"\"\"\n\n    def __init__(\n        self,\n        aabb,\n        num_images: int,\n        num_layers: int = 2,\n        hidden_dim: int = 64,\n        geo_feat_dim: int = 15,\n        num_levels: int = 16,\n        max_res: int = 2048,\n        log2_hashmap_size: int = 19,\n        num_layers_color: int = 3,\n        num_layers_transient: int = 2,\n        hidden_dim_color: int = 64,\n        hidden_dim_transient: int = 64,\n        appearance_embedding_dim: int = 32,\n        transient_embedding_dim: int = 16,\n        use_transient_embedding: bool = False,\n        use_semantics: bool = False,\n        num_semantic_classes: int = 100,\n        use_pred_normals: bool = False,\n        use_average_appearance_embedding: bool = False,\n        spatial_distortion: Optional[SpatialDistortion] = None,\n    ) -> None:\n        super().__init__()\n\n        self.aabb = Parameter(aabb, requires_grad=False)\n        self.geo_feat_dim = geo_feat_dim\n\n        self.spatial_distortion = spatial_distortion\n        self.num_images = num_images\n        self.appearance_embedding_dim = appearance_embedding_dim\n        self.embedding_appearance = Embedding(self.num_images, self.appearance_embedding_dim)\n        self.use_average_appearance_embedding = use_average_appearance_embedding\n        self.use_transient_embedding = use_transient_embedding\n        self.use_semantics = use_semantics\n        self.use_pred_normals = use_pred_normals\n\n        base_res = 16\n        features_per_level = 2\n        growth_factor = np.exp((np.log(max_res) - np.log(base_res)) / (num_levels - 1))\n\n        self.direction_encoding = tcnn.Encoding(\n            n_input_dims=3,\n            encoding_config={\n                \"otype\": \"SphericalHarmonics\",\n                \"degree\": 4,\n            },\n        )\n\n        self.position_encoding = tcnn.Encoding(\n            n_input_dims=3,\n            encoding_config={\"otype\": \"Frequency\", \"n_frequencies\": 2},\n        )\n\n        self.mlp_base = tcnn.NetworkWithInputEncoding(\n            n_input_dims=3,\n            n_output_dims=1 + self.geo_feat_dim,\n            encoding_config={\n                \"otype\": \"HashGrid\",", "metadata": {"task_id": "nerfstudio-project_nerfstudio/133", "ground_truth": "                \"n_levels\": num_levels,", "fpath_tuple": ["nerfstudio-project_nerfstudio", "nerfstudio", "fields", "nerfacto_field.py"], "context_start_lineno": 0, "line_no": 151}}
{"prompt": "# Copyright 2022 The HuggingFace Evaluate Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom numbers import Number\nfrom typing import Any, Callable, Dict, Optional, Tuple, Union\n\nfrom datasets import Dataset\nfrom typing_extensions import Literal\n\nfrom..module import EvaluationModule\nfrom..utils.file_utils import add_end_docstrings, add_start_docstrings\nfrom.base import EVALUATOR_COMPUTE_RETURN_DOCSTRING, EVALUTOR_COMPUTE_START_DOCSTRING, Evaluator\n\n\nTASK_DOCUMENTATION = r\"\"\"\n    Examples:\n    ```python\n    >>> from evaluate import evaluator\n    >>> from datasets import load_dataset\n    >>> task_evaluator = evaluator(\"image-classification\")\n    >>> data = load_dataset(\"beans\", split=\"test[:40]\")\n    >>> results = task_evaluator.compute(\n    >>>     model_or_pipeline=\"nateraw/vit-base-beans\",\n    >>>     data=data,\n    >>>     label_column=\"labels\",\n    >>>     metric=\"accuracy\",\n    >>>     label_mapping={'angular_leaf_spot': 0, 'bean_rust': 1, 'healthy': 2},\n    >>>     strategy=\"bootstrap\"\n    >>> )", "metadata": {"task_id": "huggingface_evaluate/158", "ground_truth": "    ```", "fpath_tuple": ["huggingface_evaluate", "src", "evaluate", "evaluator", "image_classification.py"], "context_start_lineno": 0, "line_no": 40}}
{"prompt": "# Copyright 2022 The HuggingFace Evaluate Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom numbers import Number\nfrom typing import Any, Callable, Dict, Optional, Tuple, Union\n\nfrom datasets import Dataset, load_dataset\nfrom typing_extensions import Literal\n\nfrom..module import EvaluationModule\nfrom..utils.file_utils import add_end_docstrings, add_start_docstrings\nfrom.base import EVALUATOR_COMPUTE_RETURN_DOCSTRING, EVALUTOR_COMPUTE_START_DOCSTRING, Evaluator", "metadata": {"task_id": "huggingface_evaluate/127", "ground_truth": "from .utils import DatasetColumnPair", "fpath_tuple": ["huggingface_evaluate", "src", "evaluate", "evaluator", "text_classification.py"], "context_start_lineno": 0, "line_no": 23}}
{"prompt": "una.prob_model.likelihood.regression import RegressionLikelihood\nfrom fortuna.prob_output_layer.classification import \\\n    ClassificationProbOutputLayer\nfrom fortuna.prob_output_layer.regression import RegressionProbOutputLayer\nfrom fortuna.utils.random import RandomNumberGenerator\nfrom tests.make_data import (make_array_random_data,\n                             make_generator_fun_random_data)\n\n\nclass TestLikelihoods(unittest.TestCase):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.shape_inputs = (3,)\n        self.n_inputs = 10\n        self.output_dim = 2\n        self.n_batches = 2\n        self.batch_size = 3\n        self.rng = random.PRNGKey(0)\n        rng = RandomNumberGenerator(seed=0)\n        reg_prob_output_layer = RegressionProbOutputLayer()\n        reg_prob_output_layer.rng = rng\n        self.reg_lik = RegressionLikelihood(\n            model_manager=RegressionModelManager(\n                model=MLP(output_dim=self.output_dim),\n                likelihood_log_variance_model=MLP(output_dim=self.output_dim),\n            ),\n            output_calib_manager=OutputCalibManager(output_calibrator=None),\n            prob_output_layer=reg_prob_output_layer,\n        )\n        self.reg_lik.rng = rng\n        class_prob_output_layer = ClassificationProbOutputLayer()\n        class_prob_output_layer.rng = rng\n        self.class_lik = ClassificationLikelihood(\n            model_manager=ClassificationModelManager(\n                model=MLP(output_dim=self.output_dim)\n            ),\n            output_calib_manager=OutputCalibManager(output_calibrator=None),\n            prob_output_layer=class_prob_output_layer,\n        )\n        self.class_lik.rng = rng\n\n        self.reg_data_arr = DataLoader.from_array_data(\n            make_array_random_data(\n                n_data=self.n_inputs,\n                shape_inputs=self.shape_inputs,\n                output_dim=self.output_dim,\n                output_type=\"continuous\",\n            )\n        )\n        self.reg_inputs_arr = InputsLoader.from_data_loader(self.reg_data_arr)\n\n        self.reg_data_gen_fun = DataLoader.from_callable_iterable(\n            make_generator_fun_random_data(\n                n_batches=self.n_batches,\n                batch_size=self.batch_size,\n                shape_inputs=self.shape_inputs,\n                output_dim=self.output_dim,\n                output_type=\"continuous\",\n            )\n        )\n        self.reg_inputs_gen_fun = InputsLoader.from_data_loader(self.reg_data_gen_fun)\n\n        self.class_data_arr = DataLoader.from_array_data(\n            make_array_random_data(\n                n_data=self.n_inputs,\n                shape_inputs=self.shape_inputs,\n                output_dim=self.output_dim,\n                output_type=\"discrete\",\n            )\n        )\n        self.class_inputs_arr = InputsLoader.from_data_loader(self.class_data_arr)\n\n        self.class_data_gen_fun = DataLoader.from_callable_iterable(\n            make_generator_fun_random_data(\n                n_batches=self.n_batches,\n                batch_size=self.batch_size,\n                shape_inputs=self.shape_inputs,\n                output_dim=self.output_dim,\n                output_type=\"discrete\",\n            )\n        )\n        self.class_inputs_gen_fun = InputsLoader.from_data_loader(\n            self.class_data_gen_fun\n        )\n\n    def test_lik_batched_log_joint_prob(self):\n        params = FrozenDict(\n            dict(\n                model=self.reg_lik.model_manager.model.init(\n                    self.rng, jnp.zeros((1,) + self.shape_inputs)\n                ),\n                lik_log_var=self.reg_lik.model_manager.likelihood_log_variance_model.init(\n                    self.rng, jnp.zeros((1,) + self.shape_inputs)\n                ),\n            )\n        )\n\n        for batch_data in self.reg_data_arr:\n            batched_log_joint_prob1 = self.reg_lik._batched_log_joint_prob(\n                params, batch_data, n_data=batch_data[1].shape[0]\n            )\n            batched_log_joint_prob2 = self.reg_lik._batched_log_joint_prob(\n                params, batch_data, n_data=2 * batch_data[1].shape[0]\n            )\n            assert jnp.allclose(batched_log_joint_prob2, 2 * batched_log_joint_prob1)\n            assert jnp.array([batched_log_joint_prob1]).shape == (1,)\n\n            _, aux = self.reg_lik._batched_log_joint_prob(\n                params,\n                batch_data,\n                n_data=batch_data[1].shape[0],\n                return_aux=[\"outputs\"],\n            )\n            assert aux[\"outputs\"].shape == (self.n_inputs, 2 * self.output_dim)\n\n    def test_lik_log_joint_prob(self):\n        params = FrozenDict(\n            dict(\n                model=self.reg_lik.model_manager.model.init(\n                    self.rng, jnp.zeros((1,) + self.shape_inputs)\n                ),\n                lik_log_var=self.reg_lik.model_manager.likelihood_log_variance_model.init(\n                    self.rng, jnp.zeros((1,) + self.shape_inputs)\n                ),\n            )\n        )\n\n        log_probs = self.reg_lik.log_prob(params, self.reg_data_arr)\n        assert log_probs.shape == (self.n_inputs,)\n\n        log_probs = self.reg_lik.log_prob(params, self.reg_data_gen_fun)\n        assert log_probs.shape == (self.n_batches * self.batch_size,)\n\n    def test_sample(self):\n        params = FrozenDict(\n            dict(\n                model=self.reg_lik.model_manager.model.init(\n                    self.rng, jnp.zeros((1,) + self.shape_inputs)\n                ),\n                lik_log_var=self.reg_lik.model_manager.likelihood_log_variance_model.init(\n                    self.rng, jnp.zeros((1,) + self.shape_inputs)\n                ),\n            )\n        )\n\n        samples = self.reg_lik.sample(10, params, self.reg_inputs_arr)\n        assert samples.shape == (10, self.n_inputs, self.output_dim)\n\n        params = FrozenDict(\n            dict(\n                model=self.class_lik.model_manager.model.init(\n                    self.rng, jnp.zeros((1,) + self.shape_inputs)", "metadata": {"task_id": "awslabs_fortuna/29", "ground_truth": "                ),", "fpath_tuple": ["awslabs_fortuna", "tests", "fortuna", "prob_model", "test_likelihood.py"], "context_start_lineno": 15, "line_no": 167}}
{"prompt": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom __future__ import annotations\n\nfrom copy import deepcopy\n\nfrom typing import Tuple\n\nimport torch\nfrom tensordict.nn import make_functional, repopulate_module\nfrom tensordict.tensordict import TensorDict, TensorDictBase\n\nfrom torchrl.modules import SafeModule\nfrom torchrl.modules.tensordict_module.actors import ActorCriticWrapper\nfrom torchrl.objectives.utils import distance_loss, hold_out_params, next_state_value\n\nfrom..envs.utils import set_exploration_mode\nfrom.common import LossModule\n\n\nclass DDPGLoss(LossModule):\n    \"\"\"The DDPG Loss class.\n\n    Args:\n        actor_network (SafeModule): a policy operator.\n        value_network (SafeModule): a Q value operator.\n        gamma (scalar): a discount factor for return computation.\n        device (str, int or torch.device, optional): a device where the losses will be computed, if it can't be found\n            via the value operator.\n        loss_function (str): loss function for the value discrepancy. Can be one of \"l1\", \"l2\" or \"smooth_l1\".\n        delay_actor (bool, optional): whether to separate the target actor networks from the actor networks used for\n            data collection. Default is :obj:`False`.\n        delay_value (bool, optional): whether to separate the target value networks from the value networks used for\n            data collection. Default is :obj:`False`.\n    \"\"\"\n\n    def __init__(\n        self,\n        actor_network: SafeModule,\n        value_network: SafeModule,\n        gamma: float,\n        loss_function: str = \"l2\",\n        delay_actor: bool = False,\n        delay_value: bool = False,\n    ) -> None:\n        super().__init__()\n        self.delay_actor = delay_actor\n        self.delay_value = delay_value\n\n        actor_critic = ActorCriticWrapper(actor_network, value_network)\n        params = make_functional(actor_critic)\n        self.actor_critic = deepcopy(actor_critic)\n        repopulate_module(actor_network, params[\"module\", \"0\"])\n        repopulate_module(value_network, params[\"module\", \"1\"])\n\n        self.convert_to_functional(\n            actor_network,\n            \"actor_network\",\n            create_target_params=self.delay_actor,\n        )\n        self.convert_to_functional(\n            value_network,\n            \"value_network\",\n            create_target_params=self.delay_value,\n            compare_against=list(actor_network.parameters()),\n        )\n        self.actor_critic.module[0] = self.actor_network\n        self.actor_critic.module[1] = self.value_network\n\n        self.actor_in_keys = actor_network.in_keys\n\n        self.register_buffer(\"gamma\", torch.tensor(gamma))\n        self.loss_funtion = loss_function\n\n    def forward(self, input_tensordict: TensorDictBase) -> TensorDict:\n        \"\"\"Computes the DDPG losses given a tensordict sampled from the replay buffer.\n\n        This function will also write a \"td_error\" key that can be used by prioritized replay buffers to assign\n            a priority to items in the tensordict.\n\n        Args:\n            input_tensordict (TensorDictBase): a tensordict with keys [\"done\", \"reward\"] and the in_keys of the actor\n                and value networks.\n\n        Returns:\n            a tuple of 2 tensors containing the DDPG loss.\n\n        \"\"\"\n        if not input_tensordict.device == self.device:\n            raise RuntimeError(\n                f\"Got device={input_tensordict.device} but \"\n                f\"actor_network.device={self.device} (self.device={self.device})\"\n            )\n\n        loss_value, td_error, pred_val, target_value = self._loss_value(\n            input_tensordict,\n        )\n        td_error = td_error.detach()\n        td_error = td_error.unsqueeze(input_tensordict.ndimension())\n        if input_tensordict.device is not None:\n            td_error = td_error.to(input_tensordict.device)\n        input_tensordict.set(\n            \"td_error\",\n            td_error,\n            inplace=True,\n        )\n        loss_actor = self._loss_actor(input_tensordict)\n        return TensorDict(\n            source={\n                \"loss_actor\": loss_actor.mean(),\n                \"loss_value\": loss_value.mean(),\n                \"pred_value\": pred_val.mean().detach(),\n                \"target_value\": target_value.mean().detach(),\n                \"pred_value_max\": pred_val.max().detach(),\n                \"target_value_max\": target_value.max().detach(),\n            },\n            batch_size=[],\n        )\n\n    def _loss_actor(\n        self,\n        tensordict: TensorDictBase,\n    ) -> torch.Tensor:\n        td_copy = tensordict.select(*self.actor_in_keys).detach()\n        td_copy = self.actor_network(\n            td_copy,\n            params=self.actor_network_params,\n        )\n        with hold_out_params(self.value_network_params) as params:\n            td_copy = self.value_network(\n                td_copy,\n                params=params,\n            )\n        return -td_copy.get(\"state_action_value\")\n\n    def _loss_value(\n        self,\n        tensordict: TensorDictBase,\n    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n        # value loss\n        td_copy = tensordict.select(*self.value_network.in_keys).detach()\n        self.value_network(\n            td_copy,\n            params=self.value_network_params,\n        )\n        pred_val = td_copy.get(\"state_action_value\").squeeze(-1)\n\n        actor_critic = self.actor_critic\n        target_params = TensorDict(\n            {\n                \"module\": {\n                    \"0\": self.target_actor_network_params,\n                    \"1\": self.target_value_network_params,\n                }\n            },\n            batch_size=self.target_actor_network_params.batch_size,\n            device=self.target_actor_network_params.device,\n        )\n        with set_exploration_mode(\"mode\"):\n            target_value = next_state_value(", "metadata": {"task_id": "pytorch_rl/172", "ground_truth": "                tensordict,", "fpath_tuple": ["pytorch_rl", "torchrl", "objectives", "ddpg.py"], "context_start_lineno": 0, "line_no": 163}}
{"prompt": "import abc\nfrom typing import Optional\n\nfrom jax._src.prng import PRNGKeyArray\n\nfrom fortuna.typing import Params\nfrom fortuna.utils.random import WithRNG\n\n\nclass Prior(WithRNG, abc.ABC):\n    \"\"\"\n    Abstract prior distribution class.\n    \"\"\"\n\n    @abc.abstractmethod\n    def log_joint_prob(self, params: Params) -> float:\n        \"\"\"\n        Evaluate the prior log-probability density function (a.k.a. log-pdf).\n\n        Parameters\n        ----------\n        params : PyTree\n            The parameters where to evaluate the log-pdf.\n\n        Returns\n        -------\n        float\n            Evaluation of the prior log-pdf.\n        \"\"\"\n        pass\n\n    @abc.abstractmethod", "metadata": {"task_id": "awslabs_fortuna/146", "ground_truth": "    def sample(self, params_like: Params, rng: Optional[PRNGKeyArray] = None) -> Params:", "fpath_tuple": ["awslabs_fortuna", "fortuna", "prob_model", "prior", "base.py"], "context_start_lineno": 0, "line_no": 32}}
{"prompt": "import math\nimport operator\nfrom typing import Optional, Union, Callable, Any\n\nfrom.base import Loader, ILoaderClass\nfrom.utils import keep, check_only\n\nNUMBER_TYPES = (int, float)\nNUMBER_TYPING = Union[int, float]\n\n\ndef numeric(int_ok: bool = True, float_ok: bool = True, inf_ok: bool = True) -> ILoaderClass:\n    if not int_ok and not float_ok:\n        raise ValueError('Either int or float should be allowed.')\n\n    def _load(value) -> NUMBER_TYPING:\n        if isinstance(value, NUMBER_TYPES):\n            if math.isnan(value):\n                raise ValueError('nan is not numeric value')\n            if isinstance(value, int) and not int_ok:\n                raise TypeError('int is not allowed but {actual} found'.format(actual=repr(value)))\n            if isinstance(value, float) and not float_ok:\n                raise TypeError('float is not allowed but {actual} found'.format(actual=repr(value)))\n            if math.isinf(value) and not inf_ok:\n                raise ValueError('inf is not allowed but {actual} found'.format(actual=repr(value)))\n\n            return value\n        else:\n            raise TypeError(\n                'numeric value should be either int, float or str, but {actual} found'.format(\n                    actual=repr(type(value).__name__)\n                )\n            )\n\n    return Loader(_load)\n\n\ndef interval(\n        left: Optional[NUMBER_TYPING] = None,\n        right: Optional[NUMBER_TYPING] = None,\n        left_ok: bool = True,\n        right_ok: bool = True,\n        eps=0.0\n) -> ILoaderClass:\n    if left is None:\n        left = -math.inf\n    if right is None:\n        right = +math.inf\n    if left > right:\n        raise ValueError(\n            \"Left bound should no more than right bound, but {left} > {right}.\".format(\n                left=repr(left), right=repr(right)\n            )\n        )\n    eps = math.fabs(eps)\n\n    def _value_compare_with_eps(a, b) -> int:\n        if math.fabs(a - b) <= eps:\n            return 0\n        elif a < b:\n            return -1\n        else:\n            return 1\n\n    def _load(value) -> NUMBER_TYPING:\n        _left_check = _value_compare_with_eps(value, left)\n        if _left_check < 0:\n            raise ValueError(\n                'value should be no less than {left} but {value} found'.format(left=repr(left), value=repr(value))\n            )\n        elif not left_ok and _left_check == 0:\n            raise ValueError(\n                'value should not be equal to left bound {left} but {value} found'.format(\n                    left=repr(left), value=repr(value)\n                )\n            )\n\n        _right_check = _value_compare_with_eps(value, right)\n        if _right_check > 0:\n            raise ValueError(\n                'value should be no more than {right} but {value} found'.format(right=repr(right), value=repr(value))\n            )\n        elif not right_ok and _right_check == 0:\n            raise ValueError(\n                'value should not be equal to right bound {right} but {value} found'.format(\n                    right=repr(right), value=repr(value)\n                )\n            )\n\n        return value\n\n    return Loader(_load)\n\n\ndef is_negative() -> ILoaderClass:\n    return Loader((lambda x: x < 0, lambda x: ValueError('negative required but {value} found'.format(value=repr(x)))))\n\n\ndef is_positive() -> ILoaderClass:\n    return Loader((lambda x: x > 0, lambda x: ValueError('positive required but {value} found'.format(value=repr(x)))))\n\n\ndef non_negative() -> ILoaderClass:\n    return Loader(\n        (lambda x: x >= 0, lambda x: ValueError('non-negative required but {value} found'.format(value=repr(x))))\n    )\n\n\ndef non_positive() -> ILoaderClass:\n    return Loader(\n        (lambda x: x <= 0, lambda x: ValueError('non-positive required but {value} found'.format(value=repr(x))))\n    )\n\n\ndef negative() -> ILoaderClass:\n    return Loader(lambda x: -x)\n\n\ndef positive() -> ILoaderClass:\n    return Loader(lambda x: +x)\n\n\ndef _math_binary(func: Callable[[Any, Any], Any], attachment) -> ILoaderClass:\n    return Loader(lambda x: func(x, Loader(attachment)(x)))\n\n\ndef plus(addend) -> ILoaderClass:\n    return _math_binary(lambda x, y: x + y, addend)\n\n\ndef minus(subtrahend) -> ILoaderClass:\n    return _math_binary(lambda x, y: x - y, subtrahend)\n\n\ndef minus_with(minuend) -> ILoaderClass:\n    return _math_binary(lambda x, y: y - x, minuend)\n\n\ndef multi(multiplier) -> ILoaderClass:\n    return _math_binary(lambda x, y: x * y, multiplier)\n\n\ndef divide(divisor) -> ILoaderClass:\n    return _math_binary(lambda x, y: x / y, divisor)\n\n\ndef divide_with(dividend) -> ILoaderClass:\n    return _math_binary(lambda x, y: y / x, dividend)\n\n\ndef power(index) -> ILoaderClass:\n    return _math_binary(lambda x, y: x ** y, index)\n\n\ndef power_with(base) -> ILoaderClass:\n    return _math_binary(lambda x, y: y ** x, base)\n\n\ndef msum(*items) -> ILoaderClass:\n\n    def _load(value):\n        return sum([item(value) for item in items])\n\n    return Loader(_load)\n\n\ndef mmulti(*items) -> ILoaderClass:\n\n    def _load(value):\n        _result = 1\n        for item in items:\n            _result *= item(value)\n        return _result\n\n    return Loader(_load)\n\n\n_COMPARE_OPERATORS = {\n    '!=': operator.__ne__,\n    '==': operator.__eq__,\n    '<': operator.__lt__,\n    '<=': operator.__le__,\n    '>': operator.__gt__,\n    '>=': operator.__ge__,\n}\n\n\ndef _msinglecmp(first, op, second) -> ILoaderClass:\n    first = Loader(first)\n    second = Loader(second)\n\n    if op in _COMPARE_OPERATORS.keys():", "metadata": {"task_id": "opendilab_ACE/37", "ground_truth": "        return Loader(", "fpath_tuple": ["opendilab_ACE", "ding", "utils", "loader", "number.py"], "context_start_lineno": 0, "line_no": 192}}
{"prompt": " a sample video (T, C, H, W), where T - number of frames,\n            # C - number of image channels (e.g. 3 for RGB), H, W - image dimensions.\n            # the first 64 frames are black and the next 64 are white\n            video = torch.cat(\n                (torch.zeros(64, 1, 32, 32), torch.full((64, 1, 32, 32), 255))\n            )\n            video = video[None, :]\n            for i in range(3):\n                logger.log_video(\n                    name=\"foo\",\n                    video=video,\n                    step=steps[i] if steps else None,\n                    fps=6,  # we can't test for the difference between fps, because the result is an encoded_string\n                )\n\n            sleep(0.01)  # wait until events are registered\n\n            event_acc = EventAccumulator(logger.experiment.get_logdir())\n            event_acc.Reload()\n            assert len(event_acc.Images(\"foo\")) == 3, str(event_acc.Images(\"foo\"))\n\n            # check that we catch the error in case the format of the tensor is wrong\n            # here the number of color channels is set to 2, which is not correct\n            video_wrong_format = torch.zeros(64, 2, 32, 32)\n            video_wrong_format = video_wrong_format[None, :]\n            with pytest.raises(Exception):\n                logger.log_video(\n                    name=\"foo\",\n                    video=video_wrong_format,\n                    step=steps[i] if steps else None,\n                )\n\n\nclass TestCSVLogger:\n    @pytest.mark.parametrize(\"steps\", [None, [1, 10, 11]])\n    def test_log_scalar(self, steps):\n        torch.manual_seed(0)\n        with tempfile.TemporaryDirectory() as log_dir:\n            exp_name = \"ramala\"\n            logger = CSVLogger(log_dir=log_dir, exp_name=exp_name)\n\n            values = torch.rand(3)\n            for i in range(3):\n                scalar_name = \"foo\"\n                scalar_value = values[i].item()\n                logger.log_scalar(\n                    value=scalar_value,\n                    name=scalar_name,\n                    step=steps[i] if steps else None,\n                )\n\n            with open(\n                os.path.join(log_dir, exp_name, \"scalars\", \"foo.csv\"), \"r\"\n            ) as file:\n                for i, row in enumerate(file.readlines()):\n                    step = steps[i] if steps else i\n                    assert row == f\"{step},{values[i].item()}\\n\"\n\n    @pytest.mark.parametrize(\"steps\", [None, [1, 10, 11]])\n    def test_log_video(self, steps):\n        torch.manual_seed(0)\n        with tempfile.TemporaryDirectory() as log_dir:\n            exp_name = \"ramala\"\n            logger = CSVLogger(log_dir=log_dir, exp_name=exp_name)\n\n            # creating a sample video (T, C, H, W), where T - number of frames,\n            # C - number of image channels (e.g. 3 for RGB), H, W - image dimensions.\n            # the first 64 frames are black and the next 64 are white\n            video = torch.cat(\n                (torch.zeros(64, 1, 32, 32), torch.full((64, 1, 32, 32), 255))\n            )\n            video = video[None, :]\n            for i in range(3):\n                logger.log_video(\n                    name=\"foo\",\n                    video=video,\n                    step=steps[i] if steps else None,\n                )\n            sleep(0.01)  # wait until events are registered\n\n            # check that the logged videos are the same as the initial video\n            video_file_name = \"foo_\" + (\"0\" if not steps else str(steps[0])) + \".pt\"\n            logged_video = torch.load(\n                os.path.join(log_dir, exp_name, \"videos\", video_file_name)\n            )\n            assert torch.equal(video, logged_video), logged_video\n\n            # check that we catch the error in case the format of the tensor is wrong\n            video_wrong_format = torch.zeros(64, 2, 32, 32)\n            video_wrong_format = video_wrong_format[None, :]\n            with pytest.raises(Exception):\n                logger.log_video(\n                    name=\"foo\",\n                    video=video_wrong_format,\n                    step=steps[i] if steps else None,\n                )\n\n\n@pytest.mark.skipif(not _has_wandb, reason=\"Wandb not installed\")\nclass TestWandbLogger:\n    @pytest.mark.parametrize(\"steps\", [None, [1, 10, 11]])\n    def test_log_scalar(self, steps):\n        torch.manual_seed(0)\n        with tempfile.TemporaryDirectory() as log_dir:\n            exp_name = \"ramala\"\n            logger = WandbLogger(log_dir=log_dir, exp_name=exp_name, offline=True)\n\n            values = torch.rand(3)\n            for i in range(3):\n                scalar_name = \"foo\"\n                scalar_value = values[i].item()\n                logger.log_scalar(\n                    value=scalar_value,\n                    name=scalar_name,\n                    step=steps[i] if steps else None,\n                )\n\n            assert logger.experiment.summary[\"foo\"] == values[-1].item()\n            assert logger.experiment.summary[\"_step\"] == i if not steps else steps[i]\n\n            logger.experiment.finish()\n            del logger\n\n    def test_log_video(self):\n        torch.manual_seed(0)\n        with tempfile.TemporaryDirectory() as log_dir:\n            exp_name = \"ramala\"\n            logger = WandbLogger(log_dir=log_dir, exp_name=exp_name, offline=True)\n\n            # creating a sample video (T, C, H, W), where T - number of frames,\n            # C - number of image channels (e.g. 3 for RGB), H, W - image dimensions.\n            # the first 64 frames are black and the next 64 are white\n            video = torch.cat(\n                (torch.zeros(64, 1, 32, 32), torch.full((64, 1, 32, 32), 255))\n            )\n            video = video[None, :]\n            logger.log_video(\n                name=\"foo\",\n                video=video,\n                fps=6,\n            )\n            logger.log_video(\n                name=\"foo_12fps\",\n                video=video,\n                fps=24,\n            )\n            sleep(0.01)  # wait until events are registered\n\n            # check that fps can be passed and that it has impact on the length of the video\n            video_6fps_size = logger.experiment.summary[\"foo\"][\"size\"]\n            video_24fps_size = logger.experiment.summary[\"foo_12fps\"][\"size\"]\n            assert video_6fps_size > video_24fps_size, video_6fps_size\n\n            # check that we catch the error in case the format of the tensor is wrong\n            video_wrong_format = torch.zeros(64, 2, 32, 32)\n            video_wrong_format = video_wrong_format[None, :]\n            with pytest.raises(Exception):\n                logger.log_video(\n                    name=\"foo\",", "metadata": {"task_id": "pytorch_rl/199", "ground_truth": "                    video=video_wrong_format,", "fpath_tuple": ["pytorch_rl", "test", "test_loggers.py"], "context_start_lineno": 70, "line_no": 229}}
{"prompt": "\n\n    def index(self, index: INDEX_TYPING, tensor_to_index: torch.Tensor) -> torch.Tensor:\n        if not isinstance(index, torch.Tensor):\n            raise ValueError(\n                f\"Only tensors are allowed for indexing using\"\n                f\" {self.__class__.__name__}.index(...)\"\n            )\n        indices = self._split(index)\n        tensor_to_index = self._split(tensor_to_index)\n\n        out = []\n        for _index, _tensor_to_index in zip(indices, tensor_to_index):\n            _index = _index.nonzero().squeeze()\n            _index = _index.expand(*_tensor_to_index.shape[:-1], _index.shape[-1])\n            out.append(_tensor_to_index.gather(-1, _index))\n        return torch.cat(out, -1)\n\n    def is_in(self, val: torch.Tensor) -> bool:\n        vals = self._split(val)\n        if vals is None:\n            return False\n        return all(\n            super(MultiOneHotDiscreteTensorSpec, self).is_in(_val) for _val in vals\n        )\n\n    def _project(self, val: torch.Tensor) -> torch.Tensor:\n        vals = self._split(val)\n        return torch.cat([super()._project(_val) for _val in vals], -1)\n\n    def to_categorical(self) -> MultiDiscreteTensorSpec:\n\n        return MultiDiscreteTensorSpec(\n            [_space.n for _space in self.space],\n            device=self.device,\n            dtype=self.dtype,\n            shape=[*self.shape[:-1], len(self.space)],\n        )\n\n    def expand(self, *shape):\n        nvecs = [space.n for space in self.space]\n        if len(shape) == 1 and isinstance(shape[0], (tuple, list, torch.Size)):\n            shape = shape[0]\n        if any(val < 0 for val in shape):\n            raise ValueError(\n                f\"{self.__class__.__name__}.extend does not support negative shapes.\"\n            )\n        if any(s1!= s2 and s2!= 1 for s1, s2 in zip(shape[-self.ndim :], self.shape)):\n            raise ValueError(\n                f\"The last {self.ndim} of the extended shape must match the\"\n                f\"shape of the CompositeSpec in CompositeSpec.extend.\"\n            )\n        return self.__class__(\n            nvec=nvecs, shape=shape, device=self.device, dtype=self.dtype\n        )\n\n\nclass DiscreteTensorSpec(TensorSpec):\n    \"\"\"A discrete tensor spec.\n\n    An alternative to OneHotTensorSpec for categorical variables in TorchRL. Instead of\n    using multiplication, categorical variables perform indexing which can speed up\n    computation and reduce memory cost for large categorical variables.\n\n    Example:\n        >>> batch, size = 3, 4\n        >>> action_value = torch.arange(batch*size)\n        >>> action_value = action_value.view(batch, size).to(torch.float)\n        >>> action = torch.argmax(action_value, dim=-1).to(torch.long)\n        >>> chosen_action_value = action_value[range(batch), action]\n        >>> print(chosen_action_value)\n        tensor([ 3.,  7., 11.])\n\n    Args:\n        n (int): number of possible outcomes.\n        shape: (torch.Size, optional): shape of the variable, default is \"torch.Size([])\".\n        device (str, int or torch.device, optional): device of the tensors.\n        dtype (str or torch.dtype, optional): dtype of the tensors.\n\n    \"\"\"\n\n    shape: torch.Size\n    space: DiscreteBox\n    device: torch.device = torch.device(\"cpu\")\n    dtype: torch.dtype = torch.float\n    domain: str = \"\"\n\n    def __init__(\n        self,\n        n: int,\n        shape: Optional[torch.Size] = None,\n        device: Optional[DEVICE_TYPING] = None,\n        dtype: Optional[Union[str, torch.dtype]] = torch.long,\n    ):\n        if shape is None:\n            shape = torch.Size([])\n        dtype, device = _default_dtype_and_device(dtype, device)\n        space = DiscreteBox(n)\n        super().__init__(shape, space, device, dtype, domain=\"discrete\")\n\n    def rand(self, shape=None) -> torch.Tensor:\n        if shape is None:\n            shape = torch.Size([])\n        return torch.randint(\n            0,\n            self.space.n,\n            torch.Size([*shape, *self.shape]),\n            device=self.device,\n            dtype=self.dtype,\n        )\n\n    def _project(self, val: torch.Tensor) -> torch.Tensor:\n        if val.dtype not in (torch.int, torch.long):\n            val = torch.round(val)\n        return val.clamp_(min=0, max=self.space.n - 1)\n\n    def is_in(self, val: torch.Tensor) -> bool:\n        return (0 <= val).all() and (val < self.space.n).all()\n\n    def __eq__(self, other):\n        return (\n            type(self) == type(other)\n            and self.shape == other.shape\n            and self.space == other.space\n            and self.device == other.device\n            and self.dtype == other.dtype\n            and self.domain == other.domain\n        )\n\n    def to_numpy(self, val: TensorDict, safe: bool = True) -> dict:\n        return super().to_numpy(val, safe)\n\n    def to_onehot(self) -> OneHotDiscreteTensorSpec:\n        # if len(self.shape) > 1:\n        #     raise RuntimeError(\n        #         f\"DiscreteTensorSpec with shape that has several dimensions can't be converted to \"\n        #         f\"OneHotDiscreteTensorSpec. Got shape={self.shape}.\"\n        #     )\n        shape = [*self.shape, self.space.n]\n        return OneHotDiscreteTensorSpec(\n            n=self.space.n, shape=shape, device=self.device, dtype=self.dtype\n        )\n\n    def expand(self, *shape):\n        if len(shape) == 1 and isinstance(shape[0], (tuple, list, torch.Size)):\n            shape = shape[0]\n        if any(val < 0 for val in shape):\n            raise ValueError(\n                f\"{self.__class__.__name__}.extend does not support negative shapes.\"\n            )\n        if any(s1!= s2 and s2!= 1 for s1, s2 in zip(shape[-self.ndim :], self.shape)):\n            raise ValueError(\n                f\"The last {self.ndim} of the extended shape must match the\"\n                f\"shape of the CompositeSpec in CompositeSpec.extend.\"\n            )\n        return self.__class__(\n            n=self.space.n, shape=shape, device=self.device, dtype=self.dtype\n        )\n\n    def to(self, dest: Union[torch.dtype, DEVICE_TYPING]) -> CompositeSpec:\n        if isinstance(dest, torch.dtype):\n            dest_dtype = dest\n            dest_device = self.device\n        else:\n            dest_dtype = self.dtype", "metadata": {"task_id": "pytorch_rl/127", "ground_truth": "            dest_device = torch.device(dest)", "fpath_tuple": ["pytorch_rl", "torchrl", "data", "tensor_specs.py"], "context_start_lineno": 1178, "line_no": 1343}}
{"prompt": "import inspect\nimport logging\nimport os\nimport random\nimport re\nimport unittest\nimport urllib.parse\nfrom distutils.util import strtobool\nfrom io import BytesIO, StringIO\nfrom pathlib import Path\nfrom typing import Optional, Union\n\nimport numpy as np\n\nimport PIL.Image\nimport PIL.ImageOps\nimport requests\nfrom packaging import version\n\nfrom.import_utils import is_flax_available, is_onnx_available, is_torch_available\n\n\nglobal_rng = random.Random()\n\n\nif is_torch_available():\n    import torch\n\n    torch_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    is_torch_higher_equal_than_1_12 = version.parse(version.parse(torch.__version__).base_version) >= version.parse(\n        \"1.12\"\n    )\n\n    if is_torch_higher_equal_than_1_12:\n        # Some builds of torch 1.12 don't have the mps backend registered. See #892 for more details\n        mps_backend_registered = hasattr(torch.backends, \"mps\")\n        torch_device = \"mps\" if (mps_backend_registered and torch.backends.mps.is_available()) else torch_device\n\n\ndef torch_all_close(a, b, *args, **kwargs):\n    if not is_torch_available():\n        raise ValueError(\"PyTorch needs to be installed to use this function.\")\n    if not torch.allclose(a, b, *args, **kwargs):\n        assert False, f\"Max diff is absolute {(a - b).abs().max()}. Diff tensor is {(a - b).abs()}.\"\n    return True\n\n\ndef print_tensor_test(tensor, filename=\"test_corrections.txt\", expected_tensor_name=\"expected_slice\"):\n    test_name = os.environ.get(\"PYTEST_CURRENT_TEST\")\n    if not torch.is_tensor(tensor):\n        tensor = torch.from_numpy(tensor)\n\n    tensor_str = str(tensor.detach().cpu().flatten().to(torch.float32)).replace(\"\\n\", \"\")\n    # format is usually:\n    # expected_slice = np.array([-0.5713, -0.3018, -0.9814, 0.04663, -0.879, 0.76, -1.734, 0.1044, 1.161])\n    output_str = tensor_str.replace(\"tensor\", f\"{expected_tensor_name} = np.array\")\n    test_file, test_class, test_fn = test_name.split(\"::\")\n    test_fn = test_fn.split()[0]\n    with open(filename, \"a\") as f:\n        print(\";\".join([test_file, test_class, test_fn, output_str]), file=f)\n\n\ndef get_tests_dir(append_path=None):\n    \"\"\"\n    Args:\n        append_path: optional path to append to the tests dir path\n    Return:\n        The full path to the `tests` dir, so that the tests can be invoked from anywhere. Optionally `append_path` is\n        joined after the `tests` dir the former is provided.\n    \"\"\"\n    # this function caller's __file__\n    caller__file__ = inspect.stack()[1][1]\n    tests_dir = os.path.abspath(os.path.dirname(caller__file__))\n\n    while not tests_dir.endswith(\"tests\"):\n        tests_dir = os.path.dirname(tests_dir)\n\n    if append_path:", "metadata": {"task_id": "huggingface_diffusers/146", "ground_truth": "        return os.path.join(tests_dir, append_path)", "fpath_tuple": ["huggingface_diffusers", "src", "diffusers", "utils", "testing_utils.py"], "context_start_lineno": 0, "line_no": 78}}
{"prompt": " self._td.get(key + \"_ssq\")\n        _count = self._td.get(key + \"_count\")\n\n        _sum = self._td.get(key + \"_sum\")\n        value_sum = _sum_left(value, _sum)\n        _sum *= self.decay\n        _sum += value_sum\n        self._td.set_(key + \"_sum\", _sum, no_check=True)\n\n        _ssq = self._td.get(key + \"_ssq\")\n        value_ssq = _sum_left(value.pow(2), _ssq)\n        _ssq *= self.decay\n        _ssq += value_ssq\n        self._td.set_(key + \"_ssq\", _ssq, no_check=True)\n\n        _count = self._td.get(key + \"_count\")\n        _count *= self.decay\n        _count += N\n        self._td.set_(key + \"_count\", _count, no_check=True)\n\n        mean = _sum / _count\n        std = (_ssq / _count - mean.pow(2)).clamp_min(self.eps).sqrt()\n        return (value - mean) / std.clamp_min(self.eps)\n\n    def to_observation_norm(self) -> Union[Compose, ObservationNorm]:\n        \"\"\"Converts VecNorm into an ObservationNorm class that can be used at inference time.\"\"\"\n        out = []\n        for key in self.in_keys:\n            _sum = self._td.get(key + \"_sum\")\n            _ssq = self._td.get(key + \"_ssq\")\n            _count = self._td.get(key + \"_count\")\n            mean = _sum / _count\n            std = (_ssq / _count - mean.pow(2)).clamp_min(self.eps).sqrt()\n\n            _out = ObservationNorm(\n                loc=mean,\n                scale=std,\n                standard_normal=True,\n                in_keys=self.in_keys,\n            )\n            if len(self.in_keys) == 1:\n                return _out\n            else:\n                out += ObservationNorm\n        return Compose(*out)\n\n    @staticmethod\n    def build_td_for_shared_vecnorm(\n        env: EnvBase,\n        keys: Optional[Sequence[str]] = None,\n        memmap: bool = False,\n    ) -> TensorDictBase:\n        \"\"\"Creates a shared tensordict for normalization across processes.\n\n        Args:\n            env (EnvBase): example environment to be used to create the\n                tensordict\n            keys (iterable of str, optional): keys that\n                have to be normalized. Default is `[\"next\", \"reward\"]`\n            memmap (bool): if True, the resulting tensordict will be cast into\n                memmory map (using `memmap_()`). Otherwise, the tensordict\n                will be placed in shared memory.\n\n        Returns:\n            A memory in shared memory to be sent to each process.\n\n        Examples:\n            >>> from torch import multiprocessing as mp\n            >>> queue = mp.Queue()\n            >>> env = make_env()\n            >>> td_shared = VecNorm.build_td_for_shared_vecnorm(env,\n           ...     [\"next\", \"reward\"])\n            >>> assert td_shared.is_shared()\n            >>> queue.put(td_shared)\n            >>> # on workers\n            >>> v = VecNorm(shared_td=queue.get())\n            >>> env = TransformedEnv(make_env(), v)\n\n        \"\"\"\n        raise NotImplementedError(\"this feature is currently put on hold.\")\n        sep = \".-|-.\"\n        if keys is None:\n            keys = [\"next\", \"reward\"]\n        td = make_tensordict(env)\n        keys = {key for key in td.keys() if key in keys}\n        td_select = td.select(*keys)\n        td_select = td_select.flatten_keys(sep)\n        if td.batch_dims:\n            raise RuntimeError(\n                f\"VecNorm should be used with non-batched environments. \"\n                f\"Got batch_size={td.batch_size}\"\n            )\n        keys = list(td_select.keys())\n        for key in keys:\n            td_select.set(key + \"_ssq\", td_select.get(key).clone())\n            td_select.set(\n                key + \"_count\",\n                torch.zeros(\n                    *td.batch_size,\n                    1,\n                    device=td_select.device,\n                    dtype=torch.float,\n                ),\n            )\n            td_select.rename_key(key, key + \"_sum\")\n        td_select.exclude(*keys).zero_()\n        td_select = td_select.unflatten_keys(sep)\n        if memmap:\n            return td_select.memmap_()\n        return td_select.share_memory_()\n\n    def get_extra_state(self) -> OrderedDict:\n        return collections.OrderedDict({\"lock\": self.lock, \"td\": self._td})\n\n    def set_extra_state(self, state: OrderedDict) -> None:\n        lock = state[\"lock\"]\n        if lock is not None:\n            \"\"\"\n            since locks can't be serialized, we have use cases for stripping them\n            for example in ParallelEnv, in which case keep the lock we already have\n            to avoid an updated tensor dict being sent between processes to erase locks\n            \"\"\"\n            self.lock = lock\n        td = state[\"td\"]\n        if td is not None and not td.is_shared():\n            raise RuntimeError(\n                \"Only shared tensordicts can be set in VecNorm transforms\"\n            )\n        self._td = td\n\n    def __repr__(self) -> str:\n        return (\n            f\"{self.__class__.__name__}(decay={self.decay:4.4f},\"\n            f\"eps={self.eps:4.4f}, keys={self.in_keys})\"\n        )\n\n\nclass RewardSum(Transform):\n    \"\"\"Tracks episode cumulative rewards.\n\n    This transform accepts a list of tensordict reward keys (i.e. \u00b4in_keys\u00b4) and tracks their cumulative\n    value along each episode. When called, the transform creates a new tensordict key for each in_key named\n    \u00b4episode_{in_key}\u00b4 where  the cumulative values are written. All \u00b4in_keys\u00b4 should be part of the env\n    reward and be present in the env reward_spec.\n\n    If no in_keys are specified, this transform assumes \u00b4reward\u00b4 to be the input key. However, multiple rewards\n    (e.g. reward1 and reward2) can also be specified. If \u00b4in_keys\u00b4 are not present in the provided tensordict,\n    this transform hos no effect.\n    \"\"\"\n\n    def __init__(\n        self,\n        in_keys: Optional[Sequence[str]] = None,\n        out_keys: Optional[Sequence[str]] = None,\n    ):\n        \"\"\"Initialises the transform. Filters out non-reward input keys and defines output keys.\"\"\"\n        if in_keys is None:\n            in_keys = [\"reward\"]\n        out_keys = [f\"episode_{in_key}\" for in_key in in_keys]\n\n        super().__init__(in_keys=in_keys, out_keys=out_keys)\n\n    def reset(self, tensordict: TensorDictBase) -> TensorDictBase:\n        \"\"\"Resets episode rewards.\"\"\"\n        # Non-batched environments\n        if len(tensordict.batch_size) < 1 or tensordict.batch_size[0] == 1:\n            for in_key, out_key in zip(self.in_keys, self.out_keys):", "metadata": {"task_id": "pytorch_rl/47", "ground_truth": "                if out_key in tensordict.keys():", "fpath_tuple": ["pytorch_rl", "torchrl", "envs", "transforms", "transforms.py"], "context_start_lineno": 2390, "line_no": 2557}}
{"prompt": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom.a2c import A2CLoss\nfrom.common import LossModule", "metadata": {"task_id": "pytorch_rl/100", "ground_truth": "from .ddpg import DDPGLoss", "fpath_tuple": ["pytorch_rl", "torchrl", "objectives", "__init__.py"], "context_start_lineno": 0, "line_no": 7}}
{"prompt": "# Copyright 2022 The Nerfstudio Team. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Base Configs\"\"\"\n\n# pylint: disable=wrong-import-position\n\nfrom __future__ import annotations\n\nimport warnings\nfrom dataclasses import dataclass\nfrom pathlib import Path\nfrom typing import Any, List, Optional, Tuple, Type\n\n# model instances\nfrom nerfstudio.utils import writer\n\nwarnings.filterwarnings(\"ignore\", module=\"torchvision\")\n\n# Pretty printing class\nclass PrintableConfig:  # pylint: disable=too-few-public-methods\n    \"\"\"Printable Config defining str function\"\"\"\n\n    def __str__(self):\n        lines = [self.__class__.__name__ + \":\"]\n        for key, val in vars(self).items():\n            if isinstance(val, Tuple):\n                flattened_val = \"[\"\n                for item in val:\n                    flattened_val += str(item) + \"\\n\"\n                flattened_val = flattened_val.rstrip(\"\\n\")\n                val = flattened_val + \"]\"", "metadata": {"task_id": "nerfstudio-project_nerfstudio/150", "ground_truth": "            lines += f\"{key}: {str(val)}\".split(\"\\n\")", "fpath_tuple": ["nerfstudio-project_nerfstudio", "nerfstudio", "configs", "base_config.py"], "context_start_lineno": 0, "line_no": 43}}
{"prompt": ".__init__)\n\n        # Perform split on different dataset\n        if 'train' in func_args:\n            # Split train to (train, val)\n            dataset_train = dataset_func(root=config.data.root,\n                                         train=True,\n                                         **filtered_args,\n                                         **transform_funcs)\n            dataset_val = None\n            dataset_test = dataset_func(root=config.data.root,\n                                        train=False,\n                                        **filtered_args,\n                                        **test_transform_funcs)\n            if splits:\n                dataset_val = dataset_func(root=config.data.root,\n                                           train=True,\n                                           **filtered_args,\n                                           **val_transform_funcs)\n\n                index = [i for i in range(len(dataset_train))]\n                np.random.shuffle(index)\n                train_size = int(splits[0] * len(dataset_train))\n                train_idx_slice, val_idx_slice = index[:train_size], \\\n                    index[train_size:]\n\n                dataset_train = Subset(dataset_train, train_idx_slice)\n                dataset_val = Subset(dataset_val, val_idx_slice)\n\n        elif'split' in func_args:\n            # Use raw split\n            dataset_train = dataset_func(root=config.data.root,\n                                         split='train',\n                                         **filtered_args,\n                                         **transform_funcs)\n            dataset_val = dataset_func(root=config.data.root,\n                                       split='valid',\n                                       **filtered_args,\n                                       **val_transform_funcs)\n            dataset_test = dataset_func(root=config.data.root,\n                                        split='test',\n                                        **filtered_args,\n                                        **test_transform_funcs)\n        elif 'classes' in func_args:\n            # Use raw split\n            dataset_train = dataset_func(root=config.data.root,\n                                         classes='train',\n                                         **filtered_args,\n                                         **transform_funcs)\n            dataset_val = dataset_func(root=config.data.root,\n                                       classes='valid',\n                                       **filtered_args,\n                                       **val_transform_funcs)\n            dataset_test = dataset_func(root=config.data.root,\n                                        classes='test',\n                                        **filtered_args,\n                                        **test_transform_funcs)\n        else:\n            # Use config.data.splits\n            dataset_train = dataset_func(root=config.data.root,\n                                         **filtered_args,\n                                         **transform_funcs)\n            dataset_val = dataset_func(root=config.data.root,\n                                       **filtered_args,\n                                       **val_transform_funcs)\n            dataset_test = dataset_func(root=config.data.root,\n                                        **filtered_args,\n                                        **test_transform_funcs)\n\n            index = [i for i in range(len(dataset_train))]\n            np.random.shuffle(index)\n\n            train_size = int(splits[0] * len(dataset_train))\n            val_size = int(splits[1] * len(dataset_train))\n\n            train_idx_slice = index[:train_size]\n            val_idx_slice = index[train_size:train_size + val_size]\n            test_idx_slice = index[train_size + val_size:]\n\n            dataset_train = Subset(dataset_train, train_idx_slice)\n            dataset_val = Subset(dataset_val, val_idx_slice)\n            dataset_test = Subset(dataset_test, test_idx_slice)\n\n        data_split_dict = {\n            'train': dataset_train,\n            'val': dataset_val,\n            'test': dataset_test\n        }\n\n        return data_split_dict\n\n    def load_torchtext_data(name, splits=None, config=None):\n        from torch.nn.utils.rnn import pad_sequence\n        from federatedscope.nlp.dataset.utils import label_to_index\n\n        dataset_func = getattr(import_module('torchtext.datasets'), name)\n        if config.data.args:\n            raw_args = config.data.args[0]\n        else:\n            raw_args = {}\n        assert'max_len' in raw_args, \"Miss key'max_len' in \" \\\n                                      \"`config.data.args`.\"\n        filtered_args = filter_dict(dataset_func.__init__, raw_args)\n        dataset = dataset_func(root=config.data.root, **filtered_args)\n\n        # torchtext.transforms requires >= 0.12.0 and torch = 1.11.0,\n        # so we do not use `get_transform` in torchtext.\n\n        # Merge all data and tokenize\n        x_list = []\n        y_list = []\n        for data_iter in dataset:\n            data, targets = [], []\n            for i, item in enumerate(data_iter):\n                data.append(item[1])\n                targets.append(item[0])\n            x_list.append(data)\n            y_list.append(targets)\n\n        x_all, y_all = [], []\n        for i in range(len(x_list)):\n            x_all += x_list[i]\n            y_all += y_list[i]\n\n        if config.model.type.endswith('transformers'):\n            from transformers import AutoTokenizer\n            cache_path = os.path.join(os.getcwd(), \"huggingface\")\n            try:\n                tokenizer = AutoTokenizer.from_pretrained(\n                    config.model.type.split('@')[0],\n                    local_files_only=True,\n                    cache_dir=cache_path)\n            except Exception as e:\n                logging.error(f\"When loading cached file form \"\n                              f\"{cache_path}, we faced the exception: \\n \"\n                              f\"{str(e)}\")\n\n            x_all = tokenizer(x_all,\n                              return_tensors='pt',\n                              padding=True,\n                              truncation=True,\n                              max_length=raw_args['max_len'])\n            data = [{key: value[i]\n                     for key, value in x_all.items()}\n                    for i in range(len(next(iter(x_all.values()))))]\n            if 'classification' in config.model.task.lower():\n                targets = label_to_index(y_all)\n            else:\n                y_all = tokenizer(y_all,\n                                  return_tensors='pt',\n                                  padding=True,\n                                  truncation=True,\n                                  max_length=raw_args['max_len'])\n                targets = [{key: value[i]\n                            for key, value in y_all.items()}\n                           for i in range(len(next(iter(y_all.values()))))]\n        else:\n            from torchtext.data import get_tokenizer\n            tokenizer = get_tokenizer(\"basic_english\")\n            if len(config.data.transform) == 0:\n                raise ValueError(\n                    \"`transform` must be one pretrained Word Embeddings from \\\n                    ['GloVe', 'FastText', 'CharNGram']\")\n            if len(config.data.transform) == 1:", "metadata": {"task_id": "alibaba_FederatedScope/93", "ground_truth": "                config.data.transform.append({})", "fpath_tuple": ["alibaba_FederatedScope", "federatedscope", "core", "data", "utils.py"], "context_start_lineno": 140, "line_no": 304}}
{"prompt": "from typing import List, Dict, Any, Tuple, Union, Optional\nfrom collections import namedtuple\nimport torch\nimport copy\nimport math\n\nfrom ding.torch_utils import Adam, RMSprop, to_device\nfrom ding.rl_utils import q_nstep_td_data, q_nstep_td_error, get_nstep_return_data, get_train_sample, l2_balance\nfrom ding.model import model_wrap\nfrom ding.utils import POLICY_REGISTRY\nfrom ding.utils.data import timestep_collate, default_collate, default_decollate\nfrom.base_policy import Policy\n\n@POLICY_REGISTRY.register('smac_ace_dqn')\nclass SMACACEDQNPolicy(Policy):\n    \"\"\"\n    Overview:\n        Policy class of ACE algorithm. ACE is a multi agent reinforcement learning algorithm, \\\n            you can view the paper in the following link https://arxiv.org/abs/2211.16068\n    Interface:\n        _init_learn, _data_preprocess_learn, _forward_learn, _reset_learn, _state_dict_learn, _load_state_dict_learn \\\n            _init_collect, _forward_collect, _reset_collect, _process_transition, _init_eval, _forward_eval \\\n            _reset_eval, _get_train_sample, default_model\n    Config:\n        == ==================== ======== ============== ======================================== =======================\n        ID Symbol               Type     Default Value  Description                              Other(Shape)\n        == ==================== ======== ============== ======================================== =======================\n        1  ``type``             str      qmix           | RL policy register name, refer to      | this arg is optional,\n                                                        | registry ``POLICY_REGISTRY``           | a placeholder\n        2  ``cuda``             bool     True           | Whether to use cuda for network        | this arg can be diff-\n                                                                                                 | erent from modes\n        3  ``on_policy``        bool     False          | Whether the RL algorithm is on-policy\n                                                        | or off-policy\n        4. ``priority``         bool     False          | Whether use priority(PER)              | priority sample,\n                                                                                                 | update priority\n        5  | ``priority_``      bool     False          | Whether use Importance Sampling        | IS weight\n           | ``IS_weight``                              | Weight to correct biased update.\n        6  | ``learn.update_``  int      20             | How many updates(iterations) to train  | this args can be vary\n           | ``per_collect``                            | after collector's one collection. Only | from envs. Bigger val\n                                                        | valid in serial training               | means more off-policy\n        7  | ``learn.target_``   float    0.001         | Target network update momentum         | between[0,1]\n           | ``update_theta``                           | parameter.\n        8  | ``learn.discount`` float    0.99           | Reward's future discount factor, aka.  | may be 1 when sparse\n           | ``_factor``                                | gamma                                  | reward env\n        == ==================== ======== ============== ======================================== =======================\n    \"\"\"\n    config = dict(\n        # (str) RL policy register name (refer to function \"POLICY_REGISTRY\").\n        type='smac_ace_dqn',\n        # (bool) Whether to use cuda for network.\n        cuda=True,\n        # (bool) Whether the RL algorithm is on-policy or off-policy.\n        on_policy=False,\n        # (bool) Whether use priority(priority sample, IS weight, update priority)\n        priority=False,\n        # (bool) Whether use Importance Sampling Weight to correct biased update. If True, priority must be True.\n        priority_IS_weight=False,\n        learn=dict(\n            # (bool) Whether to use multi gpu\n            multi_gpu=False,\n            update_per_collect=20,\n            batch_size=32,", "metadata": {"task_id": "opendilab_ACE/93", "ground_truth": "            learning_rate=0.0005,", "fpath_tuple": ["opendilab_ACE", "ding", "policy", "ace_dqn.py"], "context_start_lineno": 0, "line_no": 62}}
{"prompt": "# Copyright 2022 The Nerfstudio Team. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Helper utils for processing polycam data into the nerfstudio format.\"\"\"\n\nimport json\nimport sys\nfrom pathlib import Path\nfrom typing import List, Tuple\n\nfrom rich.console import Console\n\nfrom nerfstudio.process_data import process_data_utils\nfrom nerfstudio.process_data.process_data_utils import CAMERA_MODELS\nfrom nerfstudio.utils import io\n\nCONSOLE = Console(width=120)\n\n\ndef polycam_to_json(\n    image_filenames: List[Path],\n    depth_filenames: List[Path],\n    cameras_dir: Path,\n    output_dir: Path,\n    min_blur_score: float = 0.0,\n    crop_border_pixels: int = 0,\n) -> List[str]:\n    \"\"\"Convert Polycam data into a nerfstudio dataset.\n\n    Args:\n        image_filenames: List of paths to the original images.\n        depth_filenames: List of paths to the original depth maps.\n        cameras_dir: Path to the polycam cameras directory.\n        output_dir: Path to the output directory.\n        min_blur_score: Minimum blur score to use an image. Images below this value will be skipped.\n        crop_border_pixels: Number of pixels to crop from each border of the image.\n\n    Returns:\n        Summary of the conversion.\n    \"\"\"\n    use_depth = len(image_filenames) == len(depth_filenames)\n    data = {}\n    data[\"camera_model\"] = CAMERA_MODELS[\"perspective\"].value\n    # Needs to be a string for camera_utils.auto_orient_and_center_poses\n    data[\"orientation_override\"] = \"none\"\n\n    frames = []\n    skipped_frames = 0\n    for i, image_filename in enumerate(image_filenames):\n        json_filename = cameras_dir / f\"{image_filename.stem}.json\"\n        frame_json = io.load_from_json(json_filename)\n        if \"blur_score\" in frame_json and frame_json[\"blur_score\"] < min_blur_score:\n            skipped_frames += 1\n            continue\n        frame = {}\n        frame[\"fl_x\"] = frame_json[\"fx\"]\n        frame[\"fl_y\"] = frame_json[\"fy\"]\n        frame[\"cx\"] = frame_json[\"cx\"] - crop_border_pixels\n        frame[\"cy\"] = frame_json[\"cy\"] - crop_border_pixels\n        frame[\"w\"] = frame_json[\"width\"] - crop_border_pixels * 2\n        frame[\"h\"] = frame_json[\"height\"] - crop_border_pixels * 2\n        frame[\"file_path\"] = f\"./images/frame_{i+1:05d}{image_filename.suffix}\"\n        if use_depth:\n            frame[\"depth_map_path\"] = f\"./depth/frame_{i+1:05d}{depth_filenames[i].suffix}\"\n        # Transform matrix to nerfstudio format. Please refer to the documentation for coordinate system conventions.\n        frame[\"transform_matrix\"] = [\n            [frame_json[\"t_20\"], frame_json[\"t_21\"], frame_json[\"t_22\"], frame_json[\"t_23\"]],\n            [frame_json[\"t_00\"], frame_json[\"t_01\"], frame_json[\"t_02\"], frame_json[\"t_03\"]],\n            [frame_json[\"t_10\"], frame_json[\"t_11\"], frame_json[\"t_12\"], frame_json[\"t_13\"]],", "metadata": {"task_id": "nerfstudio-project_nerfstudio/134", "ground_truth": "            [0.0, 0.0, 0.0, 1.0],", "fpath_tuple": ["nerfstudio-project_nerfstudio", "nerfstudio", "process_data", "polycam_utils.py"], "context_start_lineno": 0, "line_no": 80}}
{"prompt": " result.fetchone()\n    if not row:\n      raise custom_errors.NotFoundError(\n          'Failed to find trial name: %s' % trial_name\n      )\n    return study_pb2.Trial.FromString(row['serialized_trial'])\n\n  def update_trial(self, trial: study_pb2.Trial) -> resources.TrialResource:\n    trial_resource = resources.TrialResource.from_name(trial.name)\n    exists_query = sqla.exists(\n        sqla.select([self._trials_table]).where(\n            self._trials_table.c.trial_name == trial.name\n        )\n    ).select()\n    update_query = (\n        sqla.update(self._trials_table)\n       .where(self._trials_table.c.trial_name == trial.name)\n       .values(\n            trial_name=trial.name,\n            owner_id=trial_resource.owner_id,\n            study_id=trial_resource.study_id,\n            trial_id=trial_resource.trial_id,\n            serialized_trial=trial.SerializeToString(),\n        )\n    )\n\n    with self._lock:\n      exists = self._connection.execute(exists_query).fetchone()[0]\n      if not exists:\n        raise custom_errors.NotFoundError(\n            'Trial %s does not exist.' % trial.name\n        )\n      self._connection.execute(update_query)\n\n    return trial_resource\n\n  def list_trials(self, study_name: str) -> List[study_pb2.Trial]:\n    study_resource = resources.StudyResource.from_name(study_name)\n    exists_query = sqla.exists(\n        sqla.select([self._studies_table]).where(\n            self._studies_table.c.study_name == study_name\n        )\n    ).select()\n    list_query = (\n        sqla.select([self._trials_table])\n       .where(self._trials_table.c.owner_id == study_resource.owner_id)\n       .where(self._trials_table.c.study_id == study_resource.study_id)\n    )\n\n    with self._lock:\n      exists = self._connection.execute(exists_query).fetchone()[0]\n      if not exists:\n        raise custom_errors.NotFoundError(\n            'Study name %s does not exist.' % study_name\n        )\n      result = self._connection.execute(list_query)\n\n    return [\n        study_pb2.Trial.FromString(row['serialized_trial']) for row in result\n    ]\n\n  def delete_trial(self, trial_name: str) -> None:\n    exists_query = sqla.exists(\n        sqla.select([self._trials_table]).where(\n            self._trials_table.c.trial_name == trial_name\n        )\n    ).select()\n    delete_query = self._trials_table.delete().where(\n        self._trials_table.c.trial_name == trial_name\n    )\n    with self._lock:\n      exists = self._connection.execute(exists_query).fetchone()[0]\n      if not exists:\n        raise custom_errors.NotFoundError(\n            'Trial %s does not exist.' % trial_name\n        )\n      self._connection.execute(delete_query)\n\n  def max_trial_id(self, study_name: str) -> int:\n    study_resource = resources.StudyResource.from_name(study_name)\n    exists_query = sqla.exists(\n        sqla.select([self._studies_table]).where(\n            self._studies_table.c.study_name == study_name\n        )\n    ).select()\n    trial_id_query = (\n        sqla.select(\n            [sqla.func.max(self._trials_table.c.trial_id, type_=sqla.INT)]\n        )\n       .where(self._trials_table.c.owner_id == study_resource.owner_id)\n       .where(self._trials_table.c.study_id == study_resource.study_id)\n    )\n\n    with self._lock:\n      exists = self._connection.execute(exists_query).fetchone()[0]\n      if not exists:\n        raise custom_errors.NotFoundError(\n            'Study %s does not exist.' % study_name\n        )\n      potential_trial_id = self._connection.execute(trial_id_query).fetchone()[\n          0\n      ]\n\n    if potential_trial_id is None:\n      return 0\n    return potential_trial_id\n\n  def create_suggestion_operation(\n      self, operation: operations_pb2.Operation\n  ) -> resources.SuggestionOperationResource:\n    resource = resources.SuggestionOperationResource.from_name(operation.name)\n    query = self._suggestion_operations_table.insert().values(\n        operation_name=operation.name,\n        owner_id=resource.owner_id,\n        study_id=resource.study_id,\n        client_id=resource.client_id,\n        operation_number=resource.operation_number,\n        serialized_op=operation.SerializeToString(),\n    )\n\n    try:\n      with self._lock:\n        self._connection.execute(query)\n      return resource\n    except sqla.exc.IntegrityError as integrity_error:\n      raise custom_errors.AlreadyExistsError(\n          'Suggest Op with name %s already exists.' % operation.name\n      ) from integrity_error\n\n  def get_suggestion_operation(\n      self, operation_name: str\n  ) -> operations_pb2.Operation:\n    query = sqla.select([self._suggestion_operations_table]).where(\n        self._suggestion_operations_table.c.operation_name == operation_name\n    )\n\n    with self._lock:\n      result = self._connection.execute(query)\n\n    row = result.fetchone()\n    if not row:\n      raise custom_errors.NotFoundError(\n          'Failed to find suggest op name: %s' % operation_name\n      )\n    return operations_pb2.Operation.FromString(row['serialized_op'])\n\n  def update_suggestion_operation(\n      self, operation: operations_pb2.Operation\n  ) -> resources.SuggestionOperationResource:\n    resource = resources.SuggestionOperationResource.from_name(operation.name)\n\n    exists_query = sqla.exists(\n        sqla.select([self._suggestion_operations_table]).where(\n            self._suggestion_operations_table.c.operation_name == operation.name\n        )\n    ).select()\n    update_query = (\n        sqla.update(self._suggestion_operations_table)\n       .where(\n            self._suggestion_operations_table.c.operation_name == operation.name\n        )\n       .values(\n            operation_name=operation.name,\n            owner_id=resource.owner_id,\n            study_id=resource.study_id,\n            client_id=resource.client_id,\n            operation_number=resource.operation_number,\n            serialized_op=operation.SerializeToString(),\n        )\n    )\n\n    with self._lock:\n      exists = self._connection.execute(exists_query).fetchone()[0]\n      if not exists:\n        raise custom_errors.NotFoundError(\n            'Suggest op %s does not exist.' % operation.name\n        )\n      self._connection.execute(update_query)\n    return resource\n\n  def list_suggestion_operations(\n      self,\n      study_name: str,", "metadata": {"task_id": "google_vizier/146", "ground_truth": "      client_id: str,", "fpath_tuple": ["google_vizier", "vizier", "service", "sql_datastore.py"], "context_start_lineno": 227, "line_no": 410}}
{"prompt": "import json\nimport os\nimport shutil\nimport subprocess\nimport tempfile\nimport unittest\n\nimport numpy as np\nimport torch\nimport transformers\nfrom datasets import load_dataset\nfrom transformers import AutoFeatureExtractor, AutoModelForImageClassification, Trainer, TrainingArguments, pipeline\n\nfrom evaluate import evaluator, load\n\nfrom.utils import slow\n\n\nclass TestEvaluatorTrainerParity(unittest.TestCase):\n    def setUp(self):\n        self.dir_path = tempfile.mkdtemp(\"evaluator_trainer_parity_test\")\n\n        transformers_version = transformers.__version__\n        branch = \"\"\n        if not transformers_version.endswith(\".dev0\"):\n            branch = f\"--branch v{transformers_version}\"\n        subprocess.run(\n            f\"git clone --depth 3 --filter=blob:none --sparse {branch} https://github.com/huggingface/transformers\",\n            shell=True,\n            cwd=self.dir_path,\n        )\n\n    def tearDown(self):\n        shutil.rmtree(self.dir_path, ignore_errors=True)\n\n    def test_text_classification_parity(self):\n        model_name = \"philschmid/tiny-bert-sst2-distilled\"\n\n        subprocess.run(\n            \"git sparse-checkout set examples/pytorch/text-classification\",\n            shell=True,\n            cwd=os.path.join(self.dir_path, \"transformers\"),\n        )\n\n        subprocess.run(\n            f\"python examples/pytorch/text-classification/run_glue.py\"\n            f\" --model_name_or_path {model_name}\"\n            f\" --task_name sst2\"\n            f\" --do_eval\"\n            f\" --max_seq_length 9999999999\"  # rely on tokenizer.model_max_length for max_length\n            f\" --output_dir {os.path.join(self.dir_path, 'textclassification_sst2_transformers')}\"\n            f\" --max_eval_samples 80\",\n            shell=True,\n            cwd=os.path.join(self.dir_path, \"transformers\"),\n        )\n\n        with open(\n            f\"{os.path.join(self.dir_path, 'textclassification_sst2_transformers', 'eval_results.json')}\", \"r\"\n        ) as f:\n            transformers_results = json.load(f)\n\n        eval_dataset = load_dataset(\"glue\", \"sst2\", split=\"validation[:80]\")\n\n        pipe = pipeline(task=\"text-classification\", model=model_name, tokenizer=model_name)\n\n        task_evaluator = evaluator(task=\"text-classification\")\n        evaluator_results = task_evaluator.compute(\n            model_or_pipeline=pipe,\n            data=eval_dataset,\n            metric=\"accuracy\",\n            input_column=\"sentence\",\n            label_column=\"label\",\n            label_mapping={\"negative\": 0, \"positive\": 1},\n            strategy=\"simple\",\n        )\n\n        self.assertEqual(transformers_results[\"eval_accuracy\"], evaluator_results[\"accuracy\"])\n\n    @slow\n    def test_text_classification_parity_two_columns(self):\n        model_name = \"prajjwal1/bert-tiny-mnli\"\n        max_eval_samples = 150\n\n        subprocess.run(\n            \"git sparse-checkout set examples/pytorch/text-classification\",\n            shell=True,\n            cwd=os.path.join(self.dir_path, \"transformers\"),\n        )\n\n        subprocess.run(\n            f\"python examples/pytorch/text-classification/run_glue.py\"\n            f\" --model_name_or_path {model_name}\"\n            f\" --task_name mnli\"\n            f\" --do_eval\"\n            f\" --max_seq_length 256\"\n            f\" --output_dir {os.path.join(self.dir_path, 'textclassification_mnli_transformers')}\"\n            f\" --max_eval_samples {max_eval_samples}\",\n            shell=True,\n            cwd=os.path.join(self.dir_path, \"transformers\"),\n        )\n\n        with open(\n            f\"{os.path.join(self.dir_path, 'textclassification_mnli_transformers', 'eval_results.json')}\", \"r\"\n        ) as f:\n            transformers_results = json.load(f)\n\n        eval_dataset = load_dataset(\"glue\", \"mnli\", split=f\"validation_matched[:{max_eval_samples}]\")\n\n        pipe = pipeline(task=\"text-classification\", model=model_name, tokenizer=model_name, max_length=256)\n\n        task_evaluator = evaluator(task=\"text-classification\")\n        evaluator_results = task_evaluator.compute(\n            model_or_pipeline=pipe,\n            data=eval_dataset,\n            metric=\"accuracy\",\n            input_column=\"premise\",\n            second_input_column=\"hypothesis\",\n            label_column=\"label\",\n            label_mapping={\"LABEL_0\": 0, \"LABEL_1\": 1, \"LABEL_2\": 2},\n        )\n\n        self.assertEqual(transformers_results[\"eval_accuracy\"], evaluator_results[\"accuracy\"])\n\n    def test_image_classification_parity(self):\n        # we can not compare to the Pytorch transformers example, that uses custom preprocessing on the images\n        model_name = \"douwekiela/resnet-18-finetuned-dogfood\"\n        dataset_name = \"beans\"\n        max_eval_samples = 120\n\n        raw_dataset = load_dataset(dataset_name, split=\"validation\")\n        eval_dataset = raw_dataset.select(range(max_eval_samples))\n\n        feature_extractor = AutoFeatureExtractor.from_pretrained(model_name)\n        model = AutoModelForImageClassification.from_pretrained(model_name)\n\n        def collate_fn(examples):", "metadata": {"task_id": "huggingface_evaluate/177", "ground_truth": "            pixel_values = torch.stack(", "fpath_tuple": ["huggingface_evaluate", "tests", "test_trainer_evaluator_parity.py"], "context_start_lineno": 0, "line_no": 136}}
{"prompt": "True,\n                )\n\n\nclass TestEarlyStoppingMixins(unittest.TestCase):\n    def test_early_stopping_is_not_active(self):\n        trainer = FakeTrainerWithEarlyStopping()\n        with self.assertRaises(AttributeError):\n            _ = trainer._early_stopping\n        self.assertFalse(trainer.is_early_stopping_active)\n\n        trainer = FakeTrainerWithEarlyStopping(\n            early_stopping_monitor=\"val_loss\",\n            early_stopping_min_delta=1,\n            early_stopping_patience=0,\n            early_stopping_mode=\"min\",\n        )\n        with self.assertRaises(AttributeError):\n            _ = trainer._early_stopping\n        self.assertFalse(trainer.is_early_stopping_active)\n\n        trainer = FakeTrainerWithEarlyStopping(\n            early_stopping_monitor=\"val_loss\",\n            early_stopping_min_delta=1,\n            early_stopping_patience=0,\n            early_stopping_mode=\"not_valid\",\n        )\n        with self.assertRaises(AttributeError):\n            _ = trainer._early_stopping\n        self.assertFalse(trainer.is_early_stopping_active)\n\n        trainer = FakeTrainerWithEarlyStopping(\n            early_stopping_monitor=\"val_loss\",\n            early_stopping_min_delta=1,\n            early_stopping_patience=0,\n            early_stopping_mode=\"not_valid\",\n        )\n        with self.assertRaises(AttributeError):\n            _ = trainer._early_stopping\n        self.assertFalse(trainer.is_early_stopping_active)\n\n    def test_is_early_stopping_active(self):\n        trainer = FakeTrainerWithEarlyStopping()\n        self.assertFalse(trainer.is_early_stopping_active)\n\n        trainer = FakeTrainerWithEarlyStopping(\n            early_stopping_monitor=\"val_loss\",\n            early_stopping_min_delta=1,\n            early_stopping_patience=2,\n            early_stopping_mode=\"min\",\n        )\n        self.assertTrue(trainer.is_early_stopping_active)\n\n    def test_early_stopping_update_when_not_active(self):\n        validation_metrics = {\"metric1\": 1, \"metric2\": 2}\n        trainer = FakeTrainerWithEarlyStopping()\n        self.assertIsNone(trainer.early_stopping_update(validation_metrics))\n\n    def test_early_stopping_update_non_existing_metric(self):\n        validation_metrics = {\"metric1\": 1, \"metric2\": 2}\n        trainer = FakeTrainerWithEarlyStopping(\n            early_stopping_monitor=\"val_loss\",\n            early_stopping_min_delta=1,\n            early_stopping_patience=2,\n            early_stopping_mode=\"min\",\n        )\n        with self.assertRaises(KeyError):\n            trainer.early_stopping_update(validation_metrics)\n\n    def test_early_stopping_update_ok_min(self):\n        validation_metrics_step1 = {\"metric1\": 1, \"metric2\": 2}\n        validation_metrics_step2 = {\"metric1\": 0.8, \"metric2\": 2}\n        validation_metrics_step3 = {\"metric1\": 0.6, \"metric2\": 2}\n        validation_metrics_step4 = {\"metric1\": 1.1, \"metric2\": 2}\n        validation_metrics_step5 = {\"metric1\": 1, \"metric2\": 2}\n\n        trainer = FakeTrainerWithEarlyStopping(\n            early_stopping_monitor=\"metric1\",\n            early_stopping_min_delta=0,\n            early_stopping_patience=1,\n            early_stopping_mode=\"min\",\n        )\n        improved = trainer.early_stopping_update(validation_metrics_step1)\n        self.assertTrue(improved)\n        improved = trainer.early_stopping_update(validation_metrics_step2)\n        self.assertTrue(improved)\n        improved = trainer.early_stopping_update(validation_metrics_step3)\n        self.assertTrue(improved)\n        improved = trainer.early_stopping_update(validation_metrics_step4)\n        self.assertFalse(improved)\n        self.assertFalse(trainer._early_stopping.should_stop)\n        improved = trainer.early_stopping_update(validation_metrics_step5)\n        self.assertFalse(improved)\n        self.assertTrue(trainer._early_stopping.should_stop)\n\n        trainer = FakeTrainerWithEarlyStopping(\n            early_stopping_monitor=\"metric1\",\n            early_stopping_min_delta=0,\n            early_stopping_patience=2,\n            early_stopping_mode=\"min\",\n        )\n        improved = trainer.early_stopping_update(validation_metrics_step1)\n        self.assertTrue(improved)\n        improved = trainer.early_stopping_update(validation_metrics_step2)\n        self.assertTrue(improved)\n        improved = trainer.early_stopping_update(validation_metrics_step3)\n        self.assertTrue(improved)\n        improved = trainer.early_stopping_update(validation_metrics_step4)\n        self.assertFalse(improved)\n        self.assertFalse(trainer._early_stopping.should_stop)\n        improved = trainer.early_stopping_update(validation_metrics_step5)\n        self.assertFalse(improved)\n        self.assertFalse(trainer._early_stopping.should_stop)\n        improved = trainer.early_stopping_update(validation_metrics_step5)\n        self.assertFalse(improved)\n        self.assertTrue(trainer._early_stopping.should_stop)\n\n    def test_early_stopping_update_ok_max(self):\n        validation_metrics_step1 = {\"metric1\": 1, \"metric2\": 2}\n        validation_metrics_step2 = {\"metric1\": 1.6, \"metric2\": 2}\n        validation_metrics_step3 = {\"metric1\": 1.8, \"metric2\": 2}\n        validation_metrics_step4 = {\"metric1\": 0.1, \"metric2\": 2}\n        validation_metrics_step5 = {\"metric1\": 0.2, \"metric2\": 2}\n\n        trainer = FakeTrainerWithEarlyStopping(\n            early_stopping_monitor=\"metric1\",\n            early_stopping_min_delta=0,\n            early_stopping_patience=2,\n            early_stopping_mode=\"max\",\n        )\n        improved = trainer.early_stopping_update(validation_metrics_step1)\n        self.assertTrue(improved)\n        improved = trainer.early_stopping_update(validation_metrics_step2)\n        self.assertTrue(improved)\n        improved = trainer.early_stopping_update(validation_metrics_step3)\n        self.assertTrue(improved)\n        improved = trainer.early_stopping_update(validation_metrics_step4)\n        self.assertFalse(improved)\n        self.assertFalse(trainer._early_stopping.should_stop)\n        improved = trainer.early_stopping_update(validation_metrics_step5)", "metadata": {"task_id": "awslabs_fortuna/152", "ground_truth": "        self.assertFalse(improved)", "fpath_tuple": ["awslabs_fortuna", "tests", "fortuna", "test_mixin.py"], "context_start_lineno": 119, "line_no": 259}}
{"prompt": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom typing import List, Optional, Union\n\nimport torch\nfrom tensordict import TensorDict\nfrom torch.hub import load_state_dict_from_url\nfrom torch.nn import Identity\n\nfrom torchrl.data.tensor_specs import (\n    CompositeSpec,\n    TensorSpec,\n    UnboundedContinuousTensorSpec,\n)\nfrom torchrl.data.utils import DEVICE_TYPING\nfrom torchrl.envs.transforms.transforms import (\n    CatTensors,\n    Compose,\n    FlattenObservation,\n    ObservationNorm,\n    Resize,\n    ToTensorImage,\n    Transform,\n    UnsqueezeTransform,\n)\n\ntry:\n    from torchvision import models\n\n    _has_tv = True\nexcept ImportError:\n    _has_tv = False\n\ntry:\n    from torchvision.models import ResNet18_Weights, ResNet34_Weights, ResNet50_Weights\n    from torchvision.models._api import WeightsEnum\nexcept ImportError:\n\n    class WeightsEnum:  # noqa: D101\n        # placeholder\n        pass\n\n\nR3M_MODEL_MAP = {\n    \"resnet18\": \"r3m_18\",\n    \"resnet34\": \"r3m_34\",\n    \"resnet50\": \"r3m_50\",\n}\n\n\nclass _R3MNet(Transform):\n\n    inplace = False\n\n    def __init__(self, in_keys, out_keys, model_name, del_keys: bool = True):\n        if not _has_tv:\n            raise ImportError(\n                \"Tried to instantiate R3M without torchvision. Make sure you have \"\n                \"torchvision installed in your environment.\"\n            )\n        self.model_name = model_name\n        if model_name == \"resnet18\":\n            # self.model_name = \"r3m_18\"\n            self.outdim = 512\n            convnet = models.resnet18(None)\n        elif model_name == \"resnet34\":\n            # self.model_name = \"r3m_34\"\n            self.outdim = 512\n            convnet = models.resnet34(None)\n        elif model_name == \"resnet50\":\n            # self.model_name = \"r3m_50\"\n            self.outdim = 2048\n            convnet = models.resnet50(None)\n        else:\n            raise NotImplementedError(\n                f\"model {model_name} is currently not supported by R3M\"\n            )\n        convnet.fc = Identity()\n        super().__init__(in_keys=in_keys, out_keys=out_keys)\n        self.convnet = convnet\n        self.del_keys = del_keys\n\n    def _call(self, tensordict):\n        tensordict_view = tensordict.view(-1)\n        super()._call(tensordict_view)\n        if self.del_keys:\n            tensordict.exclude(*self.in_keys, inplace=True)\n        return tensordict\n\n    @torch.no_grad()\n    def _apply_transform(self, obs: torch.Tensor) -> None:\n        shape = None\n        if obs.ndimension() > 4:\n            shape = obs.shape[:-3]\n            obs = obs.flatten(0, -4)\n        out = self.convnet(obs)\n        if shape is not None:\n            out = out.view(*shape, *out.shape[1:])\n        return out\n\n    def transform_observation_spec(self, observation_spec: TensorSpec) -> TensorSpec:\n        if not isinstance(observation_spec, CompositeSpec):\n            raise ValueError(\"_R3MNet can only infer CompositeSpec\")\n\n        keys = [key for key in observation_spec._specs.keys() if key in self.in_keys]\n        device = observation_spec[keys[0]].device\n        dim = observation_spec[keys[0]].shape[:-3]\n\n        observation_spec = CompositeSpec(observation_spec)", "metadata": {"task_id": "pytorch_rl/110", "ground_truth": "        if self.del_keys:", "fpath_tuple": ["pytorch_rl", "torchrl", "envs", "transforms", "r3m.py"], "context_start_lineno": 0, "line_no": 112}}
{"prompt": "import math\nimport operator\nfrom typing import Optional, Union, Callable, Any\n\nfrom.base import Loader, ILoaderClass\nfrom.utils import keep, check_only\n\nNUMBER_TYPES = (int, float)\nNUMBER_TYPING = Union[int, float]\n\n\ndef numeric(int_ok: bool = True, float_ok: bool = True, inf_ok: bool = True) -> ILoaderClass:\n    if not int_ok and not float_ok:\n        raise ValueError('Either int or float should be allowed.')\n\n    def _load(value) -> NUMBER_TYPING:\n        if isinstance(value, NUMBER_TYPES):\n            if math.isnan(value):\n                raise ValueError('nan is not numeric value')\n            if isinstance(value, int) and not int_ok:\n                raise TypeError('int is not allowed but {actual} found'.format(actual=repr(value)))\n            if isinstance(value, float) and not float_ok:\n                raise TypeError('float is not allowed but {actual} found'.format(actual=repr(value)))\n            if math.isinf(value) and not inf_ok:\n                raise ValueError('inf is not allowed but {actual} found'.format(actual=repr(value)))\n\n            return value\n        else:\n            raise TypeError(\n                'numeric value should be either int, float or str, but {actual} found'.format(\n                    actual=repr(type(value).__name__)\n                )\n            )\n\n    return Loader(_load)\n\n\ndef interval(\n        left: Optional[NUMBER_TYPING] = None,\n        right: Optional[NUMBER_TYPING] = None,\n        left_ok: bool = True,\n        right_ok: bool = True,\n        eps=0.0\n) -> ILoaderClass:\n    if left is None:\n        left = -math.inf\n    if right is None:\n        right = +math.inf\n    if left > right:\n        raise ValueError(\n            \"Left bound should no more than right bound, but {left} > {right}.\".format(\n                left=repr(left), right=repr(right)\n            )\n        )\n    eps = math.fabs(eps)\n\n    def _value_compare_with_eps(a, b) -> int:\n        if math.fabs(a - b) <= eps:\n            return 0\n        elif a < b:\n            return -1\n        else:\n            return 1\n\n    def _load(value) -> NUMBER_TYPING:\n        _left_check = _value_compare_with_eps(value, left)\n        if _left_check < 0:\n            raise ValueError(\n                'value should be no less than {left} but {value} found'.format(left=repr(left), value=repr(value))\n            )\n        elif not left_ok and _left_check == 0:\n            raise ValueError(\n                'value should not be equal to left bound {left} but {value} found'.format(\n                    left=repr(left), value=repr(value)\n                )\n            )\n\n        _right_check = _value_compare_with_eps(value, right)\n        if _right_check > 0:\n            raise ValueError(\n                'value should be no more than {right} but {value} found'.format(right=repr(right), value=repr(value))\n            )\n        elif not right_ok and _right_check == 0:\n            raise ValueError(\n                'value should not be equal to right bound {right} but {value} found'.format(\n                    right=repr(right), value=repr(value)\n                )\n            )\n\n        return value\n\n    return Loader(_load)\n\n\ndef is_negative() -> ILoaderClass:\n    return Loader((lambda x: x < 0, lambda x: ValueError('negative required but {value} found'.format(value=repr(x)))))\n\n\ndef is_positive() -> ILoaderClass:\n    return Loader((lambda x: x > 0, lambda x: ValueError('positive required but {value} found'.format(value=repr(x)))))\n\n\ndef non_negative() -> ILoaderClass:\n    return Loader(\n        (lambda x: x >= 0, lambda x: ValueError('non-negative required but {value} found'.format(value=repr(x))))\n    )\n\n\ndef non_positive() -> ILoaderClass:\n    return Loader(\n        (lambda x: x <= 0, lambda x: ValueError('non-positive required but {value} found'.format(value=repr(x))))\n    )\n\n\ndef negative() -> ILoaderClass:\n    return Loader(lambda x: -x)\n\n\ndef positive() -> ILoaderClass:\n    return Loader(lambda x: +x)\n\n\ndef _math_binary(func: Callable[[Any, Any], Any], attachment) -> ILoaderClass:\n    return Loader(lambda x: func(x, Loader(attachment)(x)))\n\n\ndef plus(addend) -> ILoaderClass:\n    return _math_binary(lambda x, y: x + y, addend)\n\n\ndef minus(subtrahend) -> ILoaderClass:\n    return _math_binary(lambda x, y: x - y, subtrahend)\n\n\ndef minus_with(minuend) -> ILoaderClass:\n    return _math_binary(lambda x, y: y - x, minuend)\n\n\ndef multi(multiplier) -> ILoaderClass:\n    return _math_binary(lambda x, y: x * y, multiplier)\n\n\ndef divide(divisor) -> ILoaderClass:\n    return _math_binary(lambda x, y: x / y, divisor)\n\n\ndef divide_with(dividend) -> ILoaderClass:\n    return _math_binary(lambda x, y: y / x, dividend)\n\n\ndef power(index) -> ILoaderClass:", "metadata": {"task_id": "opendilab_ACE/9", "ground_truth": "    return _math_binary(lambda x, y: x ** y, index)", "fpath_tuple": ["opendilab_ACE", "ding", "utils", "loader", "number.py"], "context_start_lineno": 0, "line_no": 151}}
{"prompt": "from __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport torch\nimport numpy as np\nimport scipy.sparse as sp\n\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch_geometric.data import Data\n\nfrom federatedscope.gfl.model import SAGE_Net\n\"\"\"\nhttps://proceedings.neurips.cc//paper/2021/file/ \\\n34adeb8e3242824038aa65460a47c29e-Paper.pdf\nFedsageplus models from the \"Subgraph Federated Learning with Missing\nNeighbor Generation\" (FedSage+) paper, in NeurIPS'21\nSource: https://github.com/zkhku/fedsage\n\"\"\"\n\n\nclass Sampling(nn.Module):\n    def __init__(self):\n        super(Sampling, self).__init__()\n\n    def forward(self, inputs):\n        rand = torch.normal(0, 1, size=inputs.shape)\n\n        return inputs + rand.to(inputs.device)\n\n\nclass FeatGenerator(nn.Module):\n    def __init__(self, latent_dim, dropout, num_pred, feat_shape):\n        super(FeatGenerator, self).__init__()\n        self.num_pred = num_pred\n        self.feat_shape = feat_shape\n        self.dropout = dropout\n        self.sample = Sampling()\n        self.fc1 = nn.Linear(latent_dim, 256)\n        self.fc2 = nn.Linear(256, 2048)\n        self.fc_flat = nn.Linear(2048, self.num_pred * self.feat_shape)\n\n    def forward(self, x):\n        x = self.sample(x)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = F.dropout(x, self.dropout, training=self.training)\n        x = torch.tanh(self.fc_flat(x))\n\n        return x\n\n\nclass NumPredictor(nn.Module):\n    def __init__(self, latent_dim):\n        self.latent_dim = latent_dim\n        super(NumPredictor, self).__init__()\n        self.reg_1 = nn.Linear(self.latent_dim, 1)\n\n    def forward(self, x):\n        x = F.relu(self.reg_1(x))\n        return x\n\n\n# Mend the graph via NeighGen\nclass MendGraph(nn.Module):\n    def __init__(self, num_pred):\n        super(MendGraph, self).__init__()\n        self.num_pred = num_pred\n        for param in self.parameters():\n            param.requires_grad = False\n\n    def mend_graph(self, x, edge_index, pred_degree, gen_feats):\n        device = gen_feats.device\n        num_node, num_feature = x.shape\n        new_edges = []\n        gen_feats = gen_feats.view(-1, self.num_pred, num_feature)\n\n        if pred_degree.device.type!= 'cpu':\n            pred_degree = pred_degree.cpu()\n        pred_degree = torch._cast_Int(torch.round(pred_degree)).detach()\n        x = x.detach()\n        fill_feats = torch.vstack((x, gen_feats.view(-1, num_feature)))\n\n        for i in range(num_node):\n            for j in range(min(self.num_pred, max(0, pred_degree[i]))):\n                new_edges.append(\n                    np.asarray([i, num_node + i * self.num_pred + j]))\n\n        new_edges = torch.tensor(np.asarray(new_edges).reshape((-1, 2)),\n                                 dtype=torch.int64).T\n        new_edges = new_edges.to(device)\n        if len(new_edges) > 0:\n            fill_edges = torch.hstack((edge_index, new_edges))\n        else:\n            fill_edges = torch.clone(edge_index)\n        return fill_feats, fill_edges\n\n    def forward(self, x, edge_index, pred_missing, gen_feats):\n        fill_feats, fill_edges = self.mend_graph(x, edge_index, pred_missing,\n                                                 gen_feats)\n\n        return fill_feats, fill_edges\n\n\nclass LocalSage_Plus(nn.Module):\n    def __init__(self,\n                 in_channels,\n                 out_channels,\n                 hidden,\n                 gen_hidden,\n                 dropout=0.5,\n                 num_pred=5):\n        super(LocalSage_Plus, self).__init__()\n\n        self.encoder_model = SAGE_Net(in_channels=in_channels,\n                                      out_channels=gen_hidden,\n                                      hidden=hidden,\n                                      max_depth=2,\n                                      dropout=dropout)\n        self.reg_model = NumPredictor(latent_dim=gen_hidden)\n        self.gen = FeatGenerator(latent_dim=gen_hidden,\n                                 dropout=dropout,\n                                 num_pred=num_pred,\n                                 feat_shape=in_channels)\n        self.mend_graph = MendGraph(num_pred)\n\n        self.classifier = SAGE_Net(in_channels=in_channels,\n                                   out_channels=out_channels,\n                                   hidden=hidden,\n                                   max_depth=2,\n                                   dropout=dropout)\n\n    def forward(self, data):\n        x = self.encoder_model(data)\n        degree = self.reg_model(x)\n        gen_feat = self.gen(x)\n        mend_feats, mend_edge_index = self.mend_graph(data.x, data.edge_index,\n                                                      degree, gen_feat)\n        nc_pred = self.classifier(\n            Data(x=mend_feats, edge_index=mend_edge_index))\n        return degree, gen_feat, nc_pred[:data.num_nodes]\n\n    def inference(self, impared_data, raw_data):\n        x = self.encoder_model(impared_data)\n        degree = self.reg_model(x)\n        gen_feat = self.gen(x)\n        mend_feats, mend_edge_index = self.mend_graph(raw_data.x,\n                                                      raw_data.edge_index,\n                                                      degree, gen_feat)\n        nc_pred = self.classifier(\n            Data(x=mend_feats, edge_index=mend_edge_index))", "metadata": {"task_id": "alibaba_FederatedScope/140", "ground_truth": "        return degree, gen_feat, nc_pred[:raw_data.num_nodes]", "fpath_tuple": ["alibaba_FederatedScope", "federatedscope", "gfl", "model", "fedsageplus.py"], "context_start_lineno": 0, "line_no": 152}}
{"prompt": "            and (0 in coverage_sets[1])\n        )\n\n    def test_adaptive_prediction_conformal_classifier(self):\n        val_probs = np.array([[0.1, 0.7, 0.2], [0.5, 0.4, 0.1], [0.15, 0.6, 0.35]])\n        val_targets = np.array([1, 1, 2])\n        test_probs = np.array([[0.2, 0.5, 0.3], [0.6, 0.01, 0.39]])\n\n        conformal = AdaptivePredictionConformalClassifier()\n        scores = conformal.score(val_probs, val_targets)\n        assert jnp.allclose(scores, jnp.array([0.7, 0.9, 0.95]))\n        quantile = conformal.quantile(val_probs, val_targets, 0.5, scores=scores)\n        assert (quantile > 0.9) * (quantile < 0.95)\n        coverage_sets = conformal.conformal_set(\n            val_probs, test_probs, val_targets, quantile=quantile\n        )\n        assert np.allclose(coverage_sets[0], [1, 2, 0])\n        assert np.allclose(coverage_sets[1], [0, 2])\n\n    def test_quantile_conformal_regressor(self):\n        n_val_inputs = 100\n        n_test_inputs = 100\n        val_lower_quantiles = np.random.normal(size=n_val_inputs)\n        val_upper_quantiles = np.random.normal(size=n_val_inputs)\n        test_lower_quantiles = np.random.normal(size=n_test_inputs)\n        test_upper_quantiles = np.random.normal(size=n_test_inputs)\n        val_targets = np.random.normal(size=(n_val_inputs, 1))\n\n        conformal = QuantileConformalRegressor()\n        scores = conformal.score(val_lower_quantiles, val_upper_quantiles, val_targets)\n        assert scores.shape == (n_val_inputs,)\n        quantile = conformal.quantile(\n            val_lower_quantiles, val_upper_quantiles, val_targets, 0.05, scores=scores\n        )\n        assert jnp.min(scores) <= quantile <= jnp.max(scores)\n        assert jnp.array([quantile]).shape == (1,)\n        coverage_sets = conformal.conformal_interval(\n            val_lower_quantiles,\n            val_upper_quantiles,\n            test_upper_quantiles,\n            test_lower_quantiles,\n            val_targets,\n            0.05,\n            quantile=quantile,\n        )\n        assert (\n            coverage_sets[:, 0].shape == coverage_sets[:, 1].shape == (n_test_inputs,)\n        )\n        assert (coverage_sets[:, 0] < coverage_sets[:, 1]).all()\n\n    def test_one_dimensional_uncertainty_conformal_regressor(self):\n        n_val_inputs = 100\n        n_test_inputs = 10\n        val_preds = np.random.normal(size=n_val_inputs)[:, None]\n        val_uncertainties = np.exp(np.random.normal(size=n_val_inputs))[:, None]\n        test_preds = np.random.normal(size=n_test_inputs)[:, None]\n        test_uncertainties = np.exp(np.random.normal(size=n_test_inputs))[:, None]\n        val_targets = np.random.normal(size=(n_val_inputs, 1))\n\n        conformal = OneDimensionalUncertaintyConformalRegressor()\n        scores = conformal.score(val_preds, val_uncertainties, val_targets)\n        assert (scores >= 0).all()\n        assert scores.shape == (n_val_inputs,)\n        quantile = conformal.quantile(\n            val_preds=val_preds,\n            val_uncertainties=val_uncertainties,\n            val_targets=val_targets,\n            error=0.05,\n            scores=scores,\n        )\n        assert jnp.min(scores) <= quantile <= jnp.max(scores)\n        assert jnp.array([quantile]).shape == (1,)\n        coverage_sets = conformal.conformal_interval(\n            val_preds=val_preds,\n            val_uncertainties=val_uncertainties,\n            test_preds=test_preds,\n            test_uncertainties=test_uncertainties,\n            val_targets=val_targets,\n            error=0.05,\n            quantile=quantile,\n        )\n        assert (\n            coverage_sets[:, 0].shape == coverage_sets[:, 1].shape == (n_test_inputs,)\n        )\n        assert (coverage_sets[:, 0] < coverage_sets[:, 1]).all()\n\n    def test_cvplus_conformal_regressor(self):\n        k1, k2, k3 = 100, 101, 102\n        m = 99\n        cross_val_outputs = [\n            np.random.normal(size=(k1, 1)),\n            np.random.normal(size=(k2, 1)),\n            np.random.normal(size=(k3, 1)),\n        ]\n        cross_val_targets = [\n            np.random.normal(size=(k1, 1)),\n            np.random.normal(size=(k2, 1)),\n            np.random.normal(size=(k3, 1)),\n        ]\n        cross_test_outputs = [\n            np.random.normal(size=(m, 1)),\n            np.random.normal(size=(m, 1)),\n            np.random.normal(size=(m, 1)),\n        ]\n\n        interval = CVPlusConformalRegressor().conformal_interval(cross_val_outputs, cross_val_targets, cross_test_outputs, 0.05)\n        assert interval.ndim == 2\n        assert interval.shape[0] == m\n        assert interval.shape[1] == 2\n        assert (interval[:, 0] < interval[:, 1]).all()\n        assert len(np.unique(interval[:, 0])) > 1\n        assert len(np.unique(interval[:, 1])) > 1\n\n    def test_jackknifeplus_conformal_regressor(self):\n        n = 100\n        m = 99\n        loo_val_outputs = np.random.normal(size=(n, 1))\n        loo_val_targets = np.random.normal(size=(n, 1))\n        loo_test_outputs = np.random.normal(size=(n, m, 1))\n\n        interval = JackknifePlusConformalRegressor().conformal_interval(loo_val_outputs, loo_val_targets,\n                                                                        loo_test_outputs, 0.05)\n        assert interval.ndim == 2\n        assert interval.shape[0] == m\n        assert interval.shape[1] == 2\n        assert (interval[:, 0] < interval[:, 1]).all()\n        assert len(np.unique(interval[:, 0])) > 1\n        assert len(np.unique(interval[:, 1])) > 1\n\n    def test_jackknife_minmax_conformal_regressor(self):\n        n = 100\n        m = 99", "metadata": {"task_id": "awslabs_fortuna/49", "ground_truth": "        loo_val_outputs = np.random.normal(size=(n, 1))", "fpath_tuple": ["awslabs_fortuna", "tests", "fortuna", "test_conformers.py"], "context_start_lineno": 34, "line_no": 166}}
{"prompt": "):\n    \"\"\"\n    Overview:\n        Interaction master end\n    \"\"\"\n\n    def __init__(\n        self,\n        host: Optional[str] = None,\n        port: Optional[int] = None,\n        heartbeat_tolerance: Optional[float] = None,\n        heartbeat_check_span: Optional[float] = None,\n        request_retries: Optional[int] = None,\n        request_retry_waiting: Optional[float] = None,\n        channel: Optional[int] = None,\n        my_address: Optional[str] = None\n    ):\n        \"\"\"\n        Overview:\n            Constructor of Master\n        Arguments:\n            - host (:obj:`Optional[str]`): Host of the master server, based on flask (None means `0.0.0.0`)\n            - port (:obj:`Optional[int]`): Port of the master server, based on flask (None means `7235`)\n            - heartbeat_tolerance: (:obj:`Optional[float]`): Max time tolerance of the heartbeat missing (None means \\\n                `15.0`, minimum is `0.2`, unit: second)\n            - heartbeat_check_span: (:obj:`Optional[float]`): Timespan between the heartbeat status check (None means \\\n                `1.0`, minimum is `0.1`, unit: second)\n            - request_retries (:obj:`Optional[int]`): Max times for request retries (None means `5`)\n            - request_retry_waiting (:obj:`Optional[float]`): Sleep time before requests' retrying (None means `1.0`, \\\n                unit: second)\n            - channel (:obj:`Optional[int]`): Channel id for the master client, please make sure that channel id is \\\n                equal to the slave client's channel id, or the connection cannot be established. (None means `0`, \\\n                but 0 channel is not recommended to be used in production)\n            - my_address (:obj:`Optional[str]`): The address of current server (None will grep local ip automatically, \\\n                this address will be used when connect to slave, the slave's request will be send to this address, \\\n                **so please make sure the address can be achieved by slave**)\n        \"\"\"\n        # server part\n        self.__host = host or GLOBAL_HOST\n        self.__port = port or DEFAULT_MASTER_PORT\n        self.__flask_app_value = None\n        self.__run_app_thread = Thread(target=self.__run_app, name='master_run_app')\n\n        # heartbeat part\n        self.__heartbeat_tolerance = max(heartbeat_tolerance or DEFAULT_HEARTBEAT_TOLERANCE, MIN_HEARTBEAT_SPAN)\n        self.__heartbeat_check_span = max(\n            heartbeat_check_span or DEFAULT_HEARTBEAT_CHECK_SPAN, MIN_HEARTBEAT_CHECK_SPAN\n        )\n        self.__heartbeat_check_thread = Thread(target=self.__heartbeat_check, name='master_heartbeat')\n        self.__request_retries = max(request_retries or DEFAULT_REQUEST_RETRIES, 0)\n        self.__request_retry_waiting = max(request_retry_waiting or DEFAULT_REQUEST_RETRY_WAITING, 0.0)\n\n        # self-connection part\n        self.__self_http_engine = get_http_engine_class(\n            headers={\n                'Token': lambda: self.__self_token,\n            },\n            http_error_gene=get_master_exception_by_error,\n            # )()('localhost', self.__port, False)\n        )()(self.__host, self.__port, False)  # TODO: Confirm how to ping itself\n        self.__self_token = random_token()\n\n        # slave-connection part\n        self.__channel = channel or DEFAULT_CHANNEL\n        self.__my_address = my_address or str(\n            URLObject().with_scheme('http').with_hostname(get_host_ip()).with_port(self.__port)\n        )\n\n        # slaves part\n        self.__slaves = {}  # name --> (token, slave_connection)\n        self.__token_slaves = {}  # token --> (name, slave_connection)\n        self.__slave_last_heartbeat = {}  # name --> last_heartbeat\n        self.__slave_lock = Lock()\n\n        # task part\n        self.__task_result_queue = Queue()\n        self.__task_result_process_thread = Thread(target=self.__task_result_process, name='master_task_result')\n\n        # global part\n        self.__shutdown_event = Event()\n        self.__lock = Lock()\n\n    # slave connection\n    def __connection_open(self, name: str, token: str, connection: SlaveConnectionProxy):\n        with self.__slave_lock:\n            self.__slaves[name] = (token, connection)\n            self.__token_slaves[token] = (name, connection)\n            self.__slave_last_heartbeat[name] = time.time()\n\n    # noinspection PyUnusedLocal\n    def __connection_close(self, name: str, connection: Optional[SlaveConnectionProxy] = None):\n        with self.__slave_lock:\n            token, _conn = self.__slaves[name]\n            connection = connection or _conn\n            del self.__slaves[name]\n            del self.__token_slaves[token]\n            del self.__slave_last_heartbeat[name]\n\n    # server part\n    def __generate_app(self):\n        app = Flask(__name__)\n\n        # self apis\n        app.route('/ping', methods=['GET'])(self.__check_self_request(self.__self_ping))\n        app.route('/shutdown', methods=['DELETE'])(self.__check_self_request(self.__self_shutdown))\n\n        # slave apis\n        app.route('/slave/heartbeat', methods=['GET'])(self.__check_slave_request(self.__heartbeat))\n        app.route(\n            '/slave/task/complete', methods=['PUT']\n        )(self.__check_slave_request(self.__check_task_info(self.__task_complete)))\n        app.route(\n            '/slave/task/fail', methods=['PUT']\n        )(self.__check_slave_request(self.__check_task_info(self.__task_fail)))\n\n        return app\n\n    def __flask_app(self) -> Flask:\n        return self.__flask_app_value or self.__generate_app()\n\n    def __run_app(self):\n        self.__flask_app().run(\n            host=self.__host,\n            port=self.__port,\n        )\n\n    # both method checkers\n    def __check_shutdown(self, func: Callable[[], Any]) -> Callable[[], Any]:\n\n        @wraps(func)\n        def _func():\n            if self.__shutdown_event.is_set():\n                return failure_response(\n                    code=MasterErrorCode.SYSTEM_SHUTTING_DOWN, message='System has already been shutting down.'\n                ), 401\n            else:\n                return func()\n\n        return _func\n\n    # server method checkers (self)\n    # noinspection DuplicatedCode\n    def __check_self_request(self, func: Callable[[], Any]) -> Callable[[], Any]:\n        return self.__check_shutdown(self.__check_master_token(func))\n\n    def __check_master_token(self, func: Callable[[], Any]) -> Callable[[], Any]:\n\n        @wraps(func)", "metadata": {"task_id": "opendilab_ACE/2", "ground_truth": "        def _func():", "fpath_tuple": ["opendilab_ACE", "ding", "interaction", "master", "master.py"], "context_start_lineno": 24, "line_no": 172}}
{"prompt": "# Copyright 2020 The HuggingFace Evaluate Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\" BLEURT metric. \"\"\"\n\nimport os\n\nimport datasets\nfrom bleurt import score  # From: git+https://github.com/google-research/bleurt.git\n\nimport evaluate\n\n\nlogger = evaluate.logging.get_logger(__name__)\n\n\n_CITATION = \"\"\"\\\n@inproceedings{bleurt,\n  title={BLEURT: Learning Robust Metrics for Text Generation},\n  author={Thibault Sellam and Dipanjan Das and Ankur P. Parikh},\n  booktitle={ACL},\n  year={2020},\n  url={https://arxiv.org/abs/2004.04696}\n}\n\"\"\"\n\n_DESCRIPTION = \"\"\"\\\nBLEURT a learnt evaluation metric for Natural Language Generation. It is built using multiple phases of transfer learning starting from a pretrained BERT model (Devlin et al. 2018)\nand then employing another pre-training phrase using synthetic data. Finally it is trained on WMT human annotations. You may run BLEURT out-of-the-box or fine-tune\nit for your specific application (the latter is expected to perform better).\n\nSee the project's README at https://github.com/google-research/bleurt#readme for more information.\n\"\"\"\n\n_KWARGS_DESCRIPTION = \"\"\"\nBLEURT score.\n\nArgs:\n    `predictions` (list of str): prediction/candidate sentences\n    `references` (list of str): reference sentences\n    `checkpoint` BLEURT checkpoint. Will default to BLEURT-tiny if None.\n\nReturns:\n   'scores': List of scores.", "metadata": {"task_id": "huggingface_evaluate/64", "ground_truth": "Examples:", "fpath_tuple": ["huggingface_evaluate", "metrics", "bleurt", "bleurt.py"], "context_start_lineno": 0, "line_no": 54}}
{"prompt": "traj_buffer_length = self._traj_len if self._traj_len!= INF else None\n            self._traj_buffer = {\n                env_id: [TrajBuffer(self._traj_buffer_length) for _ in range(len(policy))]\n                for env_id in range(self._env_num)\n            }\n        # self._first_update_policy = True\n\n        self._episode_result = [[] for k in range(self._env_num)]\n        self._obs_pool = CachePool('obs', self._env_num)\n        self._policy_output_pool = CachePool('policy_output', self._env_num)\n        self._total_step = 0\n        self._total_sample = 0\n        self._total_episode = 0\n\n    @property\n    def policy(self) -> List[Policy]:\n        return self._policy\n\n    # override\n    @policy.setter\n    def policy(self, _policy: List[Policy]) -> None:\n        self._policy = _policy\n        self._n_episode = _policy[0].get_attribute('cfg').collect.get('n_episode', None)\n        self._n_sample = _policy[0].get_attribute('cfg').collect.get('n_sample', None)\n        assert any(\n            [t is None for t in [self._n_sample, self._n_episode]]\n        ), \"n_episode/n_sample in policy cfg can't be not None at the same time\"\n        # TODO(nyz) the same definition of traj_len in serial and parallel\n        if self._n_episode is not None:\n            self._traj_len = INF\n        elif self._n_sample is not None:\n            self._traj_len = self._n_sample\n\n    @property\n    def env_manager(self, _env_manager) -> None:\n        self._env_manager = _env_manager\n\n    # override\n    @env_manager.setter\n    def env_manager(self, _env_manager: BaseEnvManager) -> None:\n        self._env_manager = _env_manager\n        self._env_manager.launch()\n        self._env_num = self._env_manager.env_num\n        self._predefined_episode_count = self._env_num * self._env_manager._episode_num\n\n    def _setup_env_manager(self, cfg: EasyDict) -> BaseEnvManager:\n        env_fn, collector_env_cfg, evaluator_env_cfg = get_vec_env_setting(cfg)\n        if self._eval_flag:\n            env_cfg = evaluator_env_cfg\n        else:\n            env_cfg = collector_env_cfg\n        env_manager = create_env_manager(cfg.manager, [partial(env_fn, cfg=c) for c in env_cfg])\n        return env_manager\n\n    def _start_thread(self) -> None:\n        # evaluator doesn't need to update policy periodically, only updating policy when starts\n        if not self._eval_flag:\n            self._update_policy_thread.start()\n\n    def _join_thread(self) -> None:\n        if not self._eval_flag:\n            self._update_policy_thread.join()\n            del self._update_policy_thread\n\n    # override\n    def close(self) -> None:\n        if self._end_flag:\n            return\n        self._end_flag = True\n        time.sleep(1)\n        if hasattr(self, '_env_manager'):\n            self._env_manager.close()\n        self._join_thread()\n\n    # override\n    def _policy_inference(self, obs: Dict[int, Any]) -> Dict[int, Any]:\n        env_ids = list(obs.keys())\n        if len(self._policy) > 1:\n            assert not self._eval_flag\n            obs = [{id: obs[id][i] for id in env_ids} for i in range(len(self._policy))]\n        else:\n            assert self._eval_flag\n            obs = [obs]\n        self._obs_pool.update(obs)\n        policy_outputs = []\n        for i in range(len(self._policy)):\n            if self._eval_flag:\n                policy_output = self._policy[i].forward(obs[i])\n            else:\n                policy_output = self._policy[i].forward(obs[i], **self._cfg.collect_setting)\n            policy_outputs.append(policy_output)\n        self._policy_output_pool.update(policy_outputs)\n        actions = {}\n        for env_id in env_ids:\n            action = [policy_outputs[i][env_id]['action'] for i in range(len(self._policy))]\n            action = torch.stack(action).squeeze()\n            actions[env_id] = action\n        return actions\n\n    # override\n    def _env_step(self, actions: Dict[int, Any]) -> Dict[int, Any]:\n        return self._env_manager.step(actions)\n\n    # override\n    def _process_timestep(self, timestep: Dict[int, namedtuple]) -> None:\n        for env_id, t in timestep.items():\n            if t.info.get('abnormal', False):\n                # If there is an abnormal timestep, reset all the related variables, also this env has been reset\n                for c in self._traj_buffer[env_id]:\n                    c.clear()\n                self._obs_pool.reset(env_id)\n                self._policy_output_pool.reset(env_id)\n                for p in self._policy:\n                    p.reset([env_id])\n                continue\n            self._total_step += 1\n            t = [BaseEnvTimestep(t.obs[i], t.reward[i], t.done, t.info) for i in range(len(self._policy))]\n            if t[0].done:\n                self._total_episode += 1\n            if not self._eval_flag:\n                for i in range(len(self._policy)):\n                    if self._policy_is_active[i]:\n                        # Only active policy will store transition into replay buffer.\n                        transition = self._policy[i].process_transition(\n                            self._obs_pool[env_id][i], self._policy_output_pool[env_id][i], t[i]\n                        )\n                        self._traj_buffer[env_id][i].append(transition)\n                full_indices = []\n                for i in range(len(self._traj_buffer[env_id])):\n                    if len(self._traj_buffer[env_id][i]) == self._traj_len:\n                        full_indices.append(i)\n                if t[0].done or len(full_indices) > 0:\n                    for i in full_indices:\n                        train_sample = self._policy[i].get_train_sample(self._traj_buffer[env_id][i])\n                        for s in train_sample:\n                            s = self._compressor(s)\n                            self._total_sample += 1\n                            metadata = self._get_metadata(s, env_id)\n                            self.send_stepdata(metadata['data_id'], s)\n                            self.send_metadata(metadata)\n                        self._traj_buffer[env_id][i].clear()\n            if t[0].done:\n                # env reset is done by env_manager automatically\n                self._obs_pool.reset(env_id)\n                self._policy_output_pool.reset(env_id)\n                for p in self._policy:\n                    p.reset([env_id])\n                reward = t[0].info['final_eval_reward']\n                # Only left player's reward will be recorded.\n                left_reward = reward[0]", "metadata": {"task_id": "opendilab_ACE/173", "ground_truth": "                if isinstance(left_reward, torch.Tensor):", "fpath_tuple": ["opendilab_ACE", "ding", "worker", "collector", "marine_parallel_collector.py"], "context_start_lineno": 73, "line_no": 223}}
{"prompt": "from enum import unique, IntEnum\nfrom threading import Lock\nfrom typing import Mapping, Any, Optional, Callable\nfrom uuid import UUID, uuid4\n\nimport enum_tools\nimport requests\nfrom requests import RequestException\n\nfrom.base import _BEFORE_HOOK_TYPE, _AFTER_HOOK_TYPE, _ERROR_HOOK_TYPE\nfrom..base import HttpEngine, get_values_from_response, default_func\n\n\n@enum_tools.documentation.document_enum\n@unique\nclass TaskResultType(IntEnum):\n    \"\"\"\n    Overview:\n        Types of the task result\n    \"\"\"\n    COMPLETED = 1  # doc: Task complete without error\n    FAILED = 2  # doc: Task end with error\n\n\n@enum_tools.documentation.document_enum\n@unique\nclass TaskStatus(IntEnum):\n    \"\"\"\n    Overview:\n        Status of a task\n    \"\"\"\n    IDLE = 0x00  # doc: Task not started, waiting for awake\n\n    STARTING = 0x11  # doc: Task is starting, but initialization is not completed.", "metadata": {"task_id": "opendilab_ACE/189", "ground_truth": "    STARTED = 0x12  # doc: Task started, initialization is completed.", "fpath_tuple": ["opendilab_ACE", "ding", "interaction", "master", "task.py"], "context_start_lineno": 0, "line_no": 34}}
{"prompt": "# coding=utf-8\n# Copyright 2022 HuggingFace Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport tempfile\nimport unittest\n\nfrom diffusers import (\n    DDIMScheduler,\n    DDPMScheduler,\n    DPMSolverMultistepScheduler,\n    EulerAncestralDiscreteScheduler,\n    EulerDiscreteScheduler,\n    PNDMScheduler,\n    logging,\n)\nfrom diffusers.configuration_utils import ConfigMixin, register_to_config\nfrom diffusers.utils.testing_utils import CaptureLogger\n\n\nclass SampleObject(ConfigMixin):\n    config_name = \"config.json\"\n\n    @register_to_config\n    def __init__(\n        self,\n        a=2,\n        b=5,\n        c=(2, 5),\n        d=\"for diffusion\",\n        e=[1, 3],\n    ):\n        pass\n\n\nclass SampleObject2(ConfigMixin):\n    config_name = \"config.json\"\n\n    @register_to_config\n    def __init__(\n        self,\n        a=2,\n        b=5,\n        c=(2, 5),\n        d=\"for diffusion\",\n        f=[1, 3],\n    ):\n        pass\n\n\nclass SampleObject3(ConfigMixin):\n    config_name = \"config.json\"\n\n    @register_to_config\n    def __init__(\n        self,\n        a=2,\n        b=5,\n        c=(2, 5),\n        d=\"for diffusion\",\n        e=[1, 3],\n        f=[1, 3],\n    ):\n        pass\n\n\nclass ConfigTester(unittest.TestCase):\n    def test_load_not_from_mixin(self):\n        with self.assertRaises(ValueError):\n            ConfigMixin.load_config(\"dummy_path\")\n\n    def test_register_to_config(self):\n        obj = SampleObject()\n        config = obj.config\n        assert config[\"a\"] == 2\n        assert config[\"b\"] == 5\n        assert config[\"c\"] == (2, 5)\n        assert config[\"d\"] == \"for diffusion\"\n        assert config[\"e\"] == [1, 3]\n\n        # init ignore private arguments\n        obj = SampleObject(_name_or_path=\"lalala\")\n        config = obj.config\n        assert config[\"a\"] == 2\n        assert config[\"b\"] == 5\n        assert config[\"c\"] == (2, 5)\n        assert config[\"d\"] == \"for diffusion\"\n        assert config[\"e\"] == [1, 3]\n\n        # can override default\n        obj = SampleObject(c=6)\n        config = obj.config\n        assert config[\"a\"] == 2\n        assert config[\"b\"] == 5\n        assert config[\"c\"] == 6\n        assert config[\"d\"] == \"for diffusion\"\n        assert config[\"e\"] == [1, 3]\n\n        # can use positional arguments.\n        obj = SampleObject(1, c=6)\n        config = obj.config\n        assert config[\"a\"] == 1\n        assert config[\"b\"] == 5\n        assert config[\"c\"] == 6\n        assert config[\"d\"] == \"for diffusion\"\n        assert config[\"e\"] == [1, 3]\n\n    def test_save_load(self):\n        obj = SampleObject()\n        config = obj.config\n\n        assert config[\"a\"] == 2\n        assert config[\"b\"] == 5\n        assert config[\"c\"] == (2, 5)\n        assert config[\"d\"] == \"for diffusion\"\n        assert config[\"e\"] == [1, 3]\n\n        with tempfile.TemporaryDirectory() as tmpdirname:\n            obj.save_config(tmpdirname)\n            new_obj = SampleObject.from_config(SampleObject.load_config(tmpdirname))\n            new_config = new_obj.config\n\n        # unfreeze configs\n        config = dict(config)\n        new_config = dict(new_config)\n\n        assert config.pop(\"c\") == (2, 5)  # instantiated as tuple\n        assert new_config.pop(\"c\") == [2, 5]  # saved & loaded as list because of json\n        assert config == new_config\n\n    def test_load_ddim_from_pndm(self):\n        logger = logging.get_logger(\"diffusers.configuration_utils\")\n\n        with CaptureLogger(logger) as cap_logger:\n            ddim = DDIMScheduler.from_pretrained(\n                \"hf-internal-testing/tiny-stable-diffusion-torch\", subfolder=\"scheduler\"\n            )\n\n        assert ddim.__class__ == DDIMScheduler\n        # no warning should be thrown\n        assert cap_logger.out == \"\"\n\n    def test_load_euler_from_pndm(self):\n        logger = logging.get_logger(\"diffusers.configuration_utils\")\n\n        with CaptureLogger(logger) as cap_logger:\n            euler = EulerDiscreteScheduler.from_pretrained(\n                \"hf-internal-testing/tiny-stable-diffusion-torch\", subfolder=\"scheduler\"\n            )\n\n        assert euler.__class__ == EulerDiscreteScheduler\n        # no warning should be thrown\n        assert cap_logger.out == \"\"\n\n    def test_load_euler_ancestral_from_pndm(self):\n        logger = logging.get_logger(\"diffusers.configuration_utils\")\n\n        with CaptureLogger(logger) as cap_logger:\n            euler = EulerAncestralDiscreteScheduler.from_pretrained(\n                \"hf-internal-testing/tiny-stable-diffusion-torch\", subfolder=\"scheduler\"\n            )\n\n        assert euler.__class__ == EulerAncestralDiscreteScheduler\n        # no warning should be thrown\n        assert cap_logger.out == \"\"\n\n    def test_load_pndm(self):\n        logger = logging.get_logger(\"diffusers.configuration_utils\")\n\n        with CaptureLogger(logger) as cap_logger:\n            pndm = PNDMScheduler.from_pretrained(\n                \"hf-internal-testing/tiny-stable-diffusion-torch\", subfolder=\"scheduler\"\n            )\n\n        assert pndm.__class__ == PNDMScheduler\n        # no warning should be thrown\n        assert cap_logger.out == \"\"\n\n    def test_overwrite_config_on_load(self):\n        logger = logging.get_logger(\"diffusers.configuration_utils\")\n\n        with CaptureLogger(logger) as cap_logger:\n            ddpm = DDPMScheduler.from_pretrained(\n                \"hf-internal-testing/tiny-stable-diffusion-torch\",\n                subfolder=\"scheduler\",\n                prediction_type=\"sample\",\n                beta_end=8,\n            )\n\n        with CaptureLogger(logger) as cap_logger_2:", "metadata": {"task_id": "huggingface_diffusers/114", "ground_truth": "            ddpm_2 = DDPMScheduler.from_pretrained(\"google/ddpm-celebahq-256\", beta_start=88)", "fpath_tuple": ["huggingface_diffusers", "tests", "test_config.py"], "context_start_lineno": 0, "line_no": 201}}
{"prompt": "import unittest\n\nimport jax.numpy as jnp\nfrom flax.core import FrozenDict\nfrom jax import random\n\nfrom fortuna.data.loader import DataLoader, InputsLoader\nfrom fortuna.model.mlp import MLP\nfrom fortuna.model.model_manager.classification import \\\n    ClassificationModelManager\nfrom fortuna.model.model_manager.regression import RegressionModelManager\nfrom fortuna.output_calibrator.output_calib_manager.base import \\\n    OutputCalibManager\nfrom fortuna.prob_model.likelihood.classification import \\\n    ClassificationLikelihood\nfrom fortuna.prob_model.likelihood.regression import RegressionLikelihood\nfrom fortuna.prob_output_layer.classification import \\\n    ClassificationProbOutputLayer\nfrom fortuna.prob_output_layer.regression import RegressionProbOutputLayer\nfrom fortuna.utils.random import RandomNumberGenerator\nfrom tests.make_data import (make_array_random_data,\n                             make_generator_fun_random_data)\n\n\nclass TestLikelihoods(unittest.TestCase):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.shape_inputs = (3,)\n        self.n_inputs = 10\n        self.output_dim = 2\n        self.n_batches = 2\n        self.batch_size = 3\n        self.rng = random.PRNGKey(0)\n        rng = RandomNumberGenerator(seed=0)\n        reg_prob_output_layer = RegressionProbOutputLayer()\n        reg_prob_output_layer.rng = rng\n        self.reg_lik = RegressionLikelihood(\n            model_manager=RegressionModelManager(\n                model=MLP(output_dim=self.output_dim),\n                likelihood_log_variance_model=MLP(output_dim=self.output_dim),\n            ),\n            output_calib_manager=OutputCalibManager(output_calibrator=None),\n            prob_output_layer=reg_prob_output_layer,\n        )\n        self.reg_lik.rng = rng\n        class_prob_output_layer = ClassificationProbOutputLayer()\n        class_prob_output_layer.rng = rng\n        self.class_lik = ClassificationLikelihood(\n            model_manager=ClassificationModelManager(\n                model=MLP(output_dim=self.output_dim)\n            ),\n            output_calib_manager=OutputCalibManager(output_calibrator=None),\n            prob_output_layer=class_prob_output_layer,\n        )\n        self.class_lik.rng = rng\n\n        self.reg_data_arr = DataLoader.from_array_data(\n            make_array_random_data(\n                n_data=self.n_inputs,\n                shape_inputs=self.shape_inputs,\n                output_dim=self.output_dim,\n                output_type=\"continuous\",\n            )\n        )\n        self.reg_inputs_arr = InputsLoader.from_data_loader(self.reg_data_arr)\n\n        self.reg_data_gen_fun = DataLoader.from_callable_iterable(\n            make_generator_fun_random_data(\n                n_batches=self.n_batches,\n                batch_size=self.batch_size,\n                shape_inputs=self.shape_inputs,\n                output_dim=self.output_dim,\n                output_type=\"continuous\",\n            )\n        )\n        self.reg_inputs_gen_fun = InputsLoader.from_data_loader(self.reg_data_gen_fun)\n\n        self.class_data_arr = DataLoader.from_array_data(\n            make_array_random_data(\n                n_data=self.n_inputs,\n                shape_inputs=self.shape_inputs,\n                output_dim=self.output_dim,\n                output_type=\"discrete\",\n            )\n        )\n        self.class_inputs_arr = InputsLoader.from_data_loader(self.class_data_arr)\n\n        self.class_data_gen_fun = DataLoader.from_callable_iterable(\n            make_generator_fun_random_data(\n                n_batches=self.n_batches,\n                batch_size=self.batch_size,\n                shape_inputs=self.shape_inputs,\n                output_dim=self.output_dim,\n                output_type=\"discrete\",\n            )\n        )\n        self.class_inputs_gen_fun = InputsLoader.from_data_loader(\n            self.class_data_gen_fun\n        )\n\n    def test_lik_batched_log_joint_prob(self):\n        params = FrozenDict(\n            dict(\n                model=self.reg_lik.model_manager.model.init(\n                    self.rng, jnp.zeros((1,) + self.shape_inputs)\n                ),\n                lik_log_var=self.reg_lik.model_manager.likelihood_log_variance_model.init(\n                    self.rng, jnp.zeros((1,) + self.shape_inputs)\n                ),\n            )\n        )\n\n        for batch_data in self.reg_data_arr:\n            batched_log_joint_prob1 = self.reg_lik._batched_log_joint_prob(\n                params, batch_data, n_data=batch_data[1].shape[0]\n            )\n            batched_log_joint_prob2 = self.reg_lik._batched_log_joint_prob(\n                params, batch_data, n_data=2 * batch_data[1].shape[0]\n            )\n            assert jnp.allclose(batched_log_joint_prob2, 2 * batched_log_joint_prob1)\n            assert jnp.array([batched_log_joint_prob1]).shape == (1,)\n\n            _, aux = self.reg_lik._batched_log_joint_prob(\n                params,\n                batch_data,\n                n_data=batch_data[1].shape[0],\n                return_aux=[\"outputs\"],\n            )\n            assert aux[\"outputs\"].shape == (self.n_inputs, 2 * self.output_dim)\n\n    def test_lik_log_joint_prob(self):\n        params = FrozenDict(\n            dict(\n                model=self.reg_lik.model_manager.model.init(\n                    self.rng, jnp.zeros((1,) + self.shape_inputs)\n                ),", "metadata": {"task_id": "awslabs_fortuna/69", "ground_truth": "                lik_log_var=self.reg_lik.model_manager.likelihood_log_variance_model.init(", "fpath_tuple": ["awslabs_fortuna", "tests", "fortuna", "prob_model", "test_likelihood.py"], "context_start_lineno": 0, "line_no": 136}}
{"prompt": "]\n                r4grams.append(r4gram)\n        r2gramslist.append(r2grams)\n        r3gramslist.append(r3grams)\n        r4gramslist.append(r4grams)\n\n    for i in range(0, len(s1grams) - 1):\n        if i < len(s1grams) - 1:\n            s2gram = s1grams[i] + \" \" + s1grams[i + 1]\n            s2grams.append(s2gram)\n        if i < len(s1grams) - 2:\n            s3gram = s1grams[i] + \" \" + s1grams[i + 1] + \" \" + s1grams[i + 2]\n            s3grams.append(s3gram)\n        if i < len(s1grams) - 3:\n            s4gram = s1grams[i] + \" \" + s1grams[i + 1] + \" \" + s1grams[i + 2] + \" \" + s1grams[i + 3]\n            s4grams.append(s4gram)\n\n    for i in range(0, len(c1grams) - 1):\n        if i < len(c1grams) - 1:\n            c2gram = c1grams[i] + \" \" + c1grams[i + 1]\n            c2grams.append(c2gram)\n        if i < len(c1grams) - 2:\n            c3gram = c1grams[i] + \" \" + c1grams[i + 1] + \" \" + c1grams[i + 2]\n            c3grams.append(c3gram)\n        if i < len(c1grams) - 3:\n            c4gram = c1grams[i] + \" \" + c1grams[i + 1] + \" \" + c1grams[i + 2] + \" \" + c1grams[i + 3]\n            c4grams.append(c4gram)\n\n    (keep1score, del1score, add1score) = SARIngram(s1grams, c1grams, r1gramslist, numref)\n    (keep2score, del2score, add2score) = SARIngram(s2grams, c2grams, r2gramslist, numref)\n    (keep3score, del3score, add3score) = SARIngram(s3grams, c3grams, r3gramslist, numref)\n    (keep4score, del4score, add4score) = SARIngram(s4grams, c4grams, r4gramslist, numref)\n    avgkeepscore = sum([keep1score, keep2score, keep3score, keep4score]) / 4\n    avgdelscore = sum([del1score, del2score, del3score, del4score]) / 4\n    avgaddscore = sum([add1score, add2score, add3score, add4score]) / 4\n    finalscore = (avgkeepscore + avgdelscore + avgaddscore) / 3\n    return finalscore\n\n\ndef normalize(sentence, lowercase: bool = True, tokenizer: str = \"13a\", return_str: bool = True):\n\n    # Normalization is requried for the ASSET dataset (one of the primary\n    # datasets in sentence simplification) to allow using space\n    # to split the sentence. Even though Wiki-Auto and TURK datasets,\n    # do not require normalization, we do it for consistency.\n    # Code adapted from the EASSE library [1] written by the authors of the ASSET dataset.\n    # [1] https://github.com/feralvam/easse/blob/580bba7e1378fc8289c663f864e0487188fe8067/easse/utils/preprocessing.py#L7\n\n    if lowercase:\n        sentence = sentence.lower()\n\n    if tokenizer in [\"13a\", \"intl\"]:\n        if version.parse(sacrebleu.__version__).major >= 2:\n            normalized_sent = sacrebleu.metrics.bleu._get_tokenizer(tokenizer)()(sentence)\n        else:\n            normalized_sent = sacrebleu.TOKENIZERS[tokenizer]()(sentence)\n    elif tokenizer == \"moses\":\n        normalized_sent = sacremoses.MosesTokenizer().tokenize(sentence, return_str=True, escape=False)\n    elif tokenizer == \"penn\":\n        normalized_sent = sacremoses.MosesTokenizer().penn_tokenize(sentence, return_str=True)\n    else:\n        normalized_sent = sentence\n\n    if not return_str:\n        normalized_sent = normalized_sent.split()\n\n    return normalized_sent\n\n\ndef compute_sari(sources, predictions, references):\n\n    if not (len(sources) == len(predictions) == len(references)):\n        raise ValueError(\"Sources length must match predictions and references lengths.\")\n    sari_score = 0\n    for src, pred, refs in zip(sources, predictions, references):\n        sari_score += SARIsent(normalize(src), normalize(pred), [normalize(sent) for sent in refs])\n    sari_score = sari_score / len(predictions)\n    return 100 * sari_score\n\n\ndef compute_sacrebleu(\n    predictions,\n    references,\n    smooth_method=\"exp\",\n    smooth_value=None,\n    force=False,\n    lowercase=False,\n    use_effective_order=False,\n):\n    references_per_prediction = len(references[0])\n    if any(len(refs)!= references_per_prediction for refs in references):\n        raise ValueError(\"Sacrebleu requires the same number of references for each prediction\")\n    transformed_references = [[refs[i] for refs in references] for i in range(references_per_prediction)]\n    output = sacrebleu.corpus_bleu(\n        predictions,\n        transformed_references,\n        smooth_method=smooth_method,\n        smooth_value=smooth_value,\n        force=force,\n        lowercase=lowercase,\n        use_effective_order=use_effective_order,\n    )\n    return output.score\n\n\n@evaluate.utils.file_utils.add_start_docstrings(_DESCRIPTION, _KWARGS_DESCRIPTION)\nclass WikiSplit(evaluate.Metric):\n    def _info(self):\n        return evaluate.MetricInfo(\n            description=_DESCRIPTION,\n            citation=_CITATION,\n            inputs_description=_KWARGS_DESCRIPTION,\n            features=[\n                datasets.Features(\n                    {\n                        \"predictions\": datasets.Value(\"string\", id=\"sequence\"),\n                        \"references\": datasets.Sequence(datasets.Value(\"string\", id=\"sequence\"), id=\"references\"),\n                    }\n                ),\n                datasets.Features(\n                    {\n                        \"predictions\": datasets.Value(\"string\", id=\"sequence\"),\n                        \"references\": datasets.Value(\"string\", id=\"sequence\"),\n                    }\n                ),\n            ],\n            codebase_urls=[\n                \"https://github.com/huggingface/transformers/blob/master/src/transformers/data/metrics/squad_metrics.py\",\n                \"https://github.com/cocoxu/simplification/blob/master/SARI.py\",\n                \"https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/utils/sari_hook.py\",\n                \"https://github.com/mjpost/sacreBLEU\",", "metadata": {"task_id": "huggingface_evaluate/46", "ground_truth": "            ],", "fpath_tuple": ["huggingface_evaluate", "metrics", "wiki_split", "wiki_split.py"], "context_start_lineno": 217, "line_no": 348}}
{"prompt": " of index points,\n  # conditioned on the observed data `x_observed` and `y_observed`, can be built\n  # without recomputing the Cholesky decomposition.\n  x_predicted = random.uniform(random.PRNGKey(2), shape=(5, dim))\n  pp_dist = gp_model.apply(\n      {'params': init_state['params'], **pp_state},\n      x_predicted,\n      method=gp_model.predict)\n  ```\n  \"\"\"\n\n  coroutine: ModelCoroutine\n  mean_fn: Callable[[_In], Array] = lambda _: 0.0\n\n  def setup(self):\n    \"\"\"Builds module parameters.\"\"\"\n    generator = self.coroutine()\n    try:\n      p: ModelParameter = next(generator)\n      while True:\n        # Declare a Flax variable with the name and initialization function from\n        # the `ModelParameter`.\n        param: Array = self.param(p.name, p.init_fn)\n        p: ModelParameter = generator.send(param)\n    except StopIteration:\n      # Ignore the return value from the generator since this method only builds\n      # the Flax parameters.\n      pass\n\n  def __call__(self, x: _In) -> _D:\n    \"\"\"Returns a stochastic process distribution.\n\n    If the Flax module's `apply` method is called with `mutable=True` or\n    `mutable=('losses,')` regularization losses are additionally returned.\n\n    Args:\n      x: ArrayTree of index points in the constrained space.\n\n    Returns:\n      dist: `tfd.Distribution` instance with x as index points.\n    \"\"\"\n    gen = self.coroutine(inputs=x)\n    if self.is_initializing() and isinstance(self.mean_fn, nn.Module):\n      _ = self.mean_fn(x)  # Call mean_fn so its parameters are initialized.\n    try:\n      p: ModelParameter = next(gen)\n      while True:\n        # \"params\" is the name that `nn.Module` gives to the collection of read-\n        # only variables.\n        param: Array = self.get_variable('params', p.name)\n        if p.regularizer:\n          self.sow(  # `sow` stores a value in a collection.\n              'losses',\n              f'{p.name}_regularization',\n              p.regularizer(param),\n              reduce_fn=lambda _, b: b,\n          )\n        p = gen.send(param)\n    except StopIteration as e:\n      # After the generator is exhausted, it raises a `StopIteration` error. The\n      # `StopIteration` object has a property `value` of type `_D`.\n      gp = e.value\n      return gp.copy(mean_fn=self.mean_fn)\n\n  def precompute_predictive(self, x_observed: _In, y_observed: Array) -> None:\n    \"\"\"Builds a stochastic process regression model conditioned on observations.\n\n    The mutable variable returned by this method as auxillary output should be\n    passed as state to `predict`. This avoids repeated, expensive operations\n    (often Cholesky decompositions) when computing the posterior predictive.\n\n    Args:\n      x_observed: Index points on which to condition the posterior predictive.\n      y_observed: Observations on which to condition the posterior predictive.\n    \"\"\"\n    # Call the `tfd.Distribution` object's `predict` method. This\n    # triggers an expensive computation, typically a Cholesky decomposition, and\n    # returns a new `tfd.Distribution` representing the posterior predictive.\n    # This distribution is stored in the `predictive_distribution` Flax variable\n    # and returned as auxiliary output.\n    predictive_dist = self(x_observed).posterior_predictive(\n        index_points=None, observations=y_observed)\n    self.sow(\n        'predictive', 'distribution', predictive_dist, reduce_fn=lambda _, b: b\n    )\n\n  def posterior(self, x_predictive: _In, x_observed: _In,\n                y_observed: Array) -> _D:\n    \"\"\"Returns the posterior predictive distribution.\n\n    Recommended usage:\n      Jit it as a function that takes only one argument. As long as x_observed\n      and y_observed are the same, precompute_predicitve runs only once.\n\n      ```python\n      @jax.jit\n      def posterior(x):\n        return model.apply(params, x, x_observed, y_observed,\n                           mutable=('predictive',), method=model.posterior)\n      ```\n\n    Args:\n      x_predictive: predictive index points.\n      x_observed: observed index points.\n      y_observed: observed labels.\n\n    Returns:\n      Predictive distribution on x_predictive.\n    \"\"\"\n    with jax.ensure_compile_time_eval():\n      self.precompute_predictive(x_observed, y_observed)\n    return self.predict(x_predictive)\n\n  def predict(self, x_predictive: _In) -> _D:\n    \"\"\"Returns a posterior predictive stochastic process.\n\n    The mutable variable in `predictive/distribution`, typically containing a\n    `tfd.GaussianProcessRegressionModel` or\n    `tfd.StudentTProcessRegressionModel`, is copied with the new predictive\n    index points to avoid repeated, expensive computation (often Cholesky\n    decompositions) in the distribution's constructor. See the class docstring\n    for how to use `precompute_predictive` in combination with `predict`.\n\n    Args:\n      x_predictive: Predictive index points.\n\n    Returns:\n      pp_dist: The posterior predictive distribution over `x_predictive`.\n    \"\"\"\n    if not self.has_variable('predictive', 'distribution'):\n      raise ValueError('The mutable variable containing the initial posterior '\n                       'predictive distribution must be set by '\n                       '`precompute_predictive`before `predict` is called. See '\n                       'the class docstring for an example.')\n    # Access the `tfd.Distribution` stored in the Flax variable, and copy the\n    # distribution object with new index points (avoiding recomputation).\n    return self.get_variable('predictive', 'distribution').copy(\n        index_points=x_predictive\n    )\n\n\nclass VectorToArrayTree(tfb.Chain):\n  \"\"\"Bijector that converts a vector to a dict like `params`.\n\n  The bijector splits the vector, reshapes the splits, and then packs the splits\n  into a dictionary. The bijector's `inverse` method does the reverse.\n\n  It also has aliases `to_params` and `to_vectors` for `forward()` and\n  `inverse()` methods.\n\n  Example:\n\n  ```python\n  params = {'a': [4.0, 3.0], 'b': -2.0, 'c': [[6.0]]}\n  bij = VectorToArrayTree(params)\n  bij.inverse(params)  #  => [4.0, 3.0, -2.0, 6.0]\n  bij([0.0, 1.0, 2.0, 3.0])  # => {'a': [0.0, 1.0], 'b': 2.0, 'c': [[3.0]]}\n  ```\n  \"\"\"\n\n  def __init__(self, arraytree: ArrayTree, *, validate_args: bool = False):\n    \"\"\"Init.\n\n    Args:\n      arraytree: A nested structure of Arrays.\n      validate_args: If True, does runtime validation. It may be slow.\n    \"\"\"", "metadata": {"task_id": "google_vizier/21", "ground_truth": "    parameters = dict(locals())", "fpath_tuple": ["google_vizier", "vizier", "_src", "jax", "stochastic_process_model.py"], "context_start_lineno": 323, "line_no": 490}}
{"prompt": "\"\"\"\n        inputs, targets = [], []\n        for batch_inputs, batch_targets in self._data_loader():\n            inputs.append(batch_inputs)\n            targets.append(batch_targets)\n        return np.concatenate(inputs, 0), np.concatenate(targets, 0)\n\n    def to_array_inputs(self) -> Array:\n        \"\"\"\n        Reduce a data loader to an array of target data.\n\n        Returns\n        -------\n        Array\n            Array of input data.\n        \"\"\"\n        inputs = []\n        for batch_inputs, batch_targets in self._data_loader():\n            inputs.append(batch_inputs)\n        return np.concatenate(inputs, 0)\n\n    def to_array_targets(self) -> Array:\n        \"\"\"\n        Reduce a data loader to an array of target data.\n\n        Returns\n        -------\n        Array\n            Array of input data.\n        \"\"\"\n        targets = []\n        for batch_inputs, batch_targets in self._data_loader():\n            targets.append(batch_targets)\n        return np.concatenate(targets, 0)\n\n    def to_inputs_loader(self) -> InputsLoader:\n        \"\"\"\n        Reduce a data loader to an inputs loader.\n\n        Returns\n        -------\n        InputsLoader\n            The inputs loader derived from the data loader.\n        \"\"\"\n        return InputsLoader.from_data_loader(DataLoader(data_loader=self._data_loader))\n\n    def to_targets_loader(self) -> TargetsLoader:\n        \"\"\"\n        Reduce a data loader to a targets loader.\n\n        Returns\n        -------\n        TargetsLoader\n            The targets loader derived from the data loader.\n        \"\"\"\n        return TargetsLoader.from_data_loader(DataLoader(data_loader=self._data_loader))\n\n    @classmethod\n    def chop(cls, data_loader: DataLoader, divisor: int) -> DataLoader:\n        \"\"\"\n        Chop the last part of each batch of the data loader, to make sure the number od data points per batch divides\n        `divisor`.\n\n        Parameters\n        ----------\n        data_loader : DataLoader\n            A data loader\n        divisor : int\n            Number of data points that each batched must divide.\n\n        Returns\n        -------\n        DataLoader\n            A data loader with chopped batches.\n        \"\"\"\n        return cls(data_loader=ChoppedDataLoader(data_loader=data_loader, divisor=divisor))\n\n\nclass InputsLoader:\n    def __init__(\n        self,\n        inputs_loader: Union[\n            FromArrayInputsToInputsLoader,\n            FromDataLoaderToInputsLoader,\n            FromCallableIterableToInputsLoader,\n            FromIterableToInputsLoader,\n            ChoppedInputsLoader\n        ],\n    ):\n        \"\"\"\n        An inputs loader class. Each batch is an array of inputs, with different data points over the first dimension.\n\n        Parameters\n        ----------\n        inputs_loader : Union[FromArrayInputsToInputsLoader, FromDataLoaderToInputsLoader]\n            An inputs loader.\n        \"\"\"\n        self._inputs_loader = inputs_loader\n\n    def __iter__(self):\n        yield from self._inputs_loader()\n\n    @classmethod\n    def from_data_loader(cls, data_loader: DataLoader) -> InputsLoader:\n        \"\"\"\n        Reduce a data loader to an inputs loader.\n\n        Parameters\n        ----------\n        data_loader : DataLoader\n            A data loader.\n\n        Returns\n        -------\n        InputsLoader\n            An inputs loader.\n        \"\"\"\n        return cls(inputs_loader=FromDataLoaderToInputsLoader(data_loader))\n\n    @classmethod\n    def from_array_inputs(\n        cls,\n        inputs: Array,\n        batch_size: Optional[int] = None,\n        shuffle: bool = False,\n        prefetch: bool = False,\n    ) -> InputsLoader:\n        \"\"\"\n        Build a :class:`~fortuna.data.loader.InputsLoader` object from an array of input data.\n\n        Parameters\n        ----------\n        inputs: Array\n            Input array of data.\n        batch_size: Optional[int]\n            The batch size. If not given, the inputs will not be batched.\n        shuffle: bool\n            Whether the inputs loader should shuffle at every call.\n        prefetch: bool\n            Whether to prefetch the next batch.\n\n        Returns\n        -------\n        InputsLoader\n            An inputs loader built out of the array of inputs.\n        \"\"\"\n        return cls(\n            inputs_loader=FromArrayInputsToInputsLoader(\n                inputs, batch_size=batch_size, shuffle=shuffle, prefetch=prefetch\n            )\n        )\n\n    def to_array_inputs(self) -> Array:\n        \"\"\"\n        Reduce an inputs loader to an array of inputs.\n\n        Returns\n        -------\n        Array\n            Array of input data.\n        \"\"\"\n        inputs = []\n        for batch_inputs in self._inputs_loader():\n            inputs.append(batch_inputs)\n        return np.concatenate(inputs, 0)\n\n    @classmethod\n    def from_callable_iterable(\n        cls, fun: Callable[[], Iterable[Array]],\n    ) -> InputsLoader:\n        \"\"\"\n        Transform a callable iterable into a :class:`~fortuna.data.loader.InputsLoader` object.\n\n        Parameters\n        ----------\n        fun: Callable[[], Iterable[Array]]\n            A callable iterable of input arrays.\n\n        Returns\n        -------\n        InputsLoader\n            An inputs loader object.\n        \"\"\"\n        return cls(inputs_loader=FromCallableIterableToInputsLoader(fun))\n\n    @classmethod\n    def from_iterable(cls, iterable: Iterable[Array],) -> InputsLoader:\n        \"\"\"\n        Transform an iterable into a :class:`~fortuna.data.loader.InputsLoader` object.\n\n        Parameters\n        ----------\n        iterable: Iterable[Array]\n            An iterable of input arrays.\n\n        Returns\n        -------\n        InputsLoader\n            An inputs loader object.\n        \"\"\"\n        return cls(inputs_loader=FromIterableToInputsLoader(iterable))\n\n    @classmethod\n    def chop(cls, inputs_loader: InputsLoader, divisor: int) -> InputsLoader:\n        \"\"\"\n        Chop the last part of each batch of the inputs loader, to make sure the number od data points per batch divides\n        `divisor`.\n\n        Parameters\n        ----------\n        inputs_loader : InputsLoader\n            An inputs loader.\n        divisor : int\n            Number of data points that each batched must divide.\n\n        Returns\n        -------\n        InputsLoader\n            An inputs loader with chopped batches.\n        \"\"\"\n        return cls(inputs_loader=ChoppedInputsLoader(inputs_loader=inputs_loader, divisor=divisor))\n\n\nclass TargetsLoader:\n    def __init__(\n        self,\n        targets_loader: Union[\n            FromArrayTargetsToTargetsLoader,\n            FromDataLoaderToTargetsLoader,\n            FromCallableIterableToTargetsLoader,\n            FromIterableToTargetsLoader,\n            ChoppedTargetsLoader\n        ],\n    ):\n        \"\"\"\n        A targets loader class. Each batch is an array of targets, with different data points over the first dimension.\n\n        Parameters\n        ----------\n        targets_loader : Union[FromArrayTargetsToTargetsLoader, FromDataLoaderToTargetsLoader]\n            A targets loader.\n        \"\"\"\n        self._targets_loader = targets_loader\n\n    def __iter__(self):", "metadata": {"task_id": "awslabs_fortuna/148", "ground_truth": "        yield from self._targets_loader()", "fpath_tuple": ["awslabs_fortuna", "fortuna", "data", "loader.py"], "context_start_lineno": 157, "line_no": 402}}
{"prompt": "# Copyright 2022 The HuggingFace Team. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport inspect\nfrom typing import List, Optional, Tuple, Union\n\nimport torch\nimport torch.nn as nn\nimport torch.utils.checkpoint\n\nfrom transformers import PretrainedConfig, PreTrainedModel, PreTrainedTokenizer\nfrom transformers.activations import ACT2FN\nfrom transformers.modeling_outputs import BaseModelOutput\nfrom transformers.utils import logging\n\nfrom...models import AutoencoderKL, UNet2DConditionModel, UNet2DModel, VQModel\nfrom...schedulers import DDIMScheduler, LMSDiscreteScheduler, PNDMScheduler\nfrom...utils import randn_tensor\nfrom..pipeline_utils import DiffusionPipeline, ImagePipelineOutput\n\n\nclass LDMTextToImagePipeline(DiffusionPipeline):\n    r\"\"\"\n    This model inherits from [`DiffusionPipeline`]. Check the superclass documentation for the generic methods the\n    library implements for all the pipelines (such as downloading or saving, running on a particular device, etc.)\n\n    Parameters:\n        vqvae ([`VQModel`]):\n            Vector-quantized (VQ) Model to encode and decode images to and from latent representations.\n        bert ([`LDMBertModel`]):\n            Text-encoder model based on [BERT](https://huggingface.co/docs/transformers/model_doc/bert) architecture.\n        tokenizer (`transformers.BertTokenizer`):\n            Tokenizer of class\n            [BertTokenizer](https://huggingface.co/docs/transformers/model_doc/bert#transformers.BertTokenizer).\n        unet ([`UNet2DConditionModel`]): Conditional U-Net architecture to denoise the encoded image latents.\n        scheduler ([`SchedulerMixin`]):\n            A scheduler to be used in combination with `unet` to denoise the encoded image latents. Can be one of\n            [`DDIMScheduler`], [`LMSDiscreteScheduler`], or [`PNDMScheduler`].\n    \"\"\"\n\n    def __init__(\n        self,\n        vqvae: Union[VQModel, AutoencoderKL],\n        bert: PreTrainedModel,\n        tokenizer: PreTrainedTokenizer,\n        unet: Union[UNet2DModel, UNet2DConditionModel],\n        scheduler: Union[DDIMScheduler, PNDMScheduler, LMSDiscreteScheduler],\n    ):\n        super().__init__()\n        self.register_modules(vqvae=vqvae, bert=bert, tokenizer=tokenizer, unet=unet, scheduler=scheduler)\n        self.vae_scale_factor = 2 ** (len(self.vqvae.config.block_out_channels) - 1)\n\n    @torch.no_grad()\n    def __call__(\n        self,\n        prompt: Union[str, List[str]],\n        height: Optional[int] = None,\n        width: Optional[int] = None,\n        num_inference_steps: Optional[int] = 50,\n        guidance_scale: Optional[float] = 1.0,\n        eta: Optional[float] = 0.0,\n        generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,\n        latents: Optional[torch.FloatTensor] = None,\n        output_type: Optional[str] = \"pil\",\n        return_dict: bool = True,\n        **kwargs,\n    ) -> Union[Tuple, ImagePipelineOutput]:\n        r\"\"\"\n        Args:\n            prompt (`str` or `List[str]`):\n                The prompt or prompts to guide the image generation.\n            height (`int`, *optional*, defaults to self.unet.config.sample_size * self.vae_scale_factor):\n                The height in pixels of the generated image.\n            width (`int`, *optional*, defaults to self.unet.config.sample_size * self.vae_scale_factor):\n                The width in pixels of the generated image.\n            num_inference_steps (`int`, *optional*, defaults to 50):\n                The number of denoising steps. More denoising steps usually lead to a higher quality image at the\n                expense of slower inference.\n            guidance_scale (`float`, *optional*, defaults to 1.0):\n                Guidance scale as defined in [Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598).\n                `guidance_scale` is defined as `w` of equation 2. of [Imagen\n                Paper](https://arxiv.org/pdf/2205.11487.pdf). Guidance scale is enabled by setting `guidance_scale >\n                1`. Higher guidance scale encourages to generate images that are closely linked to the text `prompt` at\n                the, usually at the expense of lower image quality.\n            generator (`torch.Generator`, *optional*):\n                One or a list of [torch generator(s)](https://pytorch.org/docs/stable/generated/torch.Generator.html)\n                to make generation deterministic.\n            latents (`torch.FloatTensor`, *optional*):", "metadata": {"task_id": "huggingface_diffusers/135", "ground_truth": "                Pre-generated noisy latents, sampled from a Gaussian distribution, to be used as inputs for image", "fpath_tuple": ["huggingface_diffusers", "src", "diffusers", "pipelines", "latent_diffusion", "pipeline_latent_diffusion.py"], "context_start_lineno": 0, "line_no": 99}}
{"prompt": "from typing import List, Optional, Union\n\nimport jax.numpy as jnp\nimport jax.scipy as jsp\nfrom jax import lax, random, vmap\nfrom jax._src.prng import PRNGKeyArray\n\nfrom fortuna.data.loader import InputsLoader\nfrom fortuna.prob_model.posterior.base import Posterior\nfrom fortuna.prob_model.predictive.base import Predictive\nfrom fortuna.typing import Array\n\n\nclass RegressionPredictive(Predictive):\n    def __init__(self, posterior: Posterior):\n        \"\"\"\n        Regression predictive distribution class.\n\n        Parameters\n        ----------\n        posterior : Posterior\n             A posterior distribution object.\n        \"\"\"\n        super().__init__(posterior)\n\n    def mode(\n        self,\n        inputs_loader: InputsLoader,\n        n_posterior_samples: int = 30,\n        means: Optional[jnp.ndarray] = None,\n        rng: Optional[PRNGKeyArray] = None,\n        distribute: bool = True,\n    ) -> jnp.ndarray:\n        if means is not None:\n            return means\n        return self.mean(\n            inputs_loader=inputs_loader,\n            n_posterior_samples=n_posterior_samples,\n            rng=rng,\n            distribute=distribute,\n        )\n\n    def aleatoric_entropy(\n        self,\n        inputs_loader: InputsLoader,\n        n_posterior_samples: int = 30,\n        n_target_samples: int = 30,\n        rng: Optional[PRNGKeyArray] = None,\n        distribute: bool = True,\n    ) -> jnp.ndarray:\n        r\"\"\"\n        Estimate the predictive aleatoric entropy, that is\n\n       .. math::\n            -\\mathbb{E}_{W|\\mathcal{D}}[\\mathbb{E}_{Y|W, x}[\\log p(Y|W, x)]],\n\n        where:\n         - :math:`x` is an observed input variable;\n         - :math:`Y` is a random target variable;\n         - :math:`\\mathcal{D}` is the observed training data set;\n         - :math:`W` denotes the random model parameters.\n\n        Parameters\n        ----------\n        inputs_loader : InputsLoader\n            A loader of input data points.\n        n_target_samples: int", "metadata": {"task_id": "awslabs_fortuna/32", "ground_truth": "            Number of target samples to draw for each input.", "fpath_tuple": ["awslabs_fortuna", "fortuna", "prob_model", "predictive", "regression.py"], "context_start_lineno": 0, "line_no": 67}}
{"prompt": "            if 'classification' in config.model.task.lower():\n                data = [\n                    vocab.get_vecs_by_tokens(tokenizer(x),\n                                             lower_case_backup=True)\n                    for x in x_all\n                ]\n                targets = label_to_index(y_all)\n            else:\n                data = [\n                    vocab.get_vecs_by_tokens(tokenizer(x),\n                                             lower_case_backup=True)\n                    for x in x_all\n                ]\n                targets = [\n                    vocab.get_vecs_by_tokens(tokenizer(y),\n                                             lower_case_backup=True)\n                    for y in y_all\n                ]\n                targets = pad_sequence(targets).transpose(\n                    0, 1)[:, :raw_args['max_len'], :]\n            data = pad_sequence(data).transpose(0,\n                                                1)[:, :raw_args['max_len'], :]\n        # Split data to raw\n        num_items = [len(ds) for ds in x_list]\n        data_list, cnt = [], 0\n        for num in num_items:\n            data_list.append([\n                (x, y)\n                for x, y in zip(data[cnt:cnt + num], targets[cnt:cnt + num])\n            ])\n            cnt += num\n\n        if len(data_list) == 3:\n            # Use raw splits\n            data_split_dict = {\n                'train': data_list[0],\n                'val': data_list[1],\n                'test': data_list[2]\n            }\n        elif len(data_list) == 2:\n            # Split train to (train, val)\n            data_split_dict = {\n                'train': data_list[0],\n                'val': None,\n                'test': data_list[1]\n            }\n            if splits:\n                train_size = int(splits[0] * len(data_split_dict['train']))\n                val_size = len(data_split_dict['train']) - train_size\n                lengths = [train_size, val_size]\n                data_split_dict['train'], data_split_dict[\n                    'val'] = torch.utils.data.dataset.random_split(\n                        data_split_dict['train'], lengths)\n        else:\n            # Use config.data.splits\n            data_split_dict = {}\n            train_size = int(splits[0] * len(data_list[0]))\n            val_size = int(splits[1] * len(data_list[0]))\n            test_size = len(data_list[0]) - train_size - val_size\n            lengths = [train_size, val_size, test_size]\n            data_split_dict['train'], data_split_dict['val'], data_split_dict[\n                'test'] = torch.utils.data.dataset.random_split(\n                    data_list[0], lengths)\n\n        return data_split_dict\n\n    def load_torchaudio_data(name, splits=None, config=None):\n\n        # dataset_func = getattr(import_module('torchaudio.datasets'), name)\n        raise NotImplementedError\n\n    def load_huggingface_datasets_data(name, splits=None, config=None):\n        import datasets\n        from datasets import load_from_disk\n\n        if config.data.args:\n            raw_args = config.data.args[0]\n        else:\n            raw_args = {}\n        assert'max_len' in raw_args, \"Miss key'max_len' in \" \\\n                                      \"`config.data.args`.\"\n        filtered_args = filter_dict(datasets.load_dataset, raw_args)\n        logger.info(\"Begin to load huggingface dataset\")\n        if \"hg_cache_dir\" in raw_args:\n            hugging_face_path = raw_args[\"hg_cache_dir\"]\n        else:\n            hugging_face_path = os.getcwd()\n\n        if \"load_disk_dir\" in raw_args:\n            load_path = raw_args[\"load_disk_dir\"]\n            try:\n                dataset = load_from_disk(load_path)\n            except Exception as e:\n                logging.error(f\"When loading cached dataset form \"\n                              f\"{load_path}, we faced the exception: \\n \"\n                              f\"{str(e)}\")\n        else:\n            dataset = datasets.load_dataset(path=config.data.root,\n                                            name=name,\n                                            **filtered_args)\n        if config.model.type.endswith('transformers'):\n            os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n            from transformers import AutoTokenizer\n            logger.info(\"To load huggingface tokenizer\")\n            tokenizer = AutoTokenizer.from_pretrained(\n                config.model.type.split('@')[0],\n                local_files_only=True,\n                cache_dir=os.path.join(hugging_face_path, \"transformers\"))\n\n        for split in dataset:\n            x_all = [i['sentence'] for i in dataset[split]]\n            targets = [i['label'] for i in dataset[split]]\n\n            if split == \"train\" and \"used_train_ratio\" in raw_args and \\\n                    1 > raw_args['used_train_ratio'] > 0:\n                selected_idx = [i for i in range(len(dataset[split]))]\n                shuffle(selected_idx)\n                selected_idx = selected_idx[:int(\n                    len(selected_idx) * raw_args['used_train_ratio'])]\n                x_all = [\n                    element for i, element in enumerate(x_all)\n                    if i in selected_idx\n                ]\n                targets = [\n                    element for i, element in enumerate(targets)\n                    if i in selected_idx\n                ]\n\n            x_all = tokenizer(x_all,\n                              return_tensors='pt',\n                              padding=True,\n                              truncation=True,\n                              max_length=raw_args['max_len'])\n            data = [{key: value[i]\n                     for key, value in x_all.items()}\n                    for i in range(len(next(iter(x_all.values()))))]\n            dataset[split] = (data, targets)\n        data_split_dict = {\n            'train': [(x, y)\n                      for x, y in zip(dataset['train'][0], dataset['train'][1])\n                      ],\n            'val': [(x, y) for x, y in zip(dataset['validation'][0],\n                                           dataset['validation'][1])],\n            'test': [\n                (x, y) for x, y in zip(dataset['test'][0], dataset['test'][1])\n            ] if (set(dataset['test'][1]) - set([-1])) else None,\n        }\n        original_train_size = len(data_split_dict[\"train\"])\n\n        if \"half_val_dummy_test\" in raw_args and raw_args[\n                \"half_val_dummy_test\"]:\n            # since the \"test\" set from GLUE dataset may be masked, we need to\n            # submit to get the ground-truth, for fast FL experiments,\n            # we split the validation set into two parts with the same size as\n            # new test/val data", "metadata": {"task_id": "alibaba_FederatedScope/161", "ground_truth": "            original_val = [(x, y) for x, y in zip(dataset['validation'][0],", "fpath_tuple": ["alibaba_FederatedScope", "federatedscope", "core", "data", "utils.py"], "context_start_lineno": 310, "line_no": 465}}
{"prompt": "): Buffer slice for sampling, such as `slice(-10, None)`, which \\\n                means only sample among the last 10 data\n        Returns:\n            - sample_data (:obj:`list`): A list of data with length ``size``.\n        \"\"\"\n        if size == 0:\n            return []\n        can_sample = self._sample_check(size)\n        if not can_sample:\n            return None\n        with self._lock:\n            indices = self._get_indices(size, sample_range)\n            sample_data = self._sample_with_indices(indices, cur_learner_iter)\n        self._periodic_thruput_monitor.sample_data_count += len(sample_data)\n        return sample_data\n\n    def _append(self, ori_data: Any, cur_collector_envstep: int = -1) -> None:\n        r\"\"\"\n        Overview:\n            Append a data item into ``self._data``.\n        Arguments:\n            - ori_data (:obj:`Any`): The data which will be inserted.\n            - cur_collector_envstep (:obj:`int`): Not used in this method, but preserved for compatibility.\n        \"\"\"\n        with self._lock:\n            if self._deepcopy:\n                data = copy.deepcopy(ori_data)\n            else:\n                data = ori_data\n            self._push_count += 1\n            if self._data[self._tail] is None:\n                self._valid_count += 1\n                self._periodic_thruput_monitor.valid_count = self._valid_count\n            elif self._enable_track_used_data:\n                self._used_data_remover.add_used_data(self._data[self._tail])\n            self._data[self._tail] = data\n            self._tail = (self._tail + 1) % self._replay_buffer_size\n\n    def _extend(self, ori_data: List[Any], cur_collector_envstep: int = -1) -> None:\n        r\"\"\"\n        Overview:\n            Extend a data list into queue.\n            Add two keys in each data item, you can refer to ``_append`` for details.\n        Arguments:\n            - ori_data (:obj:`List[Any]`): The data list.\n            - cur_collector_envstep (:obj:`int`): Not used in this method, but preserved for compatibility.\n        \"\"\"\n        with self._lock:\n            if self._deepcopy:\n                data = copy.deepcopy(ori_data)\n            else:\n                data = ori_data\n            length = len(data)\n            # When updating ``_data`` and ``_use_count``, should consider two cases regarding\n            # the relationship between \"tail + data length\" and \"replay buffer size\" to check whether\n            # data will exceed beyond buffer's max length limitation.\n            if self._tail + length <= self._replay_buffer_size:\n                if self._valid_count!= self._replay_buffer_size:\n                    self._valid_count += length\n                    self._periodic_thruput_monitor.valid_count = self._valid_count\n                elif self._enable_track_used_data:\n                    for i in range(length):\n                        self._used_data_remover.add_used_data(self._data[self._tail + i])\n                self._push_count += length\n                self._data[self._tail:self._tail + length] = data\n            else:\n                new_tail = self._tail\n                data_start = 0\n                residual_num = len(data)\n                while True:\n                    space = self._replay_buffer_size - new_tail\n                    L = min(space, residual_num)\n                    if self._valid_count!= self._replay_buffer_size:\n                        self._valid_count += L\n                        self._periodic_thruput_monitor.valid_count = self._valid_count\n                    elif self._enable_track_used_data:\n                        for i in range(L):\n                            self._used_data_remover.add_used_data(self._data[new_tail + i])\n                    self._push_count += L\n                    self._data[new_tail:new_tail + L] = data[data_start:data_start + L]\n                    residual_num -= L\n                    assert residual_num >= 0\n                    if residual_num == 0:\n                        break\n                    else:\n                        new_tail = 0\n                        data_start += L\n            # Update ``tail`` and ``next_unique_id`` after the whole list is pushed into buffer.\n            self._tail = (self._tail + length) % self._replay_buffer_size\n\n    def _sample_check(self, size: int) -> bool:\n        r\"\"\"\n        Overview:\n            Check whether this buffer has more than `size` datas to sample.\n        Arguments:\n            - size (:obj:`int`): Number of data that will be sampled.\n        Returns:\n            - can_sample (:obj:`bool`): Whether this buffer can sample enough data.\n        \"\"\"\n        if self._valid_count < size:\n            print(\"No enough elements for sampling (expect: {} / current: {})\".format(size, self._valid_count))\n            return False\n        else:\n            return True\n\n    def update(self, info: dict) -> None:\n        r\"\"\"\n        Overview:\n            Naive Buffer does not need to update any info, but this method is preserved for compatibility.\n        \"\"\"\n        print(\n            '[BUFFER WARNING] Naive Buffer does not need to update any info, \\\n                but `update` method is preserved for compatibility.'\n        )\n\n    def clear(self) -> None:\n        \"\"\"\n        Overview:\n            Clear all the data and reset the related variables.\n        \"\"\"\n        with self._lock:\n            for i in range(len(self._data)):\n                if self._data[i] is not None:\n                    if self._enable_track_used_data:\n                        self._used_data_remover.add_used_data(self._data[i])\n                    self._data[i] = None\n            self._valid_count = 0\n            self._periodic_thruput_monitor.valid_count = self._valid_count\n            self._push_count = 0\n            self._tail = 0\n\n    def __del__(self) -> None:\n        \"\"\"\n        Overview:\n            Call ``close`` to delete the object.\n        \"\"\"\n        self.close()\n\n    def _get_indices(self, size: int, sample_range: slice = None) -> list:\n        r\"\"\"\n        Overview:\n            Get the sample index list.\n        Arguments:\n            - size (:obj:`int`): The number of the data that will be sampled\n            - sample_range (:obj:`slice`): Buffer slice for sampling, such as `slice(-10, None)`, which \\\n                means only sample among the last 10 data\n        Returns:\n            - index_list (:obj:`list`): A list including all the sample indices, whose length should equal to ``size``.\n        \"\"\"\n        assert self._valid_count <= self._replay_buffer_size\n        if self._valid_count == self._replay_buffer_size:\n            tail = self._replay_buffer_size\n        else:\n            tail = self._tail\n        if sample_range is None:\n            indices = list(np.random.choice(a=tail, size=size, replace=False))\n        else:\n            indices = list(range(tail))[sample_range]\n            indices = list(np.random.choice(indices, size=size, replace=False))\n        return indices\n\n    def _sample_with_indices(self, indices: List[int], cur_learner_iter: int) -> list:\n        r\"\"\"\n        Overview:\n            Sample data with ``indices``.\n        Arguments:", "metadata": {"task_id": "opendilab_ACE/70", "ground_truth": "            - indices (:obj:`List[int]`): A list including all the sample indices.", "fpath_tuple": ["opendilab_ACE", "ding", "worker", "replay_buffer", "naive_buffer.py"], "context_start_lineno": 126, "line_no": 292}}
{"prompt": "# Copyright 2022 The HuggingFace Evaluate Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom numbers import Number\nfrom typing import Any, Callable, Dict, Optional, Tuple, Union\n\nfrom datasets import Dataset\nfrom typing_extensions import Literal\n\nfrom..module import EvaluationModule\nfrom..utils.file_utils import add_end_docstrings, add_start_docstrings\nfrom.base import EVALUATOR_COMPUTE_RETURN_DOCSTRING, EVALUTOR_COMPUTE_START_DOCSTRING, Evaluator\n\n\nTASK_DOCUMENTATION = r\"\"\"\n    Examples:\n    ```python\n    >>> from evaluate import evaluator\n    >>> from datasets import load_dataset\n    >>> task_evaluator = evaluator(\"image-classification\")\n    >>> data = load_dataset(\"beans\", split=\"test[:40]\")\n    >>> results = task_evaluator.compute(\n    >>>     model_or_pipeline=\"nateraw/vit-base-beans\",", "metadata": {"task_id": "huggingface_evaluate/44", "ground_truth": "    >>>     data=data,", "fpath_tuple": ["huggingface_evaluate", "src", "evaluate", "evaluator", "image_classification.py"], "context_start_lineno": 0, "line_no": 34}}
{"prompt": "train_loss\": jnp.array(0.05),\n            \"val_accuracy\": jnp.array(0.1),\n            \"val_loss\": jnp.array(0.21),\n        }\n        self.assertDictEqual(observed_losses_and_metrics, expected_losses_and_metrics)\n\n    def test__get_mean_losses_and_metrics_ko(self):\n        trainer = FakeTrainer(\n            predict_fn=lambda x: x, disable_training_metrics_computation=False\n        )\n        losses_and_metrics = [\n            {\n                \"train_loss\": jnp.array(0.1),\n                \"val_loss\": jnp.array(0.2),\n                \"val_accuracy\": jnp.array(0.1),\n            },\n            {\"train_loss\": jnp.array(0.05), \"val_accuracy\": jnp.array(0.1)},\n            {\n                \"train_loss\": jnp.array(0.0),\n                \"val_loss\": jnp.array(0.22),\n                \"val_accuracy\": jnp.array(0.1),\n            },\n        ]\n        with self.assertRaises(ValueError):\n            _ = trainer._get_mean_losses_and_metrics(losses_and_metrics)\n\n    def test_training_epoch_end(self):\n        trainer = FakeTrainer(\n            predict_fn=lambda x: x, disable_training_metrics_computation=False\n        )\n\n        losses_and_metrics = [\n            {\n                \"train_loss\": jnp.array(0.1),\n                \"val_loss\": jnp.array(0.2),\n                \"val_accuracy\": jnp.array(0.1),\n            },\n            {\n                \"train_loss\": jnp.array(0.0),\n                \"val_loss\": jnp.array(0.22),\n                \"val_accuracy\": jnp.array(0.1),\n            },\n        ]\n        fake_out = {\n            \"train_loss\": jnp.array(0.0),\n            \"val_loss\": jnp.array(0.22),\n            \"val_accuracy\": jnp.array(0.1),\n        }\n\n        with unittest.mock.patch.object(\n            trainer, \"_get_mean_losses_and_metrics\", return_value=fake_out\n        ) as m:\n            observed = trainer.training_epoch_end(losses_and_metrics)\n        m.assert_called_once_with(losses_and_metrics)\n        self.assertDictEqual(observed, fake_out)\n\n    def test_validation_epoch_end(self):\n        trainer = FakeTrainer(\n            predict_fn=lambda x: x,\n            disable_training_metrics_computation=False,\n            save_checkpoint_dir=\"tmp_dir\",\n            save_every_n_steps=1,\n        )\n\n        losses_and_metrics = [\n            {\n                \"train_loss\": jnp.array(0.1),\n                \"val_loss\": jnp.array(0.2),\n                \"val_accuracy\": jnp.array(0.1),\n            },\n            {\n                \"train_loss\": jnp.array(0.0),\n                \"val_loss\": jnp.array(0.22),\n                \"val_accuracy\": jnp.array(0.1),\n            },\n        ]\n        fake_out = {\n            \"train_loss\": jnp.array(0.0),\n            \"val_loss\": jnp.array(0.22),\n            \"val_accuracy\": jnp.array(0.1),\n        }\n\n        with unittest.mock.patch.multiple(\n            trainer,\n            _get_mean_losses_and_metrics=mock.DEFAULT,\n            early_stopping_update=mock.DEFAULT,\n            save_checkpoint=mock.DEFAULT,\n        ) as m:\n            m[\"_get_mean_losses_and_metrics\"].return_value = fake_out\n            m[\"early_stopping_update\"].return_value = False\n            m[\"save_checkpoint\"].return_value = fake_out\n\n            observed = trainer.validation_epoch_end(losses_and_metrics, None)\n\n        m[\"_get_mean_losses_and_metrics\"].assert_called_once_with(losses_and_metrics)\n        m[\"early_stopping_update\"].assert_called_once_with(fake_out)\n        m[\"save_checkpoint\"].assert_not_called()\n        self.assertDictEqual(observed, fake_out)\n\n        with unittest.mock.patch.multiple(\n            trainer,\n            _get_mean_losses_and_metrics=mock.DEFAULT,\n            early_stopping_update=mock.DEFAULT,\n            save_checkpoint=mock.DEFAULT,\n        ) as m:\n            m[\"_get_mean_losses_and_metrics\"].return_value = fake_out\n            m[\"early_stopping_update\"].return_value = True\n            m[\"save_checkpoint\"].return_value = fake_out\n\n            observed = trainer.validation_epoch_end(losses_and_metrics, None)\n\n        m[\"_get_mean_losses_and_metrics\"].assert_called_once_with(losses_and_metrics)\n        m[\"early_stopping_update\"].assert_called_once_with(fake_out)\n        m[\"save_checkpoint\"].assert_called_once_with(None, \"tmp_dir\", force_save=True)\n        self.assertDictEqual(observed, fake_out)\n\n    def test_should_perform_validation(self):\n        trainer = FakeTrainer(\n            predict_fn=lambda x: x, disable_training_metrics_computation=False\n        )\n        self.assertFalse(trainer.should_perform_validation(None, 1))\n\n        trainer.eval_every_n_epochs = 10\n        self.assertFalse(trainer.should_perform_validation({}, 9))\n        self.assertTrue(trainer.should_perform_validation({}, 10))\n\n    def test__validation_loop(self):\n        validation_dataloader = [\n            [jnp.array([[0, 0.0, 0.0], [0, 0.0, 0]]), jnp.array([0.0, 0.0])],\n            [jnp.array([[0.1, 0.0, 10], [0, 0.0, 0]]), jnp.array([1.0, 0.0])],\n        ]\n        trainer = FakeTrainer(\n            predict_fn=lambda x: x, disable_training_metrics_computation=False\n        )\n        (\n            observed_validation_losses_and_metrics_current_epoch,\n            observed_validation_epoch_metrics_str,\n        ) = trainer._validation_loop(\n            state=None,\n            validation_dataloader=validation_dataloader,\n            validation_dataset_size=2,\n            fun=lambda x: x,\n            rng=jax.random.PRNGKey(0),\n            metrics=(\"accuracy\",),\n            training_kwargs=FrozenDict({}),\n            unravel=None,\n            verbose=False,\n        )\n        self.assertEqual(observed_validation_epoch_metrics_str, \"\")", "metadata": {"task_id": "awslabs_fortuna/35", "ground_truth": "        self.assertDictEqual(", "fpath_tuple": ["awslabs_fortuna", "tests", "fortuna", "test_trainer.py"], "context_start_lineno": 221, "line_no": 370}}
{"prompt": "self.__class__.__name__}.index(...)\"\n            )\n        index = index.nonzero().squeeze()\n        index = index.expand(*tensor_to_index.shape[:-1], index.shape[-1])\n        return tensor_to_index.gather(-1, index)\n\n    def _project(self, val: torch.Tensor) -> torch.Tensor:\n        # idx = val.sum(-1)!= 1\n        out = torch.nn.functional.gumbel_softmax(val.to(torch.float))\n        out = (out == out.max(dim=-1, keepdim=True)[0]).to(torch.long)\n        return out\n\n    def is_in(self, val: torch.Tensor) -> bool:\n        return (val.sum(-1) == 1).all()\n\n    def __eq__(self, other):\n        return (\n            type(self) == type(other)\n            and self.shape == other.shape\n            and self.space == other.space\n            and self.device == other.device\n            and self.dtype == other.dtype\n            and self.domain == other.domain\n            and self.use_register == other.use_register\n        )\n\n    def to_categorical(self) -> DiscreteTensorSpec:\n        return DiscreteTensorSpec(\n            self.space.n, device=self.device, dtype=self.dtype, shape=self.shape[:-1]\n        )\n\n\n@dataclass(repr=False)\nclass BoundedTensorSpec(TensorSpec):\n    \"\"\"A bounded continuous tensor spec.\n\n    Args:\n        minimum (np.ndarray, torch.Tensor or number): lower bound of the box.\n        maximum (np.ndarray, torch.Tensor or number): upper bound of the box.\n        device (str, int or torch.device, optional): device of the tensors.\n        dtype (str or torch.dtype, optional): dtype of the tensors.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        minimum: Union[float, torch.Tensor, np.ndarray],\n        maximum: Union[float, torch.Tensor, np.ndarray],\n        shape: Optional[Union[torch.Size, int]] = None,\n        device: Optional[DEVICE_TYPING] = None,\n        dtype: Optional[Union[torch.dtype, str]] = None,\n    ):\n        dtype, device = _default_dtype_and_device(dtype, device)\n        if dtype is None:\n            dtype = torch.get_default_dtype()\n        if device is None:\n            device = torch._get_default_device()\n\n        if not isinstance(minimum, torch.Tensor):\n            minimum = torch.tensor(minimum, dtype=dtype, device=device)\n        if not isinstance(maximum, torch.Tensor):\n            maximum = torch.tensor(maximum, dtype=dtype, device=device)\n        if maximum.device!= device:\n            maximum = maximum.to(device)\n        if minimum.device!= device:\n            minimum = minimum.to(device)\n        if dtype is not None and minimum.dtype is not dtype:\n            minimum = minimum.to(dtype)\n        if dtype is not None and maximum.dtype is not dtype:\n            maximum = maximum.to(dtype)\n        err_msg = (\n            \"BoundedTensorSpec requires the shape to be explicitely (via \"\n            \"the shape argument) or implicitely defined (via either the \"\n            \"minimum or the maximum or both). If the maximum and/or the \"\n            \"minimum have a non-singleton shape, they must match the \"\n            \"provided shape if this one is set explicitely.\"\n        )\n        if shape is not None and not isinstance(shape, torch.Size):\n            if isinstance(shape, int):\n                shape = torch.Size([shape])\n            else:\n                shape = torch.Size(list(shape))\n\n        if maximum.ndimension():\n            if shape is not None and shape!= maximum.shape:\n                raise RuntimeError(err_msg)\n            shape = maximum.shape\n            minimum = minimum.expand(*shape).clone()\n        elif minimum.ndimension():\n            if shape is not None and shape!= minimum.shape:\n                raise RuntimeError(err_msg)\n            shape = minimum.shape\n            maximum = maximum.expand(*shape).clone()\n        elif shape is None:\n            raise RuntimeError(err_msg)\n        else:\n            minimum = minimum.expand(*shape).clone()\n            maximum = maximum.expand(*shape).clone()\n\n        if minimum.numel() > maximum.numel():\n            maximum = maximum.expand_as(minimum).clone()\n        elif maximum.numel() > minimum.numel():\n            minimum = minimum.expand_as(maximum).clone()\n        if shape is None:\n            shape = minimum.shape\n        else:\n            if isinstance(shape, float):\n                shape = torch.Size([shape])\n            elif not isinstance(shape, torch.Size):\n                shape = torch.Size(shape)\n            shape_err_msg = (\n                f\"minimum and shape mismatch, got {minimum.shape} and {shape}\"\n            )\n            if len(minimum.shape)!= len(shape):\n                raise RuntimeError(shape_err_msg)\n            if not all(_s == _sa for _s, _sa in zip(shape, minimum.shape)):\n                raise RuntimeError(shape_err_msg)\n        self.shape = shape\n\n        super().__init__(\n            shape, ContinuousBox(minimum, maximum), device, dtype, \"continuous\"\n        )\n\n    def expand(self, *shape):\n        if len(shape) == 1 and isinstance(shape[0], (tuple, list, torch.Size)):\n            shape = shape[0]\n        if any(val < 0 for val in shape):\n            raise ValueError(\n                f\"{self.__class__.__name__}.extend does not support negative shapes.\"\n            )\n        if any(s1!= s2 and s2!= 1 for s1, s2 in zip(shape[-self.ndim :], self.shape)):\n            raise ValueError(\n                f\"The last {self.ndim} of the extended shape must match the\"\n                f\"shape of the CompositeSpec in CompositeSpec.extend.\"\n            )\n        return self.__class__(\n            minimum=self.space.minimum.expand(shape).clone(),\n            maximum=self.space.maximum.expand(shape).clone(),\n            shape=shape,\n            device=self.device,\n            dtype=self.dtype,\n        )\n\n    def rand(self, shape=None) -> torch.Tensor:\n        if shape is None:\n            shape = torch.Size([])\n        a, b = self.space\n        if self.dtype in (torch.float, torch.double, torch.half):\n            shape = [*shape, *self.shape]\n            out = (\n                torch.zeros(shape, dtype=self.dtype, device=self.device).uniform_()\n                * (b - a)\n                + a\n            )\n            if (out > b).any():\n                out[out > b] = b.expand_as(out)[out > b]\n            if (out < a).any():\n                out[out < a] = a.expand_as(out)[out < a]\n            return out\n        else:\n            if self.space.maximum.dtype == torch.bool:\n                maxi = self.space.maximum.int()\n            else:\n                maxi = self.space.maximum\n            if self.space.minimum.dtype == torch.bool:\n                mini = self.space.minimum.int()\n            else:\n                mini = self.space.minimum", "metadata": {"task_id": "pytorch_rl/142", "ground_truth": "            interval = maxi - mini", "fpath_tuple": ["pytorch_rl", "torchrl", "data", "tensor_specs.py"], "context_start_lineno": 581, "line_no": 749}}
{"prompt": "import os\nimport json\nimport numpy as np\nimport logging\n\ntry:\n    import torch\n    from torch.utils.data.dataset import Dataset\nexcept ImportError:\n    torch = None\n    Dataset = None\n\nNUM_DEBUG = 20\nBOS_TOKEN_ID = -1\nEOS_TOKEN_ID = -1\nEOQ_TOKEN_ID = -1\nPAD_TOKEN_ID = -1\n\nlogger = logging.getLogger(__name__)\n\n\ndef split_sent(examples, eoq='[unused2]', tokenize=True):\n    import nltk\n    from nltk.tokenize import sent_tokenize\n\n    try:\n        nltk.data.find('tokenizers/punkt')\n    except LookupError:\n        nltk.download('punkt')\n\n    new_examples = []\n    for e in examples:\n        if tokenize:\n            e = f' {eoq} '.join(sent_tokenize(e))\n        else:\n            e = e.replace('[SEP]', eoq)\n        new_examples.append(e)\n    return new_examples\n\n\nclass DatasetDict(Dataset):\n    def __init__(self, inputs):\n        super().__init__()\n        assert all(\n            list(inputs.values())[0].size(0) == v.size(0)\n            for v in inputs.values()), \"Size mismatch between tensors\"\n        self.inputs = inputs\n\n    def __getitem__(self, index):\n        return {k: v[index] for k, v in self.inputs.items()}\n\n    def __len__(self):\n        return list(self.inputs.values())[0].size(0)\n\n\ndef setup_tokenizer(model_type,\n                    bos_token='[unused0]',\n                    eos_token='[unused1]',\n                    eoq_token='[unused2]'):\n    \"\"\"\n    Get a tokenizer, the default bos/eos/eoq token is used for Bert\n    \"\"\"\n    from transformers.models.bert import BertTokenizerFast\n    try:\n        tokenizer = BertTokenizerFast.from_pretrained(\n            model_type,\n            additional_special_tokens=[bos_token, eos_token, eoq_token],\n            skip_special_tokens=True,\n            local_files_only=True,\n        )\n    except:\n        tokenizer = BertTokenizerFast.from_pretrained(\n            model_type,\n            additional_special_tokens=[bos_token, eos_token, eoq_token],\n            skip_special_tokens=True,\n        )\n\n    tokenizer.bos_token = bos_token\n    tokenizer.eos_token = eos_token\n    tokenizer.eoq_token = eoq_token\n    tokenizer.bos_token_id = tokenizer.vocab[bos_token]\n    tokenizer.eos_token_id = tokenizer.vocab[eos_token]\n    tokenizer.eoq_token_id = tokenizer.vocab[eoq_token]\n\n    global BOS_TOKEN_ID, EOS_TOKEN_ID, EOQ_TOKEN_ID, PAD_TOKEN_ID\n    BOS_TOKEN_ID = tokenizer.bos_token_id\n    EOS_TOKEN_ID = tokenizer.eos_token_id\n    EOQ_TOKEN_ID = tokenizer.eoq_token_id\n    PAD_TOKEN_ID = tokenizer.pad_token_id\n\n    return tokenizer\n\n\ndef load_synth_data(data_config):\n    \"\"\"\n    Load the synthetic data for contrastive learning\n    \"\"\"\n    if data_config.is_debug:\n        synth_dir = 'cache_debug/synthetic/'\n    else:\n        synth_dir = os.path.join(data_config.cache_dir,'synthetic')\n\n    logger.info('Loading synthetic data from \\'{}\\''.format(synth_dir))\n    synth_prim_weight = data_config.hetero_synth_prim_weight\n    with open(os.path.join(synth_dir,'shapes.json')) as f:\n        shapes = json.load(f)\n    synth_feat_path = os.path.join(\n        synth_dir, 'feature_{}.memmap'.format(synth_prim_weight))\n    synth_tok_path = os.path.join(synth_dir,\n                                  'token_{}.memmap'.format(synth_prim_weight))\n    synth_feats = np.memmap(filename=synth_feat_path,", "metadata": {"task_id": "alibaba_FederatedScope/22", "ground_truth": "                            shape=tuple(shapes['feature']),", "fpath_tuple": ["alibaba_FederatedScope", "federatedscope", "nlp", "hetero_tasks", "dataset", "utils.py"], "context_start_lineno": 0, "line_no": 111}}
{"prompt": "   ...\n\n  def select(self,\n             parameter_name,\n             parameter_values: Optional[MonotypeParameterSequence] = None):\n    \"\"\"Selects a parameter config or its subspace.\n\n    This method is for constructing a _conditional_ search space.\n\n    EXAMPLE: Suppose we have a selector to the root of the search space with one\n    categorical parameter.\n    root = pyvizier.SearchSpace().root\n    root.add_categorical_param('model_type', ['dnn', 'linear'])\n\n    1) Select a `ParameterConfig`:\n      model = root.select('model_type')\n\n    2) Select a subspace conditioned on `model_type == 'dnn'` and add\n    a child parameter `hidden_units`:\n      dnn_subspace = root.select('model_type', ['dnn'])\n      dnn_subspace.add_int_param('hidden_layers',...)\n\n    or equivalently,\n      dnn_subspace = root.select('model_type').select_values(['dnn'])\n      dnn_subspace.add_int_param('hidden_layers',...)\n\n    3) Traverse your search space by chaining select() calls:\n      root.select('model_type', ['dnn']).select('hidden_layers', [1, 2])\n\n    4) Select more than one search space simultaneously:\n      selected = root.select('model_type', ['dnn', 'linear'])\n       .add_categorical_param('optimizer', ['adam', 'adagrad'])\n      assert len(selected) == 4  # {dnn, linear} x {adam, adagard}\n\n    Args:\n      parameter_name:\n      parameter_values: Optional parameter values for this selector, which will\n        be used to add child parameters, or traverse a conditional tree.\n\n    Returns:\n      ParameterConfigSelector for `ParameterConfig`(s) if the values are not\n        specified.\n      SearchSpaceSelector for subspace(s) if parameter_values are specified.\n    \"\"\"\n    if parameter_values is None:\n      selected_configs = []\n      for space in self._selected:\n        selected_configs.append(space.get(parameter_name))\n      return ParameterConfigSelector(selected_configs)\n    else:\n      selected_spaces = []\n      for space in self._selected:\n        selected_parameter = space.get(parameter_name)\n        for value in parameter_values:\n          selected_spaces.append(selected_parameter.subspace(value))\n      return SearchSpaceSelector(selected_spaces)\n\n  @classmethod\n  def _get_parameter_names_to_create(cls,\n                                     *,\n                                     name: str,\n                                     length: Optional[int] = None,\n                                     index: Optional[int] = None) -> List[str]:\n    \"\"\"Returns the names of all parameters which should be created.\n\n    Args:\n      name: The base parameter name.\n      length: Specifies the length of a multi-dimensional parameters. If larger\n        than 1, then multiple ParameterConfigs are added. E.g. if name='rate'\n        and length=2, then two ParameterConfigs with names 'rate[0]', 'rate[1]'\n        are added. Cannot be specified together with `index`.\n      index: Specifies the multi-dimensional index for this parameter. Cannot be\n        specified together with `length`. E.g. if name='rate' and index=1, then\n        a single ParameterConfig with name 'rate[1]' is added.\n\n    Returns:\n      List of parameter names to create.\n\n    Raises:\n      ValueError: If `length` or `index` are invalid.\n    \"\"\"\n    if length is not None and index is not None:\n      raise ValueError('Only one of `length` and `index` can be specified. Got'\n                      'length={}, index={}'.format(length, index))\n    if length is not None and length < 1:\n      raise ValueError('length must be >= 1. Got length={}'.format(length))\n    if index is not None and index < 0:\n      raise ValueError('index must be >= 0. Got index={}'.format(index))\n\n    param_names = []\n    if length is None and index is None:\n      # Add one parameter with no multi-dimensional index.\n      param_names.append(name)\n    elif index is not None:\n      # Add one parameter with a multi-dimensional index.\n      param_names.append(cls._multi_dimensional_parameter_name(name, index))\n    elif length is not None:\n      # `length > 0' is synthatic sugar for multi multi-dimensional parameter.\n      # Each multi-dimensional parameter is encoded as a list of separate\n      # parameters with names equal to `name[index]` (index is zero based).\n      for i in range(length):\n        param_names.append(cls._multi_dimensional_parameter_name(name, i))\n    return param_names\n\n  @classmethod\n  def _multi_dimensional_parameter_name(cls, name: str, index: int) -> str:\n    \"\"\"Returns the indexed parameter name.\"\"\"\n    return '{}[{}]'.format(name, index)\n\n  @classmethod\n  def parse_multi_dimensional_parameter_name(\n      cls, name: str) -> Optional[Tuple[str, int]]:\n    \"\"\"Returns the base name for a multi-dimensional parameter name.\n\n    Args:\n      name: A parameter name.\n\n    Returns:\n      (base_name, index): if name='hidden_units[10]', base_name='hidden_units'\n        and index=10.\n      Returns None if name is not in the format 'base_name[idx]'.\n    \"\"\"\n    regex = r'(?P<name>[^()]*)\\[(?P<index>\\d+)\\]$'\n    pattern = re.compile(regex)\n    matches = pattern.match(name)\n    if matches is None:\n      return None\n    return (matches.groupdict()['name'], int(matches.groupdict()['index']))\n\n  # TODO: Add def extend(space: SearchSpace)\n  def _add_parameters(\n      self, parameters: List[ParameterConfig]) -> ParameterConfigSelector:\n    \"\"\"Adds deepcopy of the ParameterConfigs.\n\n    Args:\n      parameters: The parameters to add to the search space.\n\n    Returns:\n      A list of SearchSpaceSelectors, one for each parameters added.\n    \"\"\"\n    logging.info('Adding child parameters %s to %s subspaces ',\n                 set(p.name for p in parameters), len(self._selected))\n    added = []\n    for parameter in parameters:\n      for selected in self._selected:\n        # Adds a deepcopy so that every ParameterConfig object is unique.\n        added.append(selected.add(copy.deepcopy(parameter)))\n\n    return ParameterConfigSelector(added)\n\n\n@attr.define(frozen=False, init=True, slots=True, kw_only=True)\nclass SearchSpace:\n  \"\"\"[Cross-platform] Collection of ParameterConfigs.\n\n  Vizier search space can be *conditional*.\n  Parameter names are guaranteed to be unique in any subspace.\n\n  Attribute:\n    _parameter_configs: Maps parameter names to configs.\n  \"\"\"\n  _parameter_configs: dict[str, ParameterConfig] = attr.field(\n      init=False, factory=dict)\n\n  # TODO: To be deprecated.\n  _parent_values: MonotypeParameterSequence = attr.field(\n      default=tuple(), converter=tuple, kw_only=True)\n\n  @property\n  def parameter_names(self) -> AbstractSet[str]:\n    return self._parameter_configs.keys()\n\n  def get(self, name: str) -> ParameterConfig:", "metadata": {"task_id": "google_vizier/10", "ground_truth": "    if name not in self._parameter_configs:", "fpath_tuple": ["google_vizier", "vizier", "_src", "pyvizier", "shared", "parameter_config.py"], "context_start_lineno": 955, "line_no": 1128}}
{"prompt": " stochatstic_part)\n        >>> tensordict = TensorDict({'obs': torch.randn(state_dim), '_epx_gSDE': torch.zeros(1)}, [])\n        >>> _ = stochatstic_policy(tensordict)\n        >>> print(tensordict)\n        TensorDict(\n            fields={\n                obs: Tensor(torch.Size([7]), dtype=torch.float32),\n                _epx_gSDE: Tensor(torch.Size([1]), dtype=torch.float32),\n                action: Tensor(torch.Size([5]), dtype=torch.float32),\n                loc: Tensor(torch.Size([5]), dtype=torch.float32),\n                scale: Tensor(torch.Size([5]), dtype=torch.float32),\n                _eps_gSDE: Tensor(torch.Size([5, 7]), dtype=torch.float32)},\n            batch_size=torch.Size([]),\n            device=cpu,\n            is_shared=False)\n        >>> action_first_call = tensordict.get(\"action\").clone()\n        >>> dist, *_ = stochatstic_policy.get_dist(tensordict)\n        >>> print(dist)\n        TanhNormal(loc: torch.Size([5]), scale: torch.Size([5]))\n        >>> _ = stochatstic_policy(tensordict)\n        >>> action_second_call = tensordict.get(\"action\").clone()\n        >>> assert (action_second_call == action_first_call).all()  # actions are the same\n        >>> assert (action_first_call!= dist.base_dist.base_dist.loc).all()  # actions are truly stochastic\n\n    \"\"\"\n\n    def __init__(\n        self,\n        action_dim: int,\n        state_dim: int,\n        sigma_init: float = None,\n        scale_min: float = 0.01,\n        scale_max: float = 10.0,\n        learn_sigma: bool = True,\n        transform: Optional[d.Transform] = None,\n        device: Optional[DEVICE_TYPING] = None,\n    ) -> None:\n        super().__init__()\n        self.action_dim = action_dim\n        self.state_dim = state_dim\n        self.scale_min = scale_min\n        self.scale_max = scale_max\n        self.transform = transform\n        self.learn_sigma = learn_sigma\n        if learn_sigma:\n            if sigma_init is None:\n                sigma_init = inv_softplus(math.sqrt((1.0 - scale_min) / state_dim))\n            self.register_parameter(\n                \"log_sigma\",\n                nn.Parameter(\n                    torch.zeros(\n                        (action_dim, state_dim), requires_grad=True, device=device\n                    )\n                ),\n            )\n        else:\n            if sigma_init is None:\n                sigma_init = math.sqrt((1.0 - scale_min) / state_dim)\n            self.register_buffer(\n                \"_sigma\",\n                torch.full((action_dim, state_dim), sigma_init, device=device),\n            )\n\n        if sigma_init!= 0.0:\n            self.register_buffer(\"sigma_init\", torch.tensor(sigma_init, device=device))\n\n    @property\n    def sigma(self):\n        if self.learn_sigma:\n            sigma = torch.nn.functional.softplus(self.log_sigma)\n            return sigma.clamp_min(self.scale_min)\n        else:\n            return self._sigma.clamp_min(self.scale_min)\n\n    def forward(self, mu, state, _eps_gSDE):\n        sigma = self.sigma.clamp_max(self.scale_max)\n        _err_explo = f\"gSDE behaviour for exploration mode {exploration_mode()} is not defined. Choose from 'random' or'mode'.\"\n\n        if state.shape[:-1]!= mu.shape[:-1]:\n            _err_msg = f\"mu and state are expected to have matching batch size, got shapes {mu.shape} and {state.shape}\"\n            raise RuntimeError(_err_msg)\n        if _eps_gSDE is not None and (\n            _eps_gSDE.shape[: state.ndimension() - 1]!= state.shape[:-1]\n        ):\n            _err_msg = f\"noise and state are expected to have matching batch size, got shapes {_eps_gSDE.shape} and {state.shape}\"\n            raise RuntimeError(_err_msg)\n\n        if _eps_gSDE is None and exploration_mode() == \"mode\":\n            # noise is irrelevant in with no exploration\n            _eps_gSDE = torch.zeros(\n                *state.shape[:-1], *sigma.shape, device=sigma.device, dtype=sigma.dtype\n            )\n        elif (_eps_gSDE is None and exploration_mode() == \"random\") or (\n            _eps_gSDE is not None\n            and _eps_gSDE.numel() == prod(state.shape[:-1])\n            and (_eps_gSDE == 0).all()\n        ):\n            _eps_gSDE = torch.randn(\n                *state.shape[:-1], *sigma.shape, device=sigma.device, dtype=sigma.dtype\n            )\n        elif _eps_gSDE is None:\n            raise RuntimeError(_err_explo)\n\n        gSDE_noise = sigma * _eps_gSDE\n        eps = (gSDE_noise @ state.unsqueeze(-1)).squeeze(-1)\n\n        if exploration_mode() in (\"random\",):\n            action = mu + eps\n        elif exploration_mode() in (\"mode\",):\n            action = mu\n        else:\n            raise RuntimeError(_err_explo)\n\n        sigma = (sigma * state.unsqueeze(-2)).pow(2).sum(-1).clamp_min(1e-5).sqrt()\n        if not torch.isfinite(sigma).all():\n            print(\"inf sigma\")\n\n        if self.transform is not None:\n            action = self.transform(action)\n        return mu, sigma, action, _eps_gSDE\n\n    def to(self, device_or_dtype: Union[torch.dtype, DEVICE_TYPING]):\n        if isinstance(device_or_dtype, DEVICE_TYPING_ARGS):\n            self.transform = _cast_transform_device(self.transform, device_or_dtype)\n        return super().to(device_or_dtype)\n\n\nclass LazygSDEModule(LazyModuleMixin, gSDEModule):\n    \"\"\"Lazy gSDE Module.\n\n    This module behaves exactly as gSDEModule except that it does not require the\n    user to specify the action and state dimension.\n    If the input state is multi-dimensional (i.e. more than one state is provided), the\n    sigma value is initialized such that the resulting variance will match :obj:`sigma_init`\n    (or 1 if no :obj:`sigma_init` value is provided).\n\n    \"\"\"\n\n    cls_to_become = gSDEModule\n    log_sigma: UninitializedParameter\n    _sigma: UninitializedBuffer\n    sigma_init: UninitializedBuffer\n\n    def __init__(\n        self,\n        sigma_init: float = None,\n        scale_min: float = 0.01,\n        scale_max: float = 10.0,\n        learn_sigma: bool = True,", "metadata": {"task_id": "pytorch_rl/116", "ground_truth": "        transform: Optional[d.Transform] = None,", "fpath_tuple": ["pytorch_rl", "torchrl", "modules", "models", "exploration.py"], "context_start_lineno": 277, "line_no": 426}}
{"prompt": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import annotations\n\n\"\"\"HPOB Handler copied (with slightly modified filesystem logic) from\n\nhttps://github.com/releaunifreiburg/HPO-B/blob/main/hpob_handler.py.\n\"\"\"\n# pylint:skip-file\n\nimport json\nimport os\n\nimport functools\nimport numpy as np\nimport xgboost as xgb\n\nOpen = open\nExists = os.path.exists\nIsDir = os.path.isdir\n\n\nclass HPOBHandler:\n\n  def __init__(self,\n               root_dir=\"hpob-data/\",\n               mode=\"v3-test\",\n               surrogates_dir=\"saved-surrogates/\"):\n    \"\"\"Constructor for the HPOBHandler.\n\n    Inputs:\n        * root_dir: path to directory with the benchmark data.\n        * mode: mode name indicating how to load the data. Options:\n            - v1: Loads HPO-B-v1\n            - v2: Loads HPO-B-v2\n            - v3: Loads HPO-B-v3\n            - v3-test: Loads only the meta-test split from HPO-B-v3\n            - v3-train-augmented: Loads all splits from HPO-B-v3, but\n            augmenting the meta-train data with the less frequent\n            search-spaces.\n        * surrogates_dir: path to directory with surrogates models.\n    \"\"\"\n\n    print(\"Loading HPO-B handler\")", "metadata": {"task_id": "google_vizier/139", "ground_truth": "    self.mode = mode", "fpath_tuple": ["google_vizier", "vizier", "_src", "benchmarks", "experimenters", "hpob", "handler.py"], "context_start_lineno": 0, "line_no": 56}}
{"prompt": "# Copyright 2022 The Nerfstudio Team. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\nCollection of sampling strategies\n\"\"\"\n\nfrom abc import abstractmethod\nfrom typing import Callable, List, Optional, Tuple\n\nimport nerfacc\nimport torch\nfrom nerfacc import OccupancyGrid\nfrom torch import nn\nfrom torchtyping import TensorType\n\nfrom nerfstudio.cameras.rays import Frustums, RayBundle, RaySamples\n\n\nclass Sampler(nn.Module):\n    \"\"\"Generate Samples\n\n    Args:\n        num_samples: number of samples to take\n    \"\"\"\n\n    def __init__(\n        self,\n        num_samples: Optional[int] = None,\n    ) -> None:\n        super().__init__()\n        self.num_samples = num_samples\n\n    @abstractmethod\n    def generate_ray_samples(self) -> RaySamples:\n        \"\"\"Generate Ray Samples\"\"\"\n\n    def forward(self, *args, **kwargs) -> RaySamples:\n        \"\"\"Generate ray samples\"\"\"\n        return self.generate_ray_samples(*args, **kwargs)\n\n\nclass SpacedSampler(Sampler):\n    \"\"\"Sample points according to a function.\n\n    Args:\n        num_samples: Number of samples per ray\n        spacing_fn: Function that dictates sample spacing (ie `lambda x : x` is uniform).\n        spacing_fn_inv: The inverse of spacing_fn.\n        train_stratified: Use stratified sampling during training. Defaults to True\n        single_jitter: Use a same random jitter for all samples along a ray. Defaults to False\n    \"\"\"\n\n    def __init__(\n        self,\n        spacing_fn: Callable,\n        spacing_fn_inv: Callable,\n        num_samples: Optional[int] = None,\n        train_stratified=True,\n        single_jitter=False,\n    ) -> None:\n        super().__init__(num_samples=num_samples)\n        self.train_stratified = train_stratified\n        self.single_jitter = single_jitter\n        self.spacing_fn = spacing_fn\n        self.spacing_fn_inv = spacing_fn_inv\n\n    def generate_ray_samples(\n        self,\n        ray_bundle: Optional[RayBundle] = None,\n        num_samples: Optional[int] = None,\n    ) -> RaySamples:\n        \"\"\"Generates position samples according to spacing function.\n\n        Args:\n            ray_bundle: Rays to generate samples for\n            num_samples: Number of samples per ray\n\n        Returns:\n            Positions and deltas for samples along a ray\n        \"\"\"\n        assert ray_bundle is not None\n        assert ray_bundle.nears is not None\n        assert ray_bundle.fars is not None\n\n        num_samples = num_samples or self.num_samples\n        assert num_samples is not None\n        num_rays = ray_bundle.origins.shape[0]\n\n        bins = torch.linspace(0.0, 1.0, num_samples + 1).to(ray_bundle.origins.device)[None,...]  # [1, num_samples+1]\n\n        # TODO More complicated than it needs to be.\n        if self.train_stratified and self.training:\n            if self.single_jitter:\n                t_rand = torch.rand((num_rays, 1), dtype=bins.dtype, device=bins.device)\n            else:\n                t_rand = torch.rand((num_rays, num_samples + 1), dtype=bins.dtype, device=bins.device)\n            bin_centers = (bins[..., 1:] + bins[..., :-1]) / 2.0\n            bin_upper = torch.cat([bin_centers, bins[..., -1:]], -1)\n            bin_lower = torch.cat([bins[..., :1], bin_centers], -1)\n            bins = bin_lower + (bin_upper - bin_lower) * t_rand\n\n        s_near, s_far = (self.spacing_fn(x) for x in (ray_bundle.nears, ray_bundle.fars))\n        spacing_to_euclidean_fn = lambda x: self.spacing_fn_inv(x * s_far + (1 - x) * s_near)\n        euclidean_bins = spacing_to_euclidean_fn(bins)  # [num_rays, num_samples+1]\n\n        ray_samples = ray_bundle.get_ray_samples(\n            bin_starts=euclidean_bins[..., :-1, None],\n            bin_ends=euclidean_bins[..., 1:, None],\n            spacing_starts=bins[..., :-1, None],\n            spacing_ends=bins[..., 1:, None],\n            spacing_to_euclidean_fn=spacing_to_euclidean_fn,\n        )\n\n        return ray_samples\n\n\nclass UniformSampler(SpacedSampler):\n    \"\"\"Sample uniformly along a ray\n\n    Args:\n        num_samples: Number of samples per ray\n        train_stratified: Use stratified sampling during training. Defaults to True\n        single_jitter: Use a same random jitter for all samples along a ray. Defaults to False\n    \"\"\"\n\n    def __init__(\n        self,\n        num_samples: Optional[int] = None,\n        train_stratified=True,\n        single_jitter=False,\n    ) -> None:\n        super().__init__(\n            num_samples=num_samples,\n            spacing_fn=lambda x: x,", "metadata": {"task_id": "nerfstudio-project_nerfstudio/92", "ground_truth": "            spacing_fn_inv=lambda x: x,", "fpath_tuple": ["nerfstudio-project_nerfstudio", "nerfstudio", "model_components", "ray_samplers.py"], "context_start_lineno": 0, "line_no": 146}}
{"prompt": "from typing import Optional, Tuple, Union\n\nimport flax.linen as nn\nimport jax.numpy as jnp\nfrom flax.core import FrozenDict\nfrom flax.training.checkpoints import PyTree\nfrom jax import random\nfrom jax._src.prng import PRNGKeyArray\n\nfrom fortuna.typing import Array, CalibMutable, CalibParams\nfrom fortuna.utils.random import WithRNG\n\n\nclass OutputCalibManager(WithRNG):\n    def __init__(self, output_calibrator: Optional[nn.Module] = None):\n        self.output_calibrator = output_calibrator\n\n    def apply(\n        self,\n        params: CalibParams,\n        outputs: Array,\n        mutable: Optional[CalibMutable] = None,\n        calib: bool = False,\n        rng: Optional[PRNGKeyArray] = None,\n    ) -> Union[jnp.ndarray, Tuple[jnp.ndarray, PyTree]]:\n        \"\"\"\n        Apply the output calibrator forward pass.\n\n        Parameters\n        ----------\n        params : Params\n            The random parameters of the probabilistic model.\n        outputs : Array\n            Outputs for each data point.\n        mutable : Optional[Mutable]\n            The mutable objects used to evaluate the models.\n        calib : bool\n            Whether the method is called during calibration.\n        rng: Optional[PRNGKeyArray]\n            A random number generator.\n            If not passed,\n            this will be taken from the attributes of this class.\n\n        Returns\n        -------\n        Union[jnp.ndarray, Tuple[jnp.ndarray, PyTree]]\n            The output of the model manager for each input. Mutable objects may also be returned.\n        \"\"\"\n        if self.output_calibrator is None:\n            return outputs\n        if mutable is None:\n            mutable = False\n        variables = params.unfreeze()\n\n        # setup dropout key\n        if rng is not None:\n            rng, dropout_rng = random.split(rng, 2)\n            rngs = {\"dropout\": dropout_rng}\n        else:\n            rngs = None\n\n        if mutable:\n            mutable_variables = mutable.unfreeze()\n            variables.update(mutable_variables)\n            mutable = list(mutable_variables.keys())\n        if calib and mutable:\n            outputs, mutable = self.output_calibrator.apply(\n                variables, outputs, mutable=mutable, train=calib, rngs=rngs\n            )\n            return outputs, {\"mutable\": mutable}\n        else:\n            return self.output_calibrator.apply(\n                variables, outputs, train=calib, mutable=False, rngs=rngs\n            )\n\n    def init(\n        self, output_dim: int, rng: Optional[PRNGKeyArray] = None, **kwargs\n    ) -> Optional[FrozenDict]:\n        \"\"\"\n        Initialize random parameters and mutable objects.\n\n        Parameters\n        ----------\n        output_dim: int\n            The output dimension.\n        rng: Optional[PRNGKeyArray]\n            A random number generator.\n            If not passed,\n            this will be taken from the attributes of this class.\n\n        Returns\n        -------\n        Optional[FrozenDict]\n            Initialized random parameters and mutable objects.\n        \"\"\"\n        if rng is None:\n            rng = self.rng.get()\n        rng, params_key, dropout_key = random.split(rng, 3)\n        rngs = {\"params\": params_key, \"dropout\": dropout_key}\n        return (\n            self.output_calibrator.init(rngs, jnp.zeros((1, output_dim)), **kwargs)\n            if self.output_calibrator is not None", "metadata": {"task_id": "awslabs_fortuna/142", "ground_truth": "            else None", "fpath_tuple": ["awslabs_fortuna", "fortuna", "output_calibrator", "output_calib_manager", "base.py"], "context_start_lineno": 0, "line_no": 102}}
{"prompt": "from typing import Optional, Tuple, Union\n\nimport flax.linen as nn\nimport jax.numpy as jnp\nfrom flax.core import FrozenDict\nfrom flax.training.checkpoints import PyTree\nfrom jax import random\nfrom jax._src.prng import PRNGKeyArray\n\nfrom fortuna.typing import Array, CalibMutable, CalibParams\nfrom fortuna.utils.random import WithRNG\n\n\nclass OutputCalibManager(WithRNG):\n    def __init__(self, output_calibrator: Optional[nn.Module] = None):\n        self.output_calibrator = output_calibrator\n\n    def apply(\n        self,", "metadata": {"task_id": "awslabs_fortuna/105", "ground_truth": "        params: CalibParams,", "fpath_tuple": ["awslabs_fortuna", "fortuna", "output_calibrator", "output_calib_manager", "base.py"], "context_start_lineno": 0, "line_no": 19}}
{"prompt": "import json\nimport socket\nimport time\nfrom typing import Optional, Any, Mapping, Callable, Type, Tuple\n\nimport requests\nfrom requests import HTTPError\nfrom urlobject import URLObject\nfrom urlobject.path import URLPath\n\nfrom.common import translate_dict_func\n\n\ndef get_host_ip() -> Optional[str]:\n    s = None\n    try:\n        s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n        s.connect(('8.8.8.8', 80))\n        ip = s.getsockname()[0]\n    finally:\n        if s is not None:\n            s.close()\n    return ip\n\n\n_DEFAULT_HTTP_PORT = 80\n_DEFAULT_HTTPS_PORT = 443\n\n\ndef split_http_address(address: str, default_port: Optional[int] = None) -> Tuple[str, int, bool, str]:\n    _url = URLObject(address)\n\n    _host = _url.hostname\n    _https = (_url.scheme.lower()) == 'https'\n    _port = _url.port or default_port or (_DEFAULT_HTTPS_PORT if _https else _DEFAULT_HTTP_PORT)\n    _path = str(_url.path) or ''\n\n    return _host, _port, _https, _path\n\n\nDEFAULT_RETRIES = 5\nDEFAULT_RETRY_WAITING = 1.0\n\n\nclass HttpEngine:\n\n    def __init__(self, host: str, port: int, https: bool = False, path: str = None):\n        self.__base_url = URLObject().with_scheme('https' if https else 'http') \\\n           .with_hostname(host).with_port(port).add_path(path or '')\n        self.__session = requests.session()\n        self.__session.trust_env = False\n\n    # noinspection PyMethodMayBeStatic\n    def _data_process(self, data: Optional[Mapping[str, Any]] = None) -> Mapping[str, Any]:\n        return data or {}\n\n    # noinspection PyMethodMayBeStatic\n    def _base_headers(self) -> Mapping[str, None]:\n        return {}\n\n    def _error_handler(self, err: Exception):\n        raise err\n\n    def get_url(self, path: str = None):\n        original_segments = self.__base_url.path.segments\n        path_segments = URLPath().add(path or '').segments\n        return str(self.__base_url.with_path(URLPath.join_segments(original_segments + path_segments)))\n\n    def __single_request(\n        self,\n        method: str,\n        path: str,\n        data: Optional[Mapping[str, Any]] = None,\n        headers: Optional[Mapping[str, Any]] = None,\n        params: Optional[Mapping[str, Any]] = None,\n        raise_for_status: bool = True\n    ):\n        response = self.__session.request(\n            method=method,\n            url=self.get_url(path),\n            data=json.dumps(self._data_process(data) or {}),\n            headers=headers,\n            params=params or {},\n        )\n        if raise_for_status:\n            response.raise_for_status()\n\n        return response\n\n    def request(\n            self,\n            method: str,\n            path: str,\n            data: Optional[Mapping[str, Any]] = None,\n            headers: Optional[Mapping[str, Any]] = None,\n            params: Optional[Mapping[str, Any]] = None,\n            raise_for_status: bool = True,\n            retries: Optional[int] = None,\n            retry_waiting: Optional[float] = None,\n    ) -> requests.Response:\n        _headers = dict(self._base_headers())\n        _headers.update(headers or {})\n\n        retries = retries or DEFAULT_RETRIES\n        retry_waiting = retry_waiting or DEFAULT_RETRY_WAITING\n\n        try:\n            _current_retries = 0\n            while True:\n                try:\n                    response = self.__single_request(method, path, data, _headers, params, raise_for_status)\n                except requests.exceptions.HTTPError as err:", "metadata": {"task_id": "opendilab_ACE/125", "ground_truth": "                    raise err", "fpath_tuple": ["opendilab_ACE", "ding", "interaction", "base", "network.py"], "context_start_lineno": 0, "line_no": 112}}
{"prompt": "import os\nimport logging\nimport random\nimport csv\nimport json\nimport gzip\nimport zipfile\nimport shutil\nimport copy\nimport torch\nimport numpy as np\nfrom tqdm import tqdm\nfrom federatedscope.core.data.utils import download_url\nfrom federatedscope.core.gpu_manager import GPUManager\nfrom federatedscope.nlp.hetero_tasks.model.model import ATCModel\n\nlogger = logging.getLogger(__name__)\n\n\nclass HeteroNLPDataLoader(object):\n    \"\"\"\n    Load hetero NLP task datasets (including multiple datasets), split them\n    into train/val/test, and partition into several clients.\n    \"\"\"\n    def __init__(self, data_dir, data_name, num_of_clients, split=[0.9, 0.1]):\n        self.data_dir = data_dir\n        self.data_name = data_name\n        self.num_of_clients = num_of_clients\n        self.split = split  # split for train:val\n        self.train_data = []\n        self.val_data = []\n        self.test_data = []\n\n    def get_data(self):\n        for each_data, each_client_num in zip(self.data_name,\n                                              self.num_of_clients):\n            train_and_val_data = self._load(each_data, 'train',\n                                            each_client_num)", "metadata": {"task_id": "alibaba_FederatedScope/103", "ground_truth": "            each_train_data = [", "fpath_tuple": ["alibaba_FederatedScope", "federatedscope", "nlp", "hetero_tasks", "dataset", "get_data.py"], "context_start_lineno": 0, "line_no": 38}}
{"prompt": "__set_rng()\n\n    def __set_rng(self):\n        self.model_manager.rng = self.rng\n        self.output_calib_manager.rng = self.rng\n        self.prob_output_layer.rng = self.rng\n        self.prior.rng = self.rng\n        self.likelihood.rng = self.rng\n        self.joint.rng = self.rng\n        self.posterior.rng = self.rng\n        self.predictive.rng = self.rng\n\n    def train(\n        self,\n        train_data_loader: DataLoader,\n        val_data_loader: Optional[DataLoader] = None,\n        calib_data_loader: Optional[DataLoader] = None,\n        fit_config: FitConfig = FitConfig(),\n        calib_config: CalibConfig = CalibConfig(),\n        map_fit_config: Optional[FitConfig] = None,\n    ) -> Dict[str, Status]:\n        \"\"\"\n        Train the probabilistic model. This involves fitting the posterior distribution and calibrating the\n        probabilistic model. Calibration is performed only if (1) `calib_data_loader` is passed and (2) the\n        probabilistic model contains any calibrator.\n\n        Parameters\n        ----------\n        train_data_loader : DataLoader\n            A training data loader.\n        val_data_loader : DataLoader\n            A validation data loader. This is used to validate both posterior fitting and calibration.\n        calib_data_loader : DataLoader\n            A calibration data loader. If this is not passed, no calibration is performed.\n        fit_config : FitConfig\n            An object to configure the posterior distribution fitting.\n        calib_config : CalibConfig\n            An object to configure the calibration.\n        map_fit_config : Optional[FitConfig] = None\n            An object to configure a preliminary posterior distribution fitting via the Maximum-A-Posteriori (MAP)\n            method.\n            The fit methods of several supported posterior approximations, like the ones of\n            :class:`~fortuna.prob_model.posterior.swag.swag_posterior.SWAGPosterior` and\n            :class:`~fortuna.prob_model.posterior.laplace.laplace_posterior.LaplacePosterior`, start from a preliminary\n            run of MAP, which can be configured via this object. If the method does not start from MAP, this argument is\n            ignored.\n\n        Returns\n        -------\n        Dict[str, Status]\n            Status objects for both posterior fitting and calibration.\n\n        \"\"\"\n        logging.info(\"Fit the posterior distribution...\")\n        fit_status = self.posterior.fit(\n            train_data_loader=train_data_loader,\n            val_data_loader=val_data_loader,\n            fit_config=fit_config,\n            map_fit_config=map_fit_config,\n        )\n        logging.info(\"Fit completed.\")\n\n        calib_status = None\n        if calib_data_loader:\n            calib_status = self.calibrate(\n                calib_data_loader=calib_data_loader,\n                val_data_loader=val_data_loader,\n                calib_config=calib_config,\n            )\n            logging.info(\"Calibration completed.\")\n        return dict(fit_status=fit_status, calib_status=calib_status)\n\n    def _calibrate(\n        self,\n        calib_data_loader: DataLoader,\n        uncertainty_fn: Callable[[jnp.ndarray, jnp.ndarray, Array], jnp.ndarray],\n        val_data_loader: Optional[DataLoader] = None,\n        calib_config: CalibConfig = CalibConfig(),\n    ) -> Status:\n        check_data_loader_is_not_random(calib_data_loader)\n        if val_data_loader is not None:\n            check_data_loader_is_not_random(val_data_loader)\n        if self.output_calib_manager is None:\n            logging.warning(\n                \"\"\"Nothing to calibrate. No calibrator was passed to the probabilistic model.\"\"\"\n            )\n        else:\n            if self.posterior.state is None:\n                raise ValueError(\n                    \"\"\"Before calibration, you must either train the probabilistic model (see \n                        :meth:`~fortuna.prob_model.base.ProbModel.train`), \n                        or load a state from an existing checkpoint \n                        (see :meth:`~fortuna.prob_model.base.ProbModel.load_state`).\"\"\"\n                )\n            if calib_config.monitor.verbose:\n                logging.info(\n                    \"Pre-compute ensemble of outputs on the calibration data loader.\"\n                )\n\n            distribute = jax.local_device_count() > 1\n\n            (\n                calib_ensemble_outputs_loader,\n                calib_size,\n            ) = self.predictive._sample_outputs_loader(\n                inputs_loader=calib_data_loader.to_inputs_loader(),\n                n_output_samples=calib_config.processor.n_posterior_samples,\n                return_size=True,\n                distribute=distribute,\n            )\n            if calib_config.monitor.verbose:\n                logging.info(\n                    \"Pre-compute ensemble of outputs on the validation data loader.\"\n                )\n            val_ensemble_outputs_loader, val_size = (\n                self.predictive._sample_outputs_loader(\n                    inputs_loader=val_data_loader.to_inputs_loader(),\n                    n_output_samples=calib_config.processor.n_posterior_samples,\n                    return_size=True,\n                    distribute=distribute,\n                )\n                if val_data_loader is not None\n                else (None, None)\n            )\n\n            trainer_cls = select_trainer_given_devices(\n                devices=calib_config.processor.devices,\n                BaseTrainer=ProbModelCalibrator,\n                JittedTrainer=JittedProbModelCalibrator,\n                MultiDeviceTrainer=MultiDeviceProbModelCalibrator,\n                disable_jit=calib_config.processor.disable_jit,\n            )\n\n            calibrator = trainer_cls(\n                calib_outputs_loader=calib_ensemble_outputs_loader,\n                val_outputs_loader=val_ensemble_outputs_loader,\n                predict_fn=self.prob_output_layer.predict,\n                uncertainty_fn=uncertainty_fn,\n                save_checkpoint_dir=calib_config.checkpointer.save_checkpoint_dir,\n                save_every_n_steps=calib_config.checkpointer.save_every_n_steps,\n                keep_top_n_checkpoints=calib_config.checkpointer.keep_top_n_checkpoints,\n                disable_training_metrics_computation=calib_config.monitor.disable_calibration_metrics_computation,\n                eval_every_n_epochs=calib_config.monitor.eval_every_n_epochs,\n                early_stopping_monitor=calib_config.monitor.early_stopping_monitor,\n                early_stopping_min_delta=calib_config.monitor.early_stopping_min_delta,\n                early_stopping_patience=calib_config.monitor.early_stopping_patience,\n            )\n\n            if calib_config.checkpointer.restore_checkpoint_path is None:\n                calib_dict = self.posterior.state.extract_calib_keys()\n\n                state = CalibState.init(", "metadata": {"task_id": "awslabs_fortuna/138", "ground_truth": "                    params=calib_dict[\"calib_params\"],", "fpath_tuple": ["awslabs_fortuna", "fortuna", "prob_model", "base.py"], "context_start_lineno": 27, "line_no": 179}}
{"prompt": " Client parameters\n        # necessary for compatibility with pysc2\n        from absl import flags\n        flags.FLAGS(['smac'])\n        self.agent_interface_format = sc2_env.parse_agent_interface_format(use_raw_units=True)\n        self.save_replay_episodes = cfg.save_replay_episodes\n        assert (self.save_replay_episodes is None) or isinstance(\n            self.save_replay_episodes, int\n        )  # Denote the number of replays to save\n        self.game_steps_per_episode = cfg.game_steps_per_episode\n\n        # Map parameters\n        map_name = cfg.map_name\n        assert map_name is not None\n        map_params = get_map_params(map_name)\n        self._map_name = map_name\n        self.map_type = map_params[\"map_type\"]\n        self.agent_race = map_params[\"a_race\"]\n        self.bot_race = map_params[\"b_race\"]\n        self.difficulty = cfg.difficulty\n        self.players = [sc2_env.Agent(races[self.agent_race]), sc2_env.Bot(races[self.bot_race], self.difficulty)]\n        self.n_agents = map_params[\"n_agents\"]\n        self.n_enemies = map_params[\"n_enemies\"]\n        self.n_entities = self.n_agents + self.n_enemies\n        self._episode_limit = map_params[\"limit\"]\n\n        # Reset parameters\n        self._seed = None\n        self._launch_env_flag = True\n        self._abnormal_env_flag = False\n\n        # Counter parameters\n        self._total_steps = 0\n        self._next_reset_steps = FORCE_RESTART_INTERVAL\n        self._won_count = 0\n        self._episode_count = 0\n        self._timeouts = 0\n        self._force_restarts = 0\n\n        # Reward parameters\n        self.reward_pos_scale = 1.0\n        self.reward_neg_scale = 0.0\n        self.reward_death_value = cfg.reward_death_value\n        self.reward_win = cfg.reward_win\n        self.reward_defeat = 0\n        self.reward_scale = cfg.reward_scale\n        self.reward_max = 1020 # TODO: change after the env is lannched\n        self.reward_type=cfg.reward_type\n\n        # Action parameters\n        self.n_actions_no_attack = 6\n        self.move_amount = 2\n\n    def _create_join(self):\n        # copy and overwrite original implementation\n        map_inst = random.choice(self._maps)\n        self._map_name = map_inst.name\n\n        self._step_mul = max(1, self._default_step_mul or map_inst.step_mul)\n        self._score_index = get_default(self._default_score_index,\n                                        map_inst.score_index)\n        self._score_multiplier = get_default(self._default_score_multiplier,\n                                             map_inst.score_multiplier)\n        self._episode_length = get_default(self._default_episode_length,\n                                           map_inst.game_steps_per_episode)\n        if self._episode_length <= 0 or self._episode_length > MAX_STEP_COUNT:\n            self._episode_length = MAX_STEP_COUNT\n\n        # Create the game. Set the first instance as the host.\n        create = sc_pb.RequestCreateGame(\n            disable_fog=self._disable_fog,\n            realtime=self._realtime)\n\n        if self._battle_net_map:\n            create.battlenet_map_name = map_inst.battle_net\n        else:\n            create.local_map.map_path = map_inst.path\n            map_data = map_inst.data(self._run_config)\n            if self._num_agents == 1:\n                create.local_map.map_data = map_data\n            else:\n                # Save the maps so they can access it. Don't do it in parallel since SC2\n                # doesn't respect tmpdir on windows, which leads to a race condition:\n                # https://github.com/Blizzard/s2client-proto/issues/102\n                for c in self._controllers:\n                    c.save_map(map_inst.path, map_data)\n        if self._random_seed is not None:\n            create.random_seed = self._random_seed\n        for p in self._players:\n            if isinstance(p, Agent):\n                create.player_setup.add(type=sc_pb.Participant)\n            else:\n                create.player_setup.add(\n                    type=sc_pb.Computer, race=random.choice(p.race),\n                    difficulty=p.difficulty, ai_build=random.choice(p.build))\n        if self._num_agents > 1:\n            self._controllers[1].create_game(create)\n        else:\n            self._controllers[0].create_game(create)\n\n        # Create the join requests.\n        agent_players = [p for p in self._players if isinstance(p, Agent)]\n        self.sanitized_names = crop_and_deduplicate_names(p.name for p in agent_players)\n        join_reqs = []\n        for p, name, interface in zip(agent_players, self.sanitized_names,\n                                      self._interface_options):\n            join = sc_pb.RequestJoinGame(options=interface)\n            join.race = random.choice(p.race)\n            join.player_name = name\n            if self._ports:\n                join.shared_port = 0  # unused\n                join.server_ports.game_port = self._ports[0]\n                join.server_ports.base_port = self._ports[1]\n                for i in range(self._num_agents - 1):\n                    join.client_ports.add(game_port=self._ports[i * 2 + 2],\n                                          base_port=self._ports[i * 2 + 3])\n            join_reqs.append(join)\n\n        # Join the game. This must be run in parallel because Join is a blocking\n        # call to the game that waits until all clients have joined.\n        self._parallel.run((c.join_game, join)\n                           for c, join in zip(self._controllers, join_reqs))\n\n        self._game_info = self._parallel.run(c.game_info for c in self._controllers)\n        for g, interface in zip(self._game_info, self._interface_options):\n            if g.options.render!= interface.render:\n                logging.warning(\n                    \"Actual interface options don't match requested options:\\n\"\n                    \"Requested:\\n%s\\n\\nActual:\\n%s\", interface, g.options)\n\n        self._features = None\n\n\n    def _launch(self):\n        self.old_unit_tags = set()\n        print(\"*****LAUNCH FUNCTION CALLED*****\")\n        SC2Env.__init__(\n            self,\n            map_name=self.map_name,\n            battle_net_map=False,\n            players=self.players,\n            agent_interface_format=self.agent_interface_format,\n            discount=None,\n            discount_zero_after_timeout=False,\n            visualize=False,\n            step_mul=8,\n            realtime=False,\n            save_replay_episodes=self.save_replay_episodes,\n            replay_dir=None if self.save_replay_episodes is None else \".\",\n            replay_prefix=None,", "metadata": {"task_id": "opendilab_ACE/129", "ground_truth": "            game_steps_per_episode=self.game_steps_per_episode,", "fpath_tuple": ["opendilab_ACE", "dizoo", "smac", "envs", "smac_env_ace.py"], "context_start_lineno": 125, "line_no": 275}}
{"prompt": "import torch\nimport torch.nn as nn\nfrom copy import deepcopy\nfrom federatedscope.attack.auxiliary.utils import get_generator\nimport matplotlib.pyplot as plt\n\n\nclass GANCRA():\n    '''\n    The implementation of GAN based class representative attack.\n    https://dl.acm.org/doi/abs/10.1145/3133956.3134012\n\n    References:\n\n        Hitaj, Briland, Giuseppe Ateniese, and Fernando Perez-Cruz.\n    \"Deep models under the GAN: information leakage from collaborative deep\n    learning.\" Proceedings of the 2017 ACM SIGSAC conference on computer\n    and communications security. 2017.\n\n\n\n        Args:\n            - target_label_ind (int): the label index whose representative\n            - fl_model (object):\n            - device (str or int): the device to run; 'cpu' or the device\n            index to select; default: 'cpu'.\n            - dataset_name (str): the dataset name; default: None\n            - noise_dim (int): the dimension of the noise that fed into the\n            generator; default: 100\n            - batch_size (int): the number of data generated into training;\n            default: 16\n            - generator_train_epoch (int): the number of training steps\n            when training the generator; default: 10\n            - lr (float): the learning rate of the generator training;\n            default: 0.001\n            - sav_pth (str): the path to save the generated data; default:\n            'data/'\n            - round_num (int): the FL round that starting the attack;\n            default: -1.\n\n    '''\n    def __init__(self,\n                 target_label_ind,\n                 fl_model,\n                 device='cpu',\n                 dataset_name=None,\n                 noise_dim=100,\n                 batch_size=16,\n                 generator_train_epoch=10,\n                 lr=0.001,\n                 sav_pth='data/',\n                 round_num=-1):\n\n        # get dataset's corresponding generator\n        self.generator = get_generator(dataset_name=dataset_name)().to(device)\n        self.target_label_ind = target_label_ind\n\n        self.discriminator = deepcopy(fl_model)\n\n        self.generator_loss_fun = nn.CrossEntropyLoss()\n\n        self.generator_train_epoch = generator_train_epoch\n\n        # the dimension of the noise input to generator\n        self.noise_dim = noise_dim\n        self.batch_size = batch_size\n\n        self.device = device\n\n        # define generator optimizer\n        self.generator_optimizer = torch.optim.SGD(\n            params=self.generator.parameters(), lr=lr)\n        self.sav_pth = sav_pth\n        self.round_num = round_num\n        self.generator_loss_summary = []\n\n    def update_discriminator(self, model):\n        '''\n        Copy the model of the server as the discriminator\n\n        Args:\n            model (object): the model in the server\n\n        Returns: the discriminator\n\n        '''\n\n        self.discriminator = deepcopy(model)\n\n    def discriminator_loss(self):\n        pass\n\n    def generator_loss(self, discriminator_output):\n        '''\n        Get the generator loss based on the discriminator's output\n\n        Args:\n            discriminator_output (Tensor): the discriminator's output;\n                size: batch_size * n_class\n\n        Returns: generator_loss\n\n        '''\n\n        self.num_class = discriminator_output.size()[1]\n        ideal_results = self.target_label_ind * torch.ones(\n            discriminator_output.size()[0], dtype=torch.long)\n\n        # ideal_results[:] =  self.target_label_ind\n\n        return self.generator_loss_fun(discriminator_output,\n                                       ideal_results.to(self.device))\n\n    def _gradient_closure(self, noise):\n        def closure():\n            generated_images = self.generator(noise)\n            discriminator_output = self.discriminator(generated_images)\n            generator_loss = self.generator_loss(discriminator_output)\n\n            generator_loss.backward()\n            return generator_loss\n\n        return closure\n\n    def generator_train(self):\n\n        for _ in range(self.generator_train_epoch):\n\n            self.generator.zero_grad()\n            self.generator_optimizer.zero_grad()\n            noise = torch.randn(size=(self.batch_size, self.noise_dim)).to(\n                torch.device(self.device))\n            closure = self._gradient_closure(noise)\n            tmp_loss = self.generator_optimizer.step(closure)\n            self.generator_loss_summary.append(", "metadata": {"task_id": "alibaba_FederatedScope/197", "ground_truth": "                tmp_loss.detach().to('cpu').numpy())", "fpath_tuple": ["alibaba_FederatedScope", "federatedscope", "attack", "privacy_attacks", "GAN_based_attack.py"], "context_start_lineno": 0, "line_no": 135}}
{"prompt": "batch_size': vz.ParameterValue(value=32),\n        'floating_point_param': vz.ParameterValue(value=32.0),\n        'learning_rate': vz.ParameterValue(value=0.5),\n        'units': vz.ParameterValue(value=50),\n    }\n    parameters = py_study_config._pytrial_parameters(pytrial)\n    expected = {\n        'learning_rate': 0.5,\n        'units': 50,\n        'activation':'relu',\n        'batch_size': 32,\n        'floating_point_param': 32.,\n       'synchronous': True\n    }\n    self.assertEqual(expected, parameters)\n    self.assertIsInstance(parameters['batch_size'], int)\n    self.assertIsInstance(parameters['floating_point_param'], float)\n\n  def testTrialToDictWithoutExternalType(self):\n    \"\"\"Test conversion when external types are not specified.\"\"\"\n    proto = study_pb2.StudySpec()\n    proto.parameters.add(\n        parameter_id='learning_rate',\n        double_value_spec=study_pb2.StudySpec.ParameterSpec.DoubleValueSpec(\n            min_value=1e-4, max_value=0.1),\n        scale_type=study_pb2.StudySpec.ParameterSpec.ScaleType.UNIT_LOG_SCALE)\n    proto.parameters.add(\n        parameter_id='batch_size',\n        discrete_value_spec=study_pb2.StudySpec.ParameterSpec.DiscreteValueSpec(\n            values=[1.0, 2.0, 4.0, 8.0, 16.0, 32.0, 64.0, 128.0]))\n    proto.parameters.add(\n        parameter_id='training_steps',\n        discrete_value_spec=study_pb2.StudySpec.ParameterSpec.DiscreteValueSpec(\n            values=[1000.0, 10000.0]))\n    proto.observation_noise = study_pb2.StudySpec.ObservationNoise.HIGH\n    proto.metrics.add(\n        metric_id='loss', goal=study_pb2.StudySpec.MetricSpec.MINIMIZE)\n\n    trial_proto = study_pb2.Trial()\n    trial_proto.id = str(1)\n    trial_proto.parameters.add(\n        parameter_id='batch_size', value=struct_pb2.Value(number_value=128.0))\n    trial_proto.parameters.add(\n        parameter_id='learning_rate',\n        value=struct_pb2.Value(number_value=1.2137854406366652E-4))\n    trial_proto.parameters.add(\n        parameter_id='training_steps',\n        value=struct_pb2.Value(number_value=10000.0))\n\n    py_study_config = vz.StudyConfig.from_proto(proto)\n    self.assertEqual(\n        py_study_config.observation_noise, vz.ObservationNoise.HIGH\n    )\n    parameters = py_study_config.trial_parameters(trial_proto)\n    self.assertEqual(\n        py_study_config.observation_noise, vz.ObservationNoise.HIGH\n    )\n    expected = {\n        'batch_size': 128,\n        'learning_rate': 1.2137854406366652E-4,\n        'training_steps': 10000.0\n    }\n    self.assertEqual(expected, parameters)\n    self.assertIsInstance(parameters['learning_rate'], float)\n    self.assertIsInstance(parameters['batch_size'], float)\n    self.assertIsInstance(parameters['training_steps'], float)\n\n  @absltest.skip('???')\n  def testTrialToDictMultidimensional(self):\n    py_study_config = vz.StudyConfig(\n        metric_information=[\n            vz.MetricInformation(\n                name='objective', goal=vz.ObjectiveMetricGoal.MAXIMIZE\n            )\n        ]\n    )\n    root = py_study_config.search_space.root\n    for index in (0, 1):\n      root.add_float_param('learning_rate', 0.01, 3.0, index=index)\n      root.add_int_param(\n          'units', 10, 1000, scale_type=vz.ScaleType.LOG, index=index\n      )\n      root.add_categorical_param('activation', ['tanh','relu'], index=index)\n      root.add_bool_param('synchronous', index=index)\n      root.add_discrete_param('batch_size', [8, 16, 32], index=index)\n    root.add_discrete_param(\n        'floating_point_param', [8., 16., 32.], auto_cast=False)\n\n    trial_proto = study_pb2.Trial()\n    trial_proto.id = str(2)\n    trial_proto.parameters.add(\n        parameter_id='learning_rate[0]',\n        value=struct_pb2.Value(number_value=0.5))\n    trial_proto.parameters.add(\n        parameter_id='learning_rate[1]',\n        value=struct_pb2.Value(number_value=0.1))\n    trial_proto.parameters.add(\n        parameter_id='units[0]', value=struct_pb2.Value(number_value=50))\n    trial_proto.parameters.add(\n        parameter_id='units[1]', value=struct_pb2.Value(number_value=200))\n    trial_proto.parameters.add(\n        parameter_id='activation[0]',\n        value=struct_pb2.Value(string_value='relu'))\n    trial_proto.parameters.add(\n        parameter_id='activation[1]',\n        value=struct_pb2.Value(string_value='relu'))\n    trial_proto.parameters.add(\n        parameter_id='synchronus[0]',\n        value=struct_pb2.Value(string_value='true'))\n    trial_proto.parameters.add(\n        parameter_id='synchronus[1]',\n        value=struct_pb2.Value(string_value='false'))\n    trial_proto.parameters.add(\n        parameter_id='batch_size[0]', value=struct_pb2.Value(number_value=32.0))\n    trial_proto.parameters.add(\n        parameter_id='batch_size[1]', value=struct_pb2.Value(number_value=8.0))\n    trial_proto.parameters.add(\n        parameter_id='floating_point_param',\n        value=struct_pb2.Value(number_value=16.0))\n    parameters = py_study_config.trial_parameters(trial_proto)\n    expected = {\n        'learning_rate': [0.5, 0.1],\n        'units': [50, 200],\n        'activation': ['relu','relu'],\n        'batch_size': [32, 8],\n       'synchronous': [True, False],\n        'floating_point_param': 16.,\n    }\n    self.assertEqual(expected, parameters)\n\n  def testPyTrialToDictMultidimensional(self):\n    py_study_config = vz.StudyConfig(\n        metric_information=[\n            vz.MetricInformation(\n                name='objective', goal=vz.ObjectiveMetricGoal.MAXIMIZE\n            )", "metadata": {"task_id": "google_vizier/77", "ground_truth": "        ]", "fpath_tuple": ["google_vizier", "vizier", "_src", "pyvizier", "oss", "study_config_test.py"], "context_start_lineno": 367, "line_no": 503}}
{"prompt": "import abc\nimport logging\n\nfrom collections import deque\nimport heapq\n\nimport numpy as np\n\nfrom federatedscope.core.workers import Server, Client\nfrom federatedscope.core.gpu_manager import GPUManager\nfrom federatedscope.core.auxiliaries.model_builder import get_model\nfrom federatedscope.core.auxiliaries.utils import get_resource_info\nfrom federatedscope.core.auxiliaries.feat_engr_builder import \\\n    get_feat_engr_wrapper\n\nlogger = logging.getLogger(__name__)\n\n\nclass BaseRunner(object):\n    \"\"\"\n    This class is a base class to construct an FL course, which includes \\\n    ``_set_up()`` and ``run()``.\n\n    Args:\n        data: The data used in the FL courses, which are formatted as \\\n        ``{'ID':data}`` for standalone mode. More details can be found in \\\n        federatedscope.core.auxiliaries.data_builder.\n        server_class: The server class is used for instantiating a ( \\\n        customized) server.\n        client_class: The client class is used for instantiating a ( \\\n        customized) client.\n        config: The configurations of the FL course.\n        client_configs: The clients' configurations.\n\n    Attributes:\n        data: The data used in the FL courses, which are formatted as \\\n        ``{'ID':data}`` for standalone mode. More details can be found in \\\n        federatedscope.core.auxiliaries.data_builder.\n        server: The instantiated server.\n        client: The instantiate client(s).\n        cfg : The configurations of the FL course.\n        client_cfgs: The clients' configurations.\n        mode: The run mode for FL, ``distributed`` or ``standalone``\n        gpu_manager: manager of GPU resource\n        resource_info: information of resource\n    \"\"\"\n    def __init__(self,\n                 data,\n                 server_class=Server,\n                 client_class=Client,\n                 config=None,\n                 client_configs=None):\n        self.data = data\n        self.server_class = server_class\n        self.client_class = client_class\n        assert config is not None, \\\n            \"When using Runner, you should specify the `config` para\"\n        if not config.is_ready_for_run:\n            config.ready_for_run()\n        self.cfg = config\n        self.client_cfgs = client_configs\n\n        self.mode = self.cfg.federate.mode.lower()\n        self.gpu_manager = GPUManager(gpu_available=self.cfg.use_gpu,\n                                      specified_device=self.cfg.device)\n\n        self.unseen_clients_id = []\n        self.feat_engr_wrapper_client, self.feat_engr_wrapper_server = \\\n            get_feat_engr_wrapper(config)\n        if self.cfg.federate.unseen_clients_rate > 0:\n            self.unseen_clients_id = np.random.choice(\n                np.arange(1, self.cfg.federate.client_num + 1),\n                size=max(\n                    1,\n                    int(self.cfg.federate.unseen_clients_rate *\n                        self.cfg.federate.client_num)),\n                replace=False).tolist()\n        # get resource information\n        self.resource_info = get_resource_info(\n            config.federate.resource_info_file)\n\n        # Check the completeness of msg_handler.\n        self.check()\n\n        # Set up for Runner\n        self._set_up()\n\n    @abc.abstractmethod\n    def _set_up(self):\n        \"\"\"\n        Set up and instantiate the client/server.\n        \"\"\"\n        raise NotImplementedError\n\n    @abc.abstractmethod\n    def _get_server_args(self, resource_info, client_resource_info):\n        \"\"\"\n        Get the args for instantiating the server.\n\n        Args:\n            resource_info: information of resource\n            client_resource_info: information of client's resource\n\n        Returns:\n            (server_data, model, kw): None or data which server holds; model \\\n            to be aggregated; kwargs dict to instantiate the server.\n        \"\"\"\n        raise NotImplementedError\n\n    @abc.abstractmethod\n    def _get_client_args(self, client_id, resource_info):\n        \"\"\"\n        Get the args for instantiating the server.\n\n        Args:\n            client_id: ID of client\n            resource_info: information of resource\n\n        Returns:\n            (client_data, kw): data which client holds; kwargs dict to \\\n            instantiate the client.\n        \"\"\"\n        raise NotImplementedError\n\n    @abc.abstractmethod\n    def run(self):\n        \"\"\"\n        Launch the FL course\n\n        Returns:\n            dict: best results during the FL course\n        \"\"\"\n        raise NotImplementedError\n\n    def _setup_server(self, resource_info=None, client_resource_info=None):\n        \"\"\"\n        Set up and instantiate the server.\n\n        Args:\n            resource_info: information of resource\n            client_resource_info: information of client's resource\n\n        Returns:\n            Instantiate server.\n        \"\"\"\n        assert self.server_class is not None, \\\n            \"`server_class` cannot be None.\"\n        self.server_id = 0\n        server_data, model, kw = self._get_server_args(resource_info,\n                                                       client_resource_info)\n        self._server_device = self.gpu_manager.auto_choice()\n        server = self.server_class(\n            ID=self.server_id,\n            config=self.cfg,\n            data=server_data,\n            model=model,\n            client_num=self.cfg.federate.client_num,\n            total_round_num=self.cfg.federate.total_round_num,\n            device=self._server_device,\n            unseen_clients_id=self.unseen_clients_id,\n            **kw)", "metadata": {"task_id": "alibaba_FederatedScope/195", "ground_truth": "        if self.cfg.nbafl.use:", "fpath_tuple": ["alibaba_FederatedScope", "federatedscope", "core", "fed_runner.py"], "context_start_lineno": 0, "line_no": 161}}
{"prompt": "assert_equal(expected, actual)\n\n  @parameterized.parameters([\n      dict(dtype=np.float32),\n      dict(dtype=np.float64),\n      dict(dtype='float32'),\n      dict(dtype='float64'),\n  ])\n  def test_zero_range_log_double(self, dtype):\n    converter = core.DefaultModelInputConverter(\n        pyvizier.ParameterConfig.factory(\n            'x1',\n            bounds=(np.exp(0.9), np.exp(0.9)),\n            scale_type=pyvizier.ScaleType.LOG,\n        ),\n        scale=True,\n        onehot_embed=True,\n        float_dtype=dtype,\n    )\n    actual = converter.convert([\n        Trial(parameters={'x1': pyvizier.ParameterValue(np.exp(0.9))}),\n        Trial(parameters={'x1': pyvizier.ParameterValue(np.exp(1.2))}),\n        Trial(parameters={'x1': pyvizier.ParameterValue('a')}),\n        Trial(),\n    ])\n    expected = np.asarray([[0.0], [0.0], [np.NaN], [np.NaN]], dtype=dtype)\n    np.testing.assert_equal(expected, actual)\n\n  @parameterized.parameters([\n      dict(dtype=np.float32),\n      dict(dtype=np.float64),\n      dict(dtype='float32'),\n      dict(dtype='float64'),\n  ])\n  def test_zero_range_reverse_log_double(self, dtype):\n    converter = core.DefaultModelInputConverter(\n        pyvizier.ParameterConfig.factory(\n            'x1',\n            bounds=(np.exp(0.9), np.exp(0.9)),\n            scale_type=pyvizier.ScaleType.REVERSE_LOG,\n        ),\n        scale=True,\n        onehot_embed=True,\n        float_dtype=dtype,\n    )\n    actual = converter.convert([\n        Trial(parameters={'x1': pyvizier.ParameterValue(np.exp(0.9))}),\n        Trial(parameters={'x1': pyvizier.ParameterValue(np.exp(1.2))}),\n        Trial(parameters={'x1': pyvizier.ParameterValue('a')}),\n        Trial(),\n    ])\n    expected = np.asarray([[0.0], [0.0], [np.NaN], [np.NaN]], dtype=dtype)\n    np.testing.assert_equal(expected, actual)\n\n  @parameterized.parameters([\n      dict(dtype=np.float32),\n      dict(dtype=np.float64),\n      dict(dtype='float32'),\n      dict(dtype='float64'),\n  ])\n  def test_double_into_scaled_double(self, dtype):\n    converter = core.DefaultModelInputConverter(\n        pyvizier.ParameterConfig.factory(\n            'x1', bounds=(-3.0, 3.0), scale_type=pyvizier.ScaleType.LINEAR\n        ),\n        scale=True,\n        onehot_embed=True,\n        float_dtype=dtype,\n    )\n\n    actual = converter.convert([\n        Trial(parameters={'x1': pyvizier.ParameterValue(1)}),\n        Trial(parameters={'x1': pyvizier.ParameterValue(2)}),\n        Trial(parameters={'x1': pyvizier.ParameterValue(-3)}),\n        Trial(parameters={'x1': pyvizier.ParameterValue('a')}),\n        Trial(),\n    ])\n    expected = np.asarray(\n        [[4 / 6], [5 / 6], [0 / 6], [np.NaN], [np.NaN]], dtype=dtype\n    )\n    np.testing.assert_allclose(expected, actual)\n    self.assertEqual(expected.dtype, actual.dtype)\n\n  def test_integer_discretes_into_discretes(self):\n    converter = core.DefaultModelInputConverter(\n        pyvizier.ParameterConfig.factory('x1', feasible_values=(1, 2, 3)),\n        max_discrete_indices=10,\n    )\n\n    actual = converter.convert([\n        Trial(parameters={'x1': pyvizier.ParameterValue(1)}),\n        Trial(parameters={'x1': pyvizier.ParameterValue(2)}),\n        Trial(parameters={'x1': pyvizier.ParameterValue(-3)}),\n        Trial(parameters={'x1': pyvizier.ParameterValue('a')}),\n        Trial(),\n    ])\n    expected = np.asarray([[0], [1], [3], [3], [3]], np.int32)\n    np.testing.assert_allclose(expected, actual)\n    self.assertEqual(expected.dtype, actual.dtype)\n\n  @parameterized.parameters([\n      dict(dtype=np.float32),\n      dict(dtype=np.float64),\n      dict(dtype='float32'),\n      dict(dtype='float64'),\n  ])\n  def test_integer_discretes_into_doubles(self, dtype):\n    converter = core.DefaultModelInputConverter(\n        pyvizier.ParameterConfig.factory('x1', feasible_values=(1, 2, 3)),\n        max_discrete_indices=1,\n        float_dtype=dtype,\n    )\n\n    actual = converter.convert([\n        Trial(parameters={'x1': pyvizier.ParameterValue(1)}),\n        Trial(parameters={'x1': pyvizier.ParameterValue(2)}),\n        Trial(parameters={'x1': pyvizier.ParameterValue(-3)}),\n        Trial(parameters={'x1': pyvizier.ParameterValue('a')}),\n        Trial(),\n    ])\n    expected = np.asarray(\n        [[1.0], [2.0], [-3.0], [np.NaN], [np.NaN]], dtype=dtype\n    )\n    np.testing.assert_allclose(expected, actual)\n    self.assertEqual(expected.dtype, actual.dtype)\n\n  def test_integer_discretes_into_onehot(self):\n    converter = core.DefaultModelInputConverter(\n        pyvizier.ParameterConfig.factory('x1', feasible_values=(1, 2, 3)),\n        scale=True,\n        onehot_embed=True,\n        max_discrete_indices=10,\n    )\n\n    actual = converter.convert([\n        Trial(parameters={'x1': pyvizier.ParameterValue(1)}),\n        Trial(parameters={'x1': pyvizier.ParameterValue(2)}),\n        Trial(parameters={'x1': pyvizier.ParameterValue(-3)}),\n        Trial(parameters={'x1': pyvizier.ParameterValue('a')}),\n        Trial(),\n    ])\n    expected = np.asarray(\n        [\n            [1.0, 0.0, 0.0, 0.0],\n            [0.0, 1.0, 0.0, 0.0],\n            [0.0, 0.0, 0.0, 1.0],\n            [0.0, 0.0, 0.0, 1.0],", "metadata": {"task_id": "google_vizier/17", "ground_truth": "            [0.0, 0.0, 0.0, 1.0],", "fpath_tuple": ["google_vizier", "vizier", "pyvizier", "converters", "core_test.py"], "context_start_lineno": 849, "line_no": 996}}
{"prompt": "_answer(a_pred))\n\n\ndef compute_em(predictions, references):\n    scores = [any([compute_exact(ref, pred) for ref in refs]) for pred, refs in zip(predictions, references)]\n    return (sum(scores) / len(scores)) * 100\n\n\ndef SARIngram(sgrams, cgrams, rgramslist, numref):\n    rgramsall = [rgram for rgrams in rgramslist for rgram in rgrams]\n    rgramcounter = Counter(rgramsall)\n\n    sgramcounter = Counter(sgrams)\n    sgramcounter_rep = Counter()\n    for sgram, scount in sgramcounter.items():\n        sgramcounter_rep[sgram] = scount * numref\n\n    cgramcounter = Counter(cgrams)\n    cgramcounter_rep = Counter()\n    for cgram, ccount in cgramcounter.items():\n        cgramcounter_rep[cgram] = ccount * numref\n\n    # KEEP\n    keepgramcounter_rep = sgramcounter_rep & cgramcounter_rep\n    keepgramcountergood_rep = keepgramcounter_rep & rgramcounter\n    keepgramcounterall_rep = sgramcounter_rep & rgramcounter\n\n    keeptmpscore1 = 0\n    keeptmpscore2 = 0\n    for keepgram in keepgramcountergood_rep:\n        keeptmpscore1 += keepgramcountergood_rep[keepgram] / keepgramcounter_rep[keepgram]\n        # Fix an alleged bug [2] in the keep score computation.\n        # keeptmpscore2 += keepgramcountergood_rep[keepgram] / keepgramcounterall_rep[keepgram]\n        keeptmpscore2 += keepgramcountergood_rep[keepgram]\n    # Define 0/0=1 instead of 0 to give higher scores for predictions that match\n    #      a target exactly.\n    keepscore_precision = 1\n    keepscore_recall = 1\n    if len(keepgramcounter_rep) > 0:\n        keepscore_precision = keeptmpscore1 / len(keepgramcounter_rep)\n    if len(keepgramcounterall_rep) > 0:\n        # Fix an alleged bug [2] in the keep score computation.\n        # keepscore_recall = keeptmpscore2 / len(keepgramcounterall_rep)\n        keepscore_recall = keeptmpscore2 / sum(keepgramcounterall_rep.values())\n    keepscore = 0\n    if keepscore_precision > 0 or keepscore_recall > 0:\n        keepscore = 2 * keepscore_precision * keepscore_recall / (keepscore_precision + keepscore_recall)\n\n    # DELETION\n    delgramcounter_rep = sgramcounter_rep - cgramcounter_rep\n    delgramcountergood_rep = delgramcounter_rep - rgramcounter\n    delgramcounterall_rep = sgramcounter_rep - rgramcounter\n    deltmpscore1 = 0\n    deltmpscore2 = 0\n    for delgram in delgramcountergood_rep:\n        deltmpscore1 += delgramcountergood_rep[delgram] / delgramcounter_rep[delgram]\n        deltmpscore2 += delgramcountergood_rep[delgram] / delgramcounterall_rep[delgram]\n    # Define 0/0=1 instead of 0 to give higher scores for predictions that match\n    # a target exactly.\n    delscore_precision = 1\n    if len(delgramcounter_rep) > 0:\n        delscore_precision = deltmpscore1 / len(delgramcounter_rep)\n\n    # ADDITION\n    addgramcounter = set(cgramcounter) - set(sgramcounter)\n    addgramcountergood = set(addgramcounter) & set(rgramcounter)\n    addgramcounterall = set(rgramcounter) - set(sgramcounter)\n\n    addtmpscore = 0\n    for addgram in addgramcountergood:\n        addtmpscore += 1\n\n    # Define 0/0=1 instead of 0 to give higher scores for predictions that match\n    # a target exactly.\n    addscore_precision = 1\n    addscore_recall = 1\n    if len(addgramcounter) > 0:\n        addscore_precision = addtmpscore / len(addgramcounter)\n    if len(addgramcounterall) > 0:\n        addscore_recall = addtmpscore / len(addgramcounterall)\n    addscore = 0\n    if addscore_precision > 0 or addscore_recall > 0:\n        addscore = 2 * addscore_precision * addscore_recall / (addscore_precision + addscore_recall)\n\n    return (keepscore, delscore_precision, addscore)\n\n\ndef SARIsent(ssent, csent, rsents):\n    numref = len(rsents)\n\n    s1grams = ssent.split(\" \")\n    c1grams = csent.split(\" \")\n    s2grams = []\n    c2grams = []\n    s3grams = []\n    c3grams = []\n    s4grams = []\n    c4grams = []\n\n    r1gramslist = []\n    r2gramslist = []\n    r3gramslist = []\n    r4gramslist = []\n    for rsent in rsents:\n        r1grams = rsent.split(\" \")\n        r2grams = []\n        r3grams = []\n        r4grams = []\n        r1gramslist.append(r1grams)\n        for i in range(0, len(r1grams) - 1):\n            if i < len(r1grams) - 1:\n                r2gram = r1grams[i] + \" \" + r1grams[i + 1]\n                r2grams.append(r2gram)\n            if i < len(r1grams) - 2:\n                r3gram = r1grams[i] + \" \" + r1grams[i + 1] + \" \" + r1grams[i + 2]\n                r3grams.append(r3gram)\n            if i < len(r1grams) - 3:\n                r4gram = r1grams[i] + \" \" + r1grams[i + 1] + \" \" + r1grams[i + 2] + \" \" + r1grams[i + 3]\n                r4grams.append(r4gram)\n        r2gramslist.append(r2grams)\n        r3gramslist.append(r3grams)\n        r4gramslist.append(r4grams)\n\n    for i in range(0, len(s1grams) - 1):\n        if i < len(s1grams) - 1:\n            s2gram = s1grams[i] + \" \" + s1grams[i + 1]\n            s2grams.append(s2gram)\n        if i < len(s1grams) - 2:\n            s3gram = s1grams[i] + \" \" + s1grams[i + 1] + \" \" + s1grams[i + 2]\n            s3grams.append(s3gram)\n        if i < len(s1grams) - 3:\n            s4gram = s1grams[i] + \" \" + s1grams[i + 1] + \" \" + s1grams[i + 2] + \" \" + s1grams[i + 3]\n            s4grams.append(s4gram)\n\n    for i in range(0, len(c1grams) - 1):\n        if i < len(c1grams) - 1:\n            c2gram = c1grams[i] + \" \" + c1grams[i + 1]", "metadata": {"task_id": "huggingface_evaluate/141", "ground_truth": "            c2grams.append(c2gram)", "fpath_tuple": ["huggingface_evaluate", "metrics", "wiki_split", "wiki_split.py"], "context_start_lineno": 100, "line_no": 237}}
{"prompt": "original: Add original samples to ray.\n        histogram_padding: Amount to weights prior to computing PDF.\n    \"\"\"\n\n    def __init__(\n        self,\n        num_samples: Optional[int] = None,\n        train_stratified: bool = True,\n        single_jitter: bool = False,\n        include_original: bool = True,\n        histogram_padding: float = 0.01,\n    ) -> None:\n        super().__init__(num_samples=num_samples)\n        self.train_stratified = train_stratified\n        self.include_original = include_original\n        self.histogram_padding = histogram_padding\n        self.single_jitter = single_jitter\n\n    def generate_ray_samples(\n        self,\n        ray_bundle: Optional[RayBundle] = None,\n        ray_samples: Optional[RaySamples] = None,\n        weights: TensorType[..., \"num_samples\", 1] = None,\n        num_samples: Optional[int] = None,\n        eps: float = 1e-5,\n    ) -> RaySamples:\n        \"\"\"Generates position samples given a distribution.\n\n        Args:\n            ray_bundle: Rays to generate samples for\n            ray_samples: Existing ray samples\n            weights: Weights for each bin\n            num_samples: Number of samples per ray\n            eps: Small value to prevent numerical issues.\n\n        Returns:\n            Positions and deltas for samples along a ray\n        \"\"\"\n\n        if ray_samples is None or ray_bundle is None:\n            raise ValueError(\"ray_samples and ray_bundle must be provided\")\n\n        num_samples = num_samples or self.num_samples\n        assert num_samples is not None\n        num_bins = num_samples + 1\n\n        weights = weights[..., 0] + self.histogram_padding\n\n        # Add small offset to rays with zero weight to prevent NaNs\n        weights_sum = torch.sum(weights, dim=-1, keepdim=True)\n        padding = torch.relu(eps - weights_sum)\n        weights = weights + padding / weights.shape[-1]\n        weights_sum += padding\n\n        pdf = weights / weights_sum\n        cdf = torch.min(torch.ones_like(pdf), torch.cumsum(pdf, dim=-1))\n        cdf = torch.cat([torch.zeros_like(cdf[..., :1]), cdf], dim=-1)\n\n        if self.train_stratified and self.training:\n            # Stratified samples between 0 and 1\n            u = torch.linspace(0.0, 1.0 - (1.0 / num_bins), steps=num_bins, device=cdf.device)\n            u = u.expand(size=(*cdf.shape[:-1], num_bins))\n            if self.single_jitter:\n                rand = torch.rand((*cdf.shape[:-1], 1), device=cdf.device) / num_bins\n            else:\n                rand = torch.rand((*cdf.shape[:-1], num_samples + 1), device=cdf.device) / num_bins\n            u = u + rand\n        else:\n            # Uniform samples between 0 and 1\n            u = torch.linspace(0.0, 1.0 - (1.0 / num_bins), steps=num_bins, device=cdf.device)\n            u = u + 1.0 / (2 * num_bins)\n            u = u.expand(size=(*cdf.shape[:-1], num_bins))\n        u = u.contiguous()\n\n        assert (\n            ray_samples.spacing_starts is not None and ray_samples.spacing_ends is not None\n        ), \"ray_sample spacing_starts and spacing_ends must be provided\"\n        assert ray_samples.spacing_to_euclidean_fn is not None, \"ray_samples.spacing_to_euclidean_fn must be provided\"\n        existing_bins = torch.cat(\n            [\n                ray_samples.spacing_starts[..., 0],\n                ray_samples.spacing_ends[..., -1:, 0],\n            ],\n            dim=-1,\n        )\n\n        inds = torch.searchsorted(cdf, u, side=\"right\")\n        below = torch.clamp(inds - 1, 0, existing_bins.shape[-1] - 1)\n        above = torch.clamp(inds, 0, existing_bins.shape[-1] - 1)\n        cdf_g0 = torch.gather(cdf, -1, below)\n        bins_g0 = torch.gather(existing_bins, -1, below)\n        cdf_g1 = torch.gather(cdf, -1, above)\n        bins_g1 = torch.gather(existing_bins, -1, above)\n\n        t = torch.clip(torch.nan_to_num((u - cdf_g0) / (cdf_g1 - cdf_g0), 0), 0, 1)\n        bins = bins_g0 + t * (bins_g1 - bins_g0)\n\n        if self.include_original:\n            bins, _ = torch.sort(torch.cat([existing_bins, bins], -1), -1)\n\n        # Stop gradients\n        bins = bins.detach()\n\n        euclidean_bins = ray_samples.spacing_to_euclidean_fn(bins)\n\n        ray_samples = ray_bundle.get_ray_samples(\n            bin_starts=euclidean_bins[..., :-1, None],\n            bin_ends=euclidean_bins[..., 1:, None],\n            spacing_starts=bins[..., :-1, None],\n            spacing_ends=bins[..., 1:, None],\n            spacing_to_euclidean_fn=ray_samples.spacing_to_euclidean_fn,\n        )\n\n        return ray_samples\n\n\nclass VolumetricSampler(Sampler):\n    \"\"\"Sampler inspired by the one proposed in the Instant-NGP paper.\n    Generates samples along a ray by sampling the occupancy field.\n    Optionally removes occluded samples if the density_fn is provided.\n\n    Args:\n    occupancy_grid: Occupancy grid to sample from.\n    density_fn: Function that evaluates density at a given point.\n    scene_aabb: Axis-aligned bounding box of the scene, should be set to None if the scene is unbounded.\n    \"\"\"\n\n    def __init__(\n        self,\n        occupancy_grid: Optional[OccupancyGrid] = None,\n        density_fn: Optional[Callable[[TensorType[..., 3]], TensorType[..., 1]]] = None,\n        scene_aabb: Optional[TensorType[2, 3]] = None,\n    ) -> None:\n\n        super().__init__()\n        self.scene_aabb = scene_aabb\n        self.density_fn = density_fn\n        self.occupancy_grid = occupancy_grid\n        if self.scene_aabb is not None:\n            self.scene_aabb = self.scene_aabb.to(\"cuda\").flatten()\n        print(self.scene_aabb)\n\n    def get_sigma_fn(self, origins, directions) -> Optional[Callable]:\n        \"\"\"Returns a function that returns the density of a point.\n\n        Args:\n            origins: Origins of rays\n            directions: Directions of rays\n        Returns:\n            Function that returns the density of a point or None if a density function is not provided.\n        \"\"\"\n\n        if self.density_fn is None or not self.training:", "metadata": {"task_id": "nerfstudio-project_nerfstudio/83", "ground_truth": "            return None", "fpath_tuple": ["nerfstudio-project_nerfstudio", "nerfstudio", "model_components", "ray_samplers.py"], "context_start_lineno": 255, "line_no": 408}}
{"prompt": "import os\nimport os.path as osp\nimport torch\nimport logging\nfrom federatedscope.nlp.hetero_tasks.dataset.utils import split_sent, \\\n    DatasetDict, NUM_DEBUG\n\nlogger = logging.getLogger(__name__)\n\n\nclass NewsQAExample(object):\n    def __init__(self, qa_id, question, context, train_answer, val_answer,\n                 start_pos, end_pos, context_tokens, is_impossible):\n        self.qa_id = qa_id\n        self.question = question\n        self.context = context\n        self.train_answer = train_answer\n        self.val_answer = val_answer\n        self.start_position = start_pos\n        self.end_position = end_pos\n        self.context_tokens = context_tokens\n        self.is_impossible = is_impossible\n\n\nclass NewsQAEncodedInput(object):\n    def __init__(self, token_ids, token_type_ids, attention_mask,\n                 overflow_token_ids):\n        self.token_ids = token_ids\n        self.token_type_ids = token_type_ids\n        self.attention_mask = attention_mask\n        self.overflow_token_ids = overflow_token_ids\n\n\nclass NewsQAResult(object):\n    def __init__(self, unique_id, start_logits, end_logits):\n        self.unique_id = unique_id\n        self.start_logits = start_logits\n        self.end_logits = end_logits\n\n\ndef refine_subtoken_position(context_subtokens, subtoken_start_pos,\n                             subtoken_end_pos, tokenizer, annotated_answer):\n    subtoken_answer =''.join(tokenizer.tokenize(annotated_answer))\n    for new_st in range(subtoken_start_pos, subtoken_end_pos + 1):\n        for new_ed in range(subtoken_end_pos, subtoken_start_pos - 1, -1):\n            text_span =''.join(context_subtokens[new_st:(new_ed + 1)])\n            if text_span == subtoken_answer:\n                return new_st, new_ed\n    return subtoken_start_pos, subtoken_end_pos\n\n\ndef get_char_to_word_positions(context, answer, start_char_pos, is_impossible):\n    context_tokens = []\n    char_to_word_offset = []\n    is_prev_whitespace = True\n    for c in context:\n        is_whitespace = (c =='' or c == '\\t' or c == '\\r' or c == '\\n'\n                         or ord(c) == 0x202F)\n        if is_whitespace:\n            is_prev_whitespace = True\n        else:\n            if is_prev_whitespace:\n                context_tokens.append(c)\n            else:\n                context_tokens[-1] += c\n            is_prev_whitespace = False\n        char_to_word_offset.append(len(context_tokens) - 1)\n\n    start_pos, end_pos = 0, 0\n    if start_char_pos is not None and not is_impossible:\n        start_pos = char_to_word_offset[start_char_pos]\n        end_pos = char_to_word_offset[start_char_pos + len(answer) - 1]\n    return start_pos, end_pos, context_tokens\n\n\ndef check_max_context_token(all_spans, cur_span_idx, pos):\n    best_score, best_span_idx = None, None\n    for span_idx, span in enumerate(all_spans):\n        end = span.context_start_position + span.context_len - 1\n        if pos < span.context_start_position or pos > end:\n            continue\n        num_left_context = pos - span.context_start_position\n        num_right_context = end - pos\n        score = \\\n            min(num_left_context, num_right_context) + 0.01 * span.context_len\n        if best_score is None or score > best_score:\n            best_score = score\n            best_span_idx = span_idx\n    return cur_span_idx == best_span_idx\n\n\ndef encode(tokenizer, text_a, text_b, max_seq_len, max_query_len,\n           added_trunc_size):\n    def _get_token_ids(text):\n        if isinstance(text, str):\n            return tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text))\n        elif isinstance(text, (list, tuple)) and len(text) > 0 and \\\n                isinstance(text[0], str):\n            return tokenizer.convert_tokens_to_ids(text)\n        elif isinstance(text, (list, tuple)) and len(text) > 0 and \\\n                isinstance(text[0], int):\n            return text\n        else:\n            raise ValueError('Input is not valid, should be a string, '", "metadata": {"task_id": "alibaba_FederatedScope/43", "ground_truth": "                             'a list/tuple of strings or a list/tuple of '", "fpath_tuple": ["alibaba_FederatedScope", "federatedscope", "nlp", "hetero_tasks", "dataset", "newsqa.py"], "context_start_lineno": 0, "line_no": 104}}
{"prompt": "import numpy as np\nimport logging\n\nfrom federatedscope.core.workers import Client\nfrom federatedscope.core.message import Message\nfrom federatedscope.vertical_fl.dataloader.utils import batch_iter\n\n\nclass vFLClient(Client):\n    \"\"\"\n    The client class for vertical FL, which customizes the handled\n    functions. Please refer to the tutorial for more details about the\n    implementation algorithm\n    Implementation of Vertical FL refer to `Private federated learning on\n    vertically partitioned data via entity resolution and additively\n    homomorphic encryption` [Hardy, et al., 2017]\n    (https://arxiv.org/abs/1711.10677)\n    \"\"\"\n    def __init__(self,\n                 ID=-1,\n                 server_id=None,\n                 state=-1,\n                 config=None,\n                 data=None,\n                 model=None,\n                 device='cpu',\n                 strategy=None,\n                 *args,\n                 **kwargs):\n\n        super(vFLClient,\n              self).__init__(ID, server_id, state, config, data, model, device,\n                             strategy, *args, **kwargs)\n        self.data = data\n        self.public_key = None\n        self.theta = None\n        self.batch_index = None\n        self._init_data_related_var()\n\n        self.register_handlers('public_keys',\n                               self.callback_funcs_for_public_keys)\n        self.register_handlers('model_para',\n                               self.callback_funcs_for_model_para)\n        self.register_handlers('encryped_gradient_u',\n                               self.callback_funcs_for_encryped_gradient_u)\n        self.register_handlers('encryped_gradient_v',\n                               self.callback_funcs_for_encryped_gradient_v)\n\n    def _init_data_related_var(self):\n        self.own_label = ('y' in self.data['train'])\n        self.dataloader = batch_iter(self.data['train'],\n                                     self._cfg.dataloader.batch_size,\n                                     shuffled=True)\n\n    def sample_data(self, index=None):\n        if index is None:\n            assert self.own_label\n            return next(self.dataloader)\n        else:\n            return self.data['train']['x'][index]\n\n    def callback_funcs_for_public_keys(self, message: Message):\n        self.public_key = message.content\n\n    def callback_funcs_for_model_para(self, message: Message):\n        self.theta = message.content\n        if self.own_label:", "metadata": {"task_id": "alibaba_FederatedScope/10", "ground_truth": "            index, input_x, input_y = self.sample_data()", "fpath_tuple": ["alibaba_FederatedScope", "federatedscope", "vertical_fl", "worker", "vertical_client.py"], "context_start_lineno": 0, "line_no": 67}}
{"prompt": "# coding=utf-8\n# Copyright 2022 HuggingFace Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport gc\nimport unittest\n\nimport numpy as np\nimport torch\n\nfrom diffusers import DanceDiffusionPipeline, IPNDMScheduler, UNet1DModel\nfrom diffusers.utils import slow, torch_device\nfrom diffusers.utils.testing_utils import require_torch_gpu\n\nfrom...test_pipelines_common import PipelineTesterMixin\n\n\ntorch.backends.cuda.matmul.allow_tf32 = False\n\n\nclass DanceDiffusionPipelineFastTests(PipelineTesterMixin, unittest.TestCase):\n    pipeline_class = DanceDiffusionPipeline\n    test_attention_slicing = False\n    test_cpu_offload = False\n\n    def get_dummy_components(self):\n        torch.manual_seed(0)\n        unet = UNet1DModel(", "metadata": {"task_id": "huggingface_diffusers/186", "ground_truth": "            block_out_channels=(32, 32, 64),", "fpath_tuple": ["huggingface_diffusers", "tests", "pipelines", "dance_diffusion", "test_dance_diffusion.py"], "context_start_lineno": 0, "line_no": 39}}
{"prompt": "import os\n\nimport numpy as np\nimport pytest\nfrom easydict import EasyDict\n\nfrom ding.league.player import Player, HistoricalPlayer, ActivePlayer, create_player\nfrom ding.league.shared_payoff import create_payoff\nfrom ding.league.starcraft_player import MainPlayer, MainExploiter, LeagueExploiter\nfrom ding.league.tests.league_test_default_config import league_test_config\nfrom ding.league.metric import LeagueMetricEnv\n\nONE_PHASE_STEP = 2000\nenv = LeagueMetricEnv()\n\n\n@pytest.fixture(scope='function')\ndef setup_payoff():\n    cfg = EasyDict({'type': 'battle', 'decay': 0.99})\n    return create_payoff(cfg)\n\n\n@pytest.fixture(scope='function')\ndef setup_league(setup_payoff):\n    players = []\n    for category in ['zerg', 'terran', 'protoss']:\n        # main_player\n        main_player_name = '{}_{}'.format('MainPlayer', category)\n        players.append(\n            create_player(\n                league_test_config.league,'main_player', league_test_config.league.main_player, category, setup_payoff,\n                'ckpt_{}.pth'.format(main_player_name), main_player_name, 0, env.create_rating()\n            )\n        )\n        # main_exloiter\n        main_exploiter_name = '{}_{}'.format('MainExploiter', category)\n        players.append(\n            create_player(\n                league_test_config.league,'main_exploiter', league_test_config.league.main_exploiter, category,\n                setup_payoff, 'ckpt_{}.pth'.format(main_exploiter_name), main_exploiter_name, 0, env.create_rating()\n            )\n        )\n        # league_exploiter\n        league_exploiter_name = '{}_{}'.format('LeagueExploiter', category)\n        for i in range(2):\n            players.append(\n                create_player(\n                    league_test_config.league,\n                    'league_exploiter',\n                    league_test_config.league.league_exploiter,\n                    category,\n                    setup_payoff,\n                    'ckpt_{}.pth'.format(league_exploiter_name),\n                    league_exploiter_name,\n                    0,\n                    env.create_rating(),\n                )\n            )\n        # historical player: sl player is used as initial HistoricalPlayer\n        sl_hp_name = '{}_{}_sl'.format('MainPlayer', category)\n        players.append(\n            create_player(\n                league_test_config.league,\n                'historical_player',\n                EasyDict(),\n                category,\n                setup_payoff,\n                'ckpt_sl_{}'.format(sl_hp_name),\n                sl_hp_name,\n                0,\n                env.create_rating(),\n                parent_id=main_player_name,\n            )\n        )\n    for p in players:\n        setup_payoff.add_player(p)\n    return players\n\n\n@pytest.mark.unittest\nclass TestMainPlayer:\n\n    def test_get_job(self, setup_league, setup_payoff):\n        N = 10\n        # no indicated p\n        # test get_job\n        for p in setup_league:\n            if isinstance(p, MainPlayer):\n                for i in range(N):\n                    job_dict = p.get_job()\n                    assert isinstance(job_dict, dict)\n                    opponent = job_dict['opponent']\n                    assert isinstance(opponent, Player)\n                    assert opponent in setup_league\n\n        # payoff = setup_league[np.random.randint(0, len(setup_league))].payoff  # random select reference\n        hp_list = []\n        for p in setup_league:\n            if isinstance(p, ActivePlayer):\n                p.total_agent_step = 2 * ONE_PHASE_STEP\n                hp = p.snapshot(env)\n                hp_list.append(hp)\n                setup_payoff.add_player(hp)\n        setup_league += hp_list  # 12+3 + 12\n\n        # test get_job with branch prob\n        pfsp, sp, veri = False, False, False\n        for p in setup_league:\n            if isinstance(p, MainPlayer):\n                while True:\n                    job_dict = p.get_job()\n                    opponent = job_dict['opponent']\n                    if isinstance(opponent, HistoricalPlayer) and 'MainPlayer' in opponent.parent_id:\n                        veri = True\n                    elif isinstance(opponent, HistoricalPlayer):\n                        pfsp = True\n                    elif isinstance(opponent, MainPlayer):\n                        sp = True\n                    else:\n                        raise Exception(\"Main Player selects a wrong opponent {}\", type(opponent))\n                    if veri and pfsp and sp:\n                        break\n\n    def test_snapshot(self, setup_league, setup_payoff):\n        N = 10\n        for p in setup_league:\n            for i in range(N):\n                if isinstance(p, ActivePlayer):\n                    hp = p.snapshot(env)\n                    assert isinstance(hp, HistoricalPlayer)\n                    assert id(hp.payoff) == id(p.payoff)\n                    assert hp.parent_id == p.player_id\n\n    def test_is_trained_enough(self, setup_league, setup_payoff):\n        for p in setup_league:\n            if isinstance(p, ActivePlayer):\n                assert not p.is_trained_enough()\n                assert p._last_enough_step == 0\n                # step_passed < ONE_PHASE_STEP\n                p.total_agent_step = ONE_PHASE_STEP * 0.99\n                assert not p.is_trained_enough()\n                assert p._last_enough_step == 0\n                # ONE_PHASE_STEP < step_passed < 2*ONE_PHASE_STEP, but low win rate", "metadata": {"task_id": "opendilab_ACE/147", "ground_truth": "                p.total_agent_step = ONE_PHASE_STEP + 1", "fpath_tuple": ["opendilab_ACE", "ding", "league", "tests", "test_player.py"], "context_start_lineno": 0, "line_no": 143}}
{"prompt": "        )\n    elif down_block_type == \"SkipDownBlock2D\":\n        return SkipDownBlock2D(\n            num_layers=num_layers,\n            in_channels=in_channels,\n            out_channels=out_channels,\n            temb_channels=temb_channels,\n            add_downsample=add_downsample,\n            resnet_eps=resnet_eps,\n            resnet_act_fn=resnet_act_fn,\n            downsample_padding=downsample_padding,\n            resnet_time_scale_shift=resnet_time_scale_shift,\n        )\n    elif down_block_type == \"AttnSkipDownBlock2D\":\n        return AttnSkipDownBlock2D(\n            num_layers=num_layers,\n            in_channels=in_channels,\n            out_channels=out_channels,\n            temb_channels=temb_channels,\n            add_downsample=add_downsample,\n            resnet_eps=resnet_eps,\n            resnet_act_fn=resnet_act_fn,\n            downsample_padding=downsample_padding,\n            attn_num_head_channels=attn_num_head_channels,\n            resnet_time_scale_shift=resnet_time_scale_shift,\n        )\n    elif down_block_type == \"DownEncoderBlock2D\":\n        return DownEncoderBlock2D(\n            num_layers=num_layers,\n            in_channels=in_channels,\n            out_channels=out_channels,\n            add_downsample=add_downsample,\n            resnet_eps=resnet_eps,\n            resnet_act_fn=resnet_act_fn,\n            resnet_groups=resnet_groups,\n            downsample_padding=downsample_padding,\n            resnet_time_scale_shift=resnet_time_scale_shift,\n        )\n    elif down_block_type == \"AttnDownEncoderBlock2D\":\n        return AttnDownEncoderBlock2D(\n            num_layers=num_layers,\n            in_channels=in_channels,\n            out_channels=out_channels,\n            add_downsample=add_downsample,\n            resnet_eps=resnet_eps,\n            resnet_act_fn=resnet_act_fn,\n            resnet_groups=resnet_groups,\n            downsample_padding=downsample_padding,\n            attn_num_head_channels=attn_num_head_channels,\n            resnet_time_scale_shift=resnet_time_scale_shift,\n        )\n    raise ValueError(f\"{down_block_type} does not exist.\")\n\n\ndef get_up_block(\n    up_block_type,\n    num_layers,\n    in_channels,\n    out_channels,\n    prev_output_channel,\n    temb_channels,\n    add_upsample,\n    resnet_eps,\n    resnet_act_fn,\n    attn_num_head_channels,\n    resnet_groups=None,\n    cross_attention_dim=None,\n    dual_cross_attention=False,\n    use_linear_projection=False,\n    only_cross_attention=False,\n    upcast_attention=False,\n    resnet_time_scale_shift=\"default\",\n):\n    up_block_type = up_block_type[7:] if up_block_type.startswith(\"UNetRes\") else up_block_type\n    if up_block_type == \"UpBlock2D\":\n        return UpBlock2D(\n            num_layers=num_layers,\n            in_channels=in_channels,\n            out_channels=out_channels,\n            prev_output_channel=prev_output_channel,\n            temb_channels=temb_channels,\n            add_upsample=add_upsample,\n            resnet_eps=resnet_eps,\n            resnet_act_fn=resnet_act_fn,\n            resnet_groups=resnet_groups,\n            resnet_time_scale_shift=resnet_time_scale_shift,\n        )\n    elif up_block_type == \"ResnetUpsampleBlock2D\":\n        return ResnetUpsampleBlock2D(\n            num_layers=num_layers,\n            in_channels=in_channels,\n            out_channels=out_channels,\n            prev_output_channel=prev_output_channel,\n            temb_channels=temb_channels,\n            add_upsample=add_upsample,\n            resnet_eps=resnet_eps,\n            resnet_act_fn=resnet_act_fn,\n            resnet_groups=resnet_groups,\n            resnet_time_scale_shift=resnet_time_scale_shift,\n        )\n    elif up_block_type == \"CrossAttnUpBlock2D\":\n        if cross_attention_dim is None:\n            raise ValueError(\"cross_attention_dim must be specified for CrossAttnUpBlock2D\")\n        return CrossAttnUpBlock2D(\n            num_layers=num_layers,\n            in_channels=in_channels,\n            out_channels=out_channels,\n            prev_output_channel=prev_output_channel,\n            temb_channels=temb_channels,\n            add_upsample=add_upsample,\n            resnet_eps=resnet_eps,\n            resnet_act_fn=resnet_act_fn,\n            resnet_groups=resnet_groups,\n            cross_attention_dim=cross_attention_dim,\n            attn_num_head_channels=attn_num_head_channels,\n            dual_cross_attention=dual_cross_attention,\n            use_linear_projection=use_linear_projection,\n            only_cross_attention=only_cross_attention,\n            upcast_attention=upcast_attention,\n            resnet_time_scale_shift=resnet_time_scale_shift,\n        )\n    elif up_block_type == \"SimpleCrossAttnUpBlock2D\":\n        if cross_attention_dim is None:\n            raise ValueError(\"cross_attention_dim must be specified for SimpleCrossAttnUpBlock2D\")\n        return SimpleCrossAttnUpBlock2D(\n            num_layers=num_layers,\n            in_channels=in_channels,\n            out_channels=out_channels,\n            prev_output_channel=prev_output_channel,\n            temb_channels=temb_channels,\n            add_upsample=add_upsample,\n            resnet_eps=resnet_eps,\n            resnet_act_fn=resnet_act_fn,\n            resnet_groups=resnet_groups,\n            cross_attention_dim=cross_attention_dim,\n            attn_num_head_channels=attn_num_head_channels,\n            resnet_time_scale_shift=resnet_time_scale_shift,\n        )\n    elif up_block_type == \"AttnUpBlock2D\":\n        return AttnUpBlock2D(\n            num_layers=num_layers,\n            in_channels=in_channels,\n            out_channels=out_channels,\n            prev_output_channel=prev_output_channel,\n            temb_channels=temb_channels,\n            add_upsample=add_upsample,\n            resnet_eps=resnet_eps,\n            resnet_act_fn=resnet_act_fn,", "metadata": {"task_id": "huggingface_diffusers/96", "ground_truth": "            resnet_groups=resnet_groups,", "fpath_tuple": ["huggingface_diffusers", "src", "diffusers", "models", "unet_2d_blocks.py"], "context_start_lineno": 119, "line_no": 267}}
{"prompt": "from nerfstudio.utils import profiler\n\n\ndef module_wrapper(ddp_or_model: Union[DDP, Model]) -> Model:\n    \"\"\"\n    If DDP, then return the.module. Otherwise, return the model.\n    \"\"\"\n    if isinstance(ddp_or_model, DDP):\n        return cast(Model, ddp_or_model.module)\n    return ddp_or_model\n\n\nclass Pipeline(nn.Module):\n    \"\"\"The intent of this class is to provide a higher level interface for the Model\n    that will be easy to use for our Trainer class.\n\n    This class will contain high level functions for the model like getting the loss\n    dictionaries and visualization code. It should have ways to get the next iterations\n    training loss, evaluation loss, and generate whole images for visualization. Each model\n    class should be 1:1 with a pipeline that can act as a standardized interface and hide\n    differences in how each model takes in and outputs data.\n\n    This class's function is to hide the data manager and model classes from the trainer,\n    worrying about:\n    1) Fetching data with the data manager\n    2) Feeding the model the data and fetching the loss\n    Hopefully this provides a higher level interface for the trainer to use, and\n    simplifying the model classes, which each may have different forward() methods\n    and so on.\n\n    Args:\n        config: configuration to instantiate pipeline\n        device: location to place model and data\n        test_mode:\n            'train': loads train/eval datasets into memory\n            'test': loads train/test dataset into memory\n            'inference': does not load any dataset into memory\n        world_size: total number of machines available\n        local_rank: rank of current machine\n\n    Attributes:\n        datamanager: The data manager that will be used\n        model: The model that will be used\n    \"\"\"\n\n    # pylint: disable=abstract-method\n\n    datamanager: DataManager\n    _model: Model\n\n    @property\n    def model(self):\n        \"\"\"Returns the unwrapped model if in ddp\"\"\"\n        return module_wrapper(self._model)\n\n    @property\n    def device(self):\n        \"\"\"Returns the device that the model is on.\"\"\"\n        return self.model.device\n\n    @profiler.time_function\n    def get_train_loss_dict(self, step: int):\n        \"\"\"This function gets your training loss dict. This will be responsible for\n        getting the next batch of data from the DataManager and interfacing with the\n        Model class, feeding the data to the model's forward function.\n\n        Args:\n            step: current iteration step to update sampler if using DDP (distributed)\n        \"\"\"\n        if self.world_size > 1 and step:\n            assert self.datamanager.train_sampler is not None\n            self.datamanager.train_sampler.set_epoch(step)\n        ray_bundle, batch = self.datamanager.next_train(step)\n        model_outputs = self.model(ray_bundle, batch)\n        metrics_dict = self.model.get_metrics_dict(model_outputs, batch)\n        loss_dict = self.model.get_loss_dict(model_outputs, batch, metrics_dict)\n\n        return model_outputs, loss_dict, metrics_dict\n\n    @profiler.time_function\n    def get_eval_loss_dict(self, step: int):\n        \"\"\"This function gets your evaluation loss dict. It needs to get the data\n        from the DataManager and feed it to the model's forward function\n\n        Args:\n            step: current iteration step\n        \"\"\"\n        self.eval()\n        if self.world_size > 1:\n            assert self.datamanager.eval_sampler is not None\n            self.datamanager.eval_sampler.set_epoch(step)\n        ray_bundle, batch = self.datamanager.next_eval(step)\n        model_outputs = self.model(ray_bundle, batch)\n        metrics_dict = self.model.get_metrics_dict(model_outputs, batch)\n        loss_dict = self.model.get_loss_dict(model_outputs, batch, metrics_dict)\n        self.train()\n        return model_outputs, loss_dict, metrics_dict\n\n    @abstractmethod\n    @profiler.time_function\n    def get_eval_image_metrics_and_images(self, step: int):\n        \"\"\"This function gets your evaluation loss dict. It needs to get the data\n        from the DataManager and feed it to the model's forward function\n\n        Args:\n            step: current iteration step\n        \"\"\"\n\n    @abstractmethod\n    @profiler.time_function\n    def get_average_eval_image_metrics(self, step: Optional[int] = None):\n        \"\"\"Iterate over all the images in the eval dataset and get the average.\"\"\"\n\n    def load_pipeline(self, loaded_state: Dict[str, Any], step: int) -> None:\n        \"\"\"Load the checkpoint from the given path\n\n        Args:\n            loaded_state: pre-trained model state dict\n            step: training step of the loaded checkpoint\n        \"\"\"\n\n    def get_training_callbacks(\n        self, training_callback_attributes: TrainingCallbackAttributes\n    ) -> List[TrainingCallback]:\n        \"\"\"Returns the training callbacks from both the Dataloader and the Model.\"\"\"\n\n    def get_param_groups(self) -> Dict[str, List[Parameter]]:\n        \"\"\"Get the param groups for the pipeline.\n\n        Returns:\n            A list of dictionaries containing the pipeline's param groups.\n        \"\"\"\n\n\n@dataclass\nclass VanillaPipelineConfig(cfg.InstantiateConfig):\n    \"\"\"Configuration for pipeline instantiation\"\"\"\n\n    _target: Type = field(default_factory=lambda: VanillaPipeline)\n    \"\"\"target class to instantiate\"\"\"\n    datamanager: VanillaDataManagerConfig = VanillaDataManagerConfig()\n    \"\"\"specifies the datamanager config\"\"\"\n    model: ModelConfig = ModelConfig()\n    \"\"\"specifies the model config\"\"\"\n\n\nclass VanillaPipeline(Pipeline):\n    \"\"\"The pipeline class for the vanilla nerf setup of multiple cameras for one or a few scenes.\n\n        config: configuration to instantiate pipeline\n        device: location to place model and data\n        test_mode:\n            'val': loads train/val datasets into memory\n            'test': loads train/test dataset into memory\n            'inference': does not load any dataset into memory\n        world_size: total number of machines available\n        local_rank: rank of current machine\n\n    Attributes:\n        datamanager: The data manager that will be used\n        model: The model that will be used\n    \"\"\"\n\n    def __init__(\n        self,\n        config: VanillaPipelineConfig,\n        device: str,\n        test_mode: Literal[\"test\", \"val\", \"inference\"] = \"val\",\n        world_size: int = 1,\n        local_rank: int = 0,\n    ):\n        super().__init__()\n        self.config = config\n        self.test_mode = test_mode\n        self.datamanager: VanillaDataManager = config.datamanager.setup(\n            device=device, test_mode=test_mode, world_size=world_size, local_rank=local_rank\n        )\n        self.datamanager.to(device)\n        # TODO(ethan): get rid of scene_bounds from the model\n        assert self.datamanager.train_dataset is not None, \"Missing input dataset\"\n\n        self._model = config.model.setup(\n            scene_box=self.datamanager.train_dataset.scene_box,\n            num_train_data=len(self.datamanager.train_dataset),\n            metadata=self.datamanager.train_dataset.metadata,\n        )\n        self.model.to(device)\n\n        self.world_size = world_size\n        if world_size > 1:", "metadata": {"task_id": "nerfstudio-project_nerfstudio/24", "ground_truth": "            self._model = typing.cast(Model, DDP(self._model, device_ids=[local_rank], find_unused_parameters=True))", "fpath_tuple": ["nerfstudio-project_nerfstudio", "nerfstudio", "pipelines", "base_pipeline.py"], "context_start_lineno": 47, "line_no": 237}}
{"prompt": "\"\"\"\nHere we have modified code taken from COLMAP for parsing data in the COLMAP format.\nOriginal file at:\nhttps://github.com/colmap/colmap/blob/1a4d0bad2e90aa65ce997c9d1779518eaed998d5/scripts/python/read_write_model.py.\n\"\"\"\n\n# Copyright (c) 2022, ETH Zurich and UNC Chapel Hill.\n# All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     * Redistributions of source code must retain the above copyright\n#       notice, this list of conditions and the following disclaimer.\n#\n#     * Redistributions in binary form must reproduce the above copyright\n#       notice, this list of conditions and the following disclaimer in the\n#       documentation and/or other materials provided with the distribution.\n#\n#     * Neither the name of ETH Zurich and UNC Chapel Hill nor the names of\n#       its contributors may be used to endorse or promote products derived\n#       from this software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDERS OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n#\n# Author: Johannes L. Schoenberger (jsch-at-demuc-dot-de)\n\nimport json\nimport os\nimport struct\nfrom dataclasses import dataclass\nfrom io import BufferedReader\nfrom pathlib import Path\nfrom typing import Dict, Optional, Tuple\n\nimport appdirs\nimport numpy as np\nimport requests\nfrom rich.console import Console\nfrom rich.progress import track\nfrom typing_extensions import Literal\n\nfrom nerfstudio.process_data.process_data_utils import CameraModel\nfrom nerfstudio.utils.rich_utils import status\nfrom nerfstudio.utils.scripts import run_command\n\nCONSOLE = Console(width=120)\n\n\n@dataclass", "metadata": {"task_id": "nerfstudio-project_nerfstudio/113", "ground_truth": "class ColmapCameraModel:", "fpath_tuple": ["nerfstudio-project_nerfstudio", "nerfstudio", "process_data", "colmap_utils.py"], "context_start_lineno": 0, "line_no": 60}}
{"prompt": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\nimport warnings\nfrom types import ModuleType\nfrom typing import Dict, List\nfrom warnings import warn\n\nimport torch\nfrom torchrl.data import (\n    BinaryDiscreteTensorSpec,\n    BoundedTensorSpec,\n    CompositeSpec,\n    DiscreteTensorSpec,\n    MultiDiscreteTensorSpec,\n    MultiOneHotDiscreteTensorSpec,\n    OneHotDiscreteTensorSpec,\n    TensorSpec,\n    UnboundedContinuousTensorSpec,\n)\n\nfrom..._utils import implement_for\nfrom...data.utils import numpy_to_torch_dtype_dict\n\nfrom..gym_like import default_info_dict_reader, GymLikeEnv\nfrom..utils import _classproperty\n\ntry:\n    import gym\n\n    _has_gym = True\nexcept ImportError:\n    _has_gym = False\n\n\nif _has_gym:\n    try:\n        from gym.wrappers.pixel_observation import PixelObservationWrapper\n\n        from torchrl.envs.libs.utils import (\n            GymPixelObservationWrapper as LegacyPixelObservationWrapper,\n        )\n    except ModuleNotFoundError:\n        warnings.warn(\n            f\"gym {gym.__version__} does not provide the PixelObservationWrapper\"\n            f\"used by torchrl, which will be using a patched version. \"\n            f\"Consider updating gym to a newer version.\"\n        )\n        from torchrl.envs.libs.utils import (\n            GymPixelObservationWrapper as PixelObservationWrapper,\n        )\n\n__all__ = [\"GymWrapper\", \"GymEnv\"]\n\n\ndef _gym_to_torchrl_spec_transform(\n    spec, dtype=None, device=\"cpu\", categorical_action_encoding=False\n) -> TensorSpec:", "metadata": {"task_id": "pytorch_rl/85", "ground_truth": "    if isinstance(spec, gym.spaces.tuple.Tuple):", "fpath_tuple": ["pytorch_rl", "torchrl", "envs", "libs", "gym.py"], "context_start_lineno": 0, "line_no": 59}}
{"prompt": "# Copyright 2022 The Nerfstudio Team. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Data parser for blender dataset\"\"\"\nfrom __future__ import annotations\n\nfrom dataclasses import dataclass, field\nfrom pathlib import Path\nfrom typing import Type\n\nimport imageio\nimport numpy as np\nimport torch\n\nfrom nerfstudio.cameras.cameras import Cameras, CameraType\nfrom nerfstudio.data.dataparsers.base_dataparser import (\n    DataParser,\n    DataParserConfig,\n    DataparserOutputs,\n)\nfrom nerfstudio.data.scene_box import SceneBox\nfrom nerfstudio.utils.colors import get_color\nfrom nerfstudio.utils.io import load_from_json\n\n\n@dataclass\nclass BlenderDataParserConfig(DataParserConfig):\n    \"\"\"Blender dataset parser config\"\"\"\n\n    _target: Type = field(default_factory=lambda: Blender)\n    \"\"\"target class to instantiate\"\"\"\n    data: Path = Path(\"data/blender/lego\")\n    \"\"\"Directory specifying location of data.\"\"\"", "metadata": {"task_id": "nerfstudio-project_nerfstudio/127", "ground_truth": "    scale_factor: float = 1.0", "fpath_tuple": ["nerfstudio-project_nerfstudio", "nerfstudio", "data", "dataparsers", "blender_dataparser.py"], "context_start_lineno": 0, "line_no": 44}}
{"prompt": " TensorDictBase\nfrom torch import nn, Tensor\nfrom torch.nn import Parameter\n\nfrom torchrl.modules import SafeModule\nfrom torchrl.modules.utils import Buffer\n\n_has_functorch = False\ntry:\n    import functorch as ft  # noqa\n\n    _has_functorch = True\n    FUNCTORCH_ERR = \"\"\nexcept ImportError:\n    print(\n        \"failed to import functorch. TorchRL's features that do not require \"\n        \"functional programming should work, but functionality and performance \"\n        \"may be affected. Consider installing functorch and/or upgrating pytorch.\"\n    )\n    FUNCTORCH_ERROR = \"functorch not installed. Consider installing functorch to use this functionality.\"\n\n\nclass LossModule(nn.Module):\n    \"\"\"A parent class for RL losses.\n\n    LossModule inherits from nn.Module. It is designed to read an input TensorDict and return another tensordict\n    with loss keys named \"loss_*\".\n    Splitting the loss in its component can then be used by the trainer to log the various loss values throughout\n    training. Other scalars present in the output tensordict will be logged too.\n\n    \"\"\"\n\n    def __init__(self):\n        super().__init__()\n        self._param_maps = {}\n        # self.register_forward_pre_hook(_parameters_to_tensordict)\n\n    def forward(self, tensordict: TensorDictBase) -> TensorDictBase:\n        \"\"\"It is designed to read an input TensorDict and return another tensordict with loss keys named \"loss*\".\n\n        Splitting the loss in its component can then be used by the trainer to log the various loss values throughout\n        training. Other scalars present in the output tensordict will be logged too.\n\n        Args:\n            tensordict: an input tensordict with the values required to compute the loss.\n\n        Returns:\n            A new tensordict with no batch dimension containing various loss scalars which will be named \"loss*\". It\n            is essential that the losses are returned with this name as they will be read by the trainer before\n            backpropagation.\n\n        \"\"\"\n        raise NotImplementedError\n\n    def convert_to_functional(\n        self,\n        module: SafeModule,\n        module_name: str,\n        expand_dim: Optional[int] = None,\n        create_target_params: bool = False,\n        compare_against: Optional[List[Parameter]] = None,\n        funs_to_decorate=None,\n    ) -> None:\n        if funs_to_decorate is None:\n            funs_to_decorate = [\"forward\"]\n        # To make it robust to device casting, we must register list of\n        # tensors as lazy calls to `getattr(self, name_of_tensor)`.\n        # Otherwise, casting the module to a device will keep old references\n        # to uncast tensors\n        try:\n            buffer_names = next(itertools.islice(zip(*module.named_buffers()), 1))\n        except StopIteration:\n            buffer_names = ()\n        params = make_functional(module, funs_to_decorate=funs_to_decorate)\n        functional_module = deepcopy(module)\n        repopulate_module(module, params)\n\n        params_and_buffers = params\n        # we transform the buffers in params to make sure they follow the device\n        # as tensor = nn.Parameter(tensor) keeps its identity when moved to another device\n\n        def create_buffers(tensor):\n\n            if isinstance(tensor, torch.Tensor) and not isinstance(\n                tensor, (Buffer, nn.Parameter)\n            ):\n                return Buffer(tensor, requires_grad=tensor.requires_grad)\n            return tensor\n\n        # separate params and buffers\n        params_and_buffers = params_and_buffers.apply(create_buffers)\n        for key in params_and_buffers.keys(True):\n            if \"_sep_\" in key:\n                raise KeyError(\n                    f\"The key {key} contains the '_sep_' pattern which is prohibited. Consider renaming the parameter / buffer.\"\n                )\n        params_and_buffers_flat = params_and_buffers.flatten_keys(\"_sep_\")\n        buffers = params_and_buffers_flat.select(*buffer_names)\n        params = params_and_buffers_flat.exclude(*buffer_names)\n\n        if expand_dim and not _has_functorch:\n            raise ImportError(\n                \"expanding params is only possible when functorch is installed,\"\n                \"as this feature requires calls to the vmap operator.\"\n            )\n        if expand_dim:\n            # Expands the dims of params and buffers.\n            # If the param already exist in the module, we return a simple expansion of the\n            # original one. Otherwise, we expand and resample it.\n            # For buffers, a cloned expansion (or equivalently a repeat) is returned.\n            if compare_against is not None:\n                compare_against = set(compare_against)\n            else:\n                compare_against = set()\n\n            def _compare_and_expand(param):\n\n                if param in compare_against:\n                    expanded_param = param.data.expand(expand_dim, *param.shape)\n                    # the expanded parameter must be sent to device when to()\n                    # is called:\n                    return expanded_param\n                else:\n                    p_out = param.repeat(expand_dim, *[1 for _ in param.shape])\n                    p_out = nn.Parameter(\n                        p_out.uniform_(\n                            p_out.min().item(), p_out.max().item()\n                        ).requires_grad_()\n                    )\n                    return p_out\n\n            params_udpated = params.apply(\n                _compare_and_expand, batch_size=[expand_dim, *params.shape]\n            )\n\n            params = params_udpated\n            buffers = buffers.apply(\n                lambda buffer: Buffer(buffer.expand(expand_dim, *buffer.shape).clone()),\n                batch_size=[expand_dim, *buffers.shape],\n            )\n\n            params_and_buffers.update(params.unflatten_keys(\"_sep_\"))\n            params_and_buffers.update(buffers.unflatten_keys(\"_sep_\"))\n            params_and_buffers.batch_size = params.batch_size\n\n            # self.params_to_map = params_to_map\n\n        param_name = module_name + \"_params\"\n\n        prev_set_params = set(self.parameters())\n\n        # register parameters and buffers\n        for key, parameter in params.items():\n            if parameter not in prev_set_params:\n                setattr(self, \"_sep_\".join([module_name, key]), parameter)\n            else:\n                for _param_name, p in self.named_parameters():\n                    if parameter is p:\n                        break\n                else:\n                    raise RuntimeError(\"parameter not found\")\n                setattr(self, \"_sep_\".join([module_name, key]), _param_name)\n        prev_set_buffers = set(self.buffers())\n        for key, buffer in buffers.items():\n            if buffer not in prev_set_buffers:\n                self.register_buffer(\"_sep_\".join([module_name, key]), buffer)\n            else:\n                for _buffer_name, b in self.named_buffers():\n                    if buffer is b:\n                        break\n                else:\n                    raise RuntimeError(\"buffer not found\")\n                setattr(self, \"_sep_\".join([module_name, key]), _buffer_name)\n\n        setattr(self, \"_\" + param_name, params_and_buffers)\n        setattr(\n            self.__class__,", "metadata": {"task_id": "pytorch_rl/193", "ground_truth": "            param_name,", "fpath_tuple": ["pytorch_rl", "torchrl", "objectives", "common.py"], "context_start_lineno": 15, "line_no": 192}}
{"prompt": "split_sizes):\n            return None\n        return val.split(split_sizes, dim=-1)\n\n    def to_numpy(self, val: torch.Tensor, safe: bool = True) -> np.ndarray:\n        if safe:\n            self.assert_is_in(val)\n        vals = self._split(val)\n        out = torch.stack([val.argmax(-1) for val in vals], -1).numpy()\n        return out\n\n    def index(self, index: INDEX_TYPING, tensor_to_index: torch.Tensor) -> torch.Tensor:\n        if not isinstance(index, torch.Tensor):\n            raise ValueError(\n                f\"Only tensors are allowed for indexing using\"\n                f\" {self.__class__.__name__}.index(...)\"\n            )\n        indices = self._split(index)\n        tensor_to_index = self._split(tensor_to_index)\n\n        out = []\n        for _index, _tensor_to_index in zip(indices, tensor_to_index):\n            _index = _index.nonzero().squeeze()\n            _index = _index.expand(*_tensor_to_index.shape[:-1], _index.shape[-1])\n            out.append(_tensor_to_index.gather(-1, _index))\n        return torch.cat(out, -1)\n\n    def is_in(self, val: torch.Tensor) -> bool:\n        vals = self._split(val)\n        if vals is None:\n            return False\n        return all(\n            super(MultiOneHotDiscreteTensorSpec, self).is_in(_val) for _val in vals\n        )\n\n    def _project(self, val: torch.Tensor) -> torch.Tensor:\n        vals = self._split(val)\n        return torch.cat([super()._project(_val) for _val in vals], -1)\n\n    def to_categorical(self) -> MultiDiscreteTensorSpec:\n\n        return MultiDiscreteTensorSpec(\n            [_space.n for _space in self.space],\n            device=self.device,\n            dtype=self.dtype,\n            shape=[*self.shape[:-1], len(self.space)],\n        )\n\n    def expand(self, *shape):\n        nvecs = [space.n for space in self.space]\n        if len(shape) == 1 and isinstance(shape[0], (tuple, list, torch.Size)):\n            shape = shape[0]\n        if any(val < 0 for val in shape):\n            raise ValueError(\n                f\"{self.__class__.__name__}.extend does not support negative shapes.\"\n            )\n        if any(s1!= s2 and s2!= 1 for s1, s2 in zip(shape[-self.ndim :], self.shape)):\n            raise ValueError(\n                f\"The last {self.ndim} of the extended shape must match the\"\n                f\"shape of the CompositeSpec in CompositeSpec.extend.\"\n            )\n        return self.__class__(\n            nvec=nvecs, shape=shape, device=self.device, dtype=self.dtype\n        )\n\n\nclass DiscreteTensorSpec(TensorSpec):\n    \"\"\"A discrete tensor spec.\n\n    An alternative to OneHotTensorSpec for categorical variables in TorchRL. Instead of\n    using multiplication, categorical variables perform indexing which can speed up\n    computation and reduce memory cost for large categorical variables.\n\n    Example:\n        >>> batch, size = 3, 4\n        >>> action_value = torch.arange(batch*size)\n        >>> action_value = action_value.view(batch, size).to(torch.float)\n        >>> action = torch.argmax(action_value, dim=-1).to(torch.long)\n        >>> chosen_action_value = action_value[range(batch), action]\n        >>> print(chosen_action_value)\n        tensor([ 3.,  7., 11.])\n\n    Args:\n        n (int): number of possible outcomes.\n        shape: (torch.Size, optional): shape of the variable, default is \"torch.Size([])\".\n        device (str, int or torch.device, optional): device of the tensors.\n        dtype (str or torch.dtype, optional): dtype of the tensors.\n\n    \"\"\"\n\n    shape: torch.Size\n    space: DiscreteBox\n    device: torch.device = torch.device(\"cpu\")\n    dtype: torch.dtype = torch.float\n    domain: str = \"\"\n\n    def __init__(\n        self,\n        n: int,\n        shape: Optional[torch.Size] = None,\n        device: Optional[DEVICE_TYPING] = None,\n        dtype: Optional[Union[str, torch.dtype]] = torch.long,\n    ):\n        if shape is None:\n            shape = torch.Size([])\n        dtype, device = _default_dtype_and_device(dtype, device)\n        space = DiscreteBox(n)\n        super().__init__(shape, space, device, dtype, domain=\"discrete\")\n\n    def rand(self, shape=None) -> torch.Tensor:\n        if shape is None:\n            shape = torch.Size([])\n        return torch.randint(\n            0,\n            self.space.n,\n            torch.Size([*shape, *self.shape]),\n            device=self.device,\n            dtype=self.dtype,\n        )\n\n    def _project(self, val: torch.Tensor) -> torch.Tensor:\n        if val.dtype not in (torch.int, torch.long):\n            val = torch.round(val)\n        return val.clamp_(min=0, max=self.space.n - 1)\n\n    def is_in(self, val: torch.Tensor) -> bool:\n        return (0 <= val).all() and (val < self.space.n).all()\n\n    def __eq__(self, other):\n        return (\n            type(self) == type(other)\n            and self.shape == other.shape\n            and self.space == other.space\n            and self.device == other.device\n            and self.dtype == other.dtype\n            and self.domain == other.domain\n        )\n\n    def to_numpy(self, val: TensorDict, safe: bool = True) -> dict:\n        return super().to_numpy(val, safe)\n\n    def to_onehot(self) -> OneHotDiscreteTensorSpec:\n        # if len(self.shape) > 1:\n        #     raise RuntimeError(\n        #         f\"DiscreteTensorSpec with shape that has several dimensions can't be converted to \"\n        #         f\"OneHotDiscreteTensorSpec. Got shape={self.shape}.\"\n        #     )\n        shape = [*self.shape, self.space.n]\n        return OneHotDiscreteTensorSpec(\n            n=self.space.n, shape=shape, device=self.device, dtype=self.dtype\n        )\n\n    def expand(self, *shape):\n        if len(shape) == 1 and isinstance(shape[0], (tuple, list, torch.Size)):\n            shape = shape[0]\n        if any(val < 0 for val in shape):\n            raise ValueError(\n                f\"{self.__class__.__name__}.extend does not support negative shapes.\"\n            )\n        if any(s1!= s2 and s2!= 1 for s1, s2 in zip(shape[-self.ndim :], self.shape)):\n            raise ValueError(\n                f\"The last {self.ndim} of the extended shape must match the\"\n                f\"shape of the CompositeSpec in CompositeSpec.extend.\"\n            )", "metadata": {"task_id": "pytorch_rl/21", "ground_truth": "        return self.__class__(", "fpath_tuple": ["pytorch_rl", "torchrl", "data", "tensor_specs.py"], "context_start_lineno": 1169, "line_no": 1333}}
{"prompt": " :] for i in image]\n        image = np.concatenate(image, axis=0)\n        image = np.array(image).astype(np.float32) / 255.0\n        image = image.transpose(0, 3, 1, 2)\n        image = 2.0 * image - 1.0\n        image = torch.from_numpy(image)\n    elif isinstance(image[0], torch.Tensor):\n        image = torch.cat(image, dim=0)\n    return image\n\n\nclass StableDiffusionInstructPix2PixPipeline(DiffusionPipeline):\n    r\"\"\"\n    Pipeline for pixel-level image editing by following text instructions. Based on Stable Diffusion.\n\n    This model inherits from [`DiffusionPipeline`]. Check the superclass documentation for the generic methods the\n    library implements for all the pipelines (such as downloading or saving, running on a particular device, etc.)\n\n    Args:\n        vae ([`AutoencoderKL`]):\n            Variational Auto-Encoder (VAE) Model to encode and decode images to and from latent representations.\n        text_encoder ([`CLIPTextModel`]):\n            Frozen text-encoder. Stable Diffusion uses the text portion of\n            [CLIP](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModel), specifically\n            the [clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14) variant.\n        tokenizer (`CLIPTokenizer`):\n            Tokenizer of class\n            [CLIPTokenizer](https://huggingface.co/docs/transformers/v4.21.0/en/model_doc/clip#transformers.CLIPTokenizer).\n        unet ([`UNet2DConditionModel`]): Conditional U-Net architecture to denoise the encoded image latents.\n        scheduler ([`SchedulerMixin`]):\n            A scheduler to be used in combination with `unet` to denoise the encoded image latents. Can be one of\n            [`DDIMScheduler`], [`LMSDiscreteScheduler`], or [`PNDMScheduler`].\n        safety_checker ([`StableDiffusionSafetyChecker`]):\n            Classification module that estimates whether generated images could be considered offensive or harmful.\n            Please, refer to the [model card](https://huggingface.co/runwayml/stable-diffusion-v1-5) for details.\n        feature_extractor ([`CLIPFeatureExtractor`]):\n            Model that extracts features from generated images to be used as inputs for the `safety_checker`.\n    \"\"\"\n    _optional_components = [\"safety_checker\", \"feature_extractor\"]\n\n    def __init__(\n        self,\n        vae: AutoencoderKL,\n        text_encoder: CLIPTextModel,\n        tokenizer: CLIPTokenizer,\n        unet: UNet2DConditionModel,\n        scheduler: KarrasDiffusionSchedulers,\n        safety_checker: StableDiffusionSafetyChecker,\n        feature_extractor: CLIPFeatureExtractor,\n        requires_safety_checker: bool = True,\n    ):\n        super().__init__()\n\n        if safety_checker is None and requires_safety_checker:\n            logger.warning(\n                f\"You have disabled the safety checker for {self.__class__} by passing `safety_checker=None`. Ensure\"\n                \" that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered\"\n                \" results in services or applications open to the public. Both the diffusers team and Hugging Face\"\n                \" strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling\"\n                \" it only for use-cases that involve analyzing network behavior or auditing its results. For more\"\n                \" information, please have a look at https://github.com/huggingface/diffusers/pull/254.\"\n            )\n\n        if safety_checker is not None and feature_extractor is None:\n            raise ValueError(\n                \"Make sure to define a feature extractor when loading {self.__class__} if you want to use the safety\"\n                \" checker. If you do not want to use the safety checker, you can pass `'safety_checker=None'` instead.\"\n            )\n\n        self.register_modules(\n            vae=vae,\n            text_encoder=text_encoder,\n            tokenizer=tokenizer,\n            unet=unet,\n            scheduler=scheduler,\n            safety_checker=safety_checker,\n            feature_extractor=feature_extractor,\n        )\n        self.vae_scale_factor = 2 ** (len(self.vae.config.block_out_channels) - 1)\n        self.register_to_config(requires_safety_checker=requires_safety_checker)\n\n    @torch.no_grad()\n    def __call__(\n        self,\n        prompt: Union[str, List[str]] = None,\n        image: Union[torch.FloatTensor, PIL.Image.Image] = None,\n        num_inference_steps: int = 100,\n        guidance_scale: float = 7.5,\n        image_guidance_scale: float = 1.5,\n        negative_prompt: Optional[Union[str, List[str]]] = None,\n        num_images_per_prompt: Optional[int] = 1,\n        eta: float = 0.0,\n        generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,\n        latents: Optional[torch.FloatTensor] = None,\n        prompt_embeds: Optional[torch.FloatTensor] = None,\n        negative_prompt_embeds: Optional[torch.FloatTensor] = None,\n        output_type: Optional[str] = \"pil\",\n        return_dict: bool = True,\n        callback: Optional[Callable[[int, int, torch.FloatTensor], None]] = None,\n        callback_steps: Optional[int] = 1,\n    ):\n        r\"\"\"\n        Function invoked when calling the pipeline for generation.\n\n        Args:\n             prompt (`str` or `List[str]`, *optional*):\n                The prompt or prompts to guide the image generation. If not defined, one has to pass `prompt_embeds`.\n                instead.\n            image (`PIL.Image.Image`):\n                `Image`, or tensor representing an image batch which will be repainted according to `prompt`.\n            num_inference_steps (`int`, *optional*, defaults to 100):\n                The number of denoising steps. More denoising steps usually lead to a higher quality image at the\n                expense of slower inference.\n            guidance_scale (`float`, *optional*, defaults to 7.5):\n                Guidance scale as defined in [Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598).\n                `guidance_scale` is defined as `w` of equation 2. of [Imagen\n                Paper](https://arxiv.org/pdf/2205.11487.pdf). Guidance scale is enabled by setting `guidance_scale >\n                1`. Higher guidance scale encourages to generate images that are closely linked to the text `prompt`,\n                usually at the expense of lower image quality. This pipeline requires a value of at least `1`.\n            image_guidance_scale (`float`, *optional*, defaults to 1.5):\n                Image guidance scale is to push the generated image towards the inital image `image`. Image guidance\n                scale is enabled by setting `image_guidance_scale > 1`. Higher image guidance scale encourages to\n                generate images that are closely linked to the source image `image`, usually at the expense of lower", "metadata": {"task_id": "huggingface_diffusers/130", "ground_truth": "                image quality. This pipeline requires a value of at least `1`.", "fpath_tuple": ["huggingface_diffusers", "src", "diffusers", "pipelines", "stable_diffusion", "pipeline_stable_diffusion_instruct_pix2pix.py"], "context_start_lineno": 45, "line_no": 168}}
{"prompt": "(\n            model_or_pipeline=self.pipe,\n            data=self.data,\n            input_column=\"text\",\n            label_column=\"label\",\n            label_mapping=self.label_mapping,\n        )\n\n    def test_model_init(self):\n        self.evaluator.compute(\n            model_or_pipeline=self.default_model,\n            tokenizer=self.default_tokenizer,\n            data=self.data,\n            input_column=\"text\",\n            label_column=\"label\",\n            label_mapping=self.label_mapping,\n        )\n\n    def test_model_str_init(self):\n        self.evaluator.compute(\n            model_or_pipeline=self.default_ckpt,\n            data=self.data,\n            input_column=\"text\",\n            label_column=\"label\",\n            label_mapping=self.label_mapping,\n        )\n\n\nclass TestTextClassificationEvaluator(TestCase):\n    def setUp(self):\n        self.data = Dataset.from_dict({\"label\": [1, 0], \"text\": [\"great movie\", \"horrible movie\"]})\n        self.default_model = \"lvwerra/distilbert-imdb\"\n        self.input_column = \"text\"\n        self.label_column = \"label\"\n        self.pipe = DummyTextClassificationPipeline()\n        self.perf_pipe = DummyTextClassificationPipeline(sleep_time=0.1)\n        self.evaluator = evaluator(\"text-classification\")\n        self.label_mapping = {\"NEGATIVE\": 0.0, \"POSITIVE\": 1.0}\n\n    def test_pipe_init(self):\n        results = self.evaluator.compute(\n            model_or_pipeline=self.pipe,\n            data=self.data,\n            input_column=\"text\",\n            label_column=\"label\",\n            label_mapping=self.label_mapping,\n        )\n        self.assertEqual(results[\"accuracy\"], 1.0)\n\n    @slow\n    def test_model_init(self):\n        results = self.evaluator.compute(\n            model_or_pipeline=self.default_model,\n            data=self.data,\n            metric=\"accuracy\",\n            input_column=self.input_column,\n            label_column=self.label_column,\n            label_mapping=self.label_mapping,\n        )\n\n        model = AutoModelForSequenceClassification.from_pretrained(self.default_model)\n        tokenizer = AutoTokenizer.from_pretrained(self.default_model)\n\n        self.assertEqual(results[\"accuracy\"], 1.0)\n        results = self.evaluator.compute(\n            model_or_pipeline=model,\n            data=self.data,\n            metric=\"accuracy\",\n            tokenizer=tokenizer,\n            label_mapping=self.label_mapping,\n        )\n        self.assertEqual(results[\"accuracy\"], 1.0)\n\n    def test_class_init(self):\n        evaluator = TextClassificationEvaluator()\n        self.assertEqual(evaluator.task, \"text-classification\")\n        self.assertIsNone(evaluator.default_metric_name)\n\n        results = evaluator.compute(\n            model_or_pipeline=self.pipe,\n            data=self.data,\n            metric=\"f1\",\n            label_mapping=self.label_mapping,\n        )\n        self.assertEqual(results[\"f1\"], 1.0)\n\n    @slow\n    def test_default_pipe_init(self):\n        results = self.evaluator.compute(\n            data=self.data,\n            label_mapping=self.label_mapping,\n        )\n        self.assertEqual(results[\"accuracy\"], 1.0)\n\n    def test_data_loading(self):\n\n        # Test passing in dataset by name with split\n        data = self.evaluator.load_data(\"evaluate/imdb-ci\", split=\"test[:1]\")\n        self.evaluator.prepare_data(data=data, input_column=\"text\", label_column=\"label\", second_input_column=None)\n\n        # Test passing in dataset by name without split and inferring the optimal split\n        data = self.evaluator.load_data(\"evaluate/imdb-ci\")\n        self.evaluator.prepare_data(data=data, input_column=\"text\", label_column=\"label\", second_input_column=None)\n\n        # Test that it chooses the correct one (e.g. imdb only has train and test, but no validation)\n        self.assertEqual(data.split, \"test\")\n\n        # Test that the data point returned is correct; this maps to the first example in the dataset\n        self.assertEqual(data[0][\"text\"], \"I love movies about whales!\")\n\n        # Test loading subset of a dataset with the `name` field\n        data = self.evaluator.load_data(\"evaluate/glue-ci\", subset=\"cola\", split=\"test\")\n        self.assertEqual(isinstance(data, Dataset), True)\n\n        # Test loading subset of a dataset with the `name` field and having it infer the split\n        data = self.evaluator.load_data(\"evaluate/glue-ci\", subset=\"cola\")\n        self.assertEqual(isinstance(data, Dataset), True)\n\n    def test_overwrite_default_metric(self):\n        accuracy = load(\"accuracy\")\n        results = self.evaluator.compute(\n            model_or_pipeline=self.pipe,\n            data=self.data,\n            metric=accuracy,\n            label_mapping=self.label_mapping,\n        )\n        self.assertEqual(results[\"accuracy\"], 1.0)\n        results = self.evaluator.compute(\n            model_or_pipeline=self.pipe,\n            data=self.data,\n            metric=\"accuracy\",\n            label_mapping=self.label_mapping,\n        )\n        self.assertEqual(results[\"accuracy\"], 1.0)\n\n    def test_bootstrap(self):\n        data = Dataset.from_dict({\"label\": [1, 0, 0], \"text\": [\"great movie\", \"great movie\", \"horrible movie\"]})\n\n        results = self.evaluator.compute(\n            model_or_pipeline=self.pipe,\n            data=data,\n            metric=\"accuracy\",\n            label_mapping=self.label_mapping,\n            strategy=\"bootstrap\",\n            n_resamples=10,\n            random_state=0,\n        )\n        self.assertAlmostEqual(results[\"accuracy\"][\"score\"], 0.666666, 5)\n        self.assertAlmostEqual(results[\"accuracy\"][\"confidence_interval\"][0], 0.33333, 5)\n        self.assertAlmostEqual(results[\"accuracy\"][\"confidence_interval\"][1], 0.666666, 5)\n        self.assertAlmostEqual(results[\"accuracy\"][\"standard_error\"], 0.22498, 5)\n\n    def test_perf(self):\n        results = self.evaluator.compute(\n            model_or_pipeline=self.perf_pipe,\n            data=self.data,\n            metric=\"accuracy\",\n            input_column=self.input_column,\n            label_column=self.label_column,\n            label_mapping=self.label_mapping,\n            n_resamples=10,\n            random_state=0,\n        )\n        self.assertEqual(results[\"accuracy\"], 1.0)", "metadata": {"task_id": "huggingface_evaluate/188", "ground_truth": "        self.assertAlmostEqual(results[\"total_time_in_seconds\"], 0.1, 1)", "fpath_tuple": ["huggingface_evaluate", "tests", "test_evaluator.py"], "context_start_lineno": 202, "line_no": 366}}
{"prompt": "# Copyright 2021 The HuggingFace Evaluate Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\" Chrf(++) metric as available in sacrebleu. \"\"\"\nimport datasets\nimport sacrebleu as scb\nfrom packaging import version\nfrom sacrebleu import CHRF\n\nimport evaluate\n\n\n_CITATION = \"\"\"\\\n@inproceedings{popovic-2015-chrf,\n    title = \"chr{F}: character n-gram {F}-score for automatic {MT} evaluation\",\n    author = \"Popovi{\\'c}, Maja\",\n    booktitle = \"Proceedings of the Tenth Workshop on Statistical Machine Translation\",\n    month = sep,\n    year = \"2015\",\n    address = \"Lisbon, Portugal\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/W15-3049\",\n    doi = \"10.18653/v1/W15-3049\",\n    pages = \"392--395\",\n}\n@inproceedings{popovic-2017-chrf,\n    title = \"chr{F}++: words helping character n-grams\",\n    author = \"Popovi{\\'c}, Maja\",\n    booktitle = \"Proceedings of the Second Conference on Machine Translation\",\n    month = sep,\n    year = \"2017\",\n    address = \"Copenhagen, Denmark\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/W17-4770\",\n    doi = \"10.18653/v1/W17-4770\",\n    pages = \"612--618\",\n}\n@inproceedings{post-2018-call,\n    title = \"A Call for Clarity in Reporting {BLEU} Scores\",\n    author = \"Post, Matt\",\n    booktitle = \"Proceedings of the Third Conference on Machine Translation: Research Papers\",\n    month = oct,\n    year = \"2018\",\n    address = \"Belgium, Brussels\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://www.aclweb.org/anthology/W18-6319\",\n    pages = \"186--191\",\n}\n\"\"\"\n\n_DESCRIPTION = \"\"\"\\\nChrF and ChrF++ are two MT evaluation metrics. They both use the F-score statistic for character n-gram matches,\nand ChrF++ adds word n-grams as well which correlates more strongly with direct assessment. We use the implementation\nthat is already present in sacrebleu.\n\nThe implementation here is slightly different from sacrebleu in terms of the required input format. The length of\nthe references and hypotheses lists need to be the same, so you may need to transpose your references compared to\nsacrebleu's required input format. See https://github.com/huggingface/datasets/issues/3154#issuecomment-950746534\n\nSee the README.md file at https://github.com/mjpost/sacreBLEU#chrf--chrf for more information.\n\"\"\"\n\n_KWARGS_DESCRIPTION = \"\"\"\nProduces ChrF(++) scores for hypotheses given reference translations.\n\nArgs:\n    predictions (list of str): The predicted sentences.", "metadata": {"task_id": "huggingface_evaluate/181", "ground_truth": "    references (list of list of str): The references. There should be one reference sub-list for each prediction sentence.", "fpath_tuple": ["huggingface_evaluate", "metrics", "chrf", "chrf.py"], "context_start_lineno": 0, "line_no": 77}}
{"prompt": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom dataclasses import dataclass, field as dataclass_field\nfrom typing import Any, Callable, Optional, Sequence, Tuple, Union\n\nimport torch\n\nfrom torchrl.envs import ParallelEnv\nfrom torchrl.envs.common import EnvBase\nfrom torchrl.envs.env_creator import env_creator, EnvCreator\nfrom torchrl.envs.libs.dm_control import DMControlEnv\nfrom torchrl.envs.libs.gym import GymEnv\nfrom torchrl.envs.transforms import (\n    CatFrames,\n    CatTensors,\n    CenterCrop,\n    Compose,\n    DoubleToFloat,\n    FiniteTensorDictCheck,\n    GrayScale,\n    NoopResetEnv,\n    ObservationNorm,\n    Resize,\n    RewardScaling,\n    ToTensorImage,\n    TransformedEnv,\n    VecNorm,\n)\nfrom torchrl.envs.transforms.transforms import FlattenObservation, gSDENoise\nfrom torchrl.record.loggers import Logger\nfrom torchrl.record.recorder import VideoRecorder\n\nLIBS = {\n    \"gym\": GymEnv,\n    \"dm_control\": DMControlEnv,\n}\n\n\ndef correct_for_frame_skip(cfg: \"DictConfig\") -> \"DictConfig\":  # noqa: F821\n    \"\"\"Correct the arguments for the input frame_skip, by dividing all the arguments that reflect a count of frames by the frame_skip.\n\n    This is aimed at avoiding unknowingly over-sampling from the environment, i.e. targetting a total number of frames\n    of 1M but actually collecting frame_skip * 1M frames.\n\n    Args:\n        cfg (DictConfig): DictConfig containing some frame-counting argument, including:\n            \"max_frames_per_traj\", \"total_frames\", \"frames_per_batch\", \"record_frames\", \"annealing_frames\",\n            \"init_random_frames\", \"init_env_steps\"\n\n    Returns:\n         the input DictConfig, modified in-place.\n\n    \"\"\"\n    # Adapt all frame counts wrt frame_skip\n    if cfg.frame_skip!= 1:\n        fields = [\n            \"max_frames_per_traj\",\n            \"total_frames\",\n            \"frames_per_batch\",\n            \"record_frames\",\n            \"annealing_frames\",\n            \"init_random_frames\",\n            \"init_env_steps\",\n            \"noops\",\n        ]\n        for field in fields:\n            if hasattr(cfg, field):\n                setattr(cfg, field, getattr(cfg, field) // cfg.frame_skip)", "metadata": {"task_id": "pytorch_rl/20", "ground_truth": "    return cfg", "fpath_tuple": ["pytorch_rl", "torchrl", "trainers", "helpers", "envs.py"], "context_start_lineno": 0, "line_no": 71}}
{"prompt": "qual(\n        [t.parameters for t in self._trials],\n        converter.to_parameters(converter.to_features(self._trials)),\n    )\n\n  def test_parameter_continuify(self):\n    space = pyvizier.SearchSpace()\n    root = space.root\n    root.add_float_param('double', -2.0, 2.0)\n    root.add_int_param('integer', -2, 2)\n    root.add_categorical_param('categorical', ['b', 'c'])\n    root.add_discrete_param('discrete', [-1.0, 2.0, 3.0])\n\n    converter = core.TrialToArrayConverter.from_study_config(\n        pyvizier.ProblemStatement(search_space=space)\n    )\n    trial = pyvizier.Trial(\n        parameters={\n            'double': pyvizier.ParameterValue(3.0),\n            'integer': pyvizier.ParameterValue(-1),\n            'discrete': pyvizier.ParameterValue(2.0),\n            'categorical': pyvizier.ParameterValue('d'),\n        }\n    )\n    # Scaled outputs + 1 hot embedding for categoricals.\n    expected = np.array([[1.25, 0.25, 0, 0, 1, 0.75]])\n    np.testing.assert_equal(converter.to_features([trial]), expected)\n\n  def test_onehot_embedding(self):\n    space = pyvizier.SearchSpace()\n    root = space.root\n    root.add_categorical_param('c1', ['0', '1', '2'])\n    root.add_categorical_param('c2', ['0', '1', '2', '3'])\n\n    converter = core.TrialToArrayConverter.from_study_config(\n        pyvizier.ProblemStatement(search_space=space)\n    )\n    n_trials = 10\n    n_repeat_check = 50\n    for _ in range(n_repeat_check):\n      trials = []\n      for _ in range(n_trials):\n        trials.append(\n            pyvizier.Trial(\n                parameters={\n                    'c1': pyvizier.ParameterValue(str(np.random.randint(0, 3))),\n                    'c2': pyvizier.ParameterValue(str(np.random.randint(0, 4))),\n                }\n            )\n        )\n      features = converter.to_features(trials)\n      np.testing.assert_equal(\n          np.sum(features, axis=1), np.array([2] * n_trials)\n      )\n\n\nclass DictToArrayTest(absltest.TestCase):\n\n  def test_2d(self):\n    self.assertSequenceEqual(\n        core.dict_to_array(\n            {1: np.random.random((10, 2)), 2: np.random.random((10, 1))}\n        ).shape,\n        (10, 3),\n    )\n\n  def test_4d(self):\n    self.assertSequenceEqual(\n        core.dict_to_array({\n            1: np.random.random((2, 1, 10, 2)),\n            2: np.random.random((2, 1, 10, 1)),\n        }).shape,\n        (2, 1, 10, 3),\n    )\n\n  def test_dict_like(self):\n    d = core.DictOf2DArrays({\n        'p1': np.array([[1], [2], [3]]),\n        'p2': np.array([[0.0, 0.1], [0.5, 0.5], [0.1, 0.0]]),\n    })\n    self.assertCountEqual(\n        d.dict_like(np.array([[1, 1, 0.1], [3, 0, 0.1]])),\n        {'p1': np.array([[1], [3]]), 'p2': np.array([[1, 0.1], [0, 0.1]])},\n    )\n\n  def test_dict_like_and_as_array_are_inverse(self):\n    d = core.DictOf2DArrays({\n        'p1': np.array([[1], [2], [3]]),\n        'p2': np.array([[0.0, 0.1], [0.5, 0.5], [0.1, 0.0]]),\n    })\n    self.assertCountEqual(d, d.dict_like(d.asarray()))\n\n\nrnd = np.random.random\n\n\nclass DictOf2DArraysTest(absltest.TestCase):\n\n  def test_bad_shapes(self):\n    with self.assertRaises(ValueError):\n      core.DictOf2DArrays({'a': rnd([10, 2]), 'b': rnd([5, 2])})\n\n  def test_bad_dim(self):\n    with self.assertRaises(ValueError):\n      core.DictOf2DArrays({'a': rnd([10, 2, 1])})\n\n  def test_size(self):\n    d = core.DictOf2DArrays({'a': rnd([10, 2]), 'b': rnd([10, 1])})\n    self.assertEqual(d.size, 10)\n\n  def test_addition(self):\n    d1 = core.DictOf2DArrays({'a': rnd([10, 2]), 'b': rnd([10, 1])})\n    d2 = core.DictOf2DArrays({'a': rnd([5, 2]), 'b': rnd([5, 1])})\n    d = d1 + d2\n    self.assertEqual(d.size, 15)\n    self.assertLen(d, 2)\n    self.assertSequenceEqual(d['a'].shape, (15, 2))\n    self.assertSequenceEqual(d['b'].shape, (15, 1))\n\n  def test_bad_addition(self):\n    # Shape of b doesn't match.\n    d1 = core.DictOf2DArrays({'a': rnd([10, 2]), 'b': rnd([10, 1])})\n    d2 = core.DictOf2DArrays({'a': rnd([5, 2]), 'b': rnd([5, 3])})\n    with self.assertRaises(ValueError):\n      _ = d1 + d2\n\n\nclass DefaultTrialConverterFromStudyConfigsTest(absltest.TestCase):\n\n  @property\n  def _study_configs(self):\n    study_configs = []\n    space1 = pyvizier.SearchSpace()\n    root = space1.root\n    root.add_float_param('double', -1.0, 1.0)\n    root.add_int_param('integer', -1, 1)\n    root.add_categorical_param('categorical', ['a', 'b'])\n    root.add_discrete_param('discrete', [-1.0, 1.0])\n    study_configs.append(\n        pyvizier.ProblemStatement(\n            metadata=pyvizier.Metadata({core.STUDY_ID_FIELD:'study1'}),\n            search_space=space1,\n        )\n    )\n\n    space2 = pyvizier.SearchSpace()\n    root = space2.root\n    root.add_float_param('double', -1.0, 2.0)\n    root.add_int_param('integer', -2, 1)\n    root.add_categorical_param('categorical', ['b', 'c'])", "metadata": {"task_id": "google_vizier/158", "ground_truth": "    root.add_discrete_param('discrete', [-1.0, 1.0, 2.0])", "fpath_tuple": ["google_vizier", "vizier", "pyvizier", "converters", "core_test.py"], "context_start_lineno": 56, "line_no": 206}}
{"prompt": "utf-8')\n            self.src_file = codecs.open(self.ctx.src_path, 'w', 'utf-8')\n            self.tgt_file = codecs.open(self.ctx.tgt_path, 'w', 'utf-8')\n\n        self.ctx.model.update_client_id(ID)\n\n    def update_contrast_monitor(self, contrast_monitor):\n        self.ctx.contrast_monitor = contrast_monitor\n\n    def get_model_grads(self, filter_keywords=None):\n        if filter_keywords is None:\n            filter_keywords = self.ctx.cfg.personalization.local_param\n        grads = {}\n        for n, p2 in self.ctx.model.state_dict().items():\n            if filter_by_specified_keywords(n, filter_keywords):  # preserve\n                grads[n] = p2 - self.ctx.init_params[n]\n        return grads\n\n    def parse_data(self, data):\n        init_dict = dict()\n        if isinstance(data, dict):\n            all_split = ['train', 'val', 'test'] if not \\\n                self.cfg.model.use_contrastive_loss else \\\n                ['train_raw', 'train_contrast', 'val', 'test']\n            for split in all_split:\n                init_dict['{}_data'.format(split)] = None\n                init_dict['{}_loader'.format(split)] = None\n                init_dict['num_{}_data'.format(split)] = 0\n                init_dict['{}_encoded'.format(split)] = None\n                init_dict['{}_examples'.format(split)] = None\n                if data.get(split, None) is not None:\n                    if isinstance(data.get(split)['dataloader'], DataLoader):\n                        init_dict['{}_loader'.format(split)] = \\\n                            data.get(split)['dataloader']\n                        init_dict['num_{}_data'.format(split)] = \\\n                            len(data.get(split)['dataloader'].dataset)\n                        init_dict['{}_encoded'.format(split)] = \\\n                            data.get(split)['encoded']\n                        init_dict['{}_examples'.format(split)] = \\\n                            data.get(split)['examples']\n\n                        if self.cfg.model.use_contrastive_loss and \\\n                                split == 'train_raw':\n                            init_dict['train_data'] = None\n                            init_dict['train_loader'] = \\\n                                data.get(split)['dataloader']\n                            init_dict['num_train_data'] = \\\n                                len(data.get(split)['dataloader'].dataset)\n                            init_dict['train_encoded'] = \\\n                                data.get(split)['encoded']\n                            init_dict['train_examples'] = \\\n                                data.get(split)['examples']\n                    else:\n                        raise TypeError('Type {} is not supported.'.format(\n                            type(data.get(split))))\n        else:\n            raise TypeError('Type of data should be dict.')\n\n        return init_dict\n\n    def setup_optimizer_and_scheduler(self, ctx):\n        total_steps = getattr(ctx, f'num_total_{ctx.cur_mode}_batch',\n                              None) // ctx.cfg.grad.grad_accum_count * \\\n                      ctx.cfg.federate.total_round_num\n        warmup_steps = int(ctx.cfg[ctx.cur_mode].scheduler.warmup_ratio *\n                           total_steps)\n        optimizer = get_optimizer(ctx.model, **ctx.cfg[ctx.cur_mode].optimizer)\n        scheduler = get_scheduler(optimizer,\n                                  **ctx.cfg.train.scheduler,\n                                  total_steps=total_steps,\n                                  warmup_steps=warmup_steps)\n\n        return optimizer, scheduler\n\n    def _load_model(self, ctx):\n        load_path = ctx.cfg.federate.atc_load_from\n        global_ckpt_path = os.path.join(load_path, 'global_model.pt')\n        client_ckpt_path = \\\n            os.path.join(load_path, 'client', 'client_model_{}.pt'.format(\n                self.ID))\n        if not os.path.exists(global_ckpt_path):\n            global_dir = os.path.join(load_path, 'global')\n            global_ckpt_path = \\\n                os.path.join(global_dir, 'global_model_{}.pt'.format(self.ID))\n            if not os.path.exists(global_ckpt_path):\n                raise RuntimeError(\n                    'Checkpoint NOT found in \\'{}\\''.format(global_ckpt_path))\n\n        model_ckpt = ctx.model.state_dict()\n        logger.info('Loading model from \\'{}\\''.format(global_ckpt_path))\n        global_ckpt = torch.load(global_ckpt_path, map_location='cpu')['model']\n        model_ckpt.update({\n            k: v\n            for k, v in global_ckpt.items()\n            if k in model_ckpt and v.size() == model_ckpt[k].size()\n        })\n        if os.path.exists(client_ckpt_path):\n            logger.info('Updating model from \\'{}\\''.format(client_ckpt_path))\n            client_ckpt = torch.load(client_ckpt_path,\n                                     map_location='cpu')['model']\n            model_ckpt.update({\n                k: v\n                for k, v in client_ckpt.items()\n                if k in model_ckpt and v.size() == model_ckpt[k].size()\n            })\n        ctx.model.load_state_dict(model_ckpt)\n\n    def _save_model(self, ctx):\n        if len(ctx.cfg.personalization.local_param) > 0:\n            model_ckpt = OrderedDict({\n                k: v\n                for k, v in ctx.model.state_dict().items()\n                if re.search('|'.join(ctx.cfg.personalization.local_param), k)\n                is not None\n            })\n            ckpt = {\n               'model': model_ckpt,\n                'epoch': ctx.cur_epoch_i + 1,\n                'batch': ctx.cur_batch_i + 1,\n            }\n            save_dir = os.path.join(ctx.cfg.federate.save_to, 'client')\n            os.makedirs(save_dir, exist_ok=True)\n            ckpt_path = os.path.join(save_dir,\n                                     'client_model_{}.pt'.format(self.ID))\n            torch.save(ckpt, ckpt_path)\n\n    def _remove_special_tokens(self, sent):\n        return sent.replace('[CLS]', '').replace('[SEP]', '').\\\n            replace('[PAD]', '').replace('[unused0]', '').\\\n            replace('[unused3]', '').replace('[unused1]', ''). \\\n            replace(r' +','').replace(' [unused2] ', '<q>').\\\n            replace('[unused2]', '').strip()\n\n    @property\n    def _in_contrast_prepare(self):\n        return self.use_contrastive_loss and \\\n               self.task!= 'pretrain' and \\\n               self.ctx.cur_split == 'train' and \\", "metadata": {"task_id": "alibaba_FederatedScope/83", "ground_truth": "               self.ctx.contrast_monitor.stat == 1", "fpath_tuple": ["alibaba_FederatedScope", "federatedscope", "nlp", "hetero_tasks", "trainer", "trainer.py"], "context_start_lineno": 72, "line_no": 210}}
{"prompt": "import abc\nimport collections\nimport logging\nfrom functools import partial\nfrom typing import Any, Callable, Dict, List, Optional, Tuple, Union\n\nimport jax\nimport jax.numpy as jnp\nfrom flax import jax_utils\nfrom flax.training.common_utils import stack_forest\nfrom jax import lax, random, value_and_grad\nfrom jax._src.prng import PRNGKeyArray\nfrom jax.tree_util import tree_map\nfrom tqdm import trange\nfrom tqdm.std import tqdm as TqdmDecorator\n\nfrom fortuna.calibration.state import CalibState\nfrom fortuna.data.loader import DataLoader, TargetsLoader\nfrom fortuna.training.mixin import (InputValidatorMixin,\n                                    WithCheckpointingMixin,\n                                    WithEarlyStoppingMixin)\nfrom fortuna.typing import Array, CalibMutable, CalibParams, Path, Status\nfrom fortuna.utils.builtins import HashableMixin\n\n\nclass CalibModelCalibrator(\n    HashableMixin,\n    WithCheckpointingMixin,\n    WithEarlyStoppingMixin,\n    InputValidatorMixin,\n    metaclass=abc.ABCMeta,\n):\n    def __init__(\n        self,\n        *args,\n        calib_outputs: Array,\n        calib_targets: Array,\n        predict_fn: Callable[[jnp.ndarray], jnp.ndarray],\n        uncertainty_fn: Callable[[jnp.ndarray], jnp.ndarray],\n        val_outputs: Array,\n        val_targets: Array,\n        save_checkpoint_dir: Optional[Path] = None,\n        save_every_n_steps: Optional[int] = None,\n        keep_top_n_checkpoints: int = 2,\n        disable_training_metrics_computation: bool = False,\n        eval_every_n_epochs: int = 1,\n        **kwargs,\n    ):\n        super(CalibModelCalibrator, self).__init__(*args, **kwargs)\n        self._calib_outputs = calib_outputs\n        self._calib_targets = calib_targets\n        self._val_outputs = val_outputs\n        self._val_targets = val_targets\n        self.predict_fn = predict_fn\n        self.uncertainty_fn = uncertainty_fn\n        self.save_checkpoint_dir = save_checkpoint_dir\n        self.save_every_n_steps = save_every_n_steps\n        self.keep_top_n_checkpoints = keep_top_n_checkpoints\n        self.disable_training_metrics_computation = disable_training_metrics_computation\n        self.eval_every_n_epochs = eval_every_n_epochs\n        self.multi_device = False\n\n    def train(\n        self,\n        rng: PRNGKeyArray,\n        state: CalibState,\n        fun: Callable,\n        n_epochs: int = 1,\n        metrics: Optional[\n            Tuple[Callable[[jnp.ndarray, jnp.ndarray, Array], Array],...]\n        ] = None,\n        verbose: bool = True,\n    ) -> Tuple[CalibState, Status]:\n        training_losses_and_metrics = collections.defaultdict(list)\n        val_losses_and_metrics = collections.defaultdict(list)\n\n        state, targets, outputs, rng = self.on_train_start(\n            state,\n            [self._calib_targets, self._val_targets],\n            [self._calib_outputs, self._val_outputs],\n            rng,\n        )\n        calib_targets, val_targets = targets\n        calib_outputs, val_outputs = outputs\n\n        progress_bar = trange(n_epochs, desc=\"Epoch\")\n        for epoch in progress_bar:\n            # training loop\n            (\n                state,\n                training_losses_and_metrics_current_epoch,\n                training_batch_metrics_str,\n            ) = self._training_loop(\n                epoch,\n                fun,\n                metrics,\n                rng,\n                state,\n                calib_targets,\n                calib_outputs,\n                verbose,\n                progress_bar,\n            )\n            # keep track of training losses and metrics [granularity=epoch]\n            for k in training_losses_and_metrics_current_epoch.keys():\n                training_losses_and_metrics[k].append(\n                    training_losses_and_metrics_current_epoch[k]\n                )\n\n            # validation loop", "metadata": {"task_id": "awslabs_fortuna/114", "ground_truth": "            if self.should_perform_validation(val_targets, epoch):", "fpath_tuple": ["awslabs_fortuna", "fortuna", "calib_model", "calib_model_calibrator.py"], "context_start_lineno": 0, "line_no": 110}}
{"prompt": "import torch\nimport numpy as np\nfrom torch.utils.data import Dataset\n\n\nclass WrapDataset(Dataset):\n    \"\"\"Wrap raw data into pytorch Dataset\n\n    Arguments:\n        dataset (dict): raw data dictionary contains \"x\" and \"y\"\n\n    \"\"\"\n    def __init__(self, dataset):\n        super(WrapDataset, self).__init__()\n        self.dataset = dataset\n\n    def __getitem__(self, idx):\n        if isinstance(self.dataset[\"x\"][idx], torch.Tensor):\n            return self.dataset[\"x\"][idx], self.dataset[\"y\"][idx]\n        elif isinstance(self.dataset[\"x\"][idx], np.ndarray):\n            return torch.from_numpy(", "metadata": {"task_id": "alibaba_FederatedScope/133", "ground_truth": "                self.dataset[\"x\"][idx]).float(), torch.from_numpy(", "fpath_tuple": ["alibaba_FederatedScope", "federatedscope", "core", "data", "wrap_dataset.py"], "context_start_lineno": 0, "line_no": 21}}
{"prompt": "# coding=utf-8\n# Copyright 2022 HuggingFace Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport gc\nimport random\nimport tempfile\nimport unittest\n\nimport numpy as np\nimport torch\n\nfrom diffusers import (\n    AutoencoderKL,\n    DDIMScheduler,\n    DPMSolverMultistepScheduler,\n    LMSDiscreteScheduler,\n    PNDMScheduler,\n    StableDiffusionDepth2ImgPipeline,\n    UNet2DConditionModel,\n)\nfrom diffusers.utils import floats_tensor, load_image, load_numpy, nightly, slow, torch_device\nfrom diffusers.utils.import_utils import is_accelerate_available\nfrom diffusers.utils.testing_utils import require_torch_gpu\nfrom PIL import Image\nfrom transformers import (\n    CLIPTextConfig,\n    CLIPTextModel,\n    CLIPTokenizer,\n    DPTConfig,\n    DPTFeatureExtractor,\n    DPTForDepthEstimation,\n)\n\nfrom...test_pipelines_common import PipelineTesterMixin\n\n\ntorch.backends.cuda.matmul.allow_tf32 = False\n\n\n@unittest.skipIf(torch_device == \"mps\", reason=\"The depth model does not support MPS yet\")\nclass StableDiffusionDepth2ImgPipelineFastTests(PipelineTesterMixin, unittest.TestCase):\n    pipeline_class = StableDiffusionDepth2ImgPipeline\n    test_save_load_optional_components = False\n\n    def get_dummy_components(self):\n        torch.manual_seed(0)\n        unet = UNet2DConditionModel(\n            block_out_channels=(32, 64),\n            layers_per_block=2,\n            sample_size=32,\n            in_channels=5,\n            out_channels=4,\n            down_block_types=(\"DownBlock2D\", \"CrossAttnDownBlock2D\"),\n            up_block_types=(\"CrossAttnUpBlock2D\", \"UpBlock2D\"),\n            cross_attention_dim=32,\n            attention_head_dim=(2, 4, 8, 8),\n            use_linear_projection=True,\n        )\n        scheduler = PNDMScheduler(skip_prk_steps=True)\n        torch.manual_seed(0)\n        vae = AutoencoderKL(\n            block_out_channels=[32, 64],\n            in_channels=3,\n            out_channels=3,\n            down_block_types=[\"DownEncoderBlock2D\", \"DownEncoderBlock2D\"],\n            up_block_types=[\"UpDecoderBlock2D\", \"UpDecoderBlock2D\"],\n            latent_channels=4,\n        )\n        torch.manual_seed(0)\n        text_encoder_config = CLIPTextConfig(\n            bos_token_id=0,\n            eos_token_id=2,\n            hidden_size=32,\n            intermediate_size=37,\n            layer_norm_eps=1e-05,\n            num_attention_heads=4,\n            num_hidden_layers=5,\n            pad_token_id=1,\n            vocab_size=1000,\n        )\n        text_encoder = CLIPTextModel(text_encoder_config)\n        tokenizer = CLIPTokenizer.from_pretrained(\"hf-internal-testing/tiny-random-clip\")\n\n        backbone_config = {\n            \"global_padding\": \"same\",\n            \"layer_type\": \"bottleneck\",\n            \"depths\": [3, 4, 9],\n            \"out_features\": [\"stage1\", \"stage2\", \"stage3\"],\n            \"embedding_dynamic_padding\": True,\n            \"hidden_sizes\": [96, 192, 384, 768],\n            \"num_groups\": 2,\n        }\n        depth_estimator_config = DPTConfig(\n            image_size=32,\n            patch_size=16,\n            num_channels=3,\n            hidden_size=32,\n            num_hidden_layers=4,\n            backbone_out_indices=(0, 1, 2, 3),", "metadata": {"task_id": "huggingface_diffusers/23", "ground_truth": "            num_attention_heads=4,", "fpath_tuple": ["huggingface_diffusers", "tests", "pipelines", "stable_diffusion_2", "test_stable_diffusion_depth.py"], "context_start_lineno": 0, "line_no": 111}}
{"prompt": "from typing import Union, List", "metadata": {"task_id": "opendilab_ACE/76", "ground_truth": "import torch", "fpath_tuple": ["opendilab_ACE", "ding", "model", "template", "qmix.py"], "context_start_lineno": 0, "line_no": 1}}
{"prompt": " test_tensor_ops():  # pylint: disable=(too-many-statements)\n    \"\"\"Test tensor operations\"\"\"\n\n    a = torch.ones((4, 6, 3))\n    b = torch.ones((6, 2))\n    d = {\"t1\": torch.ones((4, 6, 3)), \"t2\": {\"t3\": torch.ones((6, 4))}}\n    tensor_dataclass = DummyTensorDataclass(a=a, b=b, d=d)\n\n    assert tensor_dataclass.shape == (4, 6)\n    assert tensor_dataclass.a.shape == (4, 6, 3)\n    assert tensor_dataclass.b.shape == (4, 6, 2)\n    assert tensor_dataclass.d[\"t1\"].shape == (4, 6, 3)\n    assert tensor_dataclass.d[\"t2\"][\"t3\"].shape == (4, 6, 4)\n    assert tensor_dataclass.size == 24\n    assert tensor_dataclass.ndim == 2\n    assert len(tensor_dataclass) == 4\n\n    reshaped = tensor_dataclass.reshape((2, 12))\n    assert reshaped.shape == (2, 12)\n    assert reshaped.a.shape == (2, 12, 3)\n    assert reshaped.b.shape == (2, 12, 2)\n    assert reshaped.d[\"t1\"].shape == (2, 12, 3)\n    assert reshaped.d[\"t2\"][\"t3\"].shape == (2, 12, 4)\n\n    flattened = tensor_dataclass.flatten()\n    assert flattened.shape == (24,)\n    assert flattened.a.shape == (24, 3)\n    assert flattened.b.shape == (24, 2)\n    assert flattened.d[\"t1\"].shape == (24, 3)\n    assert flattened.d[\"t2\"][\"t3\"].shape == (24, 4)\n    assert flattened[0:4].shape == (4,)\n\n    # Test indexing operations\n    assert tensor_dataclass[:, 1].shape == (4,)\n    assert tensor_dataclass[:, 1].a.shape == (4, 3)\n    assert tensor_dataclass[:, 1].d[\"t1\"].shape == (4, 3)\n    assert tensor_dataclass[:, 1].d[\"t2\"][\"t3\"].shape == (4, 4)\n    assert tensor_dataclass[:, 0:2].shape == (4, 2)\n    assert tensor_dataclass[:, 0:2].a.shape == (4, 2, 3)\n    assert tensor_dataclass[:, 0:2].d[\"t1\"].shape == (4, 2, 3)\n    assert tensor_dataclass[:, 0:2].d[\"t2\"][\"t3\"].shape == (4, 2, 4)\n    assert tensor_dataclass[..., 1].shape == (4,)\n    assert tensor_dataclass[..., 1].a.shape == (4, 3)\n    assert tensor_dataclass[0].shape == (6,)\n    assert tensor_dataclass[0].a.shape == (6, 3)\n    assert tensor_dataclass[0].d[\"t1\"].shape == (6, 3)\n    assert tensor_dataclass[0].d[\"t2\"][\"t3\"].shape == (6, 4)\n    assert tensor_dataclass[0,...].shape == (6,)\n    assert tensor_dataclass[0,...].a.shape == (6, 3)\n\n    tensor_dataclass = DummyTensorDataclass(\n        a=torch.ones((2, 3, 4, 5)),\n        b=torch.ones((4, 5)),\n        d={\"t1\": torch.ones((2, 3, 4, 5)), \"t2\": {\"t3\": torch.ones((4, 5))}},\n    )\n    assert tensor_dataclass[0,...].shape == (3, 4)\n    assert tensor_dataclass[0,...].a.shape == (3, 4, 5)\n    assert tensor_dataclass[0,...].d[\"t1\"].shape == (3, 4, 5)\n    assert tensor_dataclass[0,...].d[\"t2\"][\"t3\"].shape == (3, 4, 5)\n    assert tensor_dataclass[0,..., 0].shape == (3,)\n    assert tensor_dataclass[0,..., 0].a.shape == (3, 5)\n    assert tensor_dataclass[0,..., 0].d[\"t1\"].shape == (3, 5)\n    assert tensor_dataclass[0,..., 0].d[\"t2\"][\"t3\"].shape == (3, 5)\n    assert tensor_dataclass[..., 0].shape == (2, 3)\n    assert tensor_dataclass[..., 0].a.shape == (2, 3, 5)\n    assert tensor_dataclass[..., 0].d[\"t1\"].shape == (2, 3, 5)\n    assert tensor_dataclass[..., 0].d[\"t2\"][\"t3\"].shape == (2, 3, 5)\n\n    mask = torch.rand(size=(2, 3)) > 0.5\n    assert tensor_dataclass[mask].ndim == 2\n\n\ndef test_nested_class():\n    \"\"\"Test nested TensorDataclasses\"\"\"\n\n    a = torch.ones((4, 6, 3))\n    b = torch.ones((6, 2))\n    c = DummyNestedClass(x=torch.ones(6, 5))\n    tensor_dataclass = DummyTensorDataclass(a=a, b=b, c=c)\n\n    assert tensor_dataclass.shape == (4, 6)\n    assert tensor_dataclass.a.shape == (4, 6, 3)\n    assert tensor_dataclass.b.shape == (4, 6, 2)\n    assert tensor_dataclass.c.shape == (4, 6)\n    assert tensor_dataclass.c.x.shape == (4, 6, 5)\n    assert tensor_dataclass.size == 24\n    assert tensor_dataclass.c.size == 24\n\n    reshaped = tensor_dataclass.reshape((2, 12))\n    assert reshaped.shape == (2, 12)\n    assert reshaped.c.shape == (2, 12)\n    assert reshaped.c.x.shape == (2, 12, 5)\n\n    flattened = tensor_dataclass.flatten()\n    assert flattened.c.shape == (24,)\n    assert flattened.c.x.shape == (24, 5)\n\n    # Test indexing operations\n    assert tensor_dataclass[:, 1].c.shape == (4,)\n    assert tensor_dataclass[:, 1].c.x.shape == (4, 5)\n\n    mask = torch.rand(size=(4,)) > 0.5\n    assert tensor_dataclass[mask].c.ndim == 2\n\n\ndef test_iter():\n    \"\"\"Test iterating over tensor dataclass\"\"\"\n    tensor_dataclass = DummyTensorDataclass(a=torch.ones((3, 4, 5)), b=torch.ones((3, 4, 5)))\n    for batch in tensor_dataclass:\n        assert batch.shape == (4,)\n        assert batch.a.shape == (4, 5)\n        assert batch.b.shape == (4, 5)\n\n\nif __name__ == \"__main__\":\n    test_init()\n    test_broadcasting()", "metadata": {"task_id": "nerfstudio-project_nerfstudio/34", "ground_truth": "    test_tensor_ops()", "fpath_tuple": ["nerfstudio-project_nerfstudio", "tests", "utils", "test_tensor_dataclass.py"], "context_start_lineno": 63, "line_no": 180}}
{"prompt": "())\n\n\nclass _dispatch_caller_serial:\n    def __init__(self, list_callable: List[Callable, Any]):\n        self.list_callable = list_callable\n\n    def __call__(self, *args, **kwargs):\n        return [_callable(*args, **kwargs) for _callable in self.list_callable]\n\n\nclass _BatchedEnv(EnvBase):\n    \"\"\"Batched environments allow the user to query an arbitrary method / attribute of the environment running remotely.\n\n    Those queries will return a list of length equal to the number of workers containing the\n    values resulting from those queries.\n        >>> env = ParallelEnv(3, my_env_fun)\n        >>> custom_attribute_list = env.custom_attribute\n        >>> custom_method_list = env.custom_method(*args)\n\n    Args:\n        num_workers: number of workers (i.e. env instances) to be deployed simultaneously;\n        create_env_fn (callable or list of callables): function (or list of functions) to be used for the environment\n            creation.\n            If a single task is used, a callable should be used and not a list of identical callables:\n            if a list of callable is provided, the environment will be executed as if multiple, diverse tasks were\n            needed, which comes with a slight compute overhead;\n        create_env_kwargs (dict or list of dicts, optional): kwargs to be used with the environments being created;\n        env_input_keys (list of str, optional): list of keys that are to be considered policy-output. If the policy has it,\n            the attribute policy.out_keys can be used.\n            Providing the env_input_keys permit to select which keys to update after the policy is called, which can\n            drastically decrease the IO burden when the tensordict is placed in shared memory / memory map.\n            env_input_keys will typically contain \"action\" and if this list is not provided this object\n            will look for corresponding keys. When working with stateless models, it is important to include the\n            state to be read by the environment. If none is provided, _BatchedEnv will use the :obj:`EnvBase.input_spec`\n            keys as indicators of the keys to be sent to the env.\n        pin_memory (bool): if True and device is \"cpu\", calls :obj:`pin_memory` on the tensordicts when created.\n        selected_keys (list of str, optional): keys that have to be returned by the environment.\n            When creating a batch of environment, it might be the case that only some of the keys are to be returned.\n            For instance, if the environment returns 'next_pixels' and 'next_vector', the user might only\n            be interested in, say,'vector'. By indicating which keys must be returned in the tensordict,\n            one can easily control the amount of data occupied in memory (for instance to limit the memory size of a\n            replay buffer) and/or limit the amount of data passed from one process to the other;\n        excluded_keys (list of str, optional): list of keys to be excluded from the returned tensordicts.\n            See selected_keys for more details;\n        share_individual_td (bool, optional): if True, a different tensordict is created for every process/worker and a lazy\n            stack is returned.\n            default = None (False if single task);\n        shared_memory (bool): whether or not the returned tensordict will be placed in shared memory;\n        memmap (bool): whether or not the returned tensordict will be placed in memory map.\n        policy_proof (callable, optional): if provided, it'll be used to get the list of\n            tensors to return through the :obj:`step()` and :obj:`reset()` methods, such as :obj:`\"hidden\"` etc.\n        device (str, int, torch.device): for consistency, this argument is kept. However this\n            argument should not be passed, as the device will be inferred from the environments.\n            It is assumed that all environments will run on the same device as a common shared\n            tensordict will be used to pass data from process to process. The device can be\n            changed after instantiation using :obj:`env.to(device)`.\n        allow_step_when_done (bool, optional): if True, batched environments can\n            execute steps after a done state is encountered.\n            Defaults to :obj:`False`.\n\n    \"\"\"\n\n    _verbose: bool = False\n    _excluded_wrapped_keys = [\n        \"is_closed\",\n        \"parent_channels\",\n        \"batch_size\",\n        \"_dummy_env_str\",\n    ]\n\n    def __init__(\n        self,\n        num_workers: int,\n        create_env_fn: Union[Callable[[], EnvBase], Sequence[Callable[[], EnvBase]]],\n        create_env_kwargs: Union[dict, Sequence[dict]] = None,\n        env_input_keys: Optional[Sequence[str]] = None,\n        pin_memory: bool = False,\n        selected_keys: Optional[Sequence[str]] = None,\n        excluded_keys: Optional[Sequence[str]] = None,\n        share_individual_td: Optional[bool] = None,\n        shared_memory: bool = True,\n        memmap: bool = False,\n        policy_proof: Optional[Callable] = None,\n        device: Optional[DEVICE_TYPING] = None,\n        allow_step_when_done: bool = False,\n    ):\n        if device is not None:\n            raise ValueError(\n                \"Device setting for batched environment can't be done at initialization. \"\n                \"The device will be inferred from the constructed environment. \"\n                \"It can be set through the `to(device)` method.\"\n            )\n\n        super().__init__(device=None)\n        self.is_closed = True\n\n        self._single_task = callable(create_env_fn) or (len(set(create_env_fn)) == 1)\n        if callable(create_env_fn):\n            create_env_fn = [create_env_fn for _ in range(num_workers)]\n        else:\n            if len(create_env_fn)!= num_workers:\n                raise RuntimeError(\n                    f\"num_workers and len(create_env_fn) mismatch, \"\n                    f\"got {len(create_env_fn)} and {num_workers}\"\n                )\n            if (\n                share_individual_td is False and not self._single_task\n            ):  # then it has been explicitly set by the user\n                raise ValueError(\n                    \"share_individual_td must be set to None or True when using multi-task batched environments\"\n                )\n            share_individual_td = True\n        create_env_kwargs = {} if create_env_kwargs is None else create_env_kwargs\n        if isinstance(create_env_kwargs, dict):\n            create_env_kwargs = [\n                deepcopy(create_env_kwargs) for _ in range(num_workers)\n            ]\n\n        self.policy_proof = policy_proof\n        self.num_workers = num_workers\n        self.create_env_fn = create_env_fn\n        self.create_env_kwargs = create_env_kwargs\n        self.env_input_keys = env_input_keys\n        self.pin_memory = pin_memory\n        self.selected_keys = selected_keys\n        self.excluded_keys = excluded_keys\n        self.share_individual_td = bool(share_individual_td)\n        self._share_memory = shared_memory\n        self._memmap = memmap\n        self.allow_step_when_done = allow_step_when_done\n        if self._share_memory and self._memmap:\n            raise RuntimeError(\n                \"memmap and shared memory are mutually exclusive features.\"\n            )\n        self._batch_size = None\n        self.__dict__[\"_observation_spec\"] = None", "metadata": {"task_id": "pytorch_rl/196", "ground_truth": "        self.__dict__[\"_input_spec\"] = None", "fpath_tuple": ["pytorch_rl", "torchrl", "envs", "vec_env.py"], "context_start_lineno": 66, "line_no": 203}}
{"prompt": " range(len(dataset)):\n        data = dataset[i]\n\n        if label_type == 'dirty':\n            # all2one attack\n            if mode == MODE.TRAIN:\n                img = np.array(data[0]).transpose(1, 2, 0) * 255.0\n                img = np.clip(img.astype('uint8'), 0, 255)\n                height = img.shape[0]\n                width = img.shape[1]\n\n                if i in perm:\n                    img = selectTrigger(img, height, width, distance, trig_h,\n                                        trig_w, trigger_type, load_path)\n\n                    dataset_.append((img, target_label))\n\n                elif 'wanet' in trigger_type and i in perm_cross:\n                    img = selectTrigger(img, width, height, distance, trig_w,\n                                        trig_h, 'wanetTriggerCross', load_path)\n                    dataset_.append((img, data[1]))\n\n                else:\n                    dataset_.append((img, data[1]))\n\n            if mode == MODE.TEST or mode == MODE.VAL:\n                if data[1] == target_label:\n                    continue\n\n                img = np.array(data[0]).transpose(1, 2, 0) * 255.0\n                img = np.clip(img.astype('uint8'), 0, 255)\n                height = img.shape[0]\n                width = img.shape[1]\n                if i in perm:\n                    img = selectTrigger(img, width, height, distance, trig_w,\n                                        trig_h, trigger_type, load_path)\n                    dataset_.append((img, target_label))\n                else:\n                    dataset_.append((img, data[1]))\n\n        elif label_type == 'clean_label':\n            pass\n\n    return dataset_\n\n\ndef load_poisoned_dataset_pixel(data, ctx, mode):\n\n    trigger_type = ctx.attack.trigger_type\n    label_type = ctx.attack.label_type\n    target_label = int(ctx.attack.target_label_ind)\n    transforms_funcs = get_transform(ctx, 'torchvision')[0]['transform']\n\n    if \"femnist\" in ctx.data.type or \"CIFAR10\" in ctx.data.type:\n        inject_portion_train = ctx.attack.poison_ratio\n    else:\n        raise RuntimeError(\n            'Now, we only support the FEMNIST and CIFAR-10 datasets')\n\n    inject_portion_test = 1.0\n\n    load_path = ctx.attack.trigger_path\n\n    if mode == MODE.TRAIN:\n        poisoned_dataset = addTrigger(data[mode].dataset,\n                                      target_label,\n                                      inject_portion_train,\n                                      mode=mode,\n                                      distance=1,\n                                      trig_h=0.1,\n                                      trig_w=0.1,\n                                      trigger_type=trigger_type,\n                                      label_type=label_type,\n                                      load_path=load_path)\n        num_dps_poisoned_dataset = len(poisoned_dataset)\n        for iii in range(num_dps_poisoned_dataset):\n            sample, label = poisoned_dataset[iii]\n            poisoned_dataset[iii] = (transforms_funcs(sample), label)\n\n        data[mode] = DataLoader(poisoned_dataset,\n                                batch_size=ctx.dataloader.batch_size,\n                                shuffle=True,\n                                num_workers=ctx.dataloader.num_workers)\n\n    if mode == MODE.TEST or mode == MODE.VAL:\n        poisoned_dataset = addTrigger(data[mode].dataset,\n                                      target_label,\n                                      inject_portion_test,\n                                      mode=mode,\n                                      distance=1,\n                                      trig_h=0.1,\n                                      trig_w=0.1,\n                                      trigger_type=trigger_type,\n                                      label_type=label_type,\n                                      load_path=load_path)\n        num_dps_poisoned_dataset = len(poisoned_dataset)\n        for iii in range(num_dps_poisoned_dataset):\n            sample, label = poisoned_dataset[iii]\n            # (channel, height, width) = sample.shape #(c,h,w)\n            poisoned_dataset[iii] = (transforms_funcs(sample), label)\n\n        data['poison_' + mode] = DataLoader(\n            poisoned_dataset,\n            batch_size=ctx.dataloader.batch_size,\n            shuffle=False,\n            num_workers=ctx.dataloader.num_workers)\n\n    return data\n\n\ndef add_trans_normalize(data, ctx):\n    '''\n    data for each client is a dictionary.\n    '''\n\n    for key in data:\n        num_dataset = len(data[key].dataset)\n        mean, std = ctx.attack.mean, ctx.attack.std\n        if \"CIFAR10\" in ctx.data.type and key == MODE.TRAIN:\n            transforms_list = []\n            transforms_list.append(transforms.RandomHorizontalFlip())\n            transforms_list.append(transforms.ToTensor())\n            tran_train = transforms.Compose(transforms_list)\n            for iii in range(num_dataset):\n                sample = np.array(data[key].dataset[iii][0]).transpose(\n                    1, 2, 0) * 255.0\n                sample = np.clip(sample.astype('uint8'), 0, 255)\n                sample = Image.fromarray(sample)\n                sample = tran_train(sample)\n                data[key].dataset[iii] = (normalize(sample, mean, std),\n                                          data[key].dataset[iii][1])\n        else:\n            for iii in range(num_dataset):\n                data[key].dataset[iii] = (normalize(data[key].dataset[iii][0],\n                                                    mean, std),\n                                          data[key].dataset[iii][1])\n\n    return data\n\n\ndef select_poisoning(data, ctx, mode):\n\n    if 'edge' in ctx.attack.trigger_type:\n        data = load_poisoned_dataset_edgeset(data, ctx, mode)\n    elif'semantic' in ctx.attack.trigger_type:\n        pass\n    else:\n        data = load_poisoned_dataset_pixel(data, ctx, mode)\n    return data\n\n\ndef poisoning(data, ctx):\n    for i in range(1, len(data) + 1):\n        if i == ctx.attack.attacker_id:\n            logger.info(50 * '-')\n            logger.info('start poisoning at Client: {}'.format(i))\n            logger.info(50 * '-')\n            data[i] = select_poisoning(data[i], ctx, mode=MODE.TRAIN)\n        data[i] = select_poisoning(data[i], ctx, mode=MODE.TEST)\n        if data[i].get(MODE.VAL):\n            data[i] = select_poisoning(data[i], ctx, mode=MODE.VAL)\n        data[i] = add_trans_normalize(data[i], ctx)\n        logger.info('finishing the clean and {} poisoning data processing \\", "metadata": {"task_id": "alibaba_FederatedScope/99", "ground_truth": "                for Client {:d}'.format(ctx.attack.trigger_type, i))", "fpath_tuple": ["alibaba_FederatedScope", "federatedscope", "attack", "auxiliary", "poisoning_data.py"], "context_start_lineno": 131, "line_no": 294}}
{"prompt": "import time\nimport torch\nfrom hpc_rll.origin.td import q_nstep_td_error_with_rescale, q_nstep_td_data\nfrom hpc_rll.rl_utils.td import QNStepTDRescale\nfrom testbase import mean_relative_error, times\n\nassert torch.cuda.is_available()\nuse_cuda = True\n\nT = 1024\nB = 64\nN = 64\ngamma = 0.95\n\n\ndef qntd_rescale_val():\n    ori_q = torch.randn(B, N)\n    ori_next_n_q = torch.randn(B, N)\n    ori_action = torch.randint(0, N, size=(B, ))\n    ori_next_n_action = torch.randint(0, N, size=(B, ))\n    ori_reward = torch.randn(T, B)\n    ori_done = torch.randn(B)\n    ori_weight = torch.randn(B)\n\n    hpc_q = ori_q.clone().detach()\n    hpc_next_n_q = ori_next_n_q.clone().detach()\n    hpc_action = ori_action.clone().detach()\n    hpc_next_n_action = ori_next_n_action.clone().detach()\n    hpc_reward = ori_reward.clone().detach()\n    hpc_done = ori_done.clone().detach()\n    hpc_weight = ori_weight.clone().detach()\n    hpc_qntd_rescale = QNStepTDRescale(T, B, N)\n\n    if use_cuda:\n        ori_q = ori_q.cuda()\n        ori_next_n_q = ori_next_n_q.cuda()\n        ori_action = ori_action.cuda()\n        ori_next_n_action = ori_next_n_action.cuda()\n        ori_reward = ori_reward.cuda()\n        ori_done = ori_done.cuda()\n        ori_weight = ori_weight.cuda()\n\n        hpc_q = hpc_q.cuda()\n        hpc_next_n_q = hpc_next_n_q.cuda()\n        hpc_action = hpc_action.cuda()\n        hpc_next_n_action = hpc_next_n_action.cuda()\n        hpc_reward = hpc_reward.cuda()\n        hpc_done = hpc_done.cuda()\n        hpc_weight = hpc_weight.cuda()", "metadata": {"task_id": "opendilab_ACE/137", "ground_truth": "        hpc_qntd_rescale = hpc_qntd_rescale.cuda()", "fpath_tuple": ["opendilab_ACE", "ding", "hpc_rl", "tests", "test_qntd_rescale.py"], "context_start_lineno": 0, "line_no": 49}}
{"prompt": "import pickle\nimport zlib\n\nimport lz4.block\n\n\ndef dummy_compressor(data):\n    r\"\"\"\n    Overview:\n        Return input data.\n    \"\"\"\n    return data\n\n\ndef zlib_data_compressor(data):\n    r\"\"\"\n    Overview:\n        Takes the input compressed data and return the compressed original data (zlib compressor) in binary format.\n    Examples:\n        >>> zlib_data_compressor(\"Hello\")\n        b'x\\x9ck`\\x99\\xca\\xc9\\x00\\x01=\\xac\\x1e\\xa999\\xf9S\\xf4\\x00%L\\x04j'\n    \"\"\"\n    return zlib.compress(pickle.dumps(data))\n\n\ndef lz4_data_compressor(data):\n    r\"\"\"\n    Overview:\n        Return the compressed original data (lz4 compressor).The compressor outputs in binary format.\n    Examples:\n        >>> lz4.block.compress(pickle.dumps(\"Hello\"))\n        b'\\x14\\x00\\x00\\x00R\\x80\\x04\\x95\\t\\x00\\x01\\x00\\x90\\x8c\\x05Hello\\x94.'\n    \"\"\"\n    return lz4.block.compress(pickle.dumps(data))\n\n\n_COMPRESSORS_MAP = {\n    'lz4': lz4_data_compressor,\n    'zlib': zlib_data_compressor,\n    'none': dummy_compressor,\n}\n\n\ndef get_data_compressor(name: str):\n    r\"\"\"\n    Overview:\n        Get the data compressor according to the input name\n    Arguments:\n        - name(:obj:`str`): Name of the compressor, support ``['lz4', 'zlib', 'none']``\n    Return:\n        - (:obj:`Callable`): Corresponding data_compressor, taking input data returning compressed data.\n    Example:\n        >>> compress_fn = get_data_compressor('lz4')\n        >>> compressed_data = compressed(input_data)\n    \"\"\"\n    return _COMPRESSORS_MAP[name]\n\n\ndef dummy_decompressor(data):\n    \"\"\"\n    Overview:\n        Return input data.\n    \"\"\"\n    return data\n\n\ndef lz4_data_decompressor(compressed_data):\n    r\"\"\"\n    Overview:", "metadata": {"task_id": "opendilab_ACE/174", "ground_truth": "        Return the decompressed original data (lz4 compressor).", "fpath_tuple": ["opendilab_ACE", "ding", "utils", "compression_helper.py"], "context_start_lineno": 0, "line_no": 69}}
{"prompt": " any learning component such as parameter updates that may be required for a complete distributed reinforcement learning algorithm implementation.\nIn this model, >= 1 data collectors workers are responsible for collecting experiences in an environment, the replay buffer worker receives all of these experiences and exposes them to a trainer that is responsible for making parameter updates to any required models.\n\"\"\"\n\nimport argparse\nimport os\nimport random\nimport sys\nimport time\n\nimport torch\nimport torch.distributed.rpc as rpc\nfrom tensordict import TensorDict\nfrom torchrl.data.replay_buffers import RemoteTensorDictReplayBuffer\nfrom torchrl.data.replay_buffers.samplers import RandomSampler\nfrom torchrl.data.replay_buffers.storages import LazyMemmapStorage\nfrom torchrl.data.replay_buffers.utils import accept_remote_rref_invocation\nfrom torchrl.data.replay_buffers.writers import RoundRobinWriter\n\nRETRY_LIMIT = 2\nRETRY_DELAY_SECS = 3\nREPLAY_BUFFER_NODE = \"ReplayBuffer\"\nTRAINER_NODE = \"Trainer\"\n\nparser = argparse.ArgumentParser(\n    description=\"RPC Replay Buffer Example\",\n    formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n)\n\nparser.add_argument(\n    \"--rank\",\n    type=int,\n    default=-1,\n    help=\"Node Rank [0 = Replay Buffer, 1 = Dummy Trainer, 2+ = Dummy Data Collector]\",\n)\n\n\nclass DummyDataCollectorNode:\n    \"\"\"Data collector node responsible for collecting experiences used for learning.\n\n    Args:\n        replay_buffer (rpc.RRef): the RRef associated with the construction of the replay buffer\n    \"\"\"\n\n    def __init__(self, replay_buffer: rpc.RRef) -> None:\n        self.id = rpc.get_worker_info().id\n        self.replay_buffer = replay_buffer\n        print(\"Data Collector Node constructed\")\n\n    def _submit_random_item_async(self) -> rpc.RRef:\n        td = TensorDict({\"a\": torch.randint(100, (1,))}, [])\n        return rpc.remote(\n            self.replay_buffer.owner(),\n            ReplayBufferNode.add,\n            args=(\n                self.replay_buffer,\n                td,\n            ),\n        )\n\n    @accept_remote_rref_invocation\n    def collect(self):\n        \"\"\"Method that begins experience collection (we just generate random TensorDicts in this example). `accept_remote_rref_invocation` enables this method to be invoked remotely provided the class instantiation `rpc.RRef` is provided in place of the object reference.\"\"\"\n        for elem in range(50):\n            time.sleep(random.randint(1, 4))\n            print(\n                f\"Collector [{self.id}] submission {elem}: {self._submit_random_item_async().to_here()}\"\n            )\n\n\nclass DummyTrainerNode:\n    \"\"\"Trainer node responsible for learning from experiences sampled from an experience replay buffer.\"\"\"\n\n    def __init__(self) -> None:\n        print(\"DummyTrainerNode\")\n        self.id = rpc.get_worker_info().id\n        self.replay_buffer = self._create_replay_buffer()\n        self._create_and_launch_data_collectors()\n\n    def train(self, iterations: int) -> None:\n        for iteration in range(iterations):\n            print(f\"[{self.id}] Training Iteration: {iteration}\")\n            time.sleep(3)\n            batch = rpc.rpc_sync(\n                self.replay_buffer.owner(),\n                ReplayBufferNode.sample,\n                args=(self.replay_buffer, 16),\n            )\n            print(f\"[{self.id}] Sample Obtained Iteration: {iteration}\")\n            print(f\"{batch}\")\n\n    def _create_replay_buffer(self) -> rpc.RRef:\n        while True:\n            try:\n                replay_buffer_info = rpc.get_worker_info(REPLAY_BUFFER_NODE)\n                buffer_rref = rpc.remote(\n                    replay_buffer_info, ReplayBufferNode, args=(10000,)\n                )\n                print(f\"Connected to replay buffer {replay_buffer_info}\")\n                return buffer_rref\n            except Exception as e:\n                print(f\"Failed to connect to replay buffer: {e}\")\n                time.sleep(RETRY_DELAY_SECS)\n\n    def _create_and_launch_data_collectors(self) -> None:\n        data_collector_number = 2\n        retries = 0\n        data_collectors = []\n        data_collector_infos = []\n        # discover launched data collector nodes (with retry to allow collectors to dynamically join)\n        while True:\n            try:\n                data_collector_info = rpc.get_worker_info(\n                    f\"DataCollector{data_collector_number}\"\n                )\n                print(f\"Data collector info: {data_collector_info}\")\n                dc_ref = rpc.remote(\n                    data_collector_info,\n                    DummyDataCollectorNode,\n                    args=(self.replay_buffer,),\n                )\n                data_collectors.append(dc_ref)\n                data_collector_infos.append(data_collector_info)\n                data_collector_number += 1\n                retries = 0\n            except Exception:\n                retries += 1\n                print(\n                    f\"Failed to connect to DataCollector{data_collector_number} with {retries} retries\"\n                )\n                if retries >= RETRY_LIMIT:\n                    print(f\"{len(data_collectors)} data collectors\")\n                    for data_collector_info, data_collector in zip(\n                        data_collector_infos, data_collectors\n                    ):\n                        rpc.remote(\n                            data_collector_info,\n                            DummyDataCollectorNode.collect,\n                            args=(data_collector,),\n                        )\n                    break\n                else:\n                    time.sleep(RETRY_DELAY_SECS)\n\n\nclass ReplayBufferNode(RemoteTensorDictReplayBuffer):\n    \"\"\"Experience replay buffer node that is capable of accepting remote connections. Being a `RemoteTensorDictReplayBuffer` means all of it's public methods are remotely invokable using `torch.rpc`.\n    Using a LazyMemmapStorage is highly advised in distributed settings with shared storage due to the lower serialisation cost of MemmapTensors as well as the ability to specify file storage locations which can improve ability to recover from node failures.\n\n    Args:\n        capacity (int): the maximum number of elements that can be stored in the replay buffer.\n    \"\"\"\n\n    def __init__(self, capacity: int):\n        super().__init__(\n            storage=LazyMemmapStorage(\n                max_size=capacity, scratch_dir=\"/tmp/\", device=torch.device(\"cpu\")\n            ),\n            sampler=RandomSampler(),\n            writer=RoundRobinWriter(),\n            collate_fn=lambda x: x,\n        )\n\n\nif __name__ == \"__main__\":\n    args = parser.parse_args()\n    rank = args.rank\n    print(f\"Rank: {rank}\")\n\n    os.environ[\"MASTER_ADDR\"] = \"localhost\"\n    os.environ[\"MASTER_PORT\"] = \"29500\"\n    os.environ[\"TORCH_DISTRIBUTED_DEBUG\"] = \"DETAIL\"\n    str_init_method = \"tcp://localhost:10000\"\n    options = rpc.TensorPipeRpcBackendOptions(\n        num_worker_threads=16, init_method=str_init_method\n    )", "metadata": {"task_id": "pytorch_rl/112", "ground_truth": "    if rank == 0:", "fpath_tuple": ["pytorch_rl", "examples", "distributed", "distributed_replay_buffer.py"], "context_start_lineno": 5, "line_no": 181}}
