{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# torchrl/modules/distributions/__init__.py\n# --------------------------------------------------\n# # Copyright (c) Meta Platforms, Inc. and affiliates.\n# #\n# # This source code is licensed under the MIT license found in the\n# # LICENSE file in the root directory of this source tree.\n# \n# from .continuous import (\n#     __all__ as _all_continuous,\n#     Delta,\n#     IndependentNormal,\n#     NormalParamWrapper,\n#     TanhDelta,\n#     TanhNormal,\n#     TruncatedNormal,\n# )\n# from .discrete import __all__ as _all_discrete, OneHotCategorical\n# \n# --------------------------------------------------\n# the below code fragment can be found in:\n# torchrl/modules/distributions/__init__.py\n# --------------------------------------------------\n# # Copyright (c) Meta Platforms, Inc. and affiliates.\n# #\n# # This source code is licensed under the MIT license found in the\n# # LICENSE file in the root directory of this source tree.\n# \n# from .continuous import (\n#     __all__ as _all_continuous,\n#     Delta,\n#     IndependentNormal,\n#     NormalParamWrapper,\n#     TanhDelta,\n#     TanhNormal,\n#     TruncatedNormal,\n# )\n# from .discrete import __all__ as _all_discrete, OneHotCategorical\n# \n# distributions_maps = {\n#     distribution_class.lower(): eval(distribution_class)\n#     for distribution_class in _all_continuous + _all_discrete\n# }\n# --------------------------------------------------\n# the below code fragment can be found in:\n# torchrl/modules/distributions/__init__.py\n# --------------------------------------------------\n# # Copyright (c) Meta Platforms, Inc. and affiliates.\n# #\n# # This source code is licensed under the MIT license found in the\n# # LICENSE file in the root directory of this source tree.\n# \n# from .continuous import (\n#     __all__ as _all_continuous,\n#     Delta,\n#     IndependentNormal,\n#     NormalParamWrapper,\n#     TanhDelta,\n#     TanhNormal,\n#     TruncatedNormal,\n# )\n# --------------------------------------------------\n# the below code fragment can be found in:\n# torchrl/modules/distributions/__init__.py\n# --------------------------------------------------\n# # Copyright (c) Meta Platforms, Inc. and affiliates.\n# #\n# # This source code is licensed under the MIT license found in the\n# # LICENSE file in the root directory of this source tree.\n# \n# from .continuous import (\n#     __all__ as _all_continuous,\n#     Delta,\n#     IndependentNormal,\n#     NormalParamWrapper,\n#     TanhDelta,\n#     TanhNormal,\n#     TruncatedNormal,\n# )\n# from .discrete import __all__ as _all_discrete, OneHotCategorical\n# \n# distributions_maps = {\n#     distribution_class.lower(): eval(distribution_class)\n# --------------------------------------------------\n# the below code fragment can be found in:\n# torchrl/modules/distributions/__init__.py\n# --------------------------------------------------\n# # Copyright (c) Meta Platforms, Inc. and affiliates.\n# #\n# # This source code is licensed under the MIT license found in the\n# # LICENSE file in the root directory of this source tree.\n# \n# from .continuous import (\n#     __all__ as _all_continuous,\n#     Delta,\n#     IndependentNormal,\n#     NormalParamWrapper,\n#     TanhDelta,\n#     TanhNormal,\n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\n# Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom .distributions import (\n    Delta,\n    distributions_maps,\n    IndependentNormal,\n    NormalParamWrapper,\n    OneHotCategorical,", "metadata": {"task_id": "pytorch_rl/126", "ground_truth": "    TanhDelta,", "fpath_tuple": ["pytorch_rl", "torchrl", "modules", "__init__.py"], "context_start_lineno": 0, "line_no": 11, "query_window": {"context": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom .distributions import (\n    Delta,\n    distributions_maps,\n    IndependentNormal,\n    NormalParamWrapper,\n    OneHotCategorical,", "metadata": {"fpath_tuple": ["pytorch_rl", "torchrl", "modules", "__init__.py"], "line_no": 11, "task_id": "pytorch_rl/126", "start_line_no": 0, "end_line_no": 11, "window_size": 20, "context_start_lineno": 0, "repo": "pytorch_rl"}}, "top_k_context": [{"context": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom .continuous import (\n    __all__ as _all_continuous,\n    Delta,\n    IndependentNormal,\n    NormalParamWrapper,", "metadata": [{"fpath_tuple": ["pytorch_rl", "torchrl", "modules", "distributions", "__init__.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 10, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}], "sim_score": 0.7258064516129032}, {"context": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom .continuous import (\n    __all__ as _all_continuous,\n    Delta,\n    IndependentNormal,\n    NormalParamWrapper,\n    TanhDelta,\n    TanhNormal,\n    TruncatedNormal,\n)\nfrom .discrete import __all__ as _all_discrete, OneHotCategorical\n", "metadata": [{"fpath_tuple": ["pytorch_rl", "torchrl", "modules", "distributions", "__init__.py"], "line_no": 6, "start_line_no": 0, "end_line_no": 16, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}], "sim_score": 0.7142857142857143}, {"context": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom .continuous import (\n    __all__ as _all_continuous,\n    Delta,\n    IndependentNormal,\n    NormalParamWrapper,\n    TanhDelta,\n    TanhNormal,", "metadata": [{"fpath_tuple": ["pytorch_rl", "torchrl", "modules", "distributions", "__init__.py"], "line_no": 2, "start_line_no": 0, "end_line_no": 12, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}], "sim_score": 0.6923076923076923}, {"context": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom .continuous import (\n    __all__ as _all_continuous,\n    Delta,\n    IndependentNormal,\n    NormalParamWrapper,\n    TanhDelta,\n    TanhNormal,\n    TruncatedNormal,\n)\nfrom .discrete import __all__ as _all_discrete, OneHotCategorical\n\ndistributions_maps = {\n    distribution_class.lower(): eval(distribution_class)", "metadata": [{"fpath_tuple": ["pytorch_rl", "torchrl", "modules", "distributions", "__init__.py"], "line_no": 8, "start_line_no": 0, "end_line_no": 18, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}], "sim_score": 0.6835443037974683}, {"context": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom .continuous import (\n    __all__ as _all_continuous,\n    Delta,\n    IndependentNormal,\n    NormalParamWrapper,\n    TanhDelta,\n    TanhNormal,\n    TruncatedNormal,\n)", "metadata": [{"fpath_tuple": ["pytorch_rl", "torchrl", "modules", "distributions", "__init__.py"], "line_no": 4, "start_line_no": 0, "end_line_no": 14, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}], "sim_score": 0.6617647058823529}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# src/diffusers/schedulers/scheduling_unclip.py\n# --------------------------------------------------\n# \n#         # 5. Compute predicted previous sample \u00b5_t\n#         # See formula (7) from https://arxiv.org/pdf/2006.11239.pdf\n#         pred_prev_sample = pred_original_sample_coeff * pred_original_sample + current_sample_coeff * sample\n# \n#         # 6. Add noise\n#         variance = 0\n#         if t > 0:\n#             variance_noise = randn_tensor(\n#                 model_output.shape, dtype=model_output.dtype, generator=generator, device=model_output.device\n#             )\n# \n#             variance = self._get_variance(\n#                 t,\n#                 predicted_variance=predicted_variance,\n#                 prev_timestep=prev_timestep,\n#             )\n# \n#             if self.variance_type == \"fixed_small_log\":\n#                 variance = variance\n# --------------------------------------------------\n# the below code fragment can be found in:\n# examples/community/bit_diffusion.py\n# --------------------------------------------------\n#     current_sample_coeff = self.alphas[t] ** (0.5) * beta_prod_t_prev / beta_prod_t\n# \n#     # 5. Compute predicted previous sample \u00b5_t\n#     # See formula (7) from https://arxiv.org/pdf/2006.11239.pdf\n#     pred_prev_sample = pred_original_sample_coeff * pred_original_sample + current_sample_coeff * sample\n# \n#     # 6. Add noise\n#     variance = 0\n#     if t > 0:\n#         noise = torch.randn(\n#             model_output.size(), dtype=model_output.dtype, layout=model_output.layout, generator=generator\n#         ).to(model_output.device)\n#         variance = (self._get_variance(t, predicted_variance=predicted_variance) ** 0.5) * noise\n# \n#     pred_prev_sample = pred_prev_sample + variance\n# \n#     if not return_dict:\n#         return (pred_prev_sample,)\n# \n#     return DDPMSchedulerOutput(prev_sample=pred_prev_sample, pred_original_sample=pred_original_sample)\n# --------------------------------------------------\n# the below code fragment can be found in:\n# examples/community/bit_diffusion.py\n# --------------------------------------------------\n#     # See formula (7) from https://arxiv.org/pdf/2006.11239.pdf\n#     pred_original_sample_coeff = (alpha_prod_t_prev ** (0.5) * self.betas[t]) / beta_prod_t\n#     current_sample_coeff = self.alphas[t] ** (0.5) * beta_prod_t_prev / beta_prod_t\n# \n#     # 5. Compute predicted previous sample \u00b5_t\n#     # See formula (7) from https://arxiv.org/pdf/2006.11239.pdf\n#     pred_prev_sample = pred_original_sample_coeff * pred_original_sample + current_sample_coeff * sample\n# \n#     # 6. Add noise\n#     variance = 0\n#     if t > 0:\n#         noise = torch.randn(\n#             model_output.size(), dtype=model_output.dtype, layout=model_output.layout, generator=generator\n#         ).to(model_output.device)\n#         variance = (self._get_variance(t, predicted_variance=predicted_variance) ** 0.5) * noise\n# \n#     pred_prev_sample = pred_prev_sample + variance\n# \n#     if not return_dict:\n#         return (pred_prev_sample,)\n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\n\n\n    def set_timesteps(\n        self, state: DDPMSchedulerState, num_inference_steps: int, shape: Tuple = ()\n    ) -> DDPMSchedulerState:\n        \"\"\"\n        Sets the discrete timesteps used for the diffusion chain. Supporting function to be run before inference.\n\n        Args:\n            state (`DDIMSchedulerState`):\n                the `FlaxDDPMScheduler` state data class instance.\n            num_inference_steps (`int`):\n                the number of diffusion steps used when generating samples with a pre-trained model.\n        \"\"\"\n\n        step_ratio = self.config.num_train_timesteps // num_inference_steps\n        # creates integer timesteps by multiplying by ratio\n        # rounding to avoid issues when num_inference_step is power of 3\n        timesteps = (jnp.arange(0, num_inference_steps) * step_ratio).round()[::-1]\n\n        return state.replace(\n            num_inference_steps=num_inference_steps,\n            timesteps=timesteps,\n        )\n\n    def _get_variance(self, state: DDPMSchedulerState, t, predicted_variance=None, variance_type=None):\n        alpha_prod_t = state.common.alphas_cumprod[t]\n        alpha_prod_t_prev = jnp.where(t > 0, state.common.alphas_cumprod[t - 1], jnp.array(1.0, dtype=self.dtype))\n\n        # For t > 0, compute predicted variance \u03b2t (see formula (6) and (7) from https://arxiv.org/pdf/2006.11239.pdf)\n        # and sample from it to get previous sample\n        # x_{t-1} ~ N(pred_prev_sample, variance) == add variance to pred_sample\n        variance = (1 - alpha_prod_t_prev) / (1 - alpha_prod_t) * state.common.betas[t]\n\n        if variance_type is None:\n            variance_type = self.config.variance_type\n\n        # hacks - were probably added for training stability\n        if variance_type == \"fixed_small\":\n            variance = jnp.clip(variance, a_min=1e-20)\n        # for rl-diffuser https://arxiv.org/abs/2205.09991\n        elif variance_type == \"fixed_small_log\":\n            variance = jnp.log(jnp.clip(variance, a_min=1e-20))\n        elif variance_type == \"fixed_large\":\n            variance = state.common.betas[t]\n        elif variance_type == \"fixed_large_log\":\n            # Glide max_log\n            variance = jnp.log(state.common.betas[t])\n        elif variance_type == \"learned\":\n            return predicted_variance\n        elif variance_type == \"learned_range\":\n            min_log = variance\n            max_log = state.common.betas[t]\n            frac = (predicted_variance + 1) / 2\n            variance = frac * max_log + (1 - frac) * min_log\n\n        return variance\n\n    def step(\n        self,\n        state: DDPMSchedulerState,\n        model_output: jnp.ndarray,\n        timestep: int,\n        sample: jnp.ndarray,\n        key: jax.random.KeyArray,\n        return_dict: bool = True,\n    ) -> Union[FlaxDDPMSchedulerOutput, Tuple]:\n        \"\"\"\n        Predict the sample at the previous timestep by reversing the SDE. Core function to propagate the diffusion\n        process from the learned model outputs (most often the predicted noise).\n\n        Args:\n            state (`DDPMSchedulerState`): the `FlaxDDPMScheduler` state data class instance.\n            model_output (`jnp.ndarray`): direct output from learned diffusion model.\n            timestep (`int`): current discrete timestep in the diffusion chain.\n            sample (`jnp.ndarray`):\n                current instance of sample being created by diffusion process.\n            key (`jax.random.KeyArray`): a PRNG key.\n            return_dict (`bool`): option for returning tuple rather than FlaxDDPMSchedulerOutput class\n\n        Returns:\n            [`FlaxDDPMSchedulerOutput`] or `tuple`: [`FlaxDDPMSchedulerOutput`] if `return_dict` is True, otherwise a\n            `tuple`. When returning a tuple, the first element is the sample tensor.\n\n        \"\"\"\n        t = timestep\n\n        if model_output.shape[1] == sample.shape[1] * 2 and self.config.variance_type in [\"learned\", \"learned_range\"]:\n            model_output, predicted_variance = jnp.split(model_output, sample.shape[1], axis=1)\n        else:\n            predicted_variance = None\n\n        # 1. compute alphas, betas\n        alpha_prod_t = state.common.alphas_cumprod[t]\n        alpha_prod_t_prev = jnp.where(t > 0, state.common.alphas_cumprod[t - 1], jnp.array(1.0, dtype=self.dtype))\n        beta_prod_t = 1 - alpha_prod_t\n        beta_prod_t_prev = 1 - alpha_prod_t_prev\n\n        # 2. compute predicted original sample from predicted noise also called\n        # \"predicted x_0\" of formula (15) from https://arxiv.org/pdf/2006.11239.pdf\n        if self.config.prediction_type == \"epsilon\":\n            pred_original_sample = (sample - beta_prod_t ** (0.5) * model_output) / alpha_prod_t ** (0.5)\n        elif self.config.prediction_type == \"sample\":\n            pred_original_sample = model_output\n        elif self.config.prediction_type == \"v_prediction\":\n            pred_original_sample = (alpha_prod_t**0.5) * sample - (beta_prod_t**0.5) * model_output\n        else:\n            raise ValueError(\n                f\"prediction_type given as {self.config.prediction_type} must be one of `epsilon`, `sample` \"\n                \" for the FlaxDDPMScheduler.\"\n            )\n\n        # 3. Clip \"predicted x_0\"\n        if self.config.clip_sample:\n            pred_original_sample = jnp.clip(pred_original_sample, -1, 1)\n\n        # 4. Compute coefficients for pred_original_sample x_0 and current sample x_t\n        # See formula (7) from https://arxiv.org/pdf/2006.11239.pdf\n        pred_original_sample_coeff = (alpha_prod_t_prev ** (0.5) * state.common.betas[t]) / beta_prod_t\n        current_sample_coeff = state.common.alphas[t] ** (0.5) * beta_prod_t_prev / beta_prod_t\n\n        # 5. Compute predicted previous sample \u00b5_t\n        # See formula (7) from https://arxiv.org/pdf/2006.11239.pdf\n        pred_prev_sample = pred_original_sample_coeff * pred_original_sample + current_sample_coeff * sample\n\n        # 6. Add noise\n        def random_variance():\n            split_key = jax.random.split(key, num=1)\n            noise = jax.random.normal(split_key, shape=model_output.shape, dtype=self.dtype)\n            return (self._get_variance(state, t, predicted_variance=predicted_variance) ** 0.5) * noise\n\n        variance = jnp.where(t > 0, random_variance(), jnp.zeros(model_output.shape, dtype=self.dtype))\n\n        pred_prev_sample = pred_prev_sample + variance\n\n        if not return_dict:", "metadata": {"task_id": "huggingface_diffusers/40", "ground_truth": "            return (pred_prev_sample, state)", "fpath_tuple": ["huggingface_diffusers", "src", "diffusers", "schedulers", "scheduling_ddpm_flax.py"], "context_start_lineno": 136, "line_no": 272, "query_window": {"context": "        # 4. Compute coefficients for pred_original_sample x_0 and current sample x_t\n        # See formula (7) from https://arxiv.org/pdf/2006.11239.pdf\n        pred_original_sample_coeff = (alpha_prod_t_prev ** (0.5) * state.common.betas[t]) / beta_prod_t\n        current_sample_coeff = state.common.alphas[t] ** (0.5) * beta_prod_t_prev / beta_prod_t\n\n        # 5. Compute predicted previous sample \u00b5_t\n        # See formula (7) from https://arxiv.org/pdf/2006.11239.pdf\n        pred_prev_sample = pred_original_sample_coeff * pred_original_sample + current_sample_coeff * sample\n\n        # 6. Add noise\n        def random_variance():\n            split_key = jax.random.split(key, num=1)\n            noise = jax.random.normal(split_key, shape=model_output.shape, dtype=self.dtype)\n            return (self._get_variance(state, t, predicted_variance=predicted_variance) ** 0.5) * noise\n\n        variance = jnp.where(t > 0, random_variance(), jnp.zeros(model_output.shape, dtype=self.dtype))\n\n        pred_prev_sample = pred_prev_sample + variance\n\n        if not return_dict:", "metadata": {"fpath_tuple": ["huggingface_diffusers", "src", "diffusers", "schedulers", "scheduling_ddpm_flax.py"], "line_no": 272, "task_id": "huggingface_diffusers/40", "start_line_no": 252, "end_line_no": 272, "window_size": 20, "context_start_lineno": 136, "repo": "huggingface_diffusers"}}, "top_k_context": [{"context": "\n    # 4. Compute coefficients for pred_original_sample x_0 and current sample x_t\n    # See formula (7) from https://arxiv.org/pdf/2006.11239.pdf\n    pred_original_sample_coeff = (alpha_prod_t_prev ** (0.5) * self.betas[t]) / beta_prod_t\n    current_sample_coeff = self.alphas[t] ** (0.5) * beta_prod_t_prev / beta_prod_t\n\n    # 5. Compute predicted previous sample \u00b5_t\n    # See formula (7) from https://arxiv.org/pdf/2006.11239.pdf\n    pred_prev_sample = pred_original_sample_coeff * pred_original_sample + current_sample_coeff * sample\n\n    # 6. Add noise\n    variance = 0\n    if t > 0:\n        noise = torch.randn(\n            model_output.size(), dtype=model_output.dtype, layout=model_output.layout, generator=generator\n        ).to(model_output.device)\n        variance = (self._get_variance(t, predicted_variance=predicted_variance) ** 0.5) * noise\n\n    pred_prev_sample = pred_prev_sample + variance\n", "metadata": [{"fpath_tuple": ["huggingface_diffusers", "examples", "community", "bit_diffusion.py"], "line_no": 196, "start_line_no": 186, "end_line_no": 206, "window_size": 20, "repo": "huggingface_diffusers", "slice_size": 10}], "sim_score": 0.6825396825396826}, {"context": "    # See formula (7) from https://arxiv.org/pdf/2006.11239.pdf\n    pred_original_sample_coeff = (alpha_prod_t_prev ** (0.5) * self.betas[t]) / beta_prod_t\n    current_sample_coeff = self.alphas[t] ** (0.5) * beta_prod_t_prev / beta_prod_t\n\n    # 5. Compute predicted previous sample \u00b5_t\n    # See formula (7) from https://arxiv.org/pdf/2006.11239.pdf\n    pred_prev_sample = pred_original_sample_coeff * pred_original_sample + current_sample_coeff * sample\n\n    # 6. Add noise\n    variance = 0\n    if t > 0:\n        noise = torch.randn(\n            model_output.size(), dtype=model_output.dtype, layout=model_output.layout, generator=generator\n        ).to(model_output.device)\n        variance = (self._get_variance(t, predicted_variance=predicted_variance) ** 0.5) * noise\n\n    pred_prev_sample = pred_prev_sample + variance\n\n    if not return_dict:\n        return (pred_prev_sample,)", "metadata": [{"fpath_tuple": ["huggingface_diffusers", "examples", "community", "bit_diffusion.py"], "line_no": 198, "start_line_no": 188, "end_line_no": 208, "window_size": 20, "repo": "huggingface_diffusers", "slice_size": 10}], "sim_score": 0.6614173228346457}, {"context": "        pred_original_sample_coeff = (alpha_prod_t_prev ** (0.5) * beta) / beta_prod_t\n        current_sample_coeff = alpha ** (0.5) * beta_prod_t_prev / beta_prod_t\n\n        # 5. Compute predicted previous sample \u00b5_t\n        # See formula (7) from https://arxiv.org/pdf/2006.11239.pdf\n        pred_prev_sample = pred_original_sample_coeff * pred_original_sample + current_sample_coeff * sample\n\n        # 6. Add noise\n        variance = 0\n        if t > 0:\n            variance_noise = randn_tensor(\n                model_output.shape, dtype=model_output.dtype, generator=generator, device=model_output.device\n            )\n\n            variance = self._get_variance(\n                t,\n                predicted_variance=predicted_variance,\n                prev_timestep=prev_timestep,\n            )\n", "metadata": [{"fpath_tuple": ["huggingface_diffusers", "src", "diffusers", "schedulers", "scheduling_unclip.py"], "line_no": 274, "start_line_no": 264, "end_line_no": 284, "window_size": 20, "repo": "huggingface_diffusers", "slice_size": 10}], "sim_score": 0.5615384615384615}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# test/test_cost.py\n# test/test_cost.py\n# --------------------------------------------------\n#         reward = torch.randn(batch, T, 1, device=device)\n#         done = torch.zeros(batch, T, 1, dtype=torch.bool, device=device)\n#         mask = ~torch.zeros(batch, T, dtype=torch.bool, device=device)\n#         td = TensorDict(\n#             batch_size=(batch, T),\n#             source={\n#                 \"observation\": obs.masked_fill_(~mask.unsqueeze(-1), 0.0),\n#                 \"next\": {\n#                     \"observation\": next_obs.masked_fill_(~mask.unsqueeze(-1), 0.0)\n#                 },\n#                 \"done\": done,\n#                 \"collector\": {\"mask\": mask},\n#                 \"reward\": reward.masked_fill_(~mask.unsqueeze(-1), 0.0),\n#                 \"action\": action.masked_fill_(~mask.unsqueeze(-1), 0.0),\n#             },\n#             device=device,\n#         )\n#         return td\n# \n#     @pytest.mark.skipif(\n# --------------------------------------------------\n# the below code fragment can be found in:\n# test/test_cost.py\n# --------------------------------------------------\n#         mask = ~torch.zeros(batch, T, dtype=torch.bool, device=device)\n#         td = TensorDict(\n#             batch_size=(batch, T),\n#             source={\n#                 \"observation\": obs.masked_fill_(~mask.unsqueeze(-1), 0.0),\n#                 \"next\": {\n#                     \"observation\": next_obs.masked_fill_(~mask.unsqueeze(-1), 0.0)\n#                 },\n#                 \"done\": done,\n#                 \"collector\": {\"mask\": mask},\n#                 \"reward\": reward.masked_fill_(~mask.unsqueeze(-1), 0.0),\n#                 \"action\": action.masked_fill_(~mask.unsqueeze(-1), 0.0),\n#                 \"action_value\": action_value.masked_fill_(~mask.unsqueeze(-1), 0.0),\n#             },\n#         )\n#         return td\n# \n#     @pytest.mark.parametrize(\"delay_value\", (False, True))\n#     @pytest.mark.parametrize(\"device\", get_available_devices())\n#     @pytest.mark.parametrize(\n# --------------------------------------------------\n# the below code fragment can be found in:\n# test/test_cost.py\n# --------------------------------------------------\n#         mask = ~torch.zeros(batch, T, dtype=torch.bool, device=device)\n#         td = TensorDict(\n#             batch_size=(batch, T),\n#             source={\n#                 \"observation\": obs.masked_fill_(~mask.unsqueeze(-1), 0.0),\n#                 \"next\": {\n#                     \"observation\": next_obs.masked_fill_(~mask.unsqueeze(-1), 0.0)\n#                 },\n#                 \"done\": done,\n#                 \"collector\": {\"mask\": mask},\n#                 \"reward\": reward.masked_fill_(~mask.unsqueeze(-1), 0.0),\n#                 \"action\": action.masked_fill_(~mask.unsqueeze(-1), 0.0),\n#             },\n#             device=device,\n#         )\n#         return td\n# \n#     @pytest.mark.skipif(\n#         not _has_functorch, reason=f\"functorch not installed: {FUNCTORCH_ERR}\"\n#     )\n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\n            np.random.seed(0)\n            loss = loss_fn(td)\n        if n == 0:\n            assert_allclose_td(td, ms_td.select(*list(td.keys())))\n            _loss = sum([item for _, item in loss.items()])\n            _loss_ms = sum([item for _, item in loss_ms.items()])\n            assert (\n                abs(_loss - _loss_ms) < 1e-3\n            ), f\"found abs(loss-loss_ms) = {abs(loss - loss_ms):4.5f} for n=0\"\n        else:\n            with pytest.raises(AssertionError):\n                assert_allclose_td(loss, loss_ms)\n\n        sum([item for _, item in loss_ms.items()]).backward()\n        named_parameters = loss_fn.named_parameters()\n        for name, p in named_parameters:\n            assert p.grad.norm() > 0.0, f\"parameter {name} has null gradient\"\n\n        # Check param update effect on targets\n        target_actor = loss_fn.target_actor_network_params.clone().values(\n            include_nested=True, leaves_only=True\n        )\n        target_qvalue = loss_fn.target_qvalue_network_params.clone().values(\n            include_nested=True, leaves_only=True\n        )\n        for p in loss_fn.parameters():\n            p.data += torch.randn_like(p)\n        target_actor2 = loss_fn.target_actor_network_params.clone().values(\n            include_nested=True, leaves_only=True\n        )\n        target_qvalue2 = loss_fn.target_qvalue_network_params.clone().values(\n            include_nested=True, leaves_only=True\n        )\n        if loss_fn.delay_actor:\n            assert all((p1 == p2).all() for p1, p2 in zip(target_actor, target_actor2))\n        else:\n            assert not any(\n                (p1 == p2).any() for p1, p2 in zip(target_actor, target_actor2)\n            )\n        if loss_fn.delay_qvalue:\n            assert all(\n                (p1 == p2).all() for p1, p2 in zip(target_qvalue, target_qvalue2)\n            )\n        else:\n            assert not any(\n                (p1 == p2).any() for p1, p2 in zip(target_qvalue, target_qvalue2)\n            )\n\n        # check that policy is updated after parameter update\n        actorp_set = set(actor.parameters())\n        loss_fnp_set = set(loss_fn.parameters())\n        assert len(actorp_set.intersection(loss_fnp_set)) == len(actorp_set)\n        parameters = [p.clone() for p in actor.parameters()]\n        for p in loss_fn.parameters():\n            p.data += torch.randn_like(p)\n        assert all((p1 != p2).all() for p1, p2 in zip(parameters, actor.parameters()))\n\n\n@pytest.mark.parametrize(\"version\", [1, 2])\nclass TestSAC:\n    seed = 0\n\n    def _create_mock_actor(self, batch=2, obs_dim=3, action_dim=4, device=\"cpu\"):\n        # Actor\n        action_spec = BoundedTensorSpec(\n            -torch.ones(action_dim), torch.ones(action_dim), (action_dim,)\n        )\n        net = NormalParamWrapper(nn.Linear(obs_dim, 2 * action_dim))\n        module = SafeModule(net, in_keys=[\"observation\"], out_keys=[\"loc\", \"scale\"])\n        actor = ProbabilisticActor(\n            module=module,\n            in_keys=[\"loc\", \"scale\"],\n            spec=action_spec,\n            distribution_class=TanhNormal,\n        )\n        return actor.to(device)\n\n    def _create_mock_qvalue(self, batch=2, obs_dim=3, action_dim=4, device=\"cpu\"):\n        class ValueClass(nn.Module):\n            def __init__(self):\n                super().__init__()\n                self.linear = nn.Linear(obs_dim + action_dim, 1)\n\n            def forward(self, obs, act):\n                return self.linear(torch.cat([obs, act], -1))\n\n        module = ValueClass()\n        qvalue = ValueOperator(\n            module=module,\n            in_keys=[\"observation\", \"action\"],\n        )\n        return qvalue.to(device)\n\n    def _create_mock_value(self, batch=2, obs_dim=3, action_dim=4, device=\"cpu\"):\n        module = nn.Linear(obs_dim, 1)\n        value = ValueOperator(\n            module=module,\n            in_keys=[\"observation\"],\n        )\n        return value.to(device)\n\n    def _create_mock_distributional_actor(\n        self, batch=2, obs_dim=3, action_dim=4, atoms=5, vmin=1, vmax=5\n    ):\n        raise NotImplementedError\n\n    def _create_mock_data_sac(\n        self, batch=16, obs_dim=3, action_dim=4, atoms=None, device=\"cpu\"\n    ):\n        # create a tensordict\n        obs = torch.randn(batch, obs_dim, device=device)\n        next_obs = torch.randn(batch, obs_dim, device=device)\n        if atoms:\n            raise NotImplementedError\n        else:\n            action = torch.randn(batch, action_dim, device=device).clamp(-1, 1)\n        reward = torch.randn(batch, 1, device=device)\n        done = torch.zeros(batch, 1, dtype=torch.bool, device=device)\n        td = TensorDict(\n            batch_size=(batch,),\n            source={\n                \"observation\": obs,\n                \"next\": {\"observation\": next_obs},\n                \"done\": done,\n                \"reward\": reward,\n                \"action\": action,\n            },\n            device=device,\n        )\n        return td\n\n    def _create_seq_mock_data_sac(\n        self, batch=8, T=4, obs_dim=3, action_dim=4, atoms=None, device=\"cpu\"\n    ):\n        # create a tensordict\n        total_obs = torch.randn(batch, T + 1, obs_dim, device=device)\n        obs = total_obs[:, :T]\n        next_obs = total_obs[:, 1:]\n        if atoms:\n            action = torch.randn(batch, T, atoms, action_dim, device=device).clamp(\n                -1, 1\n            )\n        else:\n            action = torch.randn(batch, T, action_dim, device=device).clamp(-1, 1)\n        reward = torch.randn(batch, T, 1, device=device)\n        done = torch.zeros(batch, T, 1, dtype=torch.bool, device=device)\n        mask = torch.ones(batch, T, dtype=torch.bool, device=device)\n        td = TensorDict(\n            batch_size=(batch, T),\n            source={\n                \"observation\": obs.masked_fill_(~mask.unsqueeze(-1), 0.0),\n                \"next\": {\n                    \"observation\": next_obs.masked_fill_(~mask.unsqueeze(-1), 0.0)\n                },\n                \"done\": done,\n                \"collector\": {\"mask\": mask},\n                \"reward\": reward.masked_fill_(~mask.unsqueeze(-1), 0.0),\n                \"action\": action.masked_fill_(~mask.unsqueeze(-1), 0.0),\n            },\n            device=device,\n        )\n        return td\n\n    @pytest.mark.skipif(", "metadata": {"task_id": "pytorch_rl/68", "ground_truth": "        not _has_functorch, reason=f\"functorch not installed: {FUNCTORCH_ERR}\"", "fpath_tuple": ["pytorch_rl", "test", "test_cost.py"], "context_start_lineno": 861, "line_no": 1025, "query_window": {"context": "        reward = torch.randn(batch, T, 1, device=device)\n        done = torch.zeros(batch, T, 1, dtype=torch.bool, device=device)\n        mask = torch.ones(batch, T, dtype=torch.bool, device=device)\n        td = TensorDict(\n            batch_size=(batch, T),\n            source={\n                \"observation\": obs.masked_fill_(~mask.unsqueeze(-1), 0.0),\n                \"next\": {\n                    \"observation\": next_obs.masked_fill_(~mask.unsqueeze(-1), 0.0)\n                },\n                \"done\": done,\n                \"collector\": {\"mask\": mask},\n                \"reward\": reward.masked_fill_(~mask.unsqueeze(-1), 0.0),\n                \"action\": action.masked_fill_(~mask.unsqueeze(-1), 0.0),\n            },\n            device=device,\n        )\n        return td\n\n    @pytest.mark.skipif(", "metadata": {"fpath_tuple": ["pytorch_rl", "test", "test_cost.py"], "line_no": 1025, "task_id": "pytorch_rl/68", "start_line_no": 1005, "end_line_no": 1025, "window_size": 20, "context_start_lineno": 861, "repo": "pytorch_rl"}}, "top_k_context": [{"context": "        reward = torch.randn(batch, T, 1, device=device)\n        done = torch.zeros(batch, T, 1, dtype=torch.bool, device=device)\n        mask = ~torch.zeros(batch, T, dtype=torch.bool, device=device)\n        td = TensorDict(\n            batch_size=(batch, T),\n            source={\n                \"observation\": obs.masked_fill_(~mask.unsqueeze(-1), 0.0),\n                \"next\": {\n                    \"observation\": next_obs.masked_fill_(~mask.unsqueeze(-1), 0.0)\n                },\n                \"done\": done,\n                \"collector\": {\"mask\": mask},\n                \"reward\": reward.masked_fill_(~mask.unsqueeze(-1), 0.0),\n                \"action\": action.masked_fill_(~mask.unsqueeze(-1), 0.0),\n            },\n            device=device,\n        )\n        return td\n\n    @pytest.mark.skipif(", "metadata": [{"fpath_tuple": ["pytorch_rl", "test", "test_cost.py"], "line_no": 510, "start_line_no": 500, "end_line_no": 520, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}], "sim_score": 0.9764705882352941}, {"context": "        reward = torch.randn(batch, T, 1, device=device)\n        done = torch.zeros(batch, T, 1, dtype=torch.bool, device=device)\n        mask = ~torch.zeros(batch, T, dtype=torch.bool, device=device)\n        td = TensorDict(\n            batch_size=(batch, T),\n            source={\n                \"observation\": obs.masked_fill_(~mask.unsqueeze(-1), 0.0),\n                \"next\": {\n                    \"observation\": next_obs.masked_fill_(~mask.unsqueeze(-1), 0.0)\n                },\n                \"done\": done,\n                \"collector\": {\"mask\": mask},\n                \"reward\": reward.masked_fill_(~mask.unsqueeze(-1), 0.0),\n                \"action\": action.masked_fill_(~mask.unsqueeze(-1), 0.0),\n                \"action_value\": action_value.masked_fill_(~mask.unsqueeze(-1), 0.0),\n            },\n        )\n        return td\n\n    @pytest.mark.parametrize(\"delay_value\", (False, True))", "metadata": [{"fpath_tuple": ["pytorch_rl", "test", "test_cost.py"], "line_no": 276, "start_line_no": 266, "end_line_no": 286, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}], "sim_score": 0.84375}, {"context": "        else:\n            action = torch.randn(batch, T, action_dim, device=device).clamp(-1, 1)\n        reward = torch.randn(batch, T, 1, device=device)\n        done = torch.zeros(batch, T, 1, dtype=torch.bool, device=device)\n        mask = ~torch.zeros(batch, T, dtype=torch.bool, device=device)\n        td = TensorDict(\n            batch_size=(batch, T),\n            source={\n                \"observation\": obs.masked_fill_(~mask.unsqueeze(-1), 0.0),\n                \"next\": {\n                    \"observation\": next_obs.masked_fill_(~mask.unsqueeze(-1), 0.0)\n                },\n                \"done\": done,\n                \"collector\": {\"mask\": mask},\n                \"reward\": reward.masked_fill_(~mask.unsqueeze(-1), 0.0),\n                \"action\": action.masked_fill_(~mask.unsqueeze(-1), 0.0),\n            },\n            device=device,\n        )\n        return td", "metadata": [{"fpath_tuple": ["pytorch_rl", "test", "test_cost.py"], "line_no": 508, "start_line_no": 498, "end_line_no": 518, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}, {"fpath_tuple": ["pytorch_rl", "test", "test_cost.py"], "line_no": 1442, "start_line_no": 1432, "end_line_no": 1452, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}], "sim_score": 0.8241758241758241}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# ding/interaction/master/connection.py\n# --------------------------------------------------\n#             method,\n#             path,\n#             data,\n#             retries=self.__request_retries,\n#             retry_waiting=self.__request_retry_waiting,\n#         )\n# \n#     @property\n#     def is_connected(self) -> bool:\n#         \"\"\"\n#         Overview:\n#             Check connection status\n#         Returns:\n#             - connected (:obj:`bool`): Whether this connection is still alive\n#         \"\"\"\n#         with self.__lock:\n#             return self.__is_connected\n# \n#     def _before_connect(self) -> Mapping[str, Any]:\n#         pass  # pragma: no cover\n# --------------------------------------------------\n# the below code fragment can be found in:\n# ding/interaction/master/connection.py\n# --------------------------------------------------\n#             http_error_gene=get_slave_exception_by_error,\n#         )()(host, port or DEFAULT_SLAVE_PORT, https)\n#         self.__request_retries = max(request_retries or DEFAULT_REQUEST_RETRIES, 0)\n#         self.__request_retry_waiting = max(request_retry_waiting or DEFAULT_REQUEST_RETRY_WAITING, 0.0)\n# \n#         # threading part\n#         self.__lock = Lock()\n#         self.__is_connected = False\n# \n#         # task part\n#         self.__tasks = {}\n# \n#         self.__init_triggers()\n# \n#     def __request(self, method: str, path: str, data: Optional[Mapping[str, Any]] = None) -> requests.Response:\n#         return self.__http_engine.request(\n#             method,\n#             path,\n#             data,\n#             retries=self.__request_retries,\n# --------------------------------------------------\n# the below code fragment can be found in:\n# ding/interaction/master/connection.py\n# --------------------------------------------------\n#         self.__request_retries = max(request_retries or DEFAULT_REQUEST_RETRIES, 0)\n#         self.__request_retry_waiting = max(request_retry_waiting or DEFAULT_REQUEST_RETRY_WAITING, 0.0)\n# \n#         # threading part\n#         self.__lock = Lock()\n#         self.__is_connected = False\n# \n#         # task part\n#         self.__tasks = {}\n# \n#         self.__init_triggers()\n# \n#     def __request(self, method: str, path: str, data: Optional[Mapping[str, Any]] = None) -> requests.Response:\n#         return self.__http_engine.request(\n#             method,\n#             path,\n#             data,\n#             retries=self.__request_retries,\n#             retry_waiting=self.__request_retry_waiting,\n#         )\n# --------------------------------------------------\n# the below code fragment can be found in:\n# ding/interaction/master/connection.py\n# --------------------------------------------------\n# \n#         # threading part\n#         self.__lock = Lock()\n#         self.__is_connected = False\n# \n#         # task part\n#         self.__tasks = {}\n# \n#         self.__init_triggers()\n# \n#     def __request(self, method: str, path: str, data: Optional[Mapping[str, Any]] = None) -> requests.Response:\n#         return self.__http_engine.request(\n#             method,\n#             path,\n#             data,\n#             retries=self.__request_retries,\n#             retry_waiting=self.__request_retry_waiting,\n#         )\n# \n#     @property\n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\nimport json\nimport socket\nimport time\nfrom typing import Optional, Any, Mapping, Callable, Type, Tuple\n\nimport requests\nfrom requests import HTTPError\nfrom urlobject import URLObject\nfrom urlobject.path import URLPath\n\nfrom .common import translate_dict_func\n\n\ndef get_host_ip() -> Optional[str]:\n    s = None\n    try:\n        s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n        s.connect(('8.8.8.8', 80))\n        ip = s.getsockname()[0]\n    finally:\n        if s is not None:\n            s.close()\n    return ip\n\n\n_DEFAULT_HTTP_PORT = 80\n_DEFAULT_HTTPS_PORT = 443\n\n\ndef split_http_address(address: str, default_port: Optional[int] = None) -> Tuple[str, int, bool, str]:\n    _url = URLObject(address)\n\n    _host = _url.hostname\n    _https = (_url.scheme.lower()) == 'https'\n    _port = _url.port or default_port or (_DEFAULT_HTTPS_PORT if _https else _DEFAULT_HTTP_PORT)\n    _path = str(_url.path) or ''\n\n    return _host, _port, _https, _path\n\n\nDEFAULT_RETRIES = 5\nDEFAULT_RETRY_WAITING = 1.0\n\n\nclass HttpEngine:\n\n    def __init__(self, host: str, port: int, https: bool = False, path: str = None):\n        self.__base_url = URLObject().with_scheme('https' if https else 'http') \\\n            .with_hostname(host).with_port(port).add_path(path or '')\n        self.__session = requests.session()\n        self.__session.trust_env = False\n\n    # noinspection PyMethodMayBeStatic\n    def _data_process(self, data: Optional[Mapping[str, Any]] = None) -> Mapping[str, Any]:\n        return data or {}\n\n    # noinspection PyMethodMayBeStatic\n    def _base_headers(self) -> Mapping[str, None]:\n        return {}\n\n    def _error_handler(self, err: Exception):\n        raise err\n\n    def get_url(self, path: str = None):\n        original_segments = self.__base_url.path.segments\n        path_segments = URLPath().add(path or '').segments\n        return str(self.__base_url.with_path(URLPath.join_segments(original_segments + path_segments)))\n\n    def __single_request(\n        self,\n        method: str,\n        path: str,\n        data: Optional[Mapping[str, Any]] = None,\n        headers: Optional[Mapping[str, Any]] = None,\n        params: Optional[Mapping[str, Any]] = None,\n        raise_for_status: bool = True\n    ):\n        response = self.__session.request(\n            method=method,\n            url=self.get_url(path),\n            data=json.dumps(self._data_process(data) or {}),\n            headers=headers,\n            params=params or {},\n        )\n        if raise_for_status:\n            response.raise_for_status()\n\n        return response\n\n    def request(\n            self,\n            method: str,\n            path: str,\n            data: Optional[Mapping[str, Any]] = None,\n            headers: Optional[Mapping[str, Any]] = None,\n            params: Optional[Mapping[str, Any]] = None,\n            raise_for_status: bool = True,\n            retries: Optional[int] = None,\n            retry_waiting: Optional[float] = None,\n    ) -> requests.Response:\n        _headers = dict(self._base_headers())\n        _headers.update(headers or {})\n\n        retries = retries or DEFAULT_RETRIES\n        retry_waiting = retry_waiting or DEFAULT_RETRY_WAITING\n\n        try:\n            _current_retries = 0\n            while True:\n                try:\n                    response = self.__single_request(method, path, data, _headers, params, raise_for_status)\n                except requests.exceptions.HTTPError as err:", "metadata": {"task_id": "opendilab_ACE/125", "ground_truth": "                    raise err", "fpath_tuple": ["opendilab_ACE", "ding", "interaction", "base", "network.py"], "context_start_lineno": 0, "line_no": 112, "query_window": {"context": "            path: str,\n            data: Optional[Mapping[str, Any]] = None,\n            headers: Optional[Mapping[str, Any]] = None,\n            params: Optional[Mapping[str, Any]] = None,\n            raise_for_status: bool = True,\n            retries: Optional[int] = None,\n            retry_waiting: Optional[float] = None,\n    ) -> requests.Response:\n        _headers = dict(self._base_headers())\n        _headers.update(headers or {})\n\n        retries = retries or DEFAULT_RETRIES\n        retry_waiting = retry_waiting or DEFAULT_RETRY_WAITING\n\n        try:\n            _current_retries = 0\n            while True:\n                try:\n                    response = self.__single_request(method, path, data, _headers, params, raise_for_status)\n                except requests.exceptions.HTTPError as err:", "metadata": {"fpath_tuple": ["opendilab_ACE", "ding", "interaction", "base", "network.py"], "line_no": 112, "task_id": "opendilab_ACE/125", "start_line_no": 92, "end_line_no": 112, "window_size": 20, "context_start_lineno": 0, "repo": "opendilab_ACE"}}, "top_k_context": [{"context": "        self.__request_retries = max(request_retries or DEFAULT_REQUEST_RETRIES, 0)\n        self.__request_retry_waiting = max(request_retry_waiting or DEFAULT_REQUEST_RETRY_WAITING, 0.0)\n\n        # threading part\n        self.__lock = Lock()\n        self.__is_connected = False\n\n        # task part\n        self.__tasks = {}\n\n        self.__init_triggers()\n\n    def __request(self, method: str, path: str, data: Optional[Mapping[str, Any]] = None) -> requests.Response:\n        return self.__http_engine.request(\n            method,\n            path,\n            data,\n            retries=self.__request_retries,\n            retry_waiting=self.__request_retry_waiting,\n        )", "metadata": [{"fpath_tuple": ["opendilab_ACE", "ding", "interaction", "master", "connection.py"], "line_no": 138, "start_line_no": 128, "end_line_no": 148, "window_size": 20, "repo": "opendilab_ACE", "slice_size": 10}], "sim_score": 0.4537037037037037}, {"context": "            http_error_gene=get_slave_exception_by_error,\n        )()(host, port or DEFAULT_SLAVE_PORT, https)\n        self.__request_retries = max(request_retries or DEFAULT_REQUEST_RETRIES, 0)\n        self.__request_retry_waiting = max(request_retry_waiting or DEFAULT_REQUEST_RETRY_WAITING, 0.0)\n\n        # threading part\n        self.__lock = Lock()\n        self.__is_connected = False\n\n        # task part\n        self.__tasks = {}\n\n        self.__init_triggers()\n\n    def __request(self, method: str, path: str, data: Optional[Mapping[str, Any]] = None) -> requests.Response:\n        return self.__http_engine.request(\n            method,\n            path,\n            data,\n            retries=self.__request_retries,", "metadata": [{"fpath_tuple": ["opendilab_ACE", "ding", "interaction", "master", "connection.py"], "line_no": 136, "start_line_no": 126, "end_line_no": 146, "window_size": 20, "repo": "opendilab_ACE", "slice_size": 10}], "sim_score": 0.4065040650406504}, {"context": "                'Token': lambda: self.__token,\n            },\n            http_error_gene=get_slave_exception_by_error,\n        )()(host, port or DEFAULT_SLAVE_PORT, https)\n        self.__request_retries = max(request_retries or DEFAULT_REQUEST_RETRIES, 0)\n        self.__request_retry_waiting = max(request_retry_waiting or DEFAULT_REQUEST_RETRY_WAITING, 0.0)\n\n        # threading part\n        self.__lock = Lock()\n        self.__is_connected = False\n\n        # task part\n        self.__tasks = {}\n\n        self.__init_triggers()\n\n    def __request(self, method: str, path: str, data: Optional[Mapping[str, Any]] = None) -> requests.Response:\n        return self.__http_engine.request(\n            method,\n            path,", "metadata": [{"fpath_tuple": ["opendilab_ACE", "ding", "interaction", "master", "connection.py"], "line_no": 134, "start_line_no": 124, "end_line_no": 144, "window_size": 20, "repo": "opendilab_ACE", "slice_size": 10}], "sim_score": 0.3875968992248062}, {"context": "    def __request(self, method: str, path: str, data: Optional[Mapping[str, Any]] = None) -> requests.Response:\n        return self.__http_engine.request(\n            method,\n            path,\n            data,\n            retries=self.__request_retries,\n            retry_waiting=self.__request_retry_waiting,\n        )\n\n    @property\n    def is_connected(self) -> bool:\n        \"\"\"\n        Overview:\n            Check connection status\n        Returns:\n            - connected (:obj:`bool`): Whether this connection is still alive\n        \"\"\"\n        with self.__lock:\n            return self.__is_connected\n", "metadata": [{"fpath_tuple": ["opendilab_ACE", "ding", "interaction", "master", "connection.py"], "line_no": 150, "start_line_no": 140, "end_line_no": 160, "window_size": 20, "repo": "opendilab_ACE", "slice_size": 10}], "sim_score": 0.3392857142857143}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# fortuna/prob_model/likelihood/base.py\n# --------------------------------------------------\n#         .. math::\n#             \\text{Var}_{Y|w,x}[Y],\n# \n#         where:\n#          - :math:`x` is an observed input variable;\n#          - :math:`Y` is a random target variable;\n#          - :math:`w` denotes the observed model parameters.\n# \n#         Parameters\n#         ----------\n#         params : Params\n#             The random parameters of the probabilistic model.\n#         inputs_loader : InputsLoader\n#             A loader of input data points.\n#         mutable : Optional[Mutable]\n#             The mutable objects used to evaluate the models.\n#         calib_params : Optional[CalibParams]\n#             The calibration parameters of the probabilistic model.\n#         calib_mutable : Optional[CalibMutable]\n#             The calibration mutable objects used to evaluate the calibrators.\n# --------------------------------------------------\n# the below code fragment can be found in:\n# fortuna/prob_model/likelihood/base.py\n# --------------------------------------------------\n#         Estimate the likelihood mode of the target variable, that is\n# \n#         .. math::\n#             \\text{argmax}_y\\ p(y|w, x),\n# \n#         where:\n#          - :math:`x` is an observed input variable;\n#          - :math:`w` denotes the observed model parameters;\n#          - :math:`y` is the target variable to optimize upon.\n# \n#         Parameters\n#         ----------\n#         params : Params\n#             The random parameters of the probabilistic model.\n#         inputs_loader : InputsLoader\n#             A loader of input data points.\n#         mutable : Optional[Mutable]\n#             The mutable objects used to evaluate the models.\n#         calib_params : Optional[CalibParams]\n#             The calibration parameters of the probabilistic model.\n# --------------------------------------------------\n# the below code fragment can be found in:\n# fortuna/prob_model/likelihood/base.py\n# --------------------------------------------------\n#         Estimate the likelihood mean of the target variable, that is\n# \n#         .. math::\n#             \\mathbb{E}_{Y|w, x}[Y],\n# \n#         where:\n#          - :math:`x` is an observed input variable;\n#          - :math:`Y` is a random target variable;\n#          - :math:`w` denotes the observed model parameters.\n# \n#         Parameters\n#         ----------\n#         params : Params\n#             The random parameters of the probabilistic model.\n#         inputs_loader : InputsLoader\n#             A loader of input data points.\n#         mutable : Optional[Mutable]\n#             The mutable objects used to evaluate the models.\n#         calib_params : Optional[CalibParams]\n#             The calibration parameters of the probabilistic model.\n# --------------------------------------------------\n# the below code fragment can be found in:\n# fortuna/prob_model/likelihood/base.py\n# --------------------------------------------------\n#         Estimate the likelihood variance of the target variable, that is\n# \n#         .. math::\n#             \\text{Var}_{Y|w,x}[Y],\n# \n#         where:\n#          - :math:`x` is an observed input variable;\n#          - :math:`Y` is a random target variable;\n#          - :math:`w` denotes the observed model parameters.\n# \n#         Parameters\n#         ----------\n#         params : Params\n#             The random parameters of the probabilistic model.\n#         inputs_loader : InputsLoader\n#             A loader of input data points.\n#         mutable : Optional[Mutable]\n#             The mutable objects used to evaluate the models.\n#         calib_params : Optional[CalibParams]\n#             The calibration parameters of the probabilistic model.\n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\nfrom typing import Optional\n\nimport jax\nimport jax.numpy as jnp\nfrom jax import vmap\n\nfrom fortuna.data.loader import InputsLoader\nfrom fortuna.model.model_manager.classification import \\\n    ClassificationModelManager\nfrom fortuna.output_calibrator.output_calib_manager.base import \\\n    OutputCalibManager\nfrom fortuna.prob_model.likelihood.base import Likelihood\nfrom fortuna.prob_output_layer.classification import \\\n    ClassificationProbOutputLayer\nfrom fortuna.typing import Array, CalibMutable, CalibParams, Mutable, Params\n\n\nclass ClassificationLikelihood(Likelihood):\n    def __init__(\n        self,\n        model_manager: ClassificationModelManager,\n        prob_output_layer: ClassificationProbOutputLayer,\n        output_calib_manager: OutputCalibManager,\n    ):\n        \"\"\"\n        A classification likelihood function class. In this class, the likelihood function is additionally assumed to\n        be a probability density function, i.e. positive and integrating to 1. The likelihood is formed by three\n        objects applied in sequence: the model manager, the output calibrator and the probabilistic output layer. The\n        model manager maps parameters and inputs to outputs. The output calibration takes outputs and returns some\n        calibrated version of them. The probabilistic output layer describes the probability distribution of the\n        calibrated outputs.\n\n        Parameters\n        ----------\n        model_manager : ModelManager\n            An model manager. This objects orchestrates the evaluation of the models.\n        prob_output_layer : ProbOutputLayer\n            A probabilistic output layer object. This object characterizes the probability distribution of the\n            target variable given the calibrated outputs.\n        output_calib_manager : OutputCalibManager\n            An output calibration manager object. It transforms outputs of the model manager into some\n            calibrated version of them.\n        \"\"\"\n        super().__init__(\n            model_manager, prob_output_layer, output_calib_manager=output_calib_manager\n        )\n\n    def mean(\n        self,\n        params: Params,\n        inputs_loader: InputsLoader,\n        mutable: Optional[Mutable] = None,\n        calib_params: Optional[CalibParams] = None,\n        calib_mutable: Optional[CalibMutable] = None,\n        distribute: bool = True,\n        **kwargs\n    ) -> jnp.ndarray:\n        r\"\"\"\n        Estimate the likelihood mean of the one-hot encoded target variable, that is\n\n        .. math::\n            \\mathbb{E}_{\\tilde{Y}|w, x}[\\tilde{Y}],\n\n        where:\n         - :math:`x` is an observed input variable;\n         - :math:`\\tilde{Y}` is a one-hot encoded random target variable;\n         - :math:`w` denotes the observed model parameters.\n\n        Parameters\n        ----------\n        params : Params\n            The random parameters of the probabilistic model.\n        inputs_loader : InputsLoader\n            A loader of input data points.\n        mutable : Optional[Mutable]\n            The mutable objects used to evaluate the models.\n        calib_params : Optional[CalibParams]\n            The calibration parameters of the probabilistic model.\n        calib_mutable : Optional[CalibMutable]\n            The calibration mutable objects used to evaluate the calibrators.\n        distribute: bool\n            Whether to distribute computation over multiple devices, if available.\n\n        Returns\n        -------\n        jnp.ndarray\n            An estimate of the likelihood mean for each input.\n        \"\"\"\n        return super().mean(\n            params,\n            inputs_loader,\n            mutable,\n            calib_params,\n            calib_mutable,\n            distribute,\n            **kwargs\n        )\n\n    def _batched_mean(\n        self,\n        params: Params,\n        inputs: Array,\n        mutable: Optional[Mutable] = None,\n        calib_params: Optional[CalibParams] = None,\n        calib_mutable: Optional[CalibMutable] = None,\n        **kwargs\n    ) -> jnp.ndarray:\n        outputs = self._get_batched_calibrated_outputs(\n            params, inputs, mutable, calib_params, calib_mutable, **kwargs\n        )\n        return jax.nn.softmax(outputs, -1)\n\n    def _batched_mode(\n        self,\n        params: Params,\n        inputs: Array,\n        mutable: Optional[Mutable] = None,\n        calib_params: Optional[CalibParams] = None,\n        calib_mutable: Optional[CalibMutable] = None,\n        **kwargs\n    ) -> jnp.ndarray:\n        outputs = self._get_batched_calibrated_outputs(\n            params, inputs, mutable, calib_params, calib_mutable, **kwargs\n        )\n        return jnp.argmax(outputs, -1)\n\n    def variance(\n        self,\n        params: Params,\n        inputs_loader: InputsLoader,\n        mutable: Optional[Mutable] = None,\n        calib_params: Optional[CalibParams] = None,\n        calib_mutable: Optional[CalibMutable] = None,\n        distribute: bool = True,\n        **kwargs\n    ) -> jnp.ndarray:\n        r\"\"\"\n        Estimate the likelihood variance of the one-hot encoded target variable, that is\n\n        .. math::\n            \\text{Var}_{\\tilde{Y}|w,x}[\\tilde{Y}],\n\n        where:\n         - :math:`x` is an observed input variable;\n         - :math:`\\tilde{Y}` is a one-hot encoded random target variable;\n         - :math:`w` denotes the observed model parameters.\n\n        Parameters\n        ----------\n        params : Params\n            The random parameters of the probabilistic model.\n        inputs_loader : InputsLoader\n            A loader of input data points.\n        mutable : Optional[Mutable]\n            The mutable objects used to evaluate the models.", "metadata": {"task_id": "awslabs_fortuna/176", "ground_truth": "        calib_params : Optional[CalibParams]", "fpath_tuple": ["awslabs_fortuna", "fortuna", "prob_model", "likelihood", "classification.py"], "context_start_lineno": 0, "line_no": 155, "query_window": {"context": "    ) -> jnp.ndarray:\n        r\"\"\"\n        Estimate the likelihood variance of the one-hot encoded target variable, that is\n\n        .. math::\n            \\text{Var}_{\\tilde{Y}|w,x}[\\tilde{Y}],\n\n        where:\n         - :math:`x` is an observed input variable;\n         - :math:`\\tilde{Y}` is a one-hot encoded random target variable;\n         - :math:`w` denotes the observed model parameters.\n\n        Parameters\n        ----------\n        params : Params\n            The random parameters of the probabilistic model.\n        inputs_loader : InputsLoader\n            A loader of input data points.\n        mutable : Optional[Mutable]\n            The mutable objects used to evaluate the models.", "metadata": {"fpath_tuple": ["awslabs_fortuna", "fortuna", "prob_model", "likelihood", "classification.py"], "line_no": 155, "task_id": "awslabs_fortuna/176", "start_line_no": 135, "end_line_no": 155, "window_size": 20, "context_start_lineno": 0, "repo": "awslabs_fortuna"}}, "top_k_context": [{"context": "    ) -> jnp.ndarray:\n        r\"\"\"\n        Estimate the likelihood variance of the target variable, that is\n\n        .. math::\n            \\text{Var}_{Y|w,x}[Y],\n\n        where:\n         - :math:`x` is an observed input variable;\n         - :math:`Y` is a random target variable;\n         - :math:`w` denotes the observed model parameters.\n\n        Parameters\n        ----------\n        params : Params\n            The random parameters of the probabilistic model.\n        inputs_loader : InputsLoader\n            A loader of input data points.\n        mutable : Optional[Mutable]\n            The mutable objects used to evaluate the models.", "metadata": [{"fpath_tuple": ["awslabs_fortuna", "fortuna", "prob_model", "likelihood", "base.py"], "line_no": 612, "start_line_no": 602, "end_line_no": 622, "window_size": 20, "repo": "awslabs_fortuna", "slice_size": 10}], "sim_score": 0.925531914893617}, {"context": "    ) -> jnp.ndarray:\n        r\"\"\"\n        Estimate the likelihood mean of the target variable, that is\n\n        .. math::\n            \\mathbb{E}_{Y|w, x}[Y],\n\n        where:\n         - :math:`x` is an observed input variable;\n         - :math:`Y` is a random target variable;\n         - :math:`w` denotes the observed model parameters.\n\n        Parameters\n        ----------\n        params : Params\n            The random parameters of the probabilistic model.\n        inputs_loader : InputsLoader\n            A loader of input data points.\n        mutable : Optional[Mutable]\n            The mutable objects used to evaluate the models.", "metadata": [{"fpath_tuple": ["awslabs_fortuna", "fortuna", "prob_model", "likelihood", "base.py"], "line_no": 484, "start_line_no": 474, "end_line_no": 494, "window_size": 20, "repo": "awslabs_fortuna", "slice_size": 10}], "sim_score": 0.8571428571428571}, {"context": "    ) -> jnp.ndarray:\n        r\"\"\"\n        Estimate the likelihood mode of the target variable, that is\n\n        .. math::\n            \\text{argmax}_y\\ p(y|w, x),\n\n        where:\n         - :math:`x` is an observed input variable;\n         - :math:`w` denotes the observed model parameters;\n         - :math:`y` is the target variable to optimize upon.\n\n        Parameters\n        ----------\n        params : Params\n            The random parameters of the probabilistic model.\n        inputs_loader : InputsLoader\n            A loader of input data points.\n        mutable : Optional[Mutable]\n            The mutable objects used to evaluate the models.", "metadata": [{"fpath_tuple": ["awslabs_fortuna", "fortuna", "prob_model", "likelihood", "base.py"], "line_no": 548, "start_line_no": 538, "end_line_no": 558, "window_size": 20, "repo": "awslabs_fortuna", "slice_size": 10}], "sim_score": 0.7884615384615384}, {"context": "        Estimate the likelihood variance of the target variable, that is\n\n        .. math::\n            \\text{Var}_{Y|w,x}[Y],\n\n        where:\n         - :math:`x` is an observed input variable;\n         - :math:`Y` is a random target variable;\n         - :math:`w` denotes the observed model parameters.\n\n        Parameters\n        ----------\n        params : Params\n            The random parameters of the probabilistic model.\n        inputs_loader : InputsLoader\n            A loader of input data points.\n        mutable : Optional[Mutable]\n            The mutable objects used to evaluate the models.\n        calib_params : Optional[CalibParams]\n            The calibration parameters of the probabilistic model.", "metadata": [{"fpath_tuple": ["awslabs_fortuna", "fortuna", "prob_model", "likelihood", "base.py"], "line_no": 614, "start_line_no": 604, "end_line_no": 624, "window_size": 20, "repo": "awslabs_fortuna", "slice_size": 10}], "sim_score": 0.78}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# fortuna/output_calibrator/output_calib_manager/state.py\n# --------------------------------------------------\n#         Parameters\n#         ----------\n#         d : Union[Dict, FrozenDict]\n#             A dictionary with as keys the calibrators and as values their initializations.\n# \n#         Returns\n#         -------\n#         OutputCalibManagerState\n#             An output calibration manager state.\n#         \"\"\"\n#         params = FrozenDict(\n#             {\n#                 k: FrozenDict({\"params\": v[\"params\"] if v else None})\n#                 for k, v in d.items()\n#             }\n#         )\n#         mutable = {calib_name: {} for calib_name in d}\n#         for name, variables in d.items():\n#             if variables:\n#                 for var_name, var_obj in variables.items():\n# --------------------------------------------------\n# the below code fragment can be found in:\n# fortuna/model/model_manager/state.py\n# fortuna/prob_model/state.py\n# --------------------------------------------------\n# \n#         Parameters\n#         ----------\n#         d : Union[Dict, FrozenDict]\n#             A dictionary like the output of :func:`~fortuna.model.model_manager.base.ModelManager.init`.\n# \n#         Returns\n#         -------\n#         ModelManagerState\n#             An model manager state.\n#         \"\"\"\n#         params = FrozenDict(\n#             {k: FrozenDict({\"params\": v[\"params\"]}) for k, v in d.items()}\n#         )\n#         mutable = FrozenDict(\n#             {\n#                 k: FrozenDict({_k: _v for _k, _v in v.items() if _k != \"params\"})\n#                 for k, v in d.items()\n#             }\n#         )\n# --------------------------------------------------\n# the below code fragment can be found in:\n# fortuna/model/model_manager/state.py\n# fortuna/prob_model/state.py\n# --------------------------------------------------\n#             A dictionary like the output of :func:`~fortuna.model.model_manager.base.ModelManager.init`.\n# \n#         Returns\n#         -------\n#         ModelManagerState\n#             An model manager state.\n#         \"\"\"\n#         params = FrozenDict(\n#             {k: FrozenDict({\"params\": v[\"params\"]}) for k, v in d.items()}\n#         )\n#         mutable = FrozenDict(\n#             {\n#                 k: FrozenDict({_k: _v for _k, _v in v.items() if _k != \"params\"})\n#                 for k, v in d.items()\n#             }\n#         )\n#         flag = 0\n#         for k, v in mutable.items():\n#             if len(v) > 0:\n#                 flag += 1\n# --------------------------------------------------\n# the below code fragment can be found in:\n# fortuna/model/model_manager/state.py\n# fortuna/prob_model/state.py\n# --------------------------------------------------\n#         ----------\n#         d : Union[Dict, FrozenDict]\n#             A dictionary like the output of :func:`~fortuna.model.model_manager.base.ModelManager.init`.\n# \n#         Returns\n#         -------\n#         ModelManagerState\n#             An model manager state.\n#         \"\"\"\n#         params = FrozenDict(\n#             {k: FrozenDict({\"params\": v[\"params\"]}) for k, v in d.items()}\n#         )\n#         mutable = FrozenDict(\n#             {\n#                 k: FrozenDict({_k: _v for _k, _v in v.items() if _k != \"params\"})\n#                 for k, v in d.items()\n#             }\n#         )\n#         flag = 0\n#         for k, v in mutable.items():\n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\nfrom __future__ import annotations\n\nfrom typing import Dict, Optional, Union\n\nfrom flax.core import FrozenDict\n\nfrom fortuna.typing import Mutable, Params\n\n\nclass ModelManagerState:\n    params: Params\n    mutable: Optional[Mutable] = None\n\n    def __init__(self, params: Params, mutable: Optional[Mutable] = None):\n        \"\"\"\n        An model manager state class.\n\n        Parameters\n        ----------\n        params : Params\n            The random parameters of the probabilistic model.\n        mutable : Optional[Mutable]\n            The mutable objects used to evaluate the models.\n        \"\"\"\n        self.params = params\n        self.mutable = mutable\n\n    @classmethod\n    def init_from_dict(cls, d: Union[Dict, FrozenDict]) -> ModelManagerState:\n        \"\"\"\n        Initialize the model manager state from a dictionary. This dictionary should be like the output of\n        :func:`~fortuna.model.model_manager.base.ModelManager.init`.\n\n        Parameters\n        ----------\n        d : Union[Dict, FrozenDict]\n            A dictionary like the output of :func:`~fortuna.model.model_manager.base.ModelManager.init`.\n\n        Returns\n        -------\n        ModelManagerState\n            An model manager state.\n        \"\"\"\n        params = FrozenDict(\n            {k: FrozenDict({\"params\": v[\"params\"]}) for k, v in d.items()}\n        )\n        mutable = FrozenDict(\n            {\n                k: FrozenDict({_k: _v for _k, _v in v.items() if _k != \"params\"})\n                for k, v in d.items()\n            }\n        )", "metadata": {"task_id": "awslabs_fortuna/115", "ground_truth": "        flag = 0", "fpath_tuple": ["awslabs_fortuna", "fortuna", "model", "model_manager", "state.py"], "context_start_lineno": 0, "line_no": 52, "query_window": {"context": "\n        Parameters\n        ----------\n        d : Union[Dict, FrozenDict]\n            A dictionary like the output of :func:`~fortuna.model.model_manager.base.ModelManager.init`.\n\n        Returns\n        -------\n        ModelManagerState\n            An model manager state.\n        \"\"\"\n        params = FrozenDict(\n            {k: FrozenDict({\"params\": v[\"params\"]}) for k, v in d.items()}\n        )\n        mutable = FrozenDict(\n            {\n                k: FrozenDict({_k: _v for _k, _v in v.items() if _k != \"params\"})\n                for k, v in d.items()\n            }\n        )", "metadata": {"fpath_tuple": ["awslabs_fortuna", "fortuna", "model", "model_manager", "state.py"], "line_no": 52, "task_id": "awslabs_fortuna/115", "start_line_no": 32, "end_line_no": 52, "window_size": 20, "context_start_lineno": 0, "repo": "awslabs_fortuna"}}, "top_k_context": [{"context": "\n        Parameters\n        ----------\n        d : Union[Dict, FrozenDict]\n            A dictionary like the output of :func:`~fortuna.model.model_manager.base.ModelManager.init`.\n\n        Returns\n        -------\n        ModelManagerState\n            An model manager state.\n        \"\"\"\n        params = FrozenDict(\n            {k: FrozenDict({\"params\": v[\"params\"]}) for k, v in d.items()}\n        )\n        mutable = FrozenDict(\n            {\n                k: FrozenDict({_k: _v for _k, _v in v.items() if _k != \"params\"})\n                for k, v in d.items()\n            }\n        )", "metadata": [{"fpath_tuple": ["awslabs_fortuna", "fortuna", "model", "model_manager", "state.py"], "line_no": 42, "start_line_no": 32, "end_line_no": 52, "window_size": 20, "repo": "awslabs_fortuna", "slice_size": 10}, {"fpath_tuple": ["awslabs_fortuna", "fortuna", "prob_model", "state.py"], "line_no": 42, "start_line_no": 32, "end_line_no": 52, "window_size": 20, "repo": "awslabs_fortuna", "slice_size": 10}], "sim_score": 1.0}, {"context": "        ----------\n        d : Union[Dict, FrozenDict]\n            A dictionary like the output of :func:`~fortuna.model.model_manager.base.ModelManager.init`.\n\n        Returns\n        -------\n        ModelManagerState\n            An model manager state.\n        \"\"\"\n        params = FrozenDict(\n            {k: FrozenDict({\"params\": v[\"params\"]}) for k, v in d.items()}\n        )\n        mutable = FrozenDict(\n            {\n                k: FrozenDict({_k: _v for _k, _v in v.items() if _k != \"params\"})\n                for k, v in d.items()\n            }\n        )\n        flag = 0\n        for k, v in mutable.items():", "metadata": [{"fpath_tuple": ["awslabs_fortuna", "fortuna", "model", "model_manager", "state.py"], "line_no": 44, "start_line_no": 34, "end_line_no": 54, "window_size": 20, "repo": "awslabs_fortuna", "slice_size": 10}, {"fpath_tuple": ["awslabs_fortuna", "fortuna", "prob_model", "state.py"], "line_no": 44, "start_line_no": 34, "end_line_no": 54, "window_size": 20, "repo": "awslabs_fortuna", "slice_size": 10}], "sim_score": 0.9506172839506173}, {"context": "        Initialize the model manager state from a dictionary. This dictionary should be like the output of\n        :func:`~fortuna.model.model_manager.base.ModelManager.init`.\n\n        Parameters\n        ----------\n        d : Union[Dict, FrozenDict]\n            A dictionary like the output of :func:`~fortuna.model.model_manager.base.ModelManager.init`.\n\n        Returns\n        -------\n        ModelManagerState\n            An model manager state.\n        \"\"\"\n        params = FrozenDict(\n            {k: FrozenDict({\"params\": v[\"params\"]}) for k, v in d.items()}\n        )\n        mutable = FrozenDict(\n            {\n                k: FrozenDict({_k: _v for _k, _v in v.items() if _k != \"params\"})\n                for k, v in d.items()", "metadata": [{"fpath_tuple": ["awslabs_fortuna", "fortuna", "model", "model_manager", "state.py"], "line_no": 40, "start_line_no": 30, "end_line_no": 50, "window_size": 20, "repo": "awslabs_fortuna", "slice_size": 10}, {"fpath_tuple": ["awslabs_fortuna", "fortuna", "prob_model", "state.py"], "line_no": 40, "start_line_no": 30, "end_line_no": 50, "window_size": 20, "repo": "awslabs_fortuna", "slice_size": 10}], "sim_score": 0.9058823529411765}, {"context": "        Initialize an output calibration manager state from a dictionary.\n\n        Parameters\n        ----------\n        d : Union[Dict, FrozenDict]\n            A dictionary with as keys the calibrators and as values their initializations.\n\n        Returns\n        -------\n        OutputCalibManagerState\n            An output calibration manager state.\n        \"\"\"\n        params = FrozenDict(\n            {\n                k: FrozenDict({\"params\": v[\"params\"] if v else None})\n                for k, v in d.items()\n            }\n        )\n        mutable = {calib_name: {} for calib_name in d}\n        for name, variables in d.items():", "metadata": [{"fpath_tuple": ["awslabs_fortuna", "fortuna", "output_calibrator", "output_calib_manager", "state.py"], "line_no": 40, "start_line_no": 30, "end_line_no": 50, "window_size": 20, "repo": "awslabs_fortuna", "slice_size": 10}], "sim_score": 0.5094339622641509}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# tests/fortuna/test_builtins.py\n# --------------------------------------------------\n# import unittest\n# \n# import jax.numpy as jnp\n# import numpy as np\n# \n# from fortuna.utils.builtins import HashableMixin\n# \n# \n# class ChildA(HashableMixin):\n#     def __init__(self, a, b):\n#         self.a = a\n#         self.b = b\n# --------------------------------------------------\n# the below code fragment can be found in:\n# tests/fortuna/test_temp_scaling.py\n# --------------------------------------------------\n# import unittest\n# \n# import jax.numpy as jnp\n# import optax\n# from jax import random\n# \n# from fortuna.data.loader import DataLoader\n# from fortuna.model.mlp import MLP\n# from fortuna.output_calibrator.classification import \\\n#     ClassificationTemperatureScaler\n# from fortuna.prob_model.classification import ProbClassifier\n# from fortuna.prob_model.posterior.map.map_approximator import \\\n# --------------------------------------------------\n# the below code fragment can be found in:\n# tests/fortuna/calib_model/test_calibrate.py\n# --------------------------------------------------\n# import logging\n# import tempfile\n# import unittest\n# \n# import jax.numpy as jnp\n# \n# from fortuna.calib_model.calib_config.base import CalibConfig\n# from fortuna.calib_model.calib_config.checkpointer import CalibCheckpointer\n# from fortuna.calib_model.calib_config.monitor import CalibMonitor\n# from fortuna.calib_model.calib_config.optimizer import CalibOptimizer\n# from fortuna.calib_model.classification import CalibClassifier\n# from fortuna.calib_model.regression import CalibRegressor\n# from fortuna.data.loader import DataLoader\n# from fortuna.metric.classification import accuracy, brier_score\n# --------------------------------------------------\n# the below code fragment can be found in:\n# tests/fortuna/test_model.py\n# --------------------------------------------------\n# import json\n# import os\n# import tempfile\n# import unittest\n# from types import SimpleNamespace\n# \n# import flax.linen as nn\n# import requests\n# from jax import random\n# from tqdm import tqdm\n# \n# from fortuna.model.cnn import CNN\n# from fortuna.model.mlp import MLP\n# from fortuna.model.linear import Linear\n# --------------------------------------------------\n# the below code fragment can be found in:\n# tests/fortuna/test_model.py\n# --------------------------------------------------\n# import json\n# import os\n# import tempfile\n# import unittest\n# from types import SimpleNamespace\n# \n# import flax.linen as nn\n# import requests\n# from jax import random\n# from tqdm import tqdm\n# \n# from fortuna.model.cnn import CNN\n# --------------------------------------------------\n# the below code fragment can be found in:\n# tests/fortuna/calib_model/test_calibrate.py\n# --------------------------------------------------\n# import logging\n# import tempfile\n# import unittest\n# \n# import jax.numpy as jnp\n# \n# from fortuna.calib_model.calib_config.base import CalibConfig\n# from fortuna.calib_model.calib_config.checkpointer import CalibCheckpointer\n# from fortuna.calib_model.calib_config.monitor import CalibMonitor\n# from fortuna.calib_model.calib_config.optimizer import CalibOptimizer\n# from fortuna.calib_model.classification import CalibClassifier\n# from fortuna.calib_model.regression import CalibRegressor\n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\nimport tempfile\nimport unittest\n\nimport jax.numpy as jnp\nimport optax", "metadata": {"task_id": "awslabs_fortuna/71", "ground_truth": "from jax import random", "fpath_tuple": ["awslabs_fortuna", "tests", "fortuna", "test_predictive.py"], "context_start_lineno": 0, "line_no": 5, "query_window": {"context": "import tempfile\nimport unittest\n\nimport jax.numpy as jnp\nimport optax", "metadata": {"fpath_tuple": ["awslabs_fortuna", "tests", "fortuna", "test_predictive.py"], "line_no": 5, "task_id": "awslabs_fortuna/71", "start_line_no": 0, "end_line_no": 5, "window_size": 20, "context_start_lineno": 0, "repo": "awslabs_fortuna"}}, "top_k_context": [{"context": "import logging\nimport tempfile\nimport unittest\n\nimport jax.numpy as jnp\n\nfrom fortuna.calib_model.calib_config.base import CalibConfig\nfrom fortuna.calib_model.calib_config.checkpointer import CalibCheckpointer\nfrom fortuna.calib_model.calib_config.monitor import CalibMonitor\nfrom fortuna.calib_model.calib_config.optimizer import CalibOptimizer", "metadata": [{"fpath_tuple": ["awslabs_fortuna", "tests", "fortuna", "calib_model", "test_calibrate.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 10, "window_size": 20, "repo": "awslabs_fortuna", "slice_size": 10}], "sim_score": 0.3783783783783784}, {"context": "import json\nimport os\nimport tempfile\nimport unittest\nfrom types import SimpleNamespace\n\nimport flax.linen as nn\nimport requests\nfrom jax import random\nfrom tqdm import tqdm", "metadata": [{"fpath_tuple": ["awslabs_fortuna", "tests", "fortuna", "test_model.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 10, "window_size": 20, "repo": "awslabs_fortuna", "slice_size": 10}], "sim_score": 0.375}, {"context": "import json\nimport os\nimport tempfile\nimport unittest\nfrom types import SimpleNamespace\n\nimport flax.linen as nn\nimport requests\nfrom jax import random\nfrom tqdm import tqdm\n\nfrom fortuna.model.cnn import CNN", "metadata": [{"fpath_tuple": ["awslabs_fortuna", "tests", "fortuna", "test_model.py"], "line_no": 2, "start_line_no": 0, "end_line_no": 12, "window_size": 20, "repo": "awslabs_fortuna", "slice_size": 10}], "sim_score": 0.3157894736842105}, {"context": "import logging\nimport tempfile\nimport unittest\n\nimport jax.numpy as jnp\n\nfrom fortuna.calib_model.calib_config.base import CalibConfig\nfrom fortuna.calib_model.calib_config.checkpointer import CalibCheckpointer\nfrom fortuna.calib_model.calib_config.monitor import CalibMonitor\nfrom fortuna.calib_model.calib_config.optimizer import CalibOptimizer\nfrom fortuna.calib_model.classification import CalibClassifier\nfrom fortuna.calib_model.regression import CalibRegressor", "metadata": [{"fpath_tuple": ["awslabs_fortuna", "tests", "fortuna", "calib_model", "test_calibrate.py"], "line_no": 2, "start_line_no": 0, "end_line_no": 12, "window_size": 20, "repo": "awslabs_fortuna", "slice_size": 10}], "sim_score": 0.3111111111111111}, {"context": "import unittest\n\nimport jax.numpy as jnp\nimport optax\nfrom jax import random\n\nfrom fortuna.data.loader import DataLoader\nfrom fortuna.model.mlp import MLP\nfrom fortuna.output_calibrator.classification import \\\n    ClassificationTemperatureScaler", "metadata": [{"fpath_tuple": ["awslabs_fortuna", "tests", "fortuna", "test_temp_scaling.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 10, "window_size": 20, "repo": "awslabs_fortuna", "slice_size": 10}], "sim_score": 0.30952380952380953}, {"context": "import unittest\n\nimport jax.numpy as jnp\nimport numpy as np\n\nfrom fortuna.utils.builtins import HashableMixin\n\n\nclass ChildA(HashableMixin):\n    def __init__(self, a, b):", "metadata": [{"fpath_tuple": ["awslabs_fortuna", "tests", "fortuna", "test_builtins.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 10, "window_size": 20, "repo": "awslabs_fortuna", "slice_size": 10}], "sim_score": 0.2727272727272727}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# examples/textual_inversion/textual_inversion.py\n# --------------------------------------------------\n#     )\n#     parser.add_argument(\n#         \"--gradient_accumulation_steps\",\n#         type=int,\n#         default=1,\n#         help=\"Number of updates steps to accumulate before performing a backward/update pass.\",\n#     )\n#     parser.add_argument(\n#         \"--gradient_checkpointing\",\n#         action=\"store_true\",\n#         help=\"Whether or not to use gradient checkpointing to save memory at the expense of slower backward pass.\",\n#     )\n#     parser.add_argument(\n#         \"--learning_rate\",\n#         type=float,\n#         default=1e-4,\n#         help=\"Initial learning rate (after the potential warmup period) to use.\",\n#     )\n#     parser.add_argument(\n#         \"--scale_lr\",\n# --------------------------------------------------\n# the below code fragment can be found in:\n# examples/text_to_image/train_text_to_image.py\n# examples/text_to_image/train_text_to_image_lora.py\n# --------------------------------------------------\n#         help=\"Total number of training steps to perform.  If provided, overrides num_train_epochs.\",\n#     )\n#     parser.add_argument(\n#         \"--gradient_accumulation_steps\",\n#         type=int,\n#         default=1,\n#         help=\"Number of updates steps to accumulate before performing a backward/update pass.\",\n#     )\n#     parser.add_argument(\n#         \"--gradient_checkpointing\",\n#         action=\"store_true\",\n#         help=\"Whether or not to use gradient checkpointing to save memory at the expense of slower backward pass.\",\n#     )\n#     parser.add_argument(\n#         \"--learning_rate\",\n#         type=float,\n#         default=1e-4,\n#         help=\"Initial learning rate (after the potential warmup period) to use.\",\n#     )\n#     parser.add_argument(\n# --------------------------------------------------\n# the below code fragment can be found in:\n# examples/text_to_image/train_text_to_image.py\n# examples/text_to_image/train_text_to_image_lora.py\n# --------------------------------------------------\n#     parser.add_argument(\n#         \"--gradient_accumulation_steps\",\n#         type=int,\n#         default=1,\n#         help=\"Number of updates steps to accumulate before performing a backward/update pass.\",\n#     )\n#     parser.add_argument(\n#         \"--gradient_checkpointing\",\n#         action=\"store_true\",\n#         help=\"Whether or not to use gradient checkpointing to save memory at the expense of slower backward pass.\",\n#     )\n#     parser.add_argument(\n#         \"--learning_rate\",\n#         type=float,\n#         default=1e-4,\n#         help=\"Initial learning rate (after the potential warmup period) to use.\",\n#     )\n#     parser.add_argument(\n#         \"--scale_lr\",\n#         action=\"store_true\",\n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\n#!/usr/bin/env python\n# coding=utf-8\n# Copyright 2022 The HuggingFace Inc. team. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n\nimport argparse\nimport logging\nimport math\nimport os\nimport random\nfrom pathlib import Path\nfrom typing import Optional\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nimport torch.utils.checkpoint\n\nimport datasets\nimport diffusers\nimport transformers\nfrom accelerate import Accelerator\nfrom accelerate.logging import get_logger\nfrom accelerate.utils import set_seed\nfrom datasets import load_dataset\nfrom diffusers import AutoencoderKL, DDPMScheduler, StableDiffusionPipeline, UNet2DConditionModel\nfrom diffusers.optimization import get_scheduler\nfrom diffusers.training_utils import EMAModel\nfrom diffusers.utils import check_min_version\nfrom diffusers.utils.import_utils import is_xformers_available\nfrom huggingface_hub import HfFolder, Repository, create_repo, whoami\nfrom torchvision import transforms\nfrom tqdm.auto import tqdm\nfrom transformers import CLIPTextModel, CLIPTokenizer\n\n\n# Will error if the minimal version of diffusers is not installed. Remove at your own risks.\ncheck_min_version(\"0.13.0.dev0\")\n\nlogger = get_logger(__name__, log_level=\"INFO\")\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=\"Simple example of a training script.\")\n    parser.add_argument(\n        \"--pretrained_model_name_or_path\",\n        type=str,\n        default=None,\n        required=True,\n        help=\"Path to pretrained model or model identifier from huggingface.co/models.\",\n    )\n    parser.add_argument(\n        \"--revision\",\n        type=str,\n        default=None,\n        required=False,\n        help=\"Revision of pretrained model identifier from huggingface.co/models.\",\n    )\n    parser.add_argument(\n        \"--dataset_name\",\n        type=str,\n        default=None,\n        help=(\n            \"The name of the Dataset (from the HuggingFace hub) to train on (could be your own, possibly private,\"\n            \" dataset). It can also be a path pointing to a local copy of a dataset in your filesystem,\"\n            \" or to a folder containing files that \ud83e\udd17 Datasets can understand.\"\n        ),\n    )\n    parser.add_argument(\n        \"--dataset_config_name\",\n        type=str,\n        default=None,\n        help=\"The config of the Dataset, leave as None if there's only one config.\",\n    )\n    parser.add_argument(\n        \"--train_data_dir\",\n        type=str,\n        default=None,\n        help=(\n            \"A folder containing the training data. Folder contents must follow the structure described in\"\n            \" https://huggingface.co/docs/datasets/image_dataset#imagefolder. In particular, a `metadata.jsonl` file\"\n            \" must exist to provide the captions for the images. Ignored if `dataset_name` is specified.\"\n        ),\n    )\n    parser.add_argument(\n        \"--image_column\", type=str, default=\"image\", help=\"The column of the dataset containing an image.\"\n    )\n    parser.add_argument(\n        \"--caption_column\",\n        type=str,\n        default=\"text\",\n        help=\"The column of the dataset containing a caption or a list of captions.\",\n    )\n    parser.add_argument(\n        \"--max_train_samples\",\n        type=int,\n        default=None,\n        help=(\n            \"For debugging purposes or quicker training, truncate the number of training examples to this \"\n            \"value if set.\"\n        ),\n    )\n    parser.add_argument(\n        \"--output_dir\",\n        type=str,\n        default=\"sd-model-finetuned\",\n        help=\"The output directory where the model predictions and checkpoints will be written.\",\n    )\n    parser.add_argument(\n        \"--cache_dir\",\n        type=str,\n        default=None,\n        help=\"The directory where the downloaded models and datasets will be stored.\",\n    )\n    parser.add_argument(\"--seed\", type=int, default=None, help=\"A seed for reproducible training.\")\n    parser.add_argument(\n        \"--resolution\",\n        type=int,\n        default=512,\n        help=(\n            \"The resolution for input images, all the images in the train/validation dataset will be resized to this\"\n            \" resolution\"\n        ),\n    )\n    parser.add_argument(\n        \"--center_crop\",\n        default=False,\n        action=\"store_true\",\n        help=(\n            \"Whether to center crop the input images to the resolution. If not set, the images will be randomly\"\n            \" cropped. The images will be resized to the resolution first before cropping.\"\n        ),\n    )\n    parser.add_argument(\n        \"--random_flip\",\n        action=\"store_true\",\n        help=\"whether to randomly flip images horizontally\",\n    )\n    parser.add_argument(\n        \"--train_batch_size\", type=int, default=16, help=\"Batch size (per device) for the training dataloader.\"\n    )\n    parser.add_argument(\"--num_train_epochs\", type=int, default=100)\n    parser.add_argument(\n        \"--max_train_steps\",\n        type=int,\n        default=None,\n        help=\"Total number of training steps to perform.  If provided, overrides num_train_epochs.\",\n    )\n    parser.add_argument(\n        \"--gradient_accumulation_steps\",\n        type=int,\n        default=1,\n        help=\"Number of updates steps to accumulate before performing a backward/update pass.\",\n    )\n    parser.add_argument(\n        \"--gradient_checkpointing\",\n        action=\"store_true\",\n        help=\"Whether or not to use gradient checkpointing to save memory at the expense of slower backward pass.\",\n    )\n    parser.add_argument(\n        \"--learning_rate\",\n        type=float,\n        default=1e-4,\n        help=\"Initial learning rate (after the potential warmup period) to use.\",\n    )\n    parser.add_argument(", "metadata": {"task_id": "huggingface_diffusers/97", "ground_truth": "        \"--scale_lr\",", "fpath_tuple": ["huggingface_diffusers", "examples", "text_to_image", "train_text_to_image.py"], "context_start_lineno": 0, "line_no": 176, "query_window": {"context": "        help=\"Total number of training steps to perform.  If provided, overrides num_train_epochs.\",\n    )\n    parser.add_argument(\n        \"--gradient_accumulation_steps\",\n        type=int,\n        default=1,\n        help=\"Number of updates steps to accumulate before performing a backward/update pass.\",\n    )\n    parser.add_argument(\n        \"--gradient_checkpointing\",\n        action=\"store_true\",\n        help=\"Whether or not to use gradient checkpointing to save memory at the expense of slower backward pass.\",\n    )\n    parser.add_argument(\n        \"--learning_rate\",\n        type=float,\n        default=1e-4,\n        help=\"Initial learning rate (after the potential warmup period) to use.\",\n    )\n    parser.add_argument(", "metadata": {"fpath_tuple": ["huggingface_diffusers", "examples", "text_to_image", "train_text_to_image.py"], "line_no": 176, "task_id": "huggingface_diffusers/97", "start_line_no": 156, "end_line_no": 176, "window_size": 20, "context_start_lineno": 0, "repo": "huggingface_diffusers"}}, "top_k_context": [{"context": "        help=\"Total number of training steps to perform.  If provided, overrides num_train_epochs.\",\n    )\n    parser.add_argument(\n        \"--gradient_accumulation_steps\",\n        type=int,\n        default=1,\n        help=\"Number of updates steps to accumulate before performing a backward/update pass.\",\n    )\n    parser.add_argument(\n        \"--gradient_checkpointing\",\n        action=\"store_true\",\n        help=\"Whether or not to use gradient checkpointing to save memory at the expense of slower backward pass.\",\n    )\n    parser.add_argument(\n        \"--learning_rate\",\n        type=float,\n        default=1e-4,\n        help=\"Initial learning rate (after the potential warmup period) to use.\",\n    )\n    parser.add_argument(", "metadata": [{"fpath_tuple": ["huggingface_diffusers", "examples", "text_to_image", "train_text_to_image.py"], "line_no": 166, "start_line_no": 156, "end_line_no": 176, "window_size": 20, "repo": "huggingface_diffusers", "slice_size": 10}, {"fpath_tuple": ["huggingface_diffusers", "examples", "text_to_image", "train_text_to_image_lora.py"], "line_no": 214, "start_line_no": 204, "end_line_no": 224, "window_size": 20, "repo": "huggingface_diffusers", "slice_size": 10}], "sim_score": 1.0}, {"context": "        type=int,\n        default=None,\n        help=\"Total number of training steps to perform.  If provided, overrides num_train_epochs.\",\n    )\n    parser.add_argument(\n        \"--gradient_accumulation_steps\",\n        type=int,\n        default=1,\n        help=\"Number of updates steps to accumulate before performing a backward/update pass.\",\n    )\n    parser.add_argument(\n        \"--gradient_checkpointing\",\n        action=\"store_true\",\n        help=\"Whether or not to use gradient checkpointing to save memory at the expense of slower backward pass.\",\n    )\n    parser.add_argument(\n        \"--learning_rate\",\n        type=float,\n        default=1e-4,\n        help=\"Initial learning rate (after the potential warmup period) to use.\",", "metadata": [{"fpath_tuple": ["huggingface_diffusers", "examples", "text_to_image", "train_text_to_image.py"], "line_no": 164, "start_line_no": 154, "end_line_no": 174, "window_size": 20, "repo": "huggingface_diffusers", "slice_size": 10}, {"fpath_tuple": ["huggingface_diffusers", "examples", "text_to_image", "train_text_to_image_lora.py"], "line_no": 212, "start_line_no": 202, "end_line_no": 222, "window_size": 20, "repo": "huggingface_diffusers", "slice_size": 10}], "sim_score": 0.9887640449438202}, {"context": "        default=5000,\n        help=\"Total number of training steps to perform.  If provided, overrides num_train_epochs.\",\n    )\n    parser.add_argument(\n        \"--gradient_accumulation_steps\",\n        type=int,\n        default=1,\n        help=\"Number of updates steps to accumulate before performing a backward/update pass.\",\n    )\n    parser.add_argument(\n        \"--gradient_checkpointing\",\n        action=\"store_true\",\n        help=\"Whether or not to use gradient checkpointing to save memory at the expense of slower backward pass.\",\n    )\n    parser.add_argument(\n        \"--learning_rate\",\n        type=float,\n        default=1e-4,\n        help=\"Initial learning rate (after the potential warmup period) to use.\",\n    )", "metadata": [{"fpath_tuple": ["huggingface_diffusers", "examples", "textual_inversion", "textual_inversion.py"], "line_no": 174, "start_line_no": 164, "end_line_no": 184, "window_size": 20, "repo": "huggingface_diffusers", "slice_size": 10}], "sim_score": 0.9887640449438202}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# ding/model/template/tests/test_qac_dist.py\n# --------------------------------------------------\n# T = 6\n# embedding_size = 32\n# action_shape_args = [(6, ), [\n#     1,\n# ]]\n# args = list(product(*[action_shape_args, ['regression', 'reparameterization']]))\n# \n# \n# @pytest.mark.unittest\n# @pytest.mark.parametrize('action_shape, actor_head_type', args)\n# class TestQACDIST:\n# \n#     def test_fcqac_dist(self, action_shape, actor_head_type):\n#         N = 32\n#         inputs = {'obs': torch.randn(B, N), 'action': torch.randn(B, squeeze(action_shape))}\n#         model = QACDIST(\n#             obs_shape=(N, ),\n#             action_shape=action_shape,\n#             actor_head_type=actor_head_type,\n#             critic_head_hidden_size=embedding_size,\n# --------------------------------------------------\n# the below code fragment can be found in:\n# ding/model/template/tests/test_qac_dist.py\n# --------------------------------------------------\n# ]]\n# args = list(product(*[action_shape_args, ['regression', 'reparameterization']]))\n# \n# \n# @pytest.mark.unittest\n# @pytest.mark.parametrize('action_shape, actor_head_type', args)\n# class TestQACDIST:\n# \n#     def test_fcqac_dist(self, action_shape, actor_head_type):\n#         N = 32\n#         inputs = {'obs': torch.randn(B, N), 'action': torch.randn(B, squeeze(action_shape))}\n#         model = QACDIST(\n#             obs_shape=(N, ),\n#             action_shape=action_shape,\n#             actor_head_type=actor_head_type,\n#             critic_head_hidden_size=embedding_size,\n#             actor_head_hidden_size=embedding_size,\n#         )\n#         # compute_q\n#         q = model(inputs, mode='compute_critic')\n# --------------------------------------------------\n# the below code fragment can be found in:\n# ding/model/template/tests/test_qac_dist.py\n# --------------------------------------------------\n# action_shape_args = [(6, ), [\n#     1,\n# ]]\n# args = list(product(*[action_shape_args, ['regression', 'reparameterization']]))\n# \n# \n# @pytest.mark.unittest\n# @pytest.mark.parametrize('action_shape, actor_head_type', args)\n# class TestQACDIST:\n# \n#     def test_fcqac_dist(self, action_shape, actor_head_type):\n#         N = 32\n#         inputs = {'obs': torch.randn(B, N), 'action': torch.randn(B, squeeze(action_shape))}\n#         model = QACDIST(\n#             obs_shape=(N, ),\n#             action_shape=action_shape,\n#             actor_head_type=actor_head_type,\n#             critic_head_hidden_size=embedding_size,\n#             actor_head_hidden_size=embedding_size,\n#         )\n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\nimport torch\nimport numpy as np\nimport pytest\nfrom itertools import product\n\nfrom ding.model.template import QAC\nfrom ding.torch_utils import is_differentiable\nfrom ding.utils import squeeze\n\nB = 4\nT = 6\nembedding_size = 32\naction_shape_args = [(6, ), [\n    1,\n]]\nargs = list(product(*[action_shape_args, [True, False], ['regression', 'reparameterization']]))\n\n\n@pytest.mark.unittest\n@pytest.mark.parametrize('action_shape, twin, actor_head_type', args)\nclass TestQAC:\n\n    def test_fcqac(self, action_shape, twin, actor_head_type):\n        N = 32\n        inputs = {'obs': torch.randn(B, N), 'action': torch.randn(B, squeeze(action_shape))}\n        model = QAC(\n            obs_shape=(N, ),\n            action_shape=action_shape,\n            actor_head_type=actor_head_type,\n            critic_head_hidden_size=embedding_size,", "metadata": {"task_id": "opendilab_ACE/115", "ground_truth": "            actor_head_hidden_size=embedding_size,", "fpath_tuple": ["opendilab_ACE", "ding", "model", "template", "tests", "test_qac.py"], "context_start_lineno": 0, "line_no": 30, "query_window": {"context": "T = 6\nembedding_size = 32\naction_shape_args = [(6, ), [\n    1,\n]]\nargs = list(product(*[action_shape_args, [True, False], ['regression', 'reparameterization']]))\n\n\n@pytest.mark.unittest\n@pytest.mark.parametrize('action_shape, twin, actor_head_type', args)\nclass TestQAC:\n\n    def test_fcqac(self, action_shape, twin, actor_head_type):\n        N = 32\n        inputs = {'obs': torch.randn(B, N), 'action': torch.randn(B, squeeze(action_shape))}\n        model = QAC(\n            obs_shape=(N, ),\n            action_shape=action_shape,\n            actor_head_type=actor_head_type,\n            critic_head_hidden_size=embedding_size,", "metadata": {"fpath_tuple": ["opendilab_ACE", "ding", "model", "template", "tests", "test_qac.py"], "line_no": 30, "task_id": "opendilab_ACE/115", "start_line_no": 10, "end_line_no": 30, "window_size": 20, "context_start_lineno": 0, "repo": "opendilab_ACE"}}, "top_k_context": [{"context": "T = 6\nembedding_size = 32\naction_shape_args = [(6, ), [\n    1,\n]]\nargs = list(product(*[action_shape_args, ['regression', 'reparameterization']]))\n\n\n@pytest.mark.unittest\n@pytest.mark.parametrize('action_shape, actor_head_type', args)\nclass TestQACDIST:\n\n    def test_fcqac_dist(self, action_shape, actor_head_type):\n        N = 32\n        inputs = {'obs': torch.randn(B, N), 'action': torch.randn(B, squeeze(action_shape))}\n        model = QACDIST(\n            obs_shape=(N, ),\n            action_shape=action_shape,\n            actor_head_type=actor_head_type,\n            critic_head_hidden_size=embedding_size,", "metadata": [{"fpath_tuple": ["opendilab_ACE", "ding", "model", "template", "tests", "test_qac_dist.py"], "line_no": 20, "start_line_no": 10, "end_line_no": 30, "window_size": 20, "repo": "opendilab_ACE", "slice_size": 10}], "sim_score": 0.9278350515463918}, {"context": "action_shape_args = [(6, ), [\n    1,\n]]\nargs = list(product(*[action_shape_args, ['regression', 'reparameterization']]))\n\n\n@pytest.mark.unittest\n@pytest.mark.parametrize('action_shape, actor_head_type', args)\nclass TestQACDIST:\n\n    def test_fcqac_dist(self, action_shape, actor_head_type):\n        N = 32\n        inputs = {'obs': torch.randn(B, N), 'action': torch.randn(B, squeeze(action_shape))}\n        model = QACDIST(\n            obs_shape=(N, ),\n            action_shape=action_shape,\n            actor_head_type=actor_head_type,\n            critic_head_hidden_size=embedding_size,\n            actor_head_hidden_size=embedding_size,\n        )", "metadata": [{"fpath_tuple": ["opendilab_ACE", "ding", "model", "template", "tests", "test_qac_dist.py"], "line_no": 22, "start_line_no": 12, "end_line_no": 32, "window_size": 20, "repo": "opendilab_ACE", "slice_size": 10}], "sim_score": 0.8979591836734694}, {"context": "\nB = 4\nT = 6\nembedding_size = 32\naction_shape_args = [(6, ), [\n    1,\n]]\nargs = list(product(*[action_shape_args, ['regression', 'reparameterization']]))\n\n\n@pytest.mark.unittest\n@pytest.mark.parametrize('action_shape, actor_head_type', args)\nclass TestQACDIST:\n\n    def test_fcqac_dist(self, action_shape, actor_head_type):\n        N = 32\n        inputs = {'obs': torch.randn(B, N), 'action': torch.randn(B, squeeze(action_shape))}\n        model = QACDIST(\n            obs_shape=(N, ),\n            action_shape=action_shape,", "metadata": [{"fpath_tuple": ["opendilab_ACE", "ding", "model", "template", "tests", "test_qac_dist.py"], "line_no": 18, "start_line_no": 8, "end_line_no": 28, "window_size": 20, "repo": "opendilab_ACE", "slice_size": 10}], "sim_score": 0.8877551020408163}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# scripts/convert_kakao_brain_unclip_to_diffusers.py\n# --------------------------------------------------\n#     # <original>.c_qkv -> <diffusers>.{to_q, to_k, to_v}\n#     [q_weight, k_weight, v_weight], [q_bias, k_bias, v_bias] = split_attentions(\n#         weight=checkpoint[f\"{original_attention_prefix}.c_qkv.weight\"],\n#         bias=checkpoint[f\"{original_attention_prefix}.c_qkv.bias\"],\n#         split=3,\n#         chunk_size=attention_head_dim,\n#     )\n# \n#     diffusers_checkpoint.update(\n#         {\n#             f\"{diffusers_attention_prefix}.to_q.weight\": q_weight,\n#             f\"{diffusers_attention_prefix}.to_q.bias\": q_bias,\n#             f\"{diffusers_attention_prefix}.to_k.weight\": k_weight,\n#             f\"{diffusers_attention_prefix}.to_k.bias\": k_bias,\n#             f\"{diffusers_attention_prefix}.to_v.weight\": v_weight,\n#             f\"{diffusers_attention_prefix}.to_v.bias\": v_bias,\n#         }\n#     )\n# \n#     # <original>.c_proj -> <diffusers>.to_out.0\n# --------------------------------------------------\n# the below code fragment can be found in:\n# scripts/convert_kakao_brain_unclip_to_diffusers.py\n# --------------------------------------------------\n#         weight=checkpoint[f\"{original_attention_prefix}.c_qkv.weight\"],\n#         bias=checkpoint[f\"{original_attention_prefix}.c_qkv.bias\"],\n#         split=3,\n#         chunk_size=attention_head_dim,\n#     )\n# \n#     diffusers_checkpoint.update(\n#         {\n#             f\"{diffusers_attention_prefix}.to_q.weight\": q_weight,\n#             f\"{diffusers_attention_prefix}.to_q.bias\": q_bias,\n#             f\"{diffusers_attention_prefix}.to_k.weight\": k_weight,\n#             f\"{diffusers_attention_prefix}.to_k.bias\": k_bias,\n#             f\"{diffusers_attention_prefix}.to_v.weight\": v_weight,\n#             f\"{diffusers_attention_prefix}.to_v.bias\": v_bias,\n#         }\n#     )\n# \n#     # <original>.c_proj -> <diffusers>.to_out.0\n#     diffusers_checkpoint.update(\n#         {\n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\n diffusers_resnet_prefix = f\"up_blocks.{diffusers_up_block_idx}.resnets\"\n    original_up_block_prefix = f\"{original_unet_prefix}.output_blocks\"\n\n    up_block = model.up_blocks[diffusers_up_block_idx]\n\n    num_resnets = len(up_block.resnets)\n\n    if up_block.upsamplers is None:\n        upsampler = False\n    else:\n        assert len(up_block.upsamplers) == 1\n        upsampler = True\n        # The upsample block is also a resnet\n        num_resnets += 1\n\n    has_attentions = hasattr(up_block, \"attentions\")\n\n    for resnet_idx_inc in range(num_resnets):\n        if upsampler and resnet_idx_inc == num_resnets - 1:\n            # this is an upsample block\n            if has_attentions:\n                # There is a middle attention block that we skip\n                original_resnet_block_idx = 2\n            else:\n                original_resnet_block_idx = 1\n\n            # we add the `minus 1` because the last two resnets are stuck together in the same output block\n            full_resnet_prefix = (\n                f\"{original_up_block_prefix}.{original_up_block_idx + resnet_idx_inc - 1}.{original_resnet_block_idx}\"\n            )\n\n            full_diffusers_resnet_prefix = f\"up_blocks.{diffusers_up_block_idx}.upsamplers.0\"\n        else:\n            # this is a regular resnet block\n            full_resnet_prefix = f\"{original_up_block_prefix}.{original_up_block_idx + resnet_idx_inc}.0\"\n            full_diffusers_resnet_prefix = f\"{diffusers_resnet_prefix}.{resnet_idx_inc}\"\n\n        diffusers_checkpoint.update(\n            resnet_to_diffusers_checkpoint(\n                checkpoint, resnet_prefix=full_resnet_prefix, diffusers_resnet_prefix=full_diffusers_resnet_prefix\n            )\n        )\n\n    if has_attentions:\n        num_attentions = len(up_block.attentions)\n        diffusers_attention_prefix = f\"up_blocks.{diffusers_up_block_idx}.attentions\"\n\n        for attention_idx_inc in range(num_attentions):\n            full_attention_prefix = f\"{original_up_block_prefix}.{original_up_block_idx + attention_idx_inc}.1\"\n            full_diffusers_attention_prefix = f\"{diffusers_attention_prefix}.{attention_idx_inc}\"\n\n            diffusers_checkpoint.update(\n                attention_to_diffusers_checkpoint(\n                    checkpoint,\n                    attention_prefix=full_attention_prefix,\n                    diffusers_attention_prefix=full_diffusers_attention_prefix,\n                    num_head_channels=num_head_channels,\n                )\n            )\n\n    num_original_down_blocks = num_resnets - 1 if upsampler else num_resnets\n\n    return diffusers_checkpoint, num_original_down_blocks\n\n\ndef resnet_to_diffusers_checkpoint(checkpoint, *, diffusers_resnet_prefix, resnet_prefix):\n    diffusers_checkpoint = {\n        f\"{diffusers_resnet_prefix}.norm1.weight\": checkpoint[f\"{resnet_prefix}.in_layers.0.weight\"],\n        f\"{diffusers_resnet_prefix}.norm1.bias\": checkpoint[f\"{resnet_prefix}.in_layers.0.bias\"],\n        f\"{diffusers_resnet_prefix}.conv1.weight\": checkpoint[f\"{resnet_prefix}.in_layers.2.weight\"],\n        f\"{diffusers_resnet_prefix}.conv1.bias\": checkpoint[f\"{resnet_prefix}.in_layers.2.bias\"],\n        f\"{diffusers_resnet_prefix}.time_emb_proj.weight\": checkpoint[f\"{resnet_prefix}.emb_layers.1.weight\"],\n        f\"{diffusers_resnet_prefix}.time_emb_proj.bias\": checkpoint[f\"{resnet_prefix}.emb_layers.1.bias\"],\n        f\"{diffusers_resnet_prefix}.norm2.weight\": checkpoint[f\"{resnet_prefix}.out_layers.0.weight\"],\n        f\"{diffusers_resnet_prefix}.norm2.bias\": checkpoint[f\"{resnet_prefix}.out_layers.0.bias\"],\n        f\"{diffusers_resnet_prefix}.conv2.weight\": checkpoint[f\"{resnet_prefix}.out_layers.3.weight\"],\n        f\"{diffusers_resnet_prefix}.conv2.bias\": checkpoint[f\"{resnet_prefix}.out_layers.3.bias\"],\n    }\n\n    skip_connection_prefix = f\"{resnet_prefix}.skip_connection\"\n\n    if f\"{skip_connection_prefix}.weight\" in checkpoint:\n        diffusers_checkpoint.update(\n            {\n                f\"{diffusers_resnet_prefix}.conv_shortcut.weight\": checkpoint[f\"{skip_connection_prefix}.weight\"],\n                f\"{diffusers_resnet_prefix}.conv_shortcut.bias\": checkpoint[f\"{skip_connection_prefix}.bias\"],\n            }\n        )\n\n    return diffusers_checkpoint\n\n\ndef attention_to_diffusers_checkpoint(checkpoint, *, diffusers_attention_prefix, attention_prefix, num_head_channels):\n    diffusers_checkpoint = {}\n\n    # <original>.norm -> <diffusers>.group_norm\n    diffusers_checkpoint.update(\n        {\n            f\"{diffusers_attention_prefix}.group_norm.weight\": checkpoint[f\"{attention_prefix}.norm.weight\"],\n            f\"{diffusers_attention_prefix}.group_norm.bias\": checkpoint[f\"{attention_prefix}.norm.bias\"],\n        }\n    )\n\n    # <original>.qkv -> <diffusers>.{query, key, value}\n    [q_weight, k_weight, v_weight], [q_bias, k_bias, v_bias] = split_attentions(\n        weight=checkpoint[f\"{attention_prefix}.qkv.weight\"][:, :, 0],\n        bias=checkpoint[f\"{attention_prefix}.qkv.bias\"],\n        split=3,\n        chunk_size=num_head_channels,\n    )\n\n    diffusers_checkpoint.update(\n        {\n            f\"{diffusers_attention_prefix}.to_q.weight\": q_weight,\n            f\"{diffusers_attention_prefix}.to_q.bias\": q_bias,\n            f\"{diffusers_attention_prefix}.to_k.weight\": k_weight,\n            f\"{diffusers_attention_prefix}.to_k.bias\": k_bias,\n            f\"{diffusers_attention_prefix}.to_v.weight\": v_weight,\n            f\"{diffusers_attention_prefix}.to_v.bias\": v_bias,\n        }\n    )\n\n    # <original>.encoder_kv -> <diffusers>.{context_key, context_value}\n    [encoder_k_weight, encoder_v_weight], [encoder_k_bias, encoder_v_bias] = split_attentions(\n        weight=checkpoint[f\"{attention_prefix}.encoder_kv.weight\"][:, :, 0],\n        bias=checkpoint[f\"{attention_prefix}.encoder_kv.bias\"],\n        split=2,\n        chunk_size=num_head_channels,\n    )\n\n    diffusers_checkpoint.update(\n        {\n            f\"{diffusers_attention_prefix}.add_k_proj.weight\": encoder_k_weight,", "metadata": {"task_id": "huggingface_diffusers/21", "ground_truth": "            f\"{diffusers_attention_prefix}.add_k_proj.bias\": encoder_k_bias,", "fpath_tuple": ["huggingface_diffusers", "scripts", "convert_kakao_brain_unclip_to_diffusers.py"], "context_start_lineno": 734, "line_no": 867, "query_window": {"context": "            f\"{diffusers_attention_prefix}.to_q.weight\": q_weight,\n            f\"{diffusers_attention_prefix}.to_q.bias\": q_bias,\n            f\"{diffusers_attention_prefix}.to_k.weight\": k_weight,\n            f\"{diffusers_attention_prefix}.to_k.bias\": k_bias,\n            f\"{diffusers_attention_prefix}.to_v.weight\": v_weight,\n            f\"{diffusers_attention_prefix}.to_v.bias\": v_bias,\n        }\n    )\n\n    # <original>.encoder_kv -> <diffusers>.{context_key, context_value}\n    [encoder_k_weight, encoder_v_weight], [encoder_k_bias, encoder_v_bias] = split_attentions(\n        weight=checkpoint[f\"{attention_prefix}.encoder_kv.weight\"][:, :, 0],\n        bias=checkpoint[f\"{attention_prefix}.encoder_kv.bias\"],\n        split=2,\n        chunk_size=num_head_channels,\n    )\n\n    diffusers_checkpoint.update(\n        {\n            f\"{diffusers_attention_prefix}.add_k_proj.weight\": encoder_k_weight,", "metadata": {"fpath_tuple": ["huggingface_diffusers", "scripts", "convert_kakao_brain_unclip_to_diffusers.py"], "line_no": 867, "task_id": "huggingface_diffusers/21", "start_line_no": 847, "end_line_no": 867, "window_size": 20, "context_start_lineno": 734, "repo": "huggingface_diffusers"}}, "top_k_context": [{"context": "    # <original>.c_qkv -> <diffusers>.{to_q, to_k, to_v}\n    [q_weight, k_weight, v_weight], [q_bias, k_bias, v_bias] = split_attentions(\n        weight=checkpoint[f\"{original_attention_prefix}.c_qkv.weight\"],\n        bias=checkpoint[f\"{original_attention_prefix}.c_qkv.bias\"],\n        split=3,\n        chunk_size=attention_head_dim,\n    )\n\n    diffusers_checkpoint.update(\n        {\n            f\"{diffusers_attention_prefix}.to_q.weight\": q_weight,\n            f\"{diffusers_attention_prefix}.to_q.bias\": q_bias,\n            f\"{diffusers_attention_prefix}.to_k.weight\": k_weight,\n            f\"{diffusers_attention_prefix}.to_k.bias\": k_bias,\n            f\"{diffusers_attention_prefix}.to_v.weight\": v_weight,\n            f\"{diffusers_attention_prefix}.to_v.bias\": v_bias,\n        }\n    )\n\n    # <original>.c_proj -> <diffusers>.to_out.0", "metadata": [{"fpath_tuple": ["huggingface_diffusers", "scripts", "convert_kakao_brain_unclip_to_diffusers.py"], "line_no": 186, "start_line_no": 176, "end_line_no": 196, "window_size": 20, "repo": "huggingface_diffusers", "slice_size": 10}], "sim_score": 0.7317073170731707}, {"context": "    diffusers_checkpoint = {}\n\n    # <original>.c_qkv -> <diffusers>.{to_q, to_k, to_v}\n    [q_weight, k_weight, v_weight], [q_bias, k_bias, v_bias] = split_attentions(\n        weight=checkpoint[f\"{original_attention_prefix}.c_qkv.weight\"],\n        bias=checkpoint[f\"{original_attention_prefix}.c_qkv.bias\"],\n        split=3,\n        chunk_size=attention_head_dim,\n    )\n\n    diffusers_checkpoint.update(\n        {\n            f\"{diffusers_attention_prefix}.to_q.weight\": q_weight,\n            f\"{diffusers_attention_prefix}.to_q.bias\": q_bias,\n            f\"{diffusers_attention_prefix}.to_k.weight\": k_weight,\n            f\"{diffusers_attention_prefix}.to_k.bias\": k_bias,\n            f\"{diffusers_attention_prefix}.to_v.weight\": v_weight,\n            f\"{diffusers_attention_prefix}.to_v.bias\": v_bias,\n        }\n    )", "metadata": [{"fpath_tuple": ["huggingface_diffusers", "scripts", "convert_kakao_brain_unclip_to_diffusers.py"], "line_no": 184, "start_line_no": 174, "end_line_no": 194, "window_size": 20, "repo": "huggingface_diffusers", "slice_size": 10}], "sim_score": 0.7160493827160493}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# torchrl/objectives/reinforce.py\n# --------------------------------------------------\n#         gamma: float = 0.99,\n#         advantage_key: str = \"advantage\",\n#         value_target_key: str = \"value_target\",\n#         loss_critic_type: str = \"smooth_l1\",\n#     ) -> None:\n#         super().__init__()\n# \n#         self.delay_value = delay_value\n#         self.advantage_key = advantage_key\n#         self.value_target_key = value_target_key\n#         self.loss_critic_type = loss_critic_type\n#         self.register_buffer(\"gamma\", torch.tensor(gamma))\n# \n#         # Actor\n#         self.convert_to_functional(\n#             actor_network,\n#             \"actor_network\",\n#             create_target_params=False,\n#         )\n# \n# --------------------------------------------------\n# the below code fragment can be found in:\n# torchrl/objectives/ddpg.py\n# --------------------------------------------------\n#         loss_function: str = \"l2\",\n#         delay_actor: bool = False,\n#         delay_value: bool = False,\n#     ) -> None:\n#         super().__init__()\n#         self.delay_actor = delay_actor\n#         self.delay_value = delay_value\n# \n#         actor_critic = ActorCriticWrapper(actor_network, value_network)\n#         params = make_functional(actor_critic)\n#         self.actor_critic = deepcopy(actor_critic)\n#         repopulate_module(actor_network, params[\"module\", \"0\"])\n#         repopulate_module(value_network, params[\"module\", \"1\"])\n# \n#         self.convert_to_functional(\n#             actor_network,\n#             \"actor_network\",\n#             create_target_params=self.delay_actor,\n#         )\n#         self.convert_to_functional(\n# --------------------------------------------------\n# the below code fragment can be found in:\n# torchrl/objectives/ddpg.py\n# --------------------------------------------------\n#         value_network: SafeModule,\n#         gamma: float,\n#         loss_function: str = \"l2\",\n#         delay_actor: bool = False,\n#         delay_value: bool = False,\n#     ) -> None:\n#         super().__init__()\n#         self.delay_actor = delay_actor\n#         self.delay_value = delay_value\n# \n#         actor_critic = ActorCriticWrapper(actor_network, value_network)\n#         params = make_functional(actor_critic)\n#         self.actor_critic = deepcopy(actor_critic)\n#         repopulate_module(actor_network, params[\"module\", \"0\"])\n#         repopulate_module(value_network, params[\"module\", \"1\"])\n# \n#         self.convert_to_functional(\n#             actor_network,\n#             \"actor_network\",\n#             create_target_params=self.delay_actor,\n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\n# Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom typing import Union\n\nimport torch\nfrom tensordict import TensorDict\nfrom tensordict.tensordict import TensorDictBase\nfrom torch import nn\n\nfrom torchrl.envs.utils import step_mdp\nfrom torchrl.modules import DistributionalQValueActor, QValueActor\nfrom torchrl.modules.tensordict_module.common import ensure_tensordict_compatible\n\nfrom .common import LossModule\nfrom .utils import distance_loss, next_state_value\n\n\nclass DQNLoss(LossModule):\n    \"\"\"The DQN Loss class.\n\n    Args:\n        value_network (QValueActor or nn.Module): a Q value operator.\n        gamma (scalar): a discount factor for return computation.\n        loss_function (str): loss function for the value discrepancy. Can be one of \"l1\", \"l2\" or \"smooth_l1\".\n        delay_value (bool, optional): whether to duplicate the value network into a new target value network to\n            create a double DQN. Default is :obj:`False`.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        value_network: Union[QValueActor, nn.Module],\n        gamma: float,\n        loss_function: str = \"l2\",\n        priority_key: str = \"td_error\",\n        delay_value: bool = False,\n    ) -> None:\n\n        super().__init__()\n        self.delay_value = delay_value\n\n        value_network = ensure_tensordict_compatible(\n            module=value_network, wrapper_type=QValueActor\n        )\n\n        self.convert_to_functional(\n            value_network,", "metadata": {"task_id": "pytorch_rl/2", "ground_truth": "            \"value_network\",", "fpath_tuple": ["pytorch_rl", "torchrl", "objectives", "dqn.py"], "context_start_lineno": 0, "line_no": 50, "query_window": {"context": "    \"\"\"\n\n    def __init__(\n        self,\n        value_network: Union[QValueActor, nn.Module],\n        gamma: float,\n        loss_function: str = \"l2\",\n        priority_key: str = \"td_error\",\n        delay_value: bool = False,\n    ) -> None:\n\n        super().__init__()\n        self.delay_value = delay_value\n\n        value_network = ensure_tensordict_compatible(\n            module=value_network, wrapper_type=QValueActor\n        )\n\n        self.convert_to_functional(\n            value_network,", "metadata": {"fpath_tuple": ["pytorch_rl", "torchrl", "objectives", "dqn.py"], "line_no": 50, "task_id": "pytorch_rl/2", "start_line_no": 30, "end_line_no": 50, "window_size": 20, "context_start_lineno": 0, "repo": "pytorch_rl"}}, "top_k_context": [{"context": "        self,\n        actor_network: SafeModule,\n        value_network: SafeModule,\n        gamma: float,\n        loss_function: str = \"l2\",\n        delay_actor: bool = False,\n        delay_value: bool = False,\n    ) -> None:\n        super().__init__()\n        self.delay_actor = delay_actor\n        self.delay_value = delay_value\n\n        actor_critic = ActorCriticWrapper(actor_network, value_network)\n        params = make_functional(actor_critic)\n        self.actor_critic = deepcopy(actor_critic)\n        repopulate_module(actor_network, params[\"module\", \"0\"])\n        repopulate_module(value_network, params[\"module\", \"1\"])\n\n        self.convert_to_functional(\n            actor_network,", "metadata": [{"fpath_tuple": ["pytorch_rl", "torchrl", "objectives", "ddpg.py"], "line_no": 50, "start_line_no": 40, "end_line_no": 60, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}], "sim_score": 0.4659090909090909}, {"context": "        value_network: SafeModule,\n        gamma: float,\n        loss_function: str = \"l2\",\n        delay_actor: bool = False,\n        delay_value: bool = False,\n    ) -> None:\n        super().__init__()\n        self.delay_actor = delay_actor\n        self.delay_value = delay_value\n\n        actor_critic = ActorCriticWrapper(actor_network, value_network)\n        params = make_functional(actor_critic)\n        self.actor_critic = deepcopy(actor_critic)\n        repopulate_module(actor_network, params[\"module\", \"0\"])\n        repopulate_module(value_network, params[\"module\", \"1\"])\n\n        self.convert_to_functional(\n            actor_network,\n            \"actor_network\",\n            create_target_params=self.delay_actor,", "metadata": [{"fpath_tuple": ["pytorch_rl", "torchrl", "objectives", "ddpg.py"], "line_no": 52, "start_line_no": 42, "end_line_no": 62, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}], "sim_score": 0.45652173913043476}, {"context": "        critic: Optional[SafeModule] = None,\n        delay_value: bool = False,\n        gamma: float = 0.99,\n        advantage_key: str = \"advantage\",\n        value_target_key: str = \"value_target\",\n        loss_critic_type: str = \"smooth_l1\",\n    ) -> None:\n        super().__init__()\n\n        self.delay_value = delay_value\n        self.advantage_key = advantage_key\n        self.value_target_key = value_target_key\n        self.loss_critic_type = loss_critic_type\n        self.register_buffer(\"gamma\", torch.tensor(gamma))\n\n        # Actor\n        self.convert_to_functional(\n            actor_network,\n            \"actor_network\",\n            create_target_params=False,", "metadata": [{"fpath_tuple": ["pytorch_rl", "torchrl", "objectives", "reinforce.py"], "line_no": 36, "start_line_no": 26, "end_line_no": 46, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}], "sim_score": 0.4536082474226804}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# ding/policy/impala.py\n# --------------------------------------------------\n#         Returns:\n#                - transition (:obj:`dict`): Dict type transition data, including at least ['obs','next_obs', 'logit',\\\n#                'action','reward', 'done']\n#         \"\"\"\n#         transition = {\n#             'obs': obs,\n#             'next_obs': timestep.obs,\n#             'logit': policy_output['logit'],\n#             'action': policy_output['action'],\n#             'reward': timestep.reward,\n#             'done': timestep.done,\n#         }\n#         return transition\n# \n#     def _init_eval(self) -> None:\n#         r\"\"\"\n#         Overview:\n#             Evaluate mode init method. Called by ``self.__init__``, initialize eval_model,\n#             and use argmax_sample to choose action.\n#         \"\"\"\n# --------------------------------------------------\n# the below code fragment can be found in:\n# ding/policy/collaq.py\n# --------------------------------------------------\n#         Returns:\n#             - transition (:obj:`dict`): Dict type transition data.\n#         \"\"\"\n#         transition = {\n#             'obs': obs,\n#             'next_obs': timestep.obs,\n#             'prev_state': model_output['prev_state'],\n#             'action': model_output['action'],\n#             'agent_colla_alone_q': model_output['agent_colla_alone_q'],\n#             'reward': timestep.reward,\n#             'done': timestep.done,\n#         }\n#         return transition\n# \n#     def _init_eval(self) -> None:\n#         r\"\"\"\n#         Overview:\n#             Evaluate mode init method. Called by ``self.__init__``.\n#             Init eval model with argmax strategy and the hidden_state plugin.\n#         \"\"\"\n# --------------------------------------------------\n# the below code fragment can be found in:\n# ding/policy/coma.py\n# --------------------------------------------------\n#                 (here 'obs' indicates obs after env step).\n#         Returns:\n#             - transition (:obj:`dict`): Dict type transition data.\n#         \"\"\"\n#         transition = {\n#             'obs': obs,\n#             'next_obs': timestep.obs,\n#             'prev_state': model_output['prev_state'],\n#             'action': model_output['action'],\n#             'reward': timestep.reward,\n#             'done': timestep.done,\n#         }\n#         return transition\n# \n#     def _init_eval(self) -> None:\n#         r\"\"\"\n#         Overview:\n#             Evaluate mode init method. Called by ``self.__init__``.\n#             Init eval model with argmax strategy and hidden_state plugin.\n#         \"\"\"\n# --------------------------------------------------\n# the below code fragment can be found in:\n# ding/policy/qtran.py\n# --------------------------------------------------\n#         Returns:\n#             - transition (:obj:`dict`): Dict type transition data, including 'obs', 'next_obs', 'prev_state',\\\n#                 'action', 'reward', 'done'\n#         \"\"\"\n#         transition = {\n#             'obs': obs,\n#             'next_obs': timestep.obs,\n#             'prev_state': model_output['prev_state'],\n#             'action': model_output['action'],\n#             'reward': timestep.reward,\n#             'done': timestep.done,\n#         }\n#         return transition\n# \n#     def _init_eval(self) -> None:\n#         r\"\"\"\n#         Overview:\n#             Evaluate mode init method. Called by ``self.__init__``.\n#             Init eval model with argmax strategy and the hidden_state plugin.\n#         \"\"\"\n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\nstate'][0])\n        inputs = {'obs': data['obs'], 'action': data['action']}\n        total_q = self._learn_model.forward(inputs, single_step=False)['total_q']\n\n        if self._cfg.learn.double_q:\n            next_inputs = {'obs': data['next_obs']}\n            self._learn_model.reset(state=data['prev_state'][1])\n            logit_detach = self._learn_model.forward(next_inputs, single_step=False)['logit'].clone().detach()\n            next_inputs = {'obs': data['next_obs'], 'action': logit_detach.argmax(dim=-1)}\n        else:\n            next_inputs = {'obs': data['next_obs']}\n        with torch.no_grad():\n            target_total_q = self._target_model.forward(next_inputs, single_step=False)['total_q']\n\n        with torch.no_grad():\n            if data['done'] is not None:\n                target_v = self._gamma * (1 - data['done']) * target_total_q + data['reward']\n            else:\n                target_v = self._gamma * target_total_q + data['reward']\n\n        data = v_1step_td_data(total_q, target_total_q, data['reward'], data['done'], data['weight'])\n        loss, td_error_per_sample = v_1step_td_error(data, self._gamma)\n        # ====================\n        # Q-mix update\n        # ====================\n        self._optimizer.zero_grad()\n        loss.backward()\n        grad_norm = torch.nn.utils.clip_grad_norm_(self._model.parameters(), self._cfg.learn.clip_value)\n        self._optimizer.step()\n        # =============\n        # after update\n        # =============\n        self._target_model.update(self._learn_model.state_dict())\n        return {\n            'cur_lr': self._optimizer.defaults['lr'],\n            'total_loss': loss.item(),\n            'total_q': total_q.mean().item() / self._cfg.model.agent_num,\n            'target_reward_total_q': target_v.mean().item() / self._cfg.model.agent_num,\n            'target_total_q': target_total_q.mean().item() / self._cfg.model.agent_num,\n            'grad_norm': grad_norm,\n        }\n\n    def _reset_learn(self, data_id: Optional[List[int]] = None) -> None:\n        r\"\"\"\n        Overview:\n            Reset learn model to the state indicated by data_id\n        Arguments:\n            - data_id (:obj:`Optional[List[int]]`): The id that store the state and we will reset\\\n                the model state to the state indicated by data_id\n        \"\"\"\n        self._learn_model.reset(data_id=data_id)\n\n    def _state_dict_learn(self) -> Dict[str, Any]:\n        r\"\"\"\n        Overview:\n            Return the state_dict of learn mode, usually including model and optimizer.\n        Returns:\n            - state_dict (:obj:`Dict[str, Any]`): the dict of current policy learn state, for saving and restoring.\n        \"\"\"\n        return {\n            'model': self._learn_model.state_dict(),\n            'optimizer': self._optimizer.state_dict(),\n        }\n\n    def _load_state_dict_learn(self, state_dict: Dict[str, Any]) -> None:\n        r\"\"\"\n        Overview:\n            Load the state_dict variable into policy learn mode.\n        Arguments:\n            - state_dict (:obj:`Dict[str, Any]`): the dict of policy learn state saved before.\n        .. tip::\n            If you want to only load some parts of model, you can simply set the ``strict`` argument in \\\n            load_state_dict to ``False``, or refer to ``ding.torch_utils.checkpoint_helper`` for more \\\n            complicated operation.\n        \"\"\"\n        self._learn_model.load_state_dict(state_dict['model'])\n        self._optimizer.load_state_dict(state_dict['optimizer'])\n\n    def _init_collect(self) -> None:\n        r\"\"\"\n        Overview:\n            Collect mode init method. Called by ``self.__init__``.\n            Init traj and unroll length, collect model.\n            Enable the eps_greedy_sample and the hidden_state plugin.\n        \"\"\"\n        self._unroll_len = self._cfg.collect.unroll_len\n        self._collect_model = model_wrap(\n            self._model,\n            wrapper_name='hidden_state',\n            state_num=self._cfg.collect.env_num,\n            save_prev_state=True,\n            init_fn=lambda: [None for _ in range(self._cfg.model.agent_num)]\n        )\n        self._collect_model = model_wrap(self._collect_model, wrapper_name='eps_greedy_sample')\n        self._collect_model.reset()\n\n    def _forward_collect(self, data: dict, eps: float) -> dict:\n        r\"\"\"\n        Overview:\n            Forward function for collect mode with eps_greedy\n        Arguments:\n            - data (:obj:`dict`): Dict type data, including at least ['obs'].\n            - eps (:obj:`float`): epsilon value for exploration, which is decayed by collected env step.\n        Returns:\n            - data (:obj:`dict`): The collected data\n        \"\"\"\n        data_id = list(data.keys())\n        data = default_collate(list(data.values()))\n        if self._cuda:\n            data = to_device(data, self._device)\n        data = {'obs': data}\n        self._collect_model.eval()\n        with torch.no_grad():\n            output = self._collect_model.forward(data, eps=eps, data_id=data_id)\n        if self._cuda:\n            output = to_device(output, 'cpu')\n        output = default_decollate(output)\n        return {i: d for i, d in zip(data_id, output)}\n\n    def _reset_collect(self, data_id: Optional[List[int]] = None) -> None:\n        r\"\"\"\n        Overview:\n            Reset collect model to the state indicated by data_id\n        Arguments:\n            - data_id (:obj:`Optional[List[int]]`): The id that store the state and we will reset\\\n                the model state to the state indicated by data_id\n        \"\"\"\n        self._collect_model.reset(data_id=data_id)\n\n    def _process_transition(self, obs: Any, model_output: dict, timestep: namedtuple) -> dict:\n        r\"\"\"\n        Overview:\n            Generate dict type transition data from inputs.\n        Arguments:\n            - obs (:obj:`Any`): Env observation\n            - model_output (:obj:`dict`): Output of collect model, including at least ['action', 'prev_state']\n            - timestep (:obj:`namedtuple`): Output after env step, including at least ['obs', 'reward', 'done']\\\n                (here 'obs' indicates obs after env step).\n        Returns:\n            - transition (:obj:`dict`): Dict type transition data, including 'obs', 'next_obs', 'prev_state',\\\n                'action', 'reward', 'done'\n        \"\"\"\n        transition = {\n            'obs': obs,\n            'next_obs': timestep.obs,\n            'prev_state': model_output['prev_state'],\n            'action': model_output['action'],\n            'reward': timestep.reward,\n            'done': timestep.done,\n        }\n        return transition\n\n    def _init_eval(self) -> None:\n        r\"\"\"\n        Overview:\n            Evaluate mode init method. Called by ``self.__init__``.", "metadata": {"task_id": "opendilab_ACE/123", "ground_truth": "            Init eval model with argmax strategy and the hidden_state plugin.", "fpath_tuple": ["opendilab_ACE", "ding", "policy", "qmix.py"], "context_start_lineno": 193, "line_no": 349, "query_window": {"context": "            - timestep (:obj:`namedtuple`): Output after env step, including at least ['obs', 'reward', 'done']\\\n                (here 'obs' indicates obs after env step).\n        Returns:\n            - transition (:obj:`dict`): Dict type transition data, including 'obs', 'next_obs', 'prev_state',\\\n                'action', 'reward', 'done'\n        \"\"\"\n        transition = {\n            'obs': obs,\n            'next_obs': timestep.obs,\n            'prev_state': model_output['prev_state'],\n            'action': model_output['action'],\n            'reward': timestep.reward,\n            'done': timestep.done,\n        }\n        return transition\n\n    def _init_eval(self) -> None:\n        r\"\"\"\n        Overview:\n            Evaluate mode init method. Called by ``self.__init__``.", "metadata": {"fpath_tuple": ["opendilab_ACE", "ding", "policy", "qmix.py"], "line_no": 349, "task_id": "opendilab_ACE/123", "start_line_no": 329, "end_line_no": 349, "window_size": 20, "context_start_lineno": 193, "repo": "opendilab_ACE"}}, "top_k_context": [{"context": "            - timestep (:obj:`namedtuple`): Output after env step, including at least ['obs', 'reward', 'done']\\\n                (here 'obs' indicates obs after env step).\n        Returns:\n            - transition (:obj:`dict`): Dict type transition data, including 'obs', 'next_obs', 'prev_state',\\\n                'action', 'reward', 'done'\n        \"\"\"\n        transition = {\n            'obs': obs,\n            'next_obs': timestep.obs,\n            'prev_state': model_output['prev_state'],\n            'action': model_output['action'],\n            'reward': timestep.reward,\n            'done': timestep.done,\n        }\n        return transition\n\n    def _init_eval(self) -> None:\n        r\"\"\"\n        Overview:\n            Evaluate mode init method. Called by ``self.__init__``.", "metadata": [{"fpath_tuple": ["opendilab_ACE", "ding", "policy", "qtran.py"], "line_no": 370, "start_line_no": 360, "end_line_no": 380, "window_size": 20, "repo": "opendilab_ACE", "slice_size": 10}], "sim_score": 1.0}, {"context": "            - model_output (:obj:`dict`): Output of collect model, including at least ['action', 'prev_state']\n            - timestep (:obj:`namedtuple`): Output after env step, including at least ['obs', 'reward', 'done'] \\\n                (here 'obs' indicates obs after env step).\n        Returns:\n            - transition (:obj:`dict`): Dict type transition data.\n        \"\"\"\n        transition = {\n            'obs': obs,\n            'next_obs': timestep.obs,\n            'prev_state': model_output['prev_state'],\n            'action': model_output['action'],\n            'reward': timestep.reward,\n            'done': timestep.done,\n        }\n        return transition\n\n    def _init_eval(self) -> None:\n        r\"\"\"\n        Overview:\n            Evaluate mode init method. Called by ``self.__init__``.", "metadata": [{"fpath_tuple": ["opendilab_ACE", "ding", "policy", "coma.py"], "line_no": 292, "start_line_no": 282, "end_line_no": 302, "window_size": 20, "repo": "opendilab_ACE", "slice_size": 10}], "sim_score": 0.9545454545454546}, {"context": "            - timestep (:obj:`namedtuple`): Output after env step, including at least ['obs', 'reward', 'done']\\\n                (here 'obs' indicates obs after env step).\n        Returns:\n            - transition (:obj:`dict`): Dict type transition data.\n        \"\"\"\n        transition = {\n            'obs': obs,\n            'next_obs': timestep.obs,\n            'prev_state': model_output['prev_state'],\n            'action': model_output['action'],\n            'agent_colla_alone_q': model_output['agent_colla_alone_q'],\n            'reward': timestep.reward,\n            'done': timestep.done,\n        }\n        return transition\n\n    def _init_eval(self) -> None:\n        r\"\"\"\n        Overview:\n            Evaluate mode init method. Called by ``self.__init__``.", "metadata": [{"fpath_tuple": ["opendilab_ACE", "ding", "policy", "collaq.py"], "line_no": 346, "start_line_no": 336, "end_line_no": 356, "window_size": 20, "repo": "opendilab_ACE", "slice_size": 10}], "sim_score": 0.9444444444444444}, {"context": "                - timestep (:obj:`namedtuple`): Output after env step, including at least ['obs', 'reward', 'done']\\\n                       (here 'obs' indicates obs after env step).\n        Returns:\n               - transition (:obj:`dict`): Dict type transition data, including at least ['obs','next_obs', 'logit',\\\n               'action','reward', 'done']\n        \"\"\"\n        transition = {\n            'obs': obs,\n            'next_obs': timestep.obs,\n            'logit': policy_output['logit'],\n            'action': policy_output['action'],\n            'reward': timestep.reward,\n            'done': timestep.done,\n        }\n        return transition\n\n    def _init_eval(self) -> None:\n        r\"\"\"\n        Overview:\n            Evaluate mode init method. Called by ``self.__init__``, initialize eval_model,", "metadata": [{"fpath_tuple": ["opendilab_ACE", "ding", "policy", "impala.py"], "line_no": 356, "start_line_no": 346, "end_line_no": 366, "window_size": 20, "repo": "opendilab_ACE", "slice_size": 10}], "sim_score": 0.8723404255319149}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# tests/test_optimizer.py\n# --------------------------------------------------\n#         backup_cfg = cfg.clone()\n# \n#         import torch\n#         cfg.use_gpu = torch.cuda.is_available()\n#         cfg.eval.freq = 10\n#         cfg.eval.metrics = ['acc', 'loss_regular']\n# \n#         cfg.federate.mode = 'standalone'\n#         cfg.train.local_update_steps = 5\n#         cfg.federate.total_round_num = 20\n#         cfg.federate.sample_client_num = 5\n#         cfg.federate.client_num = 10\n# \n#         cfg.data.root = 'test_data/'\n#         cfg.data.type = 'femnist'\n#         cfg.data.splits = [0.6, 0.2, 0.2]\n#         cfg.data.batch_size = 10\n#         cfg.data.subsample = 0.05\n#         cfg.data.transform = [['ToTensor'],\n#                               [\n# --------------------------------------------------\n# the below code fragment can be found in:\n# tests/test_external_dataset.py\n# --------------------------------------------------\n#         import torch\n#         cfg.use_gpu = torch.cuda.is_available()\n#         cfg.eval.freq = 10\n#         cfg.eval.metrics = ['acc']\n# \n#         cfg.federate.mode = 'standalone'\n#         cfg.train.local_update_steps = 1\n#         cfg.federate.total_round_num = 10\n#         cfg.train.batch_or_epoch = 'epoch'\n#         cfg.federate.client_num = 5\n#         cfg.federate.sample_client_rate = 0.2\n#         cfg.federate.share_local_model = True\n#         cfg.federate.online_aggr = True\n# \n#         cfg.data.root = 'test_data/'\n#         cfg.data.args = [{'max_len': 100}]\n#         cfg.data.type = 'IMDB@torchtext'\n#         cfg.data.splits = [0.6, 0.2, 0.2]\n#         cfg.data.batch_size = 10\n#         cfg.data.transform = ['GloVe', {'cache': 'test_data/', 'name': '6B'}]\n# --------------------------------------------------\n# the below code fragment can be found in:\n# tests/test_external_dataset.py\n# --------------------------------------------------\n#         import torch\n#         cfg.use_gpu = torch.cuda.is_available()\n#         cfg.eval.freq = 10\n#         cfg.eval.metrics = ['acc']\n# \n#         cfg.federate.mode = 'standalone'\n#         cfg.train.local_update_steps = 1\n#         cfg.federate.total_round_num = 20\n#         cfg.train.batch_or_epoch = 'epoch'\n#         cfg.federate.client_num = 5\n#         cfg.federate.sample_client_rate = 0.2\n#         cfg.federate.share_local_model = True\n#         cfg.federate.online_aggr = True\n# \n#         cfg.data.root = 'test_data/'\n#         cfg.data.type = 'MNIST@torchvision'\n#         cfg.data.args = [{'download': True}]\n#         cfg.data.splits = [0.6, 0.2, 0.2]\n#         cfg.data.batch_size = 10\n#         cfg.data.transform = [['ToTensor'],\n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\n# Copyright (c) Alibaba, Inc. and its affiliates.\nimport unittest\n\nfrom federatedscope.core.auxiliaries.data_builder import get_data\nfrom federatedscope.core.auxiliaries.utils import setup_seed\nfrom federatedscope.core.auxiliaries.logging import update_logger\nfrom federatedscope.core.configs.config import global_cfg\nfrom federatedscope.core.auxiliaries.runner_builder import get_runner\nfrom federatedscope.core.auxiliaries.worker_builder import get_server_cls, get_client_cls\n\n\nclass AsynCIFAR10Test(unittest.TestCase):\n    def setUp(self):\n        print(('Testing %s.%s' % (type(self).__name__, self._testMethodName)))\n\n    def set_config_cifar10_goalAchieved_afterReceiving(self, cfg):\n        backup_cfg = cfg.clone()\n\n        import torch\n        cfg.use_gpu = torch.cuda.is_available()\n        cfg.eval.freq = 5\n        cfg.eval.metrics = ['acc', 'correct']\n        cfg.eval.best_res_update_round_wise_key = 'test_acc'\n\n        cfg.federate.mode = 'standalone'\n        cfg.federate.total_round_num = 40\n        cfg.federate.sample_client_num = 13\n        cfg.federate.merge_test_data = True\n        cfg.federate.share_local_model = False\n        cfg.federate.client_num = 200\n        cfg.federate.sampler = 'group'\n        cfg.federate.resource_info_file = 'test_data/client_device_capacity'\n\n        cfg.data.root = 'test_data/'\n        cfg.data.type = 'CIFAR10@torchvision'\n        cfg.data.args = [{'download': True}]\n        cfg.data.splits = [0.8, 0.2, 0.2]\n        cfg.data.batch_size = 10\n        cfg.data.subsample = 0.2\n        cfg.data.num_workers = 0\n        cfg.data.transform = [['ToTensor'],\n                              [\n                                  'Normalize', {\n                                      'mean': [0.4914, 0.4822, 0.4465],\n                                      'std': [0.247, 0.243, 0.261]\n                                  }\n                              ]]\n        cfg.data.splitter = 'lda'\n        cfg.data.splitter_args = [{'alpha': 0.2}]\n\n        cfg.model.type = 'convnet2'\n        cfg.model.hidden = 128\n        cfg.model.out_channels = 10\n\n        cfg.train.local_update_steps = 2\n        cfg.train.batch_or_epoch = 'batch'\n        cfg.train.optimizer.lr = 0.1\n        cfg.train.optimizer.weight_decay = 0.0\n        cfg.grad.grad_clip = 5.0\n\n        cfg.criterion.type = 'CrossEntropyLoss'\n        cfg.trainer.type = 'cvtrainer'\n        cfg.seed = 123\n\n        cfg.asyn.use = True\n        cfg.asyn.overselection = False\n        cfg.asyn.staleness_discount_factor = 0.2\n        cfg.asyn.aggregator = 'goal_achieved'\n        cfg.asyn.broadcast_manner = 'after_receiving'\n        cfg.asyn.min_received_num = 10\n        cfg.asyn.staleness_toleration = 5\n\n        return backup_cfg\n\n    def set_config_cifar10_timeUp_afterAggregating(self, cfg):\n        backup_cfg = cfg.clone()\n\n        import torch\n        cfg.use_gpu = torch.cuda.is_available()\n        cfg.eval.freq = 5\n        cfg.eval.metrics = ['acc', 'correct']\n        cfg.eval.best_res_update_round_wise_key = 'test_acc'\n\n        cfg.federate.mode = 'standalone'\n        cfg.federate.total_round_num = 40\n        cfg.federate.sample_client_num = 13\n        cfg.federate.merge_test_data = True\n        cfg.federate.share_local_model = False\n        cfg.federate.client_num = 200\n        cfg.federate.sampler = 'uniform'\n        cfg.federate.resource_info_file = 'test_data/client_device_capacity'\n\n        cfg.data.root = 'test_data/'\n        cfg.data.type = 'CIFAR10@torchvision'\n        cfg.data.args = [{'download': True}]", "metadata": {"task_id": "alibaba_FederatedScope/134", "ground_truth": "        cfg.data.splits = [0.8, 0.2, 0.2]", "fpath_tuple": ["alibaba_FederatedScope", "tests", "test_asyn_cifar10.py"], "context_start_lineno": 0, "line_no": 95, "query_window": {"context": "        backup_cfg = cfg.clone()\n\n        import torch\n        cfg.use_gpu = torch.cuda.is_available()\n        cfg.eval.freq = 5\n        cfg.eval.metrics = ['acc', 'correct']\n        cfg.eval.best_res_update_round_wise_key = 'test_acc'\n\n        cfg.federate.mode = 'standalone'\n        cfg.federate.total_round_num = 40\n        cfg.federate.sample_client_num = 13\n        cfg.federate.merge_test_data = True\n        cfg.federate.share_local_model = False\n        cfg.federate.client_num = 200\n        cfg.federate.sampler = 'uniform'\n        cfg.federate.resource_info_file = 'test_data/client_device_capacity'\n\n        cfg.data.root = 'test_data/'\n        cfg.data.type = 'CIFAR10@torchvision'\n        cfg.data.args = [{'download': True}]", "metadata": {"fpath_tuple": ["alibaba_FederatedScope", "tests", "test_asyn_cifar10.py"], "line_no": 95, "task_id": "alibaba_FederatedScope/134", "start_line_no": 75, "end_line_no": 95, "window_size": 20, "context_start_lineno": 0, "repo": "alibaba_FederatedScope"}}, "top_k_context": [{"context": "        backup_cfg = cfg.clone()\n\n        import torch\n        cfg.use_gpu = torch.cuda.is_available()\n        cfg.eval.freq = 10\n        cfg.eval.metrics = ['acc']\n\n        cfg.federate.mode = 'standalone'\n        cfg.train.local_update_steps = 1\n        cfg.federate.total_round_num = 20\n        cfg.train.batch_or_epoch = 'epoch'\n        cfg.federate.client_num = 5\n        cfg.federate.sample_client_rate = 0.2\n        cfg.federate.share_local_model = True\n        cfg.federate.online_aggr = True\n\n        cfg.data.root = 'test_data/'\n        cfg.data.type = 'MNIST@torchvision'\n        cfg.data.args = [{'download': True}]\n        cfg.data.splits = [0.6, 0.2, 0.2]", "metadata": [{"fpath_tuple": ["alibaba_FederatedScope", "tests", "test_external_dataset.py"], "line_no": 26, "start_line_no": 16, "end_line_no": 36, "window_size": 20, "repo": "alibaba_FederatedScope", "slice_size": 10}], "sim_score": 0.5727272727272728}, {"context": "        backup_cfg = cfg.clone()\n\n        import torch\n        cfg.use_gpu = torch.cuda.is_available()\n        cfg.eval.freq = 10\n        cfg.eval.metrics = ['acc']\n\n        cfg.federate.mode = 'standalone'\n        cfg.train.local_update_steps = 1\n        cfg.federate.total_round_num = 10\n        cfg.train.batch_or_epoch = 'epoch'\n        cfg.federate.client_num = 5\n        cfg.federate.sample_client_rate = 0.2\n        cfg.federate.share_local_model = True\n        cfg.federate.online_aggr = True\n\n        cfg.data.root = 'test_data/'\n        cfg.data.args = [{'max_len': 100}]\n        cfg.data.type = 'IMDB@torchtext'\n        cfg.data.splits = [0.6, 0.2, 0.2]", "metadata": [{"fpath_tuple": ["alibaba_FederatedScope", "tests", "test_external_dataset.py"], "line_no": 72, "start_line_no": 62, "end_line_no": 82, "window_size": 20, "repo": "alibaba_FederatedScope", "slice_size": 10}], "sim_score": 0.5398230088495575}, {"context": "\n    def set_config_femnist(self, cfg):\n        backup_cfg = cfg.clone()\n\n        import torch\n        cfg.use_gpu = torch.cuda.is_available()\n        cfg.eval.freq = 10\n        cfg.eval.metrics = ['acc', 'loss_regular']\n\n        cfg.federate.mode = 'standalone'\n        cfg.train.local_update_steps = 5\n        cfg.federate.total_round_num = 20\n        cfg.federate.sample_client_num = 5\n        cfg.federate.client_num = 10\n\n        cfg.data.root = 'test_data/'\n        cfg.data.type = 'femnist'\n        cfg.data.splits = [0.6, 0.2, 0.2]\n        cfg.data.batch_size = 10\n        cfg.data.subsample = 0.05", "metadata": [{"fpath_tuple": ["alibaba_FederatedScope", "tests", "test_optimizer.py"], "line_no": 24, "start_line_no": 14, "end_line_no": 34, "window_size": 20, "repo": "alibaba_FederatedScope", "slice_size": 10}], "sim_score": 0.45614035087719296}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# torchrl/objectives/a2c.py\n# --------------------------------------------------\n#         self, tensordict: TensorDictBase\n#     ) -> Tuple[torch.Tensor, d.Distribution]:\n#         # current log_prob of actions\n#         action = tensordict.get(\"action\")\n#         if action.requires_grad:\n#             raise RuntimeError(\"tensordict stored action require grad.\")\n#         tensordict_clone = tensordict.select(*self.actor.in_keys).clone()\n# \n#         dist = self.actor.get_dist(tensordict_clone, params=self.actor_params)\n#         log_prob = dist.log_prob(action)\n#         log_prob = log_prob.unsqueeze(-1)\n#         return log_prob, dist\n# \n#     def loss_critic(self, tensordict: TensorDictBase) -> torch.Tensor:\n#         try:\n#             target_return = tensordict.get(self.value_target_key)\n#             tensordict_select = tensordict.select(*self.critic.in_keys)\n#             state_value = self.critic(\n#                 tensordict_select,\n#                 params=self.critic_params,\n# --------------------------------------------------\n# the below code fragment can be found in:\n# torchrl/objectives/a2c.py\n# --------------------------------------------------\n# \n#     def _log_probs(\n#         self, tensordict: TensorDictBase\n#     ) -> Tuple[torch.Tensor, d.Distribution]:\n#         # current log_prob of actions\n#         action = tensordict.get(\"action\")\n#         if action.requires_grad:\n#             raise RuntimeError(\"tensordict stored action require grad.\")\n#         tensordict_clone = tensordict.select(*self.actor.in_keys).clone()\n# \n#         dist = self.actor.get_dist(tensordict_clone, params=self.actor_params)\n#         log_prob = dist.log_prob(action)\n#         log_prob = log_prob.unsqueeze(-1)\n#         return log_prob, dist\n# \n#     def loss_critic(self, tensordict: TensorDictBase) -> torch.Tensor:\n#         try:\n#             target_return = tensordict.get(self.value_target_key)\n#             tensordict_select = tensordict.select(*self.critic.in_keys)\n#             state_value = self.critic(\n# --------------------------------------------------\n# the below code fragment can be found in:\n# torchrl/objectives/a2c.py\n# --------------------------------------------------\n#         # current log_prob of actions\n#         action = tensordict.get(\"action\")\n#         if action.requires_grad:\n#             raise RuntimeError(\"tensordict stored action require grad.\")\n#         tensordict_clone = tensordict.select(*self.actor.in_keys).clone()\n# \n#         dist = self.actor.get_dist(tensordict_clone, params=self.actor_params)\n#         log_prob = dist.log_prob(action)\n#         log_prob = log_prob.unsqueeze(-1)\n#         return log_prob, dist\n# \n#     def loss_critic(self, tensordict: TensorDictBase) -> torch.Tensor:\n#         try:\n#             target_return = tensordict.get(self.value_target_key)\n#             tensordict_select = tensordict.select(*self.critic.in_keys)\n#             state_value = self.critic(\n#                 tensordict_select,\n#                 params=self.critic_params,\n#             ).get(\"state_value\")\n#             loss_value = distance_loss(\n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\n# Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport math\nfrom typing import Tuple\n\nimport torch\nfrom tensordict.tensordict import TensorDict, TensorDictBase\nfrom torch import distributions as d\n\nfrom torchrl.modules import SafeModule\nfrom torchrl.objectives.utils import distance_loss\n\nfrom ..modules.tensordict_module import SafeProbabilisticSequential\nfrom .common import LossModule\n\n\nclass PPOLoss(LossModule):\n    \"\"\"A parent PPO loss class.\n\n    PPO (Proximal Policy Optimisation) is a model-free, online RL algorithm that makes use of a recorded (batch of)\n    trajectories to perform several optimization steps, while actively preventing the updated policy to deviate too\n    much from its original parameter configuration.\n\n    PPO loss can be found in different flavours, depending on the way the constrained optimisation is implemented:\n        ClipPPOLoss and KLPENPPOLoss.\n    Unlike its subclasses, this class does not implement any regularisation and should therefore be used cautiously.\n\n    For more details regarding PPO, refer to: \"Proximal Policy Optimization Algorithms\",\n    https://arxiv.org/abs/1707.06347\n\n    Args:\n        actor (SafeProbabilisticSequential): policy operator.\n        critic (ValueOperator): value operator.\n        advantage_key (str): the input tensordict key where the advantage is expected to be written.\n            default: \"advantage\"\n        entropy_bonus (bool): if True, an entropy bonus will be added to the loss to favour exploratory policies.\n        samples_mc_entropy (int): if the distribution retrieved from the policy operator does not have a closed form\n            formula for the entropy, a Monte-Carlo estimate will be used. samples_mc_entropy will control how many\n            samples will be used to compute this estimate.\n            default: 1\n        entropy_coef (scalar): entropy multiplier when computing the total loss.\n            default: 0.01\n        critic_coef (scalar): critic loss multiplier when computing the total loss.\n            default: 1.0\n        gamma (scalar): a discount factor for return computation.\n        loss_function (str): loss function for the value discrepancy. Can be one of \"l1\", \"l2\" or \"smooth_l1\".\n        normalize_advantage (bool): if True, the advantage will be normalized before being used.\n            Defaults to True.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        actor: SafeProbabilisticSequential,\n        critic: SafeModule,\n        advantage_key: str = \"advantage\",\n        value_target_key: str = \"value_target\",\n        entropy_bonus: bool = True,\n        samples_mc_entropy: int = 1,\n        entropy_coef: float = 0.01,\n        critic_coef: float = 1.0,\n        gamma: float = 0.99,\n        loss_critic_type: str = \"smooth_l1\",\n        normalize_advantage: bool = True,\n    ):\n        super().__init__()\n        self.convert_to_functional(\n            actor, \"actor\", funs_to_decorate=[\"forward\", \"get_dist\"]\n        )\n        # we want to make sure there are no duplicates in the params: the\n        # params of critic must be refs to actor if they're shared\n        self.convert_to_functional(critic, \"critic\", compare_against=self.actor_params)\n        self.advantage_key = advantage_key\n        self.value_target_key = value_target_key\n        self.samples_mc_entropy = samples_mc_entropy\n        self.entropy_bonus = entropy_bonus and entropy_coef\n        self.register_buffer(\n            \"entropy_coef\", torch.tensor(entropy_coef, device=self.device)\n        )\n        self.register_buffer(\n            \"critic_coef\", torch.tensor(critic_coef, device=self.device)\n        )\n        self.register_buffer(\"gamma\", torch.tensor(gamma, device=self.device))\n        self.loss_critic_type = loss_critic_type\n        self.normalize_advantage = normalize_advantage\n\n    def reset(self) -> None:\n        pass\n\n    def get_entropy_bonus(self, dist: d.Distribution) -> torch.Tensor:\n        try:\n            entropy = dist.entropy()\n        except NotImplementedError:\n            x = dist.rsample((self.samples_mc_entropy,))\n            entropy = -dist.log_prob(x)\n        return entropy.unsqueeze(-1)\n\n    def _log_weight(\n        self, tensordict: TensorDictBase\n    ) -> Tuple[torch.Tensor, d.Distribution]:\n        # current log_prob of actions\n        action = tensordict.get(\"action\")\n        if action.requires_grad:\n            raise RuntimeError(\"tensordict stored action requires grad.\")\n        tensordict_clone = tensordict.select(*self.actor.in_keys).clone()\n\n        dist = self.actor.get_dist(tensordict_clone, params=self.actor_params)\n        log_prob = dist.log_prob(action)\n\n        prev_log_prob = tensordict.get(\"sample_log_prob\")\n        if prev_log_prob.requires_grad:\n            raise RuntimeError(\"tensordict prev_log_prob requires grad.\")\n\n        log_weight = (log_prob - prev_log_prob).unsqueeze(-1)\n        return log_weight, dist\n\n    def loss_critic(self, tensordict: TensorDictBase) -> torch.Tensor:\n        try:", "metadata": {"task_id": "pytorch_rl/195", "ground_truth": "            target_return = tensordict.get(self.value_target_key)", "fpath_tuple": ["pytorch_rl", "torchrl", "objectives", "ppo.py"], "context_start_lineno": 0, "line_no": 121, "query_window": {"context": "        self, tensordict: TensorDictBase\n    ) -> Tuple[torch.Tensor, d.Distribution]:\n        # current log_prob of actions\n        action = tensordict.get(\"action\")\n        if action.requires_grad:\n            raise RuntimeError(\"tensordict stored action requires grad.\")\n        tensordict_clone = tensordict.select(*self.actor.in_keys).clone()\n\n        dist = self.actor.get_dist(tensordict_clone, params=self.actor_params)\n        log_prob = dist.log_prob(action)\n\n        prev_log_prob = tensordict.get(\"sample_log_prob\")\n        if prev_log_prob.requires_grad:\n            raise RuntimeError(\"tensordict prev_log_prob requires grad.\")\n\n        log_weight = (log_prob - prev_log_prob).unsqueeze(-1)\n        return log_weight, dist\n\n    def loss_critic(self, tensordict: TensorDictBase) -> torch.Tensor:\n        try:", "metadata": {"fpath_tuple": ["pytorch_rl", "torchrl", "objectives", "ppo.py"], "line_no": 121, "task_id": "pytorch_rl/195", "start_line_no": 101, "end_line_no": 121, "window_size": 20, "context_start_lineno": 0, "repo": "pytorch_rl"}}, "top_k_context": [{"context": "        self, tensordict: TensorDictBase\n    ) -> Tuple[torch.Tensor, d.Distribution]:\n        # current log_prob of actions\n        action = tensordict.get(\"action\")\n        if action.requires_grad:\n            raise RuntimeError(\"tensordict stored action require grad.\")\n        tensordict_clone = tensordict.select(*self.actor.in_keys).clone()\n\n        dist = self.actor.get_dist(tensordict_clone, params=self.actor_params)\n        log_prob = dist.log_prob(action)\n        log_prob = log_prob.unsqueeze(-1)\n        return log_prob, dist\n\n    def loss_critic(self, tensordict: TensorDictBase) -> torch.Tensor:\n        try:\n            target_return = tensordict.get(self.value_target_key)\n            tensordict_select = tensordict.select(*self.critic.in_keys)\n            state_value = self.critic(\n                tensordict_select,\n                params=self.critic_params,", "metadata": [{"fpath_tuple": ["pytorch_rl", "torchrl", "objectives", "a2c.py"], "line_no": 94, "start_line_no": 84, "end_line_no": 104, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}], "sim_score": 0.8541666666666666}, {"context": "            entropy = -dist.log_prob(x)\n        return entropy.unsqueeze(-1)\n\n    def _log_probs(\n        self, tensordict: TensorDictBase\n    ) -> Tuple[torch.Tensor, d.Distribution]:\n        # current log_prob of actions\n        action = tensordict.get(\"action\")\n        if action.requires_grad:\n            raise RuntimeError(\"tensordict stored action require grad.\")\n        tensordict_clone = tensordict.select(*self.actor.in_keys).clone()\n\n        dist = self.actor.get_dist(tensordict_clone, params=self.actor_params)\n        log_prob = dist.log_prob(action)\n        log_prob = log_prob.unsqueeze(-1)\n        return log_prob, dist\n\n    def loss_critic(self, tensordict: TensorDictBase) -> torch.Tensor:\n        try:\n            target_return = tensordict.get(self.value_target_key)", "metadata": [{"fpath_tuple": ["pytorch_rl", "torchrl", "objectives", "a2c.py"], "line_no": 90, "start_line_no": 80, "end_line_no": 100, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}], "sim_score": 0.8469387755102041}, {"context": "\n    def _log_probs(\n        self, tensordict: TensorDictBase\n    ) -> Tuple[torch.Tensor, d.Distribution]:\n        # current log_prob of actions\n        action = tensordict.get(\"action\")\n        if action.requires_grad:\n            raise RuntimeError(\"tensordict stored action require grad.\")\n        tensordict_clone = tensordict.select(*self.actor.in_keys).clone()\n\n        dist = self.actor.get_dist(tensordict_clone, params=self.actor_params)\n        log_prob = dist.log_prob(action)\n        log_prob = log_prob.unsqueeze(-1)\n        return log_prob, dist\n\n    def loss_critic(self, tensordict: TensorDictBase) -> torch.Tensor:\n        try:\n            target_return = tensordict.get(self.value_target_key)\n            tensordict_select = tensordict.select(*self.critic.in_keys)\n            state_value = self.critic(", "metadata": [{"fpath_tuple": ["pytorch_rl", "torchrl", "objectives", "a2c.py"], "line_no": 92, "start_line_no": 82, "end_line_no": 102, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}], "sim_score": 0.845360824742268}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# torchrl/envs/libs/dm_control.py\n# --------------------------------------------------\n#         if not len(shape):\n#             shape = torch.Size([1])\n#         return BoundedTensorSpec(\n#             shape=shape,\n#             minimum=spec.minimum,\n#             maximum=spec.maximum,\n#             dtype=dtype,\n#             device=device,\n#         )\n#     elif isinstance(spec, dm_env.specs.Array):\n#         shape = spec.shape\n#         if not len(shape):\n#             shape = torch.Size([1])\n#         if dtype is None:\n#             dtype = numpy_to_torch_dtype_dict[spec.dtype]\n#         if dtype in (torch.float, torch.double, torch.half):\n#             return UnboundedContinuousTensorSpec(\n#                 shape=shape, dtype=dtype, device=device\n#             )\n#         else:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# torchrl/envs/libs/dm_control.py\n# --------------------------------------------------\n#             minimum=spec.minimum,\n#             maximum=spec.maximum,\n#             dtype=dtype,\n#             device=device,\n#         )\n#     elif isinstance(spec, dm_env.specs.Array):\n#         shape = spec.shape\n#         if not len(shape):\n#             shape = torch.Size([1])\n#         if dtype is None:\n#             dtype = numpy_to_torch_dtype_dict[spec.dtype]\n#         if dtype in (torch.float, torch.double, torch.half):\n#             return UnboundedContinuousTensorSpec(\n#                 shape=shape, dtype=dtype, device=device\n#             )\n#         else:\n#             return UnboundedDiscreteTensorSpec(shape=shape, dtype=dtype, device=device)\n# \n#     else:\n#         raise NotImplementedError(type(spec))\n# --------------------------------------------------\n# the below code fragment can be found in:\n# torchrl/envs/libs/dm_control.py\n# --------------------------------------------------\n#         return BoundedTensorSpec(\n#             shape=shape,\n#             minimum=spec.minimum,\n#             maximum=spec.maximum,\n#             dtype=dtype,\n#             device=device,\n#         )\n#     elif isinstance(spec, dm_env.specs.Array):\n#         shape = spec.shape\n#         if not len(shape):\n#             shape = torch.Size([1])\n#         if dtype is None:\n#             dtype = numpy_to_torch_dtype_dict[spec.dtype]\n#         if dtype in (torch.float, torch.double, torch.half):\n#             return UnboundedContinuousTensorSpec(\n#                 shape=shape, dtype=dtype, device=device\n#             )\n#         else:\n#             return UnboundedDiscreteTensorSpec(shape=shape, dtype=dtype, device=device)\n# \n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\n# Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\nimport warnings\nfrom types import ModuleType\nfrom typing import Dict, List\nfrom warnings import warn\n\nimport torch\nfrom torchrl.data import (\n    BinaryDiscreteTensorSpec,\n    BoundedTensorSpec,\n    CompositeSpec,\n    DiscreteTensorSpec,\n    MultiDiscreteTensorSpec,\n    MultiOneHotDiscreteTensorSpec,\n    OneHotDiscreteTensorSpec,\n    TensorSpec,\n    UnboundedContinuousTensorSpec,\n)\n\nfrom ..._utils import implement_for\nfrom ...data.utils import numpy_to_torch_dtype_dict\n\nfrom ..gym_like import default_info_dict_reader, GymLikeEnv\nfrom ..utils import _classproperty\n\ntry:\n    import gym\n\n    _has_gym = True\nexcept ImportError:\n    _has_gym = False\n\n\nif _has_gym:\n    try:\n        from gym.wrappers.pixel_observation import PixelObservationWrapper\n\n        from torchrl.envs.libs.utils import (\n            GymPixelObservationWrapper as LegacyPixelObservationWrapper,\n        )\n    except ModuleNotFoundError:\n        warnings.warn(\n            f\"gym {gym.__version__} does not provide the PixelObservationWrapper\"\n            f\"used by torchrl, which will be using a patched version. \"\n            f\"Consider updating gym to a newer version.\"\n        )\n        from torchrl.envs.libs.utils import (\n            GymPixelObservationWrapper as PixelObservationWrapper,\n        )\n\n__all__ = [\"GymWrapper\", \"GymEnv\"]\n\n\ndef _gym_to_torchrl_spec_transform(\n    spec, dtype=None, device=\"cpu\", categorical_action_encoding=False\n) -> TensorSpec:\n    if isinstance(spec, gym.spaces.tuple.Tuple):\n        raise NotImplementedError(\"gym.spaces.tuple.Tuple mapping not yet implemented\")\n    if isinstance(spec, gym.spaces.discrete.Discrete):\n        action_space_cls = (\n            DiscreteTensorSpec\n            if categorical_action_encoding\n            else OneHotDiscreteTensorSpec\n        )\n        dtype = (\n            numpy_to_torch_dtype_dict[spec.dtype]\n            if categorical_action_encoding\n            else torch.long\n        )\n        return action_space_cls(spec.n, device=device, dtype=dtype)\n    elif isinstance(spec, gym.spaces.multi_binary.MultiBinary):\n        return BinaryDiscreteTensorSpec(\n            spec.n, device=device, dtype=numpy_to_torch_dtype_dict[spec.dtype]\n        )\n    elif isinstance(spec, gym.spaces.multi_discrete.MultiDiscrete):\n        dtype = (\n            numpy_to_torch_dtype_dict[spec.dtype]\n            if categorical_action_encoding\n            else torch.long\n        )\n        return (\n            MultiDiscreteTensorSpec(spec.nvec, device=device, dtype=dtype)\n            if categorical_action_encoding\n            else MultiOneHotDiscreteTensorSpec(spec.nvec, device=device, dtype=dtype)\n        )\n    elif isinstance(spec, gym.spaces.Box):\n        shape = spec.shape\n        if not len(shape):\n            shape = torch.Size([1])\n        if dtype is None:\n            dtype = numpy_to_torch_dtype_dict[spec.dtype]\n        low = torch.tensor(spec.low, device=device, dtype=dtype)\n        high = torch.tensor(spec.high, device=device, dtype=dtype)\n        is_unbounded = low.isinf().all() and high.isinf().all()\n        return (\n            UnboundedContinuousTensorSpec(shape, device=device, dtype=dtype)\n            if is_unbounded\n            else BoundedTensorSpec(\n                low,\n                high,\n                shape,\n                dtype=dtype,\n                device=device,\n            )\n        )\n    elif isinstance(spec, (Dict,)):\n        spec_out = {}\n        for k in spec.keys():\n            spec_out[k] = _gym_to_torchrl_spec_transform(", "metadata": {"task_id": "pytorch_rl/26", "ground_truth": "                spec[k],", "fpath_tuple": ["pytorch_rl", "torchrl", "envs", "libs", "gym.py"], "context_start_lineno": 0, "line_no": 112, "query_window": {"context": "        if dtype is None:\n            dtype = numpy_to_torch_dtype_dict[spec.dtype]\n        low = torch.tensor(spec.low, device=device, dtype=dtype)\n        high = torch.tensor(spec.high, device=device, dtype=dtype)\n        is_unbounded = low.isinf().all() and high.isinf().all()\n        return (\n            UnboundedContinuousTensorSpec(shape, device=device, dtype=dtype)\n            if is_unbounded\n            else BoundedTensorSpec(\n                low,\n                high,\n                shape,\n                dtype=dtype,\n                device=device,\n            )\n        )\n    elif isinstance(spec, (Dict,)):\n        spec_out = {}\n        for k in spec.keys():\n            spec_out[k] = _gym_to_torchrl_spec_transform(", "metadata": {"fpath_tuple": ["pytorch_rl", "torchrl", "envs", "libs", "gym.py"], "line_no": 112, "task_id": "pytorch_rl/26", "start_line_no": 92, "end_line_no": 112, "window_size": 20, "context_start_lineno": 0, "repo": "pytorch_rl"}}, "top_k_context": [{"context": "        if not len(shape):\n            shape = torch.Size([1])\n        return BoundedTensorSpec(\n            shape=shape,\n            minimum=spec.minimum,\n            maximum=spec.maximum,\n            dtype=dtype,\n            device=device,\n        )\n    elif isinstance(spec, dm_env.specs.Array):\n        shape = spec.shape\n        if not len(shape):\n            shape = torch.Size([1])\n        if dtype is None:\n            dtype = numpy_to_torch_dtype_dict[spec.dtype]\n        if dtype in (torch.float, torch.double, torch.half):\n            return UnboundedContinuousTensorSpec(\n                shape=shape, dtype=dtype, device=device\n            )\n        else:", "metadata": [{"fpath_tuple": ["pytorch_rl", "torchrl", "envs", "libs", "dm_control.py"], "line_no": 70, "start_line_no": 60, "end_line_no": 80, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}], "sim_score": 0.5416666666666666}, {"context": "        return BoundedTensorSpec(\n            shape=shape,\n            minimum=spec.minimum,\n            maximum=spec.maximum,\n            dtype=dtype,\n            device=device,\n        )\n    elif isinstance(spec, dm_env.specs.Array):\n        shape = spec.shape\n        if not len(shape):\n            shape = torch.Size([1])\n        if dtype is None:\n            dtype = numpy_to_torch_dtype_dict[spec.dtype]\n        if dtype in (torch.float, torch.double, torch.half):\n            return UnboundedContinuousTensorSpec(\n                shape=shape, dtype=dtype, device=device\n            )\n        else:\n            return UnboundedDiscreteTensorSpec(shape=shape, dtype=dtype, device=device)\n", "metadata": [{"fpath_tuple": ["pytorch_rl", "torchrl", "envs", "libs", "dm_control.py"], "line_no": 72, "start_line_no": 62, "end_line_no": 82, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}], "sim_score": 0.5408163265306123}, {"context": "            dtype = numpy_to_torch_dtype_dict[spec.dtype]\n        shape = spec.shape\n        if not len(shape):\n            shape = torch.Size([1])\n        return BoundedTensorSpec(\n            shape=shape,\n            minimum=spec.minimum,\n            maximum=spec.maximum,\n            dtype=dtype,\n            device=device,\n        )\n    elif isinstance(spec, dm_env.specs.Array):\n        shape = spec.shape\n        if not len(shape):\n            shape = torch.Size([1])\n        if dtype is None:\n            dtype = numpy_to_torch_dtype_dict[spec.dtype]\n        if dtype in (torch.float, torch.double, torch.half):\n            return UnboundedContinuousTensorSpec(\n                shape=shape, dtype=dtype, device=device", "metadata": [{"fpath_tuple": ["pytorch_rl", "torchrl", "envs", "libs", "dm_control.py"], "line_no": 68, "start_line_no": 58, "end_line_no": 78, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}], "sim_score": 0.53125}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# vizier/_src/pyvizier/shared/parameter_config.py\n# --------------------------------------------------\n#       value = value.value\n#     try:\n#       self._assert_feasible(value)\n#     except (TypeError, ValueError):\n#       return False\n#     return True\n# \n#   @property\n#   def num_feasible_values(self) -> Union[float, int]:\n#     if self.type == ParameterType.DOUBLE:\n#       return float('inf')\n#     elif self.type == ParameterType.INTEGER:\n#       return self.bounds[1] - self.bounds[0] + 1\n#     else:\n#       return len(self.feasible_values)\n# \n#   def _assert_bounds(self, value: trial.ParameterValueTypes) -> None:\n#     if not self.bounds[0] <= value <= self.bounds[1]:\n#       raise ValueError(f'Parameter {self.name} has bounds: {self.bounds}. '\n#                        f'Given: {value}')\n# --------------------------------------------------\n# the below code fragment can be found in:\n# vizier/_src/pyvizier/shared/parameter_config.py\n# --------------------------------------------------\n#   def contains(\n#       self, value: Union[trial.ParameterValueTypes,\n#                          trial.ParameterValue]) -> bool:\n#     \"\"\"Check if the `value` is a valid value for this parameter config.\"\"\"\n#     if isinstance(value, trial.ParameterValue):\n#       # TODO: Extract the raw value.\n#       value = value.value\n#     try:\n#       self._assert_feasible(value)\n#     except (TypeError, ValueError):\n#       return False\n#     return True\n# \n#   @property\n#   def num_feasible_values(self) -> Union[float, int]:\n#     if self.type == ParameterType.DOUBLE:\n#       return float('inf')\n#     elif self.type == ParameterType.INTEGER:\n#       return self.bounds[1] - self.bounds[0] + 1\n#     else:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# vizier/_src/pyvizier/oss/study_config.py\n# --------------------------------------------------\n#     return external_values\n# \n#   def trial_parameters(\n#       self, proto: study_pb2.Trial) -> Dict[str, ParameterValueSequence]:\n#     \"\"\"Returns the trial values, cast to external types, if they exist.\n# \n#     Args:\n#       proto:\n# \n#     Returns:\n#       Parameter values dict: cast to each parameter's external_type, if exists.\n#       NOTE that the values in the dict may be a Sequence as opposed to a single\n#       element.\n# \n#     Raises:\n#       ValueError: If the trial parameters do not exist in this search space.\n#       ValueError: If the trial contains duplicate parameters.\n#     \"\"\"\n#     pytrial = proto_converters.TrialConverter.from_proto(proto)\n#     return self._pytrial_parameters(pytrial)\n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\n# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import annotations\n\n\"\"\"Wrapper classes for Trial protos and other messages in them.\"\"\"\n\nimport collections\nfrom collections import abc\nimport copy\nimport dataclasses\nimport datetime\nimport enum\nfrom typing import Any, Dict, List, Mapping, MutableMapping, Optional, Union, FrozenSet\n\nfrom absl import logging\nimport attr\nimport numpy as np\n\nfrom vizier._src.pyvizier.shared import common\n\nParameterValueTypes = Union[str, int, float, bool]\n\n# TODO: These constants should be deleted.\nTRUE_VALUE = 'True'\nFALSE_VALUE = 'False'\n\n\nclass ParameterType(enum.Enum):\n  \"\"\"Valid Values for ParameterConfig.type.\"\"\"\n  DOUBLE = 'DOUBLE'\n  INTEGER = 'INTEGER'\n  CATEGORICAL = 'CATEGORICAL'\n  DISCRETE = 'DISCRETE'\n\n  def is_numeric(self) -> bool:\n    return self in [self.DOUBLE, self.INTEGER, self.DISCRETE]\n\n  def is_continuous(self) -> bool:\n    return self == self.DOUBLE\n\n  def _raise_type_error(self, value: ParameterValueTypes) -> None:\n    raise TypeError(f'Type {self} is not compatible with value: {value}')\n\n  def assert_correct_type(self, value: ParameterValueTypes) -> None:\n    if self.is_numeric() and float(value) != value:\n      self._raise_type_error(value)\n\n  # TODO: Accepting boolean into categorical is unintuitive.\n    elif (self\n          == ParameterType.CATEGORICAL) and (not isinstance(value,\n                                                            (str, bool))):\n      self._raise_type_error(value)\n\n    if self == self.INTEGER and int(value) != value:\n      self._raise_type_error(value)\n\n\n# TODO: Trial class should not depend on these.\nclass ExternalType(enum.Enum):\n  \"\"\"Valid Values for ParameterConfig.external_type.\"\"\"\n  INTERNAL = 'INTERNAL'\n  BOOLEAN = 'BOOLEAN'\n  INTEGER = 'INTEGER'\n  FLOAT = 'FLOAT'\n\n\n# Values should NEVER be removed from the enums below, only added.\nclass TrialStatus(enum.Enum):\n  \"\"\"Values for Trial.Status.\"\"\"\n  UNKNOWN = 'UNKNOWN'\n  REQUESTED = 'REQUESTED'\n  ACTIVE = 'ACTIVE'\n  COMPLETED = 'COMPLETED'\n  STOPPING = 'STOPPING'\n\n\n@attr.s(frozen=True, init=True, slots=True, kw_only=False)\nclass Metric:\n  \"\"\"Enhanced immutable wrapper for vizier_pb2.Metric proto.\n\n  It has an optional field \"std\" for internal usage. This field gets lost\n  when the object is converted to proto.\n  \"\"\"\n\n  def _std_not_negative(self, _, stddev: Optional[float]) -> bool:\n    if (stddev is not None) and (not stddev >= 0):\n      raise ValueError(\n          'Standard deviation must be a non-negative finite number.')\n\n  value: float = attr.ib(\n      converter=float,\n      init=True,\n      validator=[attr.validators.instance_of(float)],\n      kw_only=False)\n  std: Optional[float] = attr.ib(\n      converter=lambda x: float(x) if x is not None else None,\n      validator=[\n          attr.validators.optional(attr.validators.instance_of(float)),\n          _std_not_negative\n      ],\n      init=True,\n      default=None,\n      kw_only=True)\n\n\n# Use when you want to preserve the shapes or reduce if-else statements.\n# e.g. `metrics.get('metric_name', NaNMetric).value` to get NaN or the actual\n# value.\nNaNMetric = Metric(value=np.nan)\n\n\n# TODO: This class should be deleted in the future.\n@attr.s(auto_attribs=True, frozen=True, init=True, slots=True, repr=False)\nclass ParameterValue:\n  \"\"\"Immutable wrapper for vizier_pb2.Parameter.value, which is a oneof field.\n\n  Has accessors (properties) that cast the value into the type according\n  to StudyConfiguration class behavior. In particular, 'true' and FALSE_VALUE\n  are\n  treated as special strings that are cast to a numeric value of 1 and 0,\n  respectively, and boolean value of True and False, repectively.\n  \"\"\"\n\n  value: ParameterValueTypes = attr.ib(\n      init=True,\n      validator=[\n          attr.validators.instance_of((str, int, float, bool)),\n      ])\n\n  def cast_as_internal(self,\n                       internal_type: ParameterType) -> ParameterValueTypes:\n    \"\"\"Cast to the internal type.\"\"\"\n    internal_type.assert_correct_type(self.value)\n\n    if internal_type in (ParameterType.DOUBLE, ParameterType.DISCRETE):\n      return self.as_float\n    elif internal_type == ParameterType.INTEGER:\n      return self.as_int\n    elif internal_type == ParameterType.CATEGORICAL:\n      return self.as_str\n    else:\n      raise RuntimeError(f'Unknown type {internal_type}')\n\n  def cast(\n      self,\n      external_type: ExternalType,\n  ) -> ParameterValueTypes:\n    \"\"\"Returns ParameterValue cast to external_type.\n\n    Args:\n      external_type:\n\n    Returns:\n      self.value if external_type is INTERNAL.\n      self.as_bool if external_type is BOOLEAN.\n      self.as_int if external_type is INTEGER.\n      self.as_float if external_type is FLOAT.\n\n    Raises:\n      ValueError: If external_type is not valid.\n    \"\"\"\n    if external_type == ExternalType.INTERNAL:\n      return self.value", "metadata": {"task_id": "google_vizier/79", "ground_truth": "    elif external_type == ExternalType.BOOLEAN:", "fpath_tuple": ["google_vizier", "vizier", "_src", "pyvizier", "shared", "trial.py"], "context_start_lineno": 0, "line_no": 175, "query_window": {"context": "  def cast(\n      self,\n      external_type: ExternalType,\n  ) -> ParameterValueTypes:\n    \"\"\"Returns ParameterValue cast to external_type.\n\n    Args:\n      external_type:\n\n    Returns:\n      self.value if external_type is INTERNAL.\n      self.as_bool if external_type is BOOLEAN.\n      self.as_int if external_type is INTEGER.\n      self.as_float if external_type is FLOAT.\n\n    Raises:\n      ValueError: If external_type is not valid.\n    \"\"\"\n    if external_type == ExternalType.INTERNAL:\n      return self.value", "metadata": {"fpath_tuple": ["google_vizier", "vizier", "_src", "pyvizier", "shared", "trial.py"], "line_no": 175, "task_id": "google_vizier/79", "start_line_no": 155, "end_line_no": 175, "window_size": 20, "context_start_lineno": 0, "repo": "google_vizier"}}, "top_k_context": [{"context": "      external_values[pc.name] = external_value\n      remaining_parameters.pop(pc.name)\n    return external_values\n\n  def trial_parameters(\n      self, proto: study_pb2.Trial) -> Dict[str, ParameterValueSequence]:\n    \"\"\"Returns the trial values, cast to external types, if they exist.\n\n    Args:\n      proto:\n\n    Returns:\n      Parameter values dict: cast to each parameter's external_type, if exists.\n      NOTE that the values in the dict may be a Sequence as opposed to a single\n      element.\n\n    Raises:\n      ValueError: If the trial parameters do not exist in this search space.\n      ValueError: If the trial contains duplicate parameters.\n    \"\"\"", "metadata": [{"fpath_tuple": ["google_vizier", "vizier", "_src", "pyvizier", "oss", "study_config.py"], "line_no": 328, "start_line_no": 318, "end_line_no": 338, "window_size": 20, "repo": "google_vizier", "slice_size": 10}], "sim_score": 0.30357142857142855}, {"context": "\n  # TODO: Rename to `validate_value or is_feasible`\n  def contains(\n      self, value: Union[trial.ParameterValueTypes,\n                         trial.ParameterValue]) -> bool:\n    \"\"\"Check if the `value` is a valid value for this parameter config.\"\"\"\n    if isinstance(value, trial.ParameterValue):\n      # TODO: Extract the raw value.\n      value = value.value\n    try:\n      self._assert_feasible(value)\n    except (TypeError, ValueError):\n      return False\n    return True\n\n  @property\n  def num_feasible_values(self) -> Union[float, int]:\n    if self.type == ParameterType.DOUBLE:\n      return float('inf')\n    elif self.type == ParameterType.INTEGER:", "metadata": [{"fpath_tuple": ["google_vizier", "vizier", "_src", "pyvizier", "shared", "parameter_config.py"], "line_no": 538, "start_line_no": 528, "end_line_no": 548, "window_size": 20, "repo": "google_vizier", "slice_size": 10}], "sim_score": 0.2833333333333333}, {"context": "    if isinstance(value, trial.ParameterValue):\n      # TODO: Extract the raw value.\n      value = value.value\n    try:\n      self._assert_feasible(value)\n    except (TypeError, ValueError):\n      return False\n    return True\n\n  @property\n  def num_feasible_values(self) -> Union[float, int]:\n    if self.type == ParameterType.DOUBLE:\n      return float('inf')\n    elif self.type == ParameterType.INTEGER:\n      return self.bounds[1] - self.bounds[0] + 1\n    else:\n      return len(self.feasible_values)\n\n  def _assert_bounds(self, value: trial.ParameterValueTypes) -> None:\n    if not self.bounds[0] <= value <= self.bounds[1]:", "metadata": [{"fpath_tuple": ["google_vizier", "vizier", "_src", "pyvizier", "shared", "parameter_config.py"], "line_no": 544, "start_line_no": 534, "end_line_no": 554, "window_size": 20, "repo": "google_vizier", "slice_size": 10}], "sim_score": 0.2831858407079646}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# torchrl/modules/tensordict_module/actors.py\n# --------------------------------------------------\n#         >>> from torchrl.modules import Actor\n#         >>> td = TensorDict({\"observation\": torch.randn(3, 4)}, [3,])\n#         >>> action_spec = UnboundedContinuousTensorSpec(4)\n#         >>> module = torch.nn.Linear(4, 4)\n#         >>> td_module = Actor(\n#         ...    module=module,\n#         ...    spec=action_spec,\n#         ...    )\n#         >>> td_module(td)\n#         >>> print(td.get(\"action\"))\n# \n#     \"\"\"\n# \n#     def __init__(\n#         self,\n#         *args,\n#         in_keys: Optional[Sequence[str]] = None,\n#         out_keys: Optional[Sequence[str]] = None,\n#         spec: Optional[TensorSpec] = None,\n#         **kwargs,\n# --------------------------------------------------\n# the below code fragment can be found in:\n# torchrl/modules/tensordict_module/common.py\n# --------------------------------------------------\n#         >>> import torch\n#         >>> from tensordict import TensorDict\n#         >>> from tensordict.nn.functional_modules import make_functional\n#         >>> from torchrl.data import UnboundedContinuousTensorSpec\n#         >>> from torchrl.modules import SafeModule\n#         >>> td = TensorDict({\"input\": torch.randn(3, 4), \"hidden\": torch.randn(3, 8)}, [3,])\n#         >>> spec = UnboundedContinuousTensorSpec(8)\n#         >>> module = torch.nn.GRUCell(4, 8)\n#         >>> td_fmodule = SafeModule(\n#         ...    module=module,\n#         ...    spec=spec,\n#         ...    in_keys=[\"input\", \"hidden\"],\n#         ...    out_keys=[\"output\"],\n#         ...    )\n#         >>> params = make_functional(td_fmodule)\n#         >>> td_functional = td_fmodule(td.clone(), params=params)\n#         >>> print(td_functional)\n#         TensorDict(\n#             fields={\n#                 hidden: Tensor(torch.Size([3, 8]), dtype=torch.float32),\n# --------------------------------------------------\n# the below code fragment can be found in:\n# torchrl/modules/tensordict_module/common.py\n# --------------------------------------------------\n#         >>> from tensordict.nn.functional_modules import make_functional\n#         >>> from torchrl.data import UnboundedContinuousTensorSpec\n#         >>> from torchrl.modules import SafeModule\n#         >>> td = TensorDict({\"input\": torch.randn(3, 4), \"hidden\": torch.randn(3, 8)}, [3,])\n#         >>> spec = UnboundedContinuousTensorSpec(8)\n#         >>> module = torch.nn.GRUCell(4, 8)\n#         >>> td_fmodule = SafeModule(\n#         ...    module=module,\n#         ...    spec=spec,\n#         ...    in_keys=[\"input\", \"hidden\"],\n#         ...    out_keys=[\"output\"],\n#         ...    )\n#         >>> params = make_functional(td_fmodule)\n#         >>> td_functional = td_fmodule(td.clone(), params=params)\n#         >>> print(td_functional)\n#         TensorDict(\n#             fields={\n#                 hidden: Tensor(torch.Size([3, 8]), dtype=torch.float32),\n#                 input: Tensor(torch.Size([3, 4]), dtype=torch.float32),\n#                 output: Tensor(torch.Size([3, 8]), dtype=torch.float32)},\n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\n# Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport argparse\n\nimport pytest\nimport torch\nfrom tensordict import TensorDict\nfrom tensordict.nn.functional_modules import make_functional\nfrom torch import nn\nfrom torchrl.data.tensor_specs import (\n    BoundedTensorSpec,\n    CompositeSpec,\n    UnboundedContinuousTensorSpec,\n)\nfrom torchrl.envs.utils import set_exploration_mode\nfrom torchrl.modules import NormalParamWrapper, SafeModule, TanhNormal\nfrom torchrl.modules.tensordict_module.common import (\n    ensure_tensordict_compatible,\n    is_tensordict_compatible,\n)\nfrom torchrl.modules.tensordict_module.probabilistic import (\n    SafeProbabilisticModule,\n    SafeProbabilisticSequential,\n)\nfrom torchrl.modules.tensordict_module.sequence import SafeSequential\n\n_has_functorch = False\ntry:\n    from functorch import vmap\n\n    _has_functorch = True\nexcept ImportError:\n    pass\n\n\nclass TestTDModule:\n    def test_multiple_output(self):\n        class MultiHeadLinear(nn.Module):\n            def __init__(self, in_1, out_1, out_2, out_3):\n                super().__init__()\n                self.linear_1 = nn.Linear(in_1, out_1)\n                self.linear_2 = nn.Linear(in_1, out_2)\n                self.linear_3 = nn.Linear(in_1, out_3)\n\n            def forward(self, x):\n                return self.linear_1(x), self.linear_2(x), self.linear_3(x)\n\n        tensordict_module = SafeModule(\n            MultiHeadLinear(5, 4, 3, 2),\n            in_keys=[\"input\"],\n            out_keys=[\"out_1\", \"out_2\", \"out_3\"],\n        )\n        td = TensorDict({\"input\": torch.randn(3, 5)}, batch_size=[3])\n        td = tensordict_module(td)\n        assert td.shape == torch.Size([3])\n        assert \"input\" in td.keys()\n        assert \"out_1\" in td.keys()\n        assert \"out_2\" in td.keys()\n        assert \"out_3\" in td.keys()\n        assert td.get(\"out_3\").shape == torch.Size([3, 2])\n\n        # Using \"_\" key to ignore some output\n        tensordict_module = SafeModule(\n            MultiHeadLinear(5, 4, 3, 2),\n            in_keys=[\"input\"],\n            out_keys=[\"_\", \"_\", \"out_3\"],\n        )\n        td = TensorDict({\"input\": torch.randn(3, 5)}, batch_size=[3])\n        td = tensordict_module(td)\n        assert td.shape == torch.Size([3])\n        assert \"input\" in td.keys()\n        assert \"out_3\" in td.keys()\n        assert \"_\" not in td.keys()\n        assert td.get(\"out_3\").shape == torch.Size([3, 2])\n\n    def test_spec_key_warning(self):\n        class MultiHeadLinear(nn.Module):\n            def __init__(self, in_1, out_1, out_2):\n                super().__init__()\n                self.linear_1 = nn.Linear(in_1, out_1)\n                self.linear_2 = nn.Linear(in_1, out_2)\n\n            def forward(self, x):\n                return self.linear_1(x), self.linear_2(x)\n\n        spec_dict = {\n            \"_\": UnboundedContinuousTensorSpec((4,)),\n            \"out_2\": UnboundedContinuousTensorSpec((3,)),\n        }\n\n        # warning due to \"_\" in spec keys\n        with pytest.warns(UserWarning, match='got a spec with key \"_\"'):\n            tensordict_module = SafeModule(\n                MultiHeadLinear(5, 4, 3),\n                in_keys=[\"input\"],\n                out_keys=[\"_\", \"out_2\"],\n                spec=CompositeSpec(**spec_dict),\n            )\n\n    @pytest.mark.parametrize(\"safe\", [True, False])\n    @pytest.mark.parametrize(\"spec_type\", [None, \"bounded\", \"unbounded\"])\n    @pytest.mark.parametrize(\"lazy\", [True, False])\n    def test_stateful(self, safe, spec_type, lazy):\n        torch.manual_seed(0)\n        param_multiplier = 1\n        if lazy:\n            net = nn.LazyLinear(4 * param_multiplier)\n        else:\n            net = nn.Linear(3, 4 * param_multiplier)\n\n        if spec_type is None:\n            spec = None\n        elif spec_type == \"bounded\":\n            spec = BoundedTensorSpec(-0.1, 0.1, 4)\n        elif spec_type == \"unbounded\":\n            spec = UnboundedContinuousTensorSpec(4)\n\n        if safe and spec is None:\n            with pytest.raises(\n                RuntimeError,\n                match=\"is not a valid configuration as the tensor specs are not \"\n                \"specified\",\n            ):\n                tensordict_module = SafeModule(\n                    module=net,\n                    spec=spec,\n                    in_keys=[\"in\"],\n                    out_keys=[\"out\"],\n                    safe=safe,\n                )\n            return\n        else:\n            tensordict_module = SafeModule(\n                module=net,\n                spec=spec,\n                in_keys=[\"in\"],\n                out_keys=[\"out\"],\n                safe=safe,\n            )\n\n        td = TensorDict({\"in\": torch.randn(3, 3)}, [3])\n        tensordict_module(td)\n        assert td.shape == torch.Size([3])\n        assert td.get(\"out\").shape == torch.Size([3, 4])\n\n        # test bounds", "metadata": {"task_id": "pytorch_rl/82", "ground_truth": "        if not safe and spec_type == \"bounded\":", "fpath_tuple": ["pytorch_rl", "test", "test_tensordictmodules.py"], "context_start_lineno": 0, "line_no": 149, "query_window": {"context": "                    in_keys=[\"in\"],\n                    out_keys=[\"out\"],\n                    safe=safe,\n                )\n            return\n        else:\n            tensordict_module = SafeModule(\n                module=net,\n                spec=spec,\n                in_keys=[\"in\"],\n                out_keys=[\"out\"],\n                safe=safe,\n            )\n\n        td = TensorDict({\"in\": torch.randn(3, 3)}, [3])\n        tensordict_module(td)\n        assert td.shape == torch.Size([3])\n        assert td.get(\"out\").shape == torch.Size([3, 4])\n\n        # test bounds", "metadata": {"fpath_tuple": ["pytorch_rl", "test", "test_tensordictmodules.py"], "line_no": 149, "task_id": "pytorch_rl/82", "start_line_no": 129, "end_line_no": 149, "window_size": 20, "context_start_lineno": 0, "repo": "pytorch_rl"}}, "top_k_context": [{"context": "        >>> import torch\n        >>> from tensordict import TensorDict\n        >>> from tensordict.nn.functional_modules import make_functional\n        >>> from torchrl.data import UnboundedContinuousTensorSpec\n        >>> from torchrl.modules import SafeModule\n        >>> td = TensorDict({\"input\": torch.randn(3, 4), \"hidden\": torch.randn(3, 8)}, [3,])\n        >>> spec = UnboundedContinuousTensorSpec(8)\n        >>> module = torch.nn.GRUCell(4, 8)\n        >>> td_fmodule = SafeModule(\n        ...    module=module,\n        ...    spec=spec,\n        ...    in_keys=[\"input\", \"hidden\"],\n        ...    out_keys=[\"output\"],\n        ...    )\n        >>> params = make_functional(td_fmodule)\n        >>> td_functional = td_fmodule(td.clone(), params=params)\n        >>> print(td_functional)\n        TensorDict(\n            fields={\n                hidden: Tensor(torch.Size([3, 8]), dtype=torch.float32),", "metadata": [{"fpath_tuple": ["pytorch_rl", "torchrl", "modules", "tensordict_module", "common.py"], "line_no": 130, "start_line_no": 120, "end_line_no": 140, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}], "sim_score": 0.4036697247706422}, {"context": "\n    Examples:\n        >>> import torch\n        >>> from tensordict import TensorDict\n        >>> from tensordict.nn.functional_modules import make_functional\n        >>> from torchrl.data import UnboundedContinuousTensorSpec\n        >>> from torchrl.modules import SafeModule\n        >>> td = TensorDict({\"input\": torch.randn(3, 4), \"hidden\": torch.randn(3, 8)}, [3,])\n        >>> spec = UnboundedContinuousTensorSpec(8)\n        >>> module = torch.nn.GRUCell(4, 8)\n        >>> td_fmodule = SafeModule(\n        ...    module=module,\n        ...    spec=spec,\n        ...    in_keys=[\"input\", \"hidden\"],\n        ...    out_keys=[\"output\"],\n        ...    )\n        >>> params = make_functional(td_fmodule)\n        >>> td_functional = td_fmodule(td.clone(), params=params)\n        >>> print(td_functional)\n        TensorDict(", "metadata": [{"fpath_tuple": ["pytorch_rl", "torchrl", "modules", "tensordict_module", "common.py"], "line_no": 128, "start_line_no": 118, "end_line_no": 138, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}], "sim_score": 0.4}, {"context": "        >>> from tensordict import TensorDict\n        >>> from torchrl.data import UnboundedContinuousTensorSpec\n        >>> from torchrl.modules import Actor\n        >>> td = TensorDict({\"observation\": torch.randn(3, 4)}, [3,])\n        >>> action_spec = UnboundedContinuousTensorSpec(4)\n        >>> module = torch.nn.Linear(4, 4)\n        >>> td_module = Actor(\n        ...    module=module,\n        ...    spec=action_spec,\n        ...    )\n        >>> td_module(td)\n        >>> print(td.get(\"action\"))\n\n    \"\"\"\n\n    def __init__(\n        self,\n        *args,\n        in_keys: Optional[Sequence[str]] = None,\n        out_keys: Optional[Sequence[str]] = None,", "metadata": [{"fpath_tuple": ["pytorch_rl", "torchrl", "modules", "tensordict_module", "actors.py"], "line_no": 40, "start_line_no": 30, "end_line_no": 50, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}], "sim_score": 0.37142857142857144}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# test/test_cost.py\n# --------------------------------------------------\n#         v1 = vec_td_lambda_advantage_estimate(\n#             gamma, lmbda, state_value, next_state_value, reward, done\n#         )\n#         v2 = vec_td_lambda_advantage_estimate(\n#             gamma_tensor, lmbda, state_value, next_state_value, reward, done\n#         )\n# \n#         torch.testing.assert_close(v1, v2, rtol=1e-4, atol=1e-4)\n# \n#         # # same with last done being true\n#         done[..., -1, :] = True  # terminating trajectory\n#         gamma_tensor[..., -1, :] = 0.0\n# \n#         v1 = vec_td_lambda_advantage_estimate(\n#             gamma, lmbda, state_value, next_state_value, reward, done\n#         )\n#         v2 = vec_td_lambda_advantage_estimate(\n#             gamma_tensor, lmbda, state_value, next_state_value, reward, done\n#         )\n# \n# --------------------------------------------------\n# the below code fragment can be found in:\n# test/test_cost.py\n# --------------------------------------------------\n#         )\n#         v2 = vec_td_lambda_advantage_estimate(\n#             gamma_tensor, lmbda, state_value, next_state_value, reward, done\n#         )\n# \n#         torch.testing.assert_close(v1, v2, rtol=1e-4, atol=1e-4)\n# \n#         # # same with last done being true\n#         done[..., -1, :] = True  # terminating trajectory\n#         gamma_tensor[..., -1, :] = 0.0\n# \n#         v1 = vec_td_lambda_advantage_estimate(\n#             gamma, lmbda, state_value, next_state_value, reward, done\n#         )\n#         v2 = vec_td_lambda_advantage_estimate(\n#             gamma_tensor, lmbda, state_value, next_state_value, reward, done\n#         )\n# \n#         torch.testing.assert_close(v1, v2, rtol=1e-4, atol=1e-4)\n# \n# --------------------------------------------------\n# the below code fragment can be found in:\n# test/test_cost.py\n# --------------------------------------------------\n#             gamma_tensor, lmbda, state_value, next_state_value, reward, done\n#         )\n# \n#         torch.testing.assert_close(v1, v2, rtol=1e-4, atol=1e-4)\n# \n#         # # same with last done being true\n#         done[..., -1, :] = True  # terminating trajectory\n#         gamma_tensor[..., -1, :] = 0.0\n# \n#         v1 = vec_td_lambda_advantage_estimate(\n#             gamma, lmbda, state_value, next_state_value, reward, done\n#         )\n#         v2 = vec_td_lambda_advantage_estimate(\n#             gamma_tensor, lmbda, state_value, next_state_value, reward, done\n#         )\n# \n#         torch.testing.assert_close(v1, v2, rtol=1e-4, atol=1e-4)\n# \n#     @pytest.mark.parametrize(\"device\", get_available_devices())\n#     @pytest.mark.parametrize(\"gamma\", [0.5, 0.99, 0.1])\n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\n = vec_td_lambda_advantage_estimate(\n            gamma_tensor, lmbda, state_value, next_state_value, reward, done\n        )\n\n        torch.testing.assert_close(v1, v2, rtol=1e-4, atol=1e-4)\n\n    @pytest.mark.parametrize(\"device\", get_available_devices())\n    @pytest.mark.parametrize(\"lmbda\", [0.1, 0.5, 0.99])\n    @pytest.mark.parametrize(\"N\", [(3,), (7, 3)])\n    @pytest.mark.parametrize(\"T\", [50, 3])\n    @pytest.mark.parametrize(\"rolling_gamma\", [True, False, None])\n    @pytest.mark.parametrize(\"has_done\", [True, False])\n    @pytest.mark.parametrize(\"seed\", range(1))\n    def test_vectdlambda_rand_gamma(\n        self, device, lmbda, N, T, rolling_gamma, dtype_fixture, has_done, seed  # noqa\n    ):\n        \"\"\"Tests td_lambda_advantage_estimate against vec_td_lambda_advantage_estimate\n        with gamma being a random tensor\n\n        \"\"\"\n        torch.manual_seed(seed)\n\n        done = torch.zeros(*N, T, 1, device=device, dtype=torch.bool)\n        if has_done:\n            done = done.bernoulli_(0.1)\n        reward = torch.randn(*N, T, 1, device=device)\n        state_value = torch.randn(*N, T, 1, device=device)\n        next_state_value = torch.randn(*N, T, 1, device=device)\n\n        # avoid low values of gamma\n        gamma_tensor = 0.5 + torch.rand_like(next_state_value) / 2\n\n        v1 = td_lambda_advantage_estimate(\n            gamma_tensor,\n            lmbda,\n            state_value,\n            next_state_value,\n            reward,\n            done,\n            rolling_gamma,\n        )\n        if rolling_gamma is False and not done[..., 1:, :][done[..., :-1, :]].all():\n            # if a not-done follows a done, then rolling_gamma=False cannot be used\n            with pytest.raises(\n                NotImplementedError, match=\"When using rolling_gamma=False\"\n            ):\n                vec_td_lambda_advantage_estimate(\n                    gamma_tensor,\n                    lmbda,\n                    state_value,\n                    next_state_value,\n                    reward,\n                    done,\n                    rolling_gamma,\n                )\n            return\n        v2 = vec_td_lambda_advantage_estimate(\n            gamma_tensor,\n            lmbda,\n            state_value,\n            next_state_value,\n            reward,\n            done,\n            rolling_gamma,\n        )\n        torch.testing.assert_close(v1, v2, rtol=1e-4, atol=1e-4)\n\n    @pytest.mark.parametrize(\"device\", get_available_devices())\n    @pytest.mark.parametrize(\"gamma\", [0.99, \"rand\"])\n    @pytest.mark.parametrize(\"N\", [(3,), (3, 7)])\n    @pytest.mark.parametrize(\"T\", [3, 5, 200])\n    @pytest.mark.parametrize(\"rolling_gamma\", [True, False])\n    def test_custom_conv1d_tensor(self, device, gamma, N, T, rolling_gamma):\n        \"\"\"\n        Tests the _custom_conv1d logic against a manual for-loop implementation\n        \"\"\"\n        torch.manual_seed(0)\n\n        if gamma == \"rand\":\n            gamma = torch.rand(*N, T, 1, device=device)\n            rand_gamma = True\n        else:\n            gamma = torch.full((*N, T, 1), gamma, device=device)\n            rand_gamma = False\n\n        values = torch.randn(*N, 1, T, device=device)\n        out = torch.zeros(*N, 1, T, device=device)\n        if rand_gamma and not rolling_gamma:\n            for i in range(T):\n                for j in reversed(range(i, T)):\n                    out[..., i] = out[..., i] * gamma[..., i, :] + values[..., j]\n        else:\n            prev_val = 0.0\n            for i in reversed(range(T)):\n                prev_val = out[..., i] = prev_val * gamma[..., i, :] + values[..., i]\n\n        gammas = _make_gammas_tensor(gamma, T, rolling_gamma)\n        gammas = gammas.cumprod(-2)\n        out_custom = _custom_conv1d(values.view(-1, 1, T), gammas).reshape(values.shape)\n\n        torch.testing.assert_close(out, out_custom, rtol=1e-4, atol=1e-4)\n\n    @pytest.mark.parametrize(\"device\", get_available_devices())\n    @pytest.mark.parametrize(\"N\", [(3,), (3, 7)])\n    @pytest.mark.parametrize(\"T\", [3, 5, 200])\n    @pytest.mark.parametrize(\"rolling_gamma\", [True, False])\n    def test_successive_traj_tdlambda(self, device, N, T, rolling_gamma):\n        \"\"\"Tests td_lambda_advantage_estimate against vec_td_lambda_advantage_estimate\n        with gamma being a random tensor\n\n        \"\"\"\n        torch.manual_seed(0)\n\n        lmbda = torch.rand([]).item()\n\n        done = torch.zeros(*N, T, 1, device=device, dtype=torch.bool)\n        done[..., T // 2 - 1, :] = 1\n\n        reward = torch.randn(*N, T, 1, device=device)\n        state_value = torch.randn(*N, T, 1, device=device)\n        next_state_value = torch.randn(*N, T, 1, device=device)\n\n        # avoid low values of gamma\n        gamma_tensor = 0.5 + torch.rand_like(next_state_value) / 2\n\n        v1 = td_lambda_advantage_estimate(\n            gamma_tensor,\n            lmbda,\n            state_value,\n            next_state_value,\n            reward,\n            done,\n            rolling_gamma,\n        )\n        v1a = td_lambda_advantage_estimate(\n            gamma_tensor[..., : T // 2, :],\n            lmbda,\n            state_value[..., : T // 2, :],\n            next_state_value[..., : T // 2, :],\n            reward[..., : T // 2, :],\n            done[..., : T // 2, :],\n            rolling_gamma,\n        )\n        v1b = td_lambda_advantage_estimate(\n            gamma_tensor[..., T // 2 :, :],\n            lmbda,\n            state_value[..., T // 2 :, :],\n            next_state_value[..., T // 2 :, :],\n            reward[..., T // 2 :, :],\n            done[..., T // 2 :, :],\n            rolling_gamma,\n        )\n        torch.testing.assert_close(v1, torch.cat([v1a, v1b], -2), rtol=1e-4, atol=1e-4)\n\n        if not rolling_gamma:\n            with pytest.raises(\n                NotImplementedError, match=\"When using rolling_gamma=False\"\n            ):\n                vec_td_lambda_advantage_estimate(\n                    gamma_tensor,\n                    lmbda,", "metadata": {"task_id": "pytorch_rl/78", "ground_truth": "                    state_value,", "fpath_tuple": ["pytorch_rl", "test", "test_cost.py"], "context_start_lineno": 3095, "line_no": 3256, "query_window": {"context": "            rolling_gamma,\n        )\n        v1b = td_lambda_advantage_estimate(\n            gamma_tensor[..., T // 2 :, :],\n            lmbda,\n            state_value[..., T // 2 :, :],\n            next_state_value[..., T // 2 :, :],\n            reward[..., T // 2 :, :],\n            done[..., T // 2 :, :],\n            rolling_gamma,\n        )\n        torch.testing.assert_close(v1, torch.cat([v1a, v1b], -2), rtol=1e-4, atol=1e-4)\n\n        if not rolling_gamma:\n            with pytest.raises(\n                NotImplementedError, match=\"When using rolling_gamma=False\"\n            ):\n                vec_td_lambda_advantage_estimate(\n                    gamma_tensor,\n                    lmbda,", "metadata": {"fpath_tuple": ["pytorch_rl", "test", "test_cost.py"], "line_no": 3256, "task_id": "pytorch_rl/78", "start_line_no": 3236, "end_line_no": 3256, "window_size": 20, "context_start_lineno": 3095, "repo": "pytorch_rl"}}, "top_k_context": [{"context": "        )\n        v2 = vec_td_lambda_advantage_estimate(\n            gamma_tensor, lmbda, state_value, next_state_value, reward, done\n        )\n\n        torch.testing.assert_close(v1, v2, rtol=1e-4, atol=1e-4)\n\n        # # same with last done being true\n        done[..., -1, :] = True  # terminating trajectory\n        gamma_tensor[..., -1, :] = 0.0\n\n        v1 = vec_td_lambda_advantage_estimate(\n            gamma, lmbda, state_value, next_state_value, reward, done\n        )\n        v2 = vec_td_lambda_advantage_estimate(\n            gamma_tensor, lmbda, state_value, next_state_value, reward, done\n        )\n\n        torch.testing.assert_close(v1, v2, rtol=1e-4, atol=1e-4)\n", "metadata": [{"fpath_tuple": ["pytorch_rl", "test", "test_cost.py"], "line_no": 3044, "start_line_no": 3034, "end_line_no": 3054, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}], "sim_score": 0.5208333333333334}, {"context": "        v1 = vec_td_lambda_advantage_estimate(\n            gamma, lmbda, state_value, next_state_value, reward, done\n        )\n        v2 = vec_td_lambda_advantage_estimate(\n            gamma_tensor, lmbda, state_value, next_state_value, reward, done\n        )\n\n        torch.testing.assert_close(v1, v2, rtol=1e-4, atol=1e-4)\n\n        # # same with last done being true\n        done[..., -1, :] = True  # terminating trajectory\n        gamma_tensor[..., -1, :] = 0.0\n\n        v1 = vec_td_lambda_advantage_estimate(\n            gamma, lmbda, state_value, next_state_value, reward, done\n        )\n        v2 = vec_td_lambda_advantage_estimate(\n            gamma_tensor, lmbda, state_value, next_state_value, reward, done\n        )\n", "metadata": [{"fpath_tuple": ["pytorch_rl", "test", "test_cost.py"], "line_no": 3042, "start_line_no": 3032, "end_line_no": 3052, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}], "sim_score": 0.5208333333333334}, {"context": "        gamma_tensor = torch.full((*N, T, 1), gamma, device=device)\n\n        v1 = vec_td_lambda_advantage_estimate(\n            gamma, lmbda, state_value, next_state_value, reward, done\n        )\n        v2 = vec_td_lambda_advantage_estimate(\n            gamma_tensor, lmbda, state_value, next_state_value, reward, done\n        )\n\n        torch.testing.assert_close(v1, v2, rtol=1e-4, atol=1e-4)\n\n        # # same with last done being true\n        done[..., -1, :] = True  # terminating trajectory\n        gamma_tensor[..., -1, :] = 0.0\n\n        v1 = vec_td_lambda_advantage_estimate(\n            gamma, lmbda, state_value, next_state_value, reward, done\n        )\n        v2 = vec_td_lambda_advantage_estimate(\n            gamma_tensor, lmbda, state_value, next_state_value, reward, done", "metadata": [{"fpath_tuple": ["pytorch_rl", "test", "test_cost.py"], "line_no": 3040, "start_line_no": 3030, "end_line_no": 3050, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}], "sim_score": 0.5048543689320388}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# examples/distributed/distributed_replay_buffer.py\n# --------------------------------------------------\n# from torchrl.data.replay_buffers.writers import RoundRobinWriter\n# \n# RETRY_LIMIT = 2\n# RETRY_DELAY_SECS = 3\n# REPLAY_BUFFER_NODE = \"ReplayBuffer\"\n# TRAINER_NODE = \"Trainer\"\n# \n# parser = argparse.ArgumentParser(\n#     description=\"RPC Replay Buffer Example\",\n#     formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n# )\n# \n# parser.add_argument(\n#     \"--rank\",\n#     type=int,\n#     default=-1,\n#     help=\"Node Rank [0 = Replay Buffer, 1 = Dummy Trainer, 2+ = Dummy Data Collector]\",\n# )\n# \n# \n# --------------------------------------------------\n# the below code fragment can be found in:\n# examples/distributed/distributed_replay_buffer.py\n# --------------------------------------------------\n#     description=\"RPC Replay Buffer Example\",\n#     formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n# )\n# \n# parser.add_argument(\n#     \"--rank\",\n#     type=int,\n#     default=-1,\n#     help=\"Node Rank [0 = Replay Buffer, 1 = Dummy Trainer, 2+ = Dummy Data Collector]\",\n# )\n# \n# \n# class DummyDataCollectorNode:\n#     \"\"\"Data collector node responsible for collecting experiences used for learning.\n# \n#     Args:\n#         replay_buffer (rpc.RRef): the RRef associated with the construction of the replay buffer\n#     \"\"\"\n# \n#     def __init__(self, replay_buffer: rpc.RRef) -> None:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# examples/distributed/distributed_replay_buffer.py\n# --------------------------------------------------\n# RETRY_LIMIT = 2\n# RETRY_DELAY_SECS = 3\n# REPLAY_BUFFER_NODE = \"ReplayBuffer\"\n# TRAINER_NODE = \"Trainer\"\n# \n# parser = argparse.ArgumentParser(\n#     description=\"RPC Replay Buffer Example\",\n#     formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n# )\n# \n# parser.add_argument(\n#     \"--rank\",\n#     type=int,\n#     default=-1,\n#     help=\"Node Rank [0 = Replay Buffer, 1 = Dummy Trainer, 2+ = Dummy Data Collector]\",\n# )\n# \n# \n# class DummyDataCollectorNode:\n#     \"\"\"Data collector node responsible for collecting experiences used for learning.\n# --------------------------------------------------\n# the below code fragment can be found in:\n# examples/distributed/distributed_replay_buffer.py\n# --------------------------------------------------\n# \n# parser = argparse.ArgumentParser(\n#     description=\"RPC Replay Buffer Example\",\n#     formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n# )\n# \n# parser.add_argument(\n#     \"--rank\",\n#     type=int,\n#     default=-1,\n#     help=\"Node Rank [0 = Replay Buffer, 1 = Dummy Trainer, 2+ = Dummy Data Collector]\",\n# )\n# \n# \n# class DummyDataCollectorNode:\n#     \"\"\"Data collector node responsible for collecting experiences used for learning.\n# \n#     Args:\n#         replay_buffer (rpc.RRef): the RRef associated with the construction of the replay buffer\n#     \"\"\"\n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\n# Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\n\"\"\"\nSample latency benchmarking (using RPC)\n======================================\nA rough benchmark of sample latency using different storage types over the network using `torch.rpc`.\nRun this script with --rank=0 and --rank=1 flags set in separate processes - these ranks correspond to the trainer worker and buffer worker respectively, and both need to be initialised.\ne.g. to benchmark LazyMemmapStorage, run the following commands using either two separate shells or multiprocessing.\n    - python3 benchmark_sample_latency_over_rpc.py --rank=0 --storage=LazyMemmapStorage\n    - python3 benchmark_sample_latency_over_rpc.py --rank=1 --storage=LazyMemmapStorage\nThis code is based on examples/distributed/distributed_replay_buffer.py.\n\"\"\"\nimport argparse\nimport os\nimport pickle\nimport sys\nimport time\nimport timeit\nfrom datetime import datetime\n\nimport torch\nimport torch.distributed.rpc as rpc\nfrom tensordict import TensorDict\nfrom torchrl.data.replay_buffers import RemoteTensorDictReplayBuffer\nfrom torchrl.data.replay_buffers.samplers import RandomSampler\nfrom torchrl.data.replay_buffers.storages import (\n    LazyMemmapStorage,\n    LazyTensorStorage,\n    ListStorage,\n)\nfrom torchrl.data.replay_buffers.writers import RoundRobinWriter\n\nRETRY_LIMIT = 2\nRETRY_DELAY_SECS = 3\nREPLAY_BUFFER_NODE = \"ReplayBuffer\"\nTRAINER_NODE = \"Trainer\"\nTENSOR_SIZE = 3 * 86 * 86\nBUFFER_SIZE = 1001\nBATCH_SIZE = 256\nREPEATS = 1000\n\nstorage_options = {\n    \"LazyMemmapStorage\": LazyMemmapStorage,\n    \"LazyTensorStorage\": LazyTensorStorage,\n    \"ListStorage\": ListStorage,\n}\n\nstorage_arg_options = {\n    \"LazyMemmapStorage\": {\"scratch_dir\": \"/tmp/\", \"device\": torch.device(\"cpu\")},\n    \"LazyTensorStorage\": {},\n    \"ListStorage\": {},\n}\nparser = argparse.ArgumentParser(\n    description=\"RPC Replay Buffer Example\",\n    formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n)\n\nparser.add_argument(\n    \"--rank\",\n    type=int,\n    default=-1,\n    help=\"Node Rank [0 = Replay Buffer, 1 = Dummy Trainer, 2+ = Dummy Data Collector]\",\n)\n\nparser.add_argument(\n    \"--storage\",\n    type=str,", "metadata": {"task_id": "pytorch_rl/9", "ground_truth": "    default=\"LazyMemmapStorage\",", "fpath_tuple": ["pytorch_rl", "benchmarks", "storage", "benchmark_sample_latency_over_rpc.py"], "context_start_lineno": 0, "line_no": 70, "query_window": {"context": "storage_arg_options = {\n    \"LazyMemmapStorage\": {\"scratch_dir\": \"/tmp/\", \"device\": torch.device(\"cpu\")},\n    \"LazyTensorStorage\": {},\n    \"ListStorage\": {},\n}\nparser = argparse.ArgumentParser(\n    description=\"RPC Replay Buffer Example\",\n    formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n)\n\nparser.add_argument(\n    \"--rank\",\n    type=int,\n    default=-1,\n    help=\"Node Rank [0 = Replay Buffer, 1 = Dummy Trainer, 2+ = Dummy Data Collector]\",\n)\n\nparser.add_argument(\n    \"--storage\",\n    type=str,", "metadata": {"fpath_tuple": ["pytorch_rl", "benchmarks", "storage", "benchmark_sample_latency_over_rpc.py"], "line_no": 70, "task_id": "pytorch_rl/9", "start_line_no": 50, "end_line_no": 70, "window_size": 20, "context_start_lineno": 0, "repo": "pytorch_rl"}}, "top_k_context": [{"context": "REPLAY_BUFFER_NODE = \"ReplayBuffer\"\nTRAINER_NODE = \"Trainer\"\n\nparser = argparse.ArgumentParser(\n    description=\"RPC Replay Buffer Example\",\n    formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n)\n\nparser.add_argument(\n    \"--rank\",\n    type=int,\n    default=-1,\n    help=\"Node Rank [0 = Replay Buffer, 1 = Dummy Trainer, 2+ = Dummy Data Collector]\",\n)\n\n\nclass DummyDataCollectorNode:\n    \"\"\"Data collector node responsible for collecting experiences used for learning.\n\n    Args:", "metadata": [{"fpath_tuple": ["pytorch_rl", "examples", "distributed", "distributed_replay_buffer.py"], "line_no": 36, "start_line_no": 26, "end_line_no": 46, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}], "sim_score": 0.49557522123893805}, {"context": "from torchrl.data.replay_buffers.writers import RoundRobinWriter\n\nRETRY_LIMIT = 2\nRETRY_DELAY_SECS = 3\nREPLAY_BUFFER_NODE = \"ReplayBuffer\"\nTRAINER_NODE = \"Trainer\"\n\nparser = argparse.ArgumentParser(\n    description=\"RPC Replay Buffer Example\",\n    formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n)\n\nparser.add_argument(\n    \"--rank\",\n    type=int,\n    default=-1,\n    help=\"Node Rank [0 = Replay Buffer, 1 = Dummy Trainer, 2+ = Dummy Data Collector]\",\n)\n\n", "metadata": [{"fpath_tuple": ["pytorch_rl", "examples", "distributed", "distributed_replay_buffer.py"], "line_no": 32, "start_line_no": 22, "end_line_no": 42, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}], "sim_score": 0.4915254237288136}, {"context": "\nparser = argparse.ArgumentParser(\n    description=\"RPC Replay Buffer Example\",\n    formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n)\n\nparser.add_argument(\n    \"--rank\",\n    type=int,\n    default=-1,\n    help=\"Node Rank [0 = Replay Buffer, 1 = Dummy Trainer, 2+ = Dummy Data Collector]\",\n)\n\n\nclass DummyDataCollectorNode:\n    \"\"\"Data collector node responsible for collecting experiences used for learning.\n\n    Args:\n        replay_buffer (rpc.RRef): the RRef associated with the construction of the replay buffer\n    \"\"\"", "metadata": [{"fpath_tuple": ["pytorch_rl", "examples", "distributed", "distributed_replay_buffer.py"], "line_no": 38, "start_line_no": 28, "end_line_no": 48, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}], "sim_score": 0.48672566371681414}, {"context": "from torchrl.data.replay_buffers.storages import LazyMemmapStorage\nfrom torchrl.data.replay_buffers.utils import accept_remote_rref_invocation\nfrom torchrl.data.replay_buffers.writers import RoundRobinWriter\n\nRETRY_LIMIT = 2\nRETRY_DELAY_SECS = 3\nREPLAY_BUFFER_NODE = \"ReplayBuffer\"\nTRAINER_NODE = \"Trainer\"\n\nparser = argparse.ArgumentParser(\n    description=\"RPC Replay Buffer Example\",\n    formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n)\n\nparser.add_argument(\n    \"--rank\",\n    type=int,\n    default=-1,\n    help=\"Node Rank [0 = Replay Buffer, 1 = Dummy Trainer, 2+ = Dummy Data Collector]\",\n)", "metadata": [{"fpath_tuple": ["pytorch_rl", "examples", "distributed", "distributed_replay_buffer.py"], "line_no": 30, "start_line_no": 20, "end_line_no": 40, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}], "sim_score": 0.484375}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# fortuna/model/model_manager/classification.py\n# --------------------------------------------------\n#     def __init__(self, model: nn.Module):\n#         r\"\"\"\n#         Classification model manager class. It orchestrates the forward pass of the model in the probabilistic model.\n# \n#         Parameters\n#         ----------\n#         model : nn.Module\n#             A model describing the deterministic relation between inputs and outputs. The outputs must correspond to\n#             the logits of a softmax probability vector. The output dimension must be the same as the number of classes.\n#             Let :math:`x` be input variables and :math:`w` the random model parameters. Then the model is described by\n#             a function :math:`f(w, x)`, where each component of :math:`f` corresponds to one of the classes.\n#         \"\"\"\n#         self.model = model\n# \n#     def apply(\n#         self,\n#         params: Params,\n#         inputs: Array,\n#         mutable: Optional[Mutable] = None,\n#         train: bool = False,\n# --------------------------------------------------\n# the below code fragment can be found in:\n# fortuna/prob_model/regression.py\n# --------------------------------------------------\n#         output_calibrator: Optional[nn.Module] = RegressionTemperatureScaler(),\n#         seed: int = 0,\n#     ):\n#         r\"\"\"\n#         A probabilistic regressor class.\n# \n#         Parameters\n#         ----------\n#         model : nn.Module\n#             A model describing the deterministic relation between inputs and outputs. It characterizes the mean model\n#             of the likelihood function. The outputs must belong to the same space as the target variables.\n#             Let :math:`x` be input variables and :math:`w` the random model parameters. Then the model is described by\n#             a function :math:`\\mu(w, x)`.\n#         likelihood_log_variance_model: nn.Module\n#             A model characterizing the log-variance of a Gaussian likelihood function. The outputs must belong to the\n#             same space as the target variables. Let :math:`x` be input variables and :math:`w` the random model\n#             parameters. Then the model is described by a function :math:`\\log\\sigma^2(w, x)`.\n#         prior : Prior\n#             A prior distribution object. The default is an isotropic standard Gaussian. Let :math:`w` be the random\n#             model parameters. Then the prior is defined by a distribution :math:`p(w)`.\n# --------------------------------------------------\n# the below code fragment can be found in:\n# fortuna/prob_model/regression.py\n# --------------------------------------------------\n#         prior: Prior = IsotropicGaussianPrior(),\n#         posterior_approximator: PosteriorApproximator = SWAGPosteriorApproximator(),\n#         output_calibrator: Optional[nn.Module] = RegressionTemperatureScaler(),\n#         seed: int = 0,\n#     ):\n#         r\"\"\"\n#         A probabilistic regressor class.\n# \n#         Parameters\n#         ----------\n#         model : nn.Module\n#             A model describing the deterministic relation between inputs and outputs. It characterizes the mean model\n#             of the likelihood function. The outputs must belong to the same space as the target variables.\n#             Let :math:`x` be input variables and :math:`w` the random model parameters. Then the model is described by\n#             a function :math:`\\mu(w, x)`.\n#         likelihood_log_variance_model: nn.Module\n#             A model characterizing the log-variance of a Gaussian likelihood function. The outputs must belong to the\n#             same space as the target variables. Let :math:`x` be input variables and :math:`w` the random model\n#             parameters. Then the model is described by a function :math:`\\log\\sigma^2(w, x)`.\n#         prior : Prior\n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\nfrom typing import Dict, Optional\n\nimport flax.linen as nn\nimport numpy as np\n\nfrom fortuna.data.loader import DataLoader\nfrom fortuna.model.model_manager.classification import \\\n    ClassificationModelManager\nfrom fortuna.output_calibrator.classification import \\\n    ClassificationTemperatureScaler\nfrom fortuna.output_calibrator.output_calib_manager.base import \\\n    OutputCalibManager\nfrom fortuna.prob_model.base import ProbModel\nfrom fortuna.prob_model.calib_config.base import CalibConfig\nfrom fortuna.prob_model.fit_config import FitConfig\nfrom fortuna.prob_model.joint.base import Joint\nfrom fortuna.prob_model.likelihood.classification import \\\n    ClassificationLikelihood\nfrom fortuna.prob_model.posterior.base import PosteriorApproximator\nfrom fortuna.prob_model.posterior.posterior_approximations import \\\n    PosteriorApproximations\nfrom fortuna.prob_model.posterior.swag.swag_approximator import \\\n    SWAGPosteriorApproximator\nfrom fortuna.prob_model.predictive.classification import \\\n    ClassificationPredictive\nfrom fortuna.prob_model.prior import IsotropicGaussianPrior\nfrom fortuna.prob_model.prior.base import Prior\nfrom fortuna.prob_output_layer.classification import \\\n    ClassificationProbOutputLayer\nfrom fortuna.typing import Status\n\n\nclass ProbClassifier(ProbModel):\n    def __init__(\n        self,\n        model: nn.Module,\n        prior: Prior = IsotropicGaussianPrior(),\n        posterior_approximator: PosteriorApproximator = SWAGPosteriorApproximator(),\n        output_calibrator: Optional[nn.Module] = ClassificationTemperatureScaler(),\n        seed: int = 0,\n    ):\n        r\"\"\"\n        A probabilistic classifier class.\n\n        Parameters\n        ----------\n        model : nn.Module\n            A model describing the deterministic relation between inputs and outputs. The outputs must correspond to\n            the logits of a softmax probability vector. The output dimension must be the same as the number of classes.\n            Let :math:`x` be input variables and :math:`w` the random model parameters. Then the model is described by\n            a function :math:`f(w, x)`, where each component of :math:`f` corresponds to one of the classes.\n        prior : Prior\n            A prior distribution object. The default is an isotropic standard Gaussian. Let :math:`w` be the random\n            model parameters. Then the prior is defined by a distribution :math:`p(w)`.\n        posterior_approximator : PosteriorApproximator", "metadata": {"task_id": "awslabs_fortuna/61", "ground_truth": "            A posterior approximation method. The default method is SWAG.", "fpath_tuple": ["awslabs_fortuna", "fortuna", "prob_model", "classification.py"], "context_start_lineno": 0, "line_no": 55, "query_window": {"context": "        model: nn.Module,\n        prior: Prior = IsotropicGaussianPrior(),\n        posterior_approximator: PosteriorApproximator = SWAGPosteriorApproximator(),\n        output_calibrator: Optional[nn.Module] = ClassificationTemperatureScaler(),\n        seed: int = 0,\n    ):\n        r\"\"\"\n        A probabilistic classifier class.\n\n        Parameters\n        ----------\n        model : nn.Module\n            A model describing the deterministic relation between inputs and outputs. The outputs must correspond to\n            the logits of a softmax probability vector. The output dimension must be the same as the number of classes.\n            Let :math:`x` be input variables and :math:`w` the random model parameters. Then the model is described by\n            a function :math:`f(w, x)`, where each component of :math:`f` corresponds to one of the classes.\n        prior : Prior\n            A prior distribution object. The default is an isotropic standard Gaussian. Let :math:`w` be the random\n            model parameters. Then the prior is defined by a distribution :math:`p(w)`.\n        posterior_approximator : PosteriorApproximator", "metadata": {"fpath_tuple": ["awslabs_fortuna", "fortuna", "prob_model", "classification.py"], "line_no": 55, "task_id": "awslabs_fortuna/61", "start_line_no": 35, "end_line_no": 55, "window_size": 20, "context_start_lineno": 0, "repo": "awslabs_fortuna"}}, "top_k_context": [{"context": "        model: nn.Module,\n        likelihood_log_variance_model: nn.Module,\n        prior: Prior = IsotropicGaussianPrior(),\n        posterior_approximator: PosteriorApproximator = SWAGPosteriorApproximator(),\n        output_calibrator: Optional[nn.Module] = RegressionTemperatureScaler(),\n        seed: int = 0,\n    ):\n        r\"\"\"\n        A probabilistic regressor class.\n\n        Parameters\n        ----------\n        model : nn.Module\n            A model describing the deterministic relation between inputs and outputs. It characterizes the mean model\n            of the likelihood function. The outputs must belong to the same space as the target variables.\n            Let :math:`x` be input variables and :math:`w` the random model parameters. Then the model is described by\n            a function :math:`\\mu(w, x)`.\n        likelihood_log_variance_model: nn.Module\n            A model characterizing the log-variance of a Gaussian likelihood function. The outputs must belong to the\n            same space as the target variables. Let :math:`x` be input variables and :math:`w` the random model", "metadata": [{"fpath_tuple": ["awslabs_fortuna", "fortuna", "prob_model", "regression.py"], "line_no": 40, "start_line_no": 30, "end_line_no": 50, "window_size": 20, "repo": "awslabs_fortuna", "slice_size": 10}], "sim_score": 0.6666666666666666}, {"context": "        prior: Prior = IsotropicGaussianPrior(),\n        posterior_approximator: PosteriorApproximator = SWAGPosteriorApproximator(),\n        output_calibrator: Optional[nn.Module] = RegressionTemperatureScaler(),\n        seed: int = 0,\n    ):\n        r\"\"\"\n        A probabilistic regressor class.\n\n        Parameters\n        ----------\n        model : nn.Module\n            A model describing the deterministic relation between inputs and outputs. It characterizes the mean model\n            of the likelihood function. The outputs must belong to the same space as the target variables.\n            Let :math:`x` be input variables and :math:`w` the random model parameters. Then the model is described by\n            a function :math:`\\mu(w, x)`.\n        likelihood_log_variance_model: nn.Module\n            A model characterizing the log-variance of a Gaussian likelihood function. The outputs must belong to the\n            same space as the target variables. Let :math:`x` be input variables and :math:`w` the random model\n            parameters. Then the model is described by a function :math:`\\log\\sigma^2(w, x)`.\n        prior : Prior", "metadata": [{"fpath_tuple": ["awslabs_fortuna", "fortuna", "prob_model", "regression.py"], "line_no": 42, "start_line_no": 32, "end_line_no": 52, "window_size": 20, "repo": "awslabs_fortuna", "slice_size": 10}], "sim_score": 0.6486486486486487}, {"context": "\nclass ClassificationModelManager(ModelManager):\n    def __init__(self, model: nn.Module):\n        r\"\"\"\n        Classification model manager class. It orchestrates the forward pass of the model in the probabilistic model.\n\n        Parameters\n        ----------\n        model : nn.Module\n            A model describing the deterministic relation between inputs and outputs. The outputs must correspond to\n            the logits of a softmax probability vector. The output dimension must be the same as the number of classes.\n            Let :math:`x` be input variables and :math:`w` the random model parameters. Then the model is described by\n            a function :math:`f(w, x)`, where each component of :math:`f` corresponds to one of the classes.\n        \"\"\"\n        self.model = model\n\n    def apply(\n        self,\n        params: Params,\n        inputs: Array,", "metadata": [{"fpath_tuple": ["awslabs_fortuna", "fortuna", "model", "model_manager", "classification.py"], "line_no": 22, "start_line_no": 12, "end_line_no": 32, "window_size": 20, "repo": "awslabs_fortuna", "slice_size": 10}], "sim_score": 0.5202702702702703}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# fortuna/calibration/calibrator.py\n# --------------------------------------------------\n#             # training loop\n#             (\n#                 state,\n#                 training_losses_and_metrics_current_epoch,\n#                 training_batch_metrics_str,\n#             ) = self._training_loop(\n#                 epoch,\n#                 fun,\n#                 metrics,\n#                 rng,\n#                 state,\n#                 training_data_loader,\n#                 calib_outputs_loader,\n#                 training_dataset_size,\n#                 verbose,\n#                 progress_bar,\n#             )\n#             # keep track of training losses and metrics [granularity=epoch]\n#             for k in training_losses_and_metrics_current_epoch.keys():\n#                 training_losses_and_metrics[k].append(\n# --------------------------------------------------\n# the below code fragment can be found in:\n# fortuna/calib_model/calib_model_calibrator.py\n# --------------------------------------------------\n# \n#         progress_bar = trange(n_epochs, desc=\"Epoch\")\n#         for epoch in progress_bar:\n#             # training loop\n#             (\n#                 state,\n#                 training_losses_and_metrics_current_epoch,\n#                 training_batch_metrics_str,\n#             ) = self._training_loop(\n#                 epoch,\n#                 fun,\n#                 metrics,\n#                 rng,\n#                 state,\n#                 calib_targets,\n#                 calib_outputs,\n#                 verbose,\n#                 progress_bar,\n#             )\n#             # keep track of training losses and metrics [granularity=epoch]\n# --------------------------------------------------\n# the below code fragment can be found in:\n# fortuna/calibration/calibrator.py\n# --------------------------------------------------\n#         calib_outputs_loader, val_outputs_loader = outputs_loaders\n# \n#         progress_bar = trange(n_epochs, desc=\"Epoch\")\n#         for epoch in progress_bar:\n#             # training loop\n#             (\n#                 state,\n#                 training_losses_and_metrics_current_epoch,\n#                 training_batch_metrics_str,\n#             ) = self._training_loop(\n#                 epoch,\n#                 fun,\n#                 metrics,\n#                 rng,\n#                 state,\n#                 training_data_loader,\n#                 calib_outputs_loader,\n#                 training_dataset_size,\n#                 verbose,\n#                 progress_bar,\n# --------------------------------------------------\n# the below code fragment can be found in:\n# fortuna/calibration/calibrator.py\n# --------------------------------------------------\n#         progress_bar = trange(n_epochs, desc=\"Epoch\")\n#         for epoch in progress_bar:\n#             # training loop\n#             (\n#                 state,\n#                 training_losses_and_metrics_current_epoch,\n#                 training_batch_metrics_str,\n#             ) = self._training_loop(\n#                 epoch,\n#                 fun,\n#                 metrics,\n#                 rng,\n#                 state,\n#                 training_data_loader,\n#                 calib_outputs_loader,\n#                 training_dataset_size,\n#                 verbose,\n#                 progress_bar,\n#             )\n#             # keep track of training losses and metrics [granularity=epoch]\n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\narray], jnp.ndarray],\n        save_checkpoint_dir: Optional[Path] = None,\n        save_every_n_steps: Optional[int] = None,\n        keep_top_n_checkpoints: int = 2,\n        disable_training_metrics_computation: bool = False,\n        eval_every_n_epochs: int = 1,\n        **kwargs,\n    ):\n        super(TrainerABC, self).__init__(*args, **kwargs)\n        self.predict_fn = predict_fn\n        self.save_checkpoint_dir = save_checkpoint_dir\n        self.save_every_n_steps = save_every_n_steps\n        self.keep_top_n_checkpoints = keep_top_n_checkpoints\n        self.disable_training_metrics_computation = disable_training_metrics_computation\n        self.eval_every_n_epochs = eval_every_n_epochs\n        self._unravel = None\n        self.multi_device = False\n\n    def training_step(\n        self,\n        state: TrainState,\n        batch: Batch,\n        fun: Callable,\n        rng: PRNGKeyArray,\n        n_data: int,\n        unravel: Optional[Callable[[any], PyTree]] = None,\n        kwargs: FrozenDict[str, Any] = FrozenDict(),\n    ) -> Tuple[TrainState, Dict[str, Any]]:\n        # ensure to use a different key at each step\n        model_key = random.fold_in(rng, state.step)\n\n        grad_fn = value_and_grad(\n            lambda params: self.training_loss_step(\n                fun,\n                params,\n                batch,\n                state.mutable,\n                model_key,\n                n_data,\n                unravel,\n                state.calib_params,\n                state.calib_mutable,\n                kwargs,\n            ),\n            has_aux=True,\n        )\n        (loss, aux), grad = grad_fn(state.params)\n        grad, loss = self.sync_gradients_and_loss(grad, loss)\n\n        state = state.apply_gradients(grads=grad, mutable=aux[\"mutable\"])\n        return (\n            state,\n            {\n                \"loss\": loss,\n                \"outputs\": aux[\"outputs\"],\n                \"logging_kwargs\": aux[\"logging_kwargs\"],\n            },\n        )\n\n    @abc.abstractmethod\n    def validation_step(\n        self,\n        state: TrainState,\n        batch: Batch,\n        fun: Callable,\n        rng: PRNGKeyArray,\n        n_data: int,\n        metrics: Optional[Tuple[Callable[[jnp.ndarray, Array], float], ...]] = None,\n        unravel: Optional[Callable[[any], PyTree]] = None,\n        kwargs: FrozenDict[str, Any] = FrozenDict(),\n    ) -> Dict[str, jnp.ndarray]:\n        pass\n\n    def training_step_end(\n        self,\n        current_epoch: int,\n        state: TrainState,\n        aux: Dict[str, Any],\n        batch: Batch,\n        metrics: Optional[Tuple[Callable[[jnp.ndarray, Array], float], ...]],\n        kwargs: FrozenDict[str, Any] = FrozenDict(),\n    ) -> Dict[str, jnp.ndarray]:\n        if (\n            self.save_checkpoint_dir is not None\n            and self.save_every_n_steps is not None\n            and current_epoch % self.save_every_n_steps == 0\n        ):\n            self.save_checkpoint(\n                state, self.save_checkpoint_dir, keep=self.keep_top_n_checkpoints\n            )\n        training_losses_and_metrics = {\"loss\": aux[\"loss\"]}\n\n        if aux[\"logging_kwargs\"] is not None:\n            for k, v in aux[\"logging_kwargs\"].items():\n                training_losses_and_metrics[k] = v\n\n        if not self.disable_training_metrics_computation and metrics is not None:\n            preds = self.predict_fn(aux[\"outputs\"])\n            if self.multi_device:\n                training_batch_metrics = self.compute_metrics(\n                    preds.reshape((preds.shape[0] * preds.shape[1],) + preds.shape[2:]),\n                    batch[1].reshape(\n                        (batch[1].shape[0] * batch[1].shape[1],) + batch[1].shape[2:]\n                    ),\n                    metrics,\n                )\n            else:\n                training_batch_metrics = self.compute_metrics(preds, batch[1], metrics)\n            for k, v in training_batch_metrics.items():\n                training_losses_and_metrics[k] = v\n        return training_losses_and_metrics\n\n    def training_epoch_end(\n        self, training_losses_and_metrics_current_epoch: List[Dict[str, jnp.ndarray]]\n    ) -> Dict[str, float]:\n        return self._get_mean_losses_and_metrics(\n            training_losses_and_metrics_current_epoch\n        )\n\n    def validation_epoch_end(\n        self,\n        validation_losses_and_metrics_current_epoch: List[Dict[str, jnp.ndarray]],\n        state: TrainState,\n    ) -> Dict[str, float]:\n        validation_losses_and_metrics_current_epoch = self._get_mean_losses_and_metrics(\n            validation_losses_and_metrics_current_epoch\n        )\n        # early stopping\n        improved = self.early_stopping_update(\n            validation_losses_and_metrics_current_epoch\n        )\n        if improved and self.save_checkpoint_dir:\n            self.save_checkpoint(state, self.save_checkpoint_dir, force_save=True)\n        return validation_losses_and_metrics_current_epoch\n\n    def train(\n        self,\n        rng: PRNGKeyArray,\n        state: TrainState,\n        fun: Callable,\n        training_dataloader: DataLoader,\n        training_dataset_size: int,\n        n_epochs: int = 1,\n        metrics: Optional[Tuple[Callable[[jnp.ndarray, Array], float], ...]] = None,\n        validation_dataloader: Optional[DataLoader] = None,\n        validation_dataset_size: Optional[int] = None,\n        verbose: bool = True,\n        unravel: Optional[Callable[[any], PyTree]] = None,\n        **kwargs,\n    ) -> Tuple[TrainState, Status]:\n        training_kwargs = FrozenDict(kwargs)\n        if validation_dataloader:\n            assert (\n                validation_dataset_size is not None\n            ), \"`validation_dataset_size` is required when `validation_dataloader` is provided.\"\n\n        training_losses_and_metrics = collections.defaultdict(list)\n        validation_losses_and_metrics = collections.defaultdict(list)\n\n        state, dataloaders, rng = self.on_train_start(\n            state, [training_dataloader, validation_dataloader], rng\n        )\n        training_dataloader, validation_dataloader = dataloaders\n\n        progress_bar = trange(n_epochs, desc=\"Epoch\")\n        for epoch in progress_bar:\n            # training loop\n            (\n                state,\n                training_losses_and_metrics_current_epoch,\n                training_batch_metrics_str,\n            ) = self._training_loop(\n                epoch,\n                fun,\n                metrics,\n                rng,\n                state,\n                training_dataloader,\n                training_dataset_size,\n                training_kwargs,\n                verbose,", "metadata": {"task_id": "awslabs_fortuna/90", "ground_truth": "                progress_bar,", "fpath_tuple": ["awslabs_fortuna", "fortuna", "training", "trainer.py"], "context_start_lineno": 37, "line_no": 218, "query_window": {"context": "        )\n        training_dataloader, validation_dataloader = dataloaders\n\n        progress_bar = trange(n_epochs, desc=\"Epoch\")\n        for epoch in progress_bar:\n            # training loop\n            (\n                state,\n                training_losses_and_metrics_current_epoch,\n                training_batch_metrics_str,\n            ) = self._training_loop(\n                epoch,\n                fun,\n                metrics,\n                rng,\n                state,\n                training_dataloader,\n                training_dataset_size,\n                training_kwargs,\n                verbose,", "metadata": {"fpath_tuple": ["awslabs_fortuna", "fortuna", "training", "trainer.py"], "line_no": 218, "task_id": "awslabs_fortuna/90", "start_line_no": 198, "end_line_no": 218, "window_size": 20, "context_start_lineno": 37, "repo": "awslabs_fortuna"}}, "top_k_context": [{"context": "        calib_outputs_loader, val_outputs_loader = outputs_loaders\n\n        progress_bar = trange(n_epochs, desc=\"Epoch\")\n        for epoch in progress_bar:\n            # training loop\n            (\n                state,\n                training_losses_and_metrics_current_epoch,\n                training_batch_metrics_str,\n            ) = self._training_loop(\n                epoch,\n                fun,\n                metrics,\n                rng,\n                state,\n                training_data_loader,\n                calib_outputs_loader,\n                training_dataset_size,\n                verbose,\n                progress_bar,", "metadata": [{"fpath_tuple": ["awslabs_fortuna", "fortuna", "calibration", "calibrator.py"], "line_no": 94, "start_line_no": 84, "end_line_no": 104, "window_size": 20, "repo": "awslabs_fortuna", "slice_size": 10}], "sim_score": 0.7714285714285715}, {"context": "        )\n        training_data_loader, val_data_loader = data_loaders\n        calib_outputs_loader, val_outputs_loader = outputs_loaders\n\n        progress_bar = trange(n_epochs, desc=\"Epoch\")\n        for epoch in progress_bar:\n            # training loop\n            (\n                state,\n                training_losses_and_metrics_current_epoch,\n                training_batch_metrics_str,\n            ) = self._training_loop(\n                epoch,\n                fun,\n                metrics,\n                rng,\n                state,\n                training_data_loader,\n                calib_outputs_loader,\n                training_dataset_size,", "metadata": [{"fpath_tuple": ["awslabs_fortuna", "fortuna", "calibration", "calibrator.py"], "line_no": 92, "start_line_no": 82, "end_line_no": 102, "window_size": 20, "repo": "awslabs_fortuna", "slice_size": 10}], "sim_score": 0.7323943661971831}, {"context": "        calib_targets, val_targets = targets\n        calib_outputs, val_outputs = outputs\n\n        progress_bar = trange(n_epochs, desc=\"Epoch\")\n        for epoch in progress_bar:\n            # training loop\n            (\n                state,\n                training_losses_and_metrics_current_epoch,\n                training_batch_metrics_str,\n            ) = self._training_loop(\n                epoch,\n                fun,\n                metrics,\n                rng,\n                state,\n                calib_targets,\n                calib_outputs,\n                verbose,\n                progress_bar,", "metadata": [{"fpath_tuple": ["awslabs_fortuna", "fortuna", "calib_model", "calib_model_calibrator.py"], "line_no": 92, "start_line_no": 82, "end_line_no": 102, "window_size": 20, "repo": "awslabs_fortuna", "slice_size": 10}], "sim_score": 0.6901408450704225}, {"context": "        progress_bar = trange(n_epochs, desc=\"Epoch\")\n        for epoch in progress_bar:\n            # training loop\n            (\n                state,\n                training_losses_and_metrics_current_epoch,\n                training_batch_metrics_str,\n            ) = self._training_loop(\n                epoch,\n                fun,\n                metrics,\n                rng,\n                state,\n                training_data_loader,\n                calib_outputs_loader,\n                training_dataset_size,\n                verbose,\n                progress_bar,\n            )\n            # keep track of training losses and metrics [granularity=epoch]", "metadata": [{"fpath_tuple": ["awslabs_fortuna", "fortuna", "calibration", "calibrator.py"], "line_no": 96, "start_line_no": 86, "end_line_no": 106, "window_size": 20, "repo": "awslabs_fortuna", "slice_size": 10}], "sim_score": 0.6753246753246753}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# federatedscope/nlp/hetero_tasks/trainer/trainer.py\n# --------------------------------------------------\n#                 hook(self.ctx)\n# \n#             for hook in hooks_set[\"on_batch_backward\"]:\n#                 hook(self.ctx)\n# \n#             for hook in hooks_set[\"on_batch_end\"]:\n#                 hook(self.ctx)\n# \n#             # Break in the final epoch\n#             if self.ctx.cur_mode in [MODE.TRAIN, MODE.FINETUNE] and \\\n#                 self.ctx.cur_epoch_i == getattr(\n#                     self.ctx, f'num_{self.ctx.cur_mode}_epoch', None) - 1:\n#                 if batch_i >= \\\n#                         getattr(self.ctx,\n#                                 f'num_{self.ctx.cur_mode}_batch_last_epoch',\n#                                 None) - 1:\n#                     break\n# \n#     def _hook_on_fit_start_init(self, ctx):\n#         ctx.model.to(ctx.device)\n# --------------------------------------------------\n# the below code fragment can be found in:\n# federatedscope/nlp/hetero_tasks/trainer/trainer.py\n# --------------------------------------------------\n#             for hook in hooks_set[\"on_batch_backward\"]:\n#                 hook(self.ctx)\n# \n#             for hook in hooks_set[\"on_batch_end\"]:\n#                 hook(self.ctx)\n# \n#             # Break in the final epoch\n#             if self.ctx.cur_mode in [MODE.TRAIN, MODE.FINETUNE] and \\\n#                 self.ctx.cur_epoch_i == getattr(\n#                     self.ctx, f'num_{self.ctx.cur_mode}_epoch', None) - 1:\n#                 if batch_i >= \\\n#                         getattr(self.ctx,\n#                                 f'num_{self.ctx.cur_mode}_batch_last_epoch',\n#                                 None) - 1:\n#                     break\n# \n#     def _hook_on_fit_start_init(self, ctx):\n#         ctx.model.to(ctx.device)\n# \n#         if ctx.cur_mode in [MODE.TRAIN, MODE.FINETUNE]:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# federatedscope/nlp/hetero_tasks/trainer/trainer.py\n# --------------------------------------------------\n# \n#             for hook in hooks_set[\"on_batch_end\"]:\n#                 hook(self.ctx)\n# \n#             # Break in the final epoch\n#             if self.ctx.cur_mode in [MODE.TRAIN, MODE.FINETUNE] and \\\n#                 self.ctx.cur_epoch_i == getattr(\n#                     self.ctx, f'num_{self.ctx.cur_mode}_epoch', None) - 1:\n#                 if batch_i >= \\\n#                         getattr(self.ctx,\n#                                 f'num_{self.ctx.cur_mode}_batch_last_epoch',\n#                                 None) - 1:\n#                     break\n# \n#     def _hook_on_fit_start_init(self, ctx):\n#         ctx.model.to(ctx.device)\n# \n#         if ctx.cur_mode in [MODE.TRAIN, MODE.FINETUNE]:\n#             ctx.optimizer = ctx.get(f'{ctx.cur_mode}_optimizer', None)\n#             ctx.scheduler = ctx.get(f'{ctx.cur_mode}_scheduler', None)\n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\nidx = -1  # -1 indicates del the whole list\n        else:\n            for hook_idx in range(len(hooks_dict[target_trigger])):\n                if target_hook_name == hooks_dict[target_trigger][\n                        hook_idx].__name__:\n                    del_one = hooks_dict[target_trigger].pop(hook_idx)\n                    logger.info(f\"Remove the hook `{del_one.__name__}` from \"\n                                f\"hooks_set at trigger `{target_trigger}`\")\n                    del_one_hook_idx = hook_idx\n                    break\n            if del_one_hook_idx is None:\n                logger.warning(\n                    f\"In hook del procedure, can't find the target hook \"\n                    f\"named {target_hook_name}\")\n        return del_one_hook_idx\n\n    def register_hook_in_train(self,\n                               new_hook,\n                               trigger,\n                               insert_pos=None,\n                               base_hook=None,\n                               insert_mode=\"before\"):\n        hooks_dict = self.hooks_in_train\n        self._register_hook(base_hook, hooks_dict, insert_mode, insert_pos,\n                            new_hook, trigger)\n\n    def register_hook_in_ft(self,\n                            new_hook,\n                            trigger,\n                            insert_pos=None,\n                            base_hook=None,\n                            insert_mode=\"before\"):\n        hooks_dict = self.hooks_in_ft\n        self._register_hook(base_hook, hooks_dict, insert_mode, insert_pos,\n                            new_hook, trigger)\n\n    def register_hook_in_eval(self,\n                              new_hook,\n                              trigger,\n                              insert_pos=None,\n                              base_hook=None,\n                              insert_mode=\"before\"):\n        hooks_dict = self.hooks_in_eval\n        self._register_hook(base_hook, hooks_dict, insert_mode, insert_pos,\n                            new_hook, trigger)\n\n    def _register_hook(self, base_hook, hooks_dict, insert_mode, insert_pos,\n                       new_hook, trigger):\n        assert trigger in self.HOOK_TRIGGER, \\\n            f\"Got {trigger} as hook trigger, you should specify a string \" \\\n            f\"within {self.HOOK_TRIGGER}.\"\n        # parse the insertion position\n        target_hook_set = hooks_dict[trigger]\n        if insert_pos is not None:\n            assert (insert_pos == -1) or (insert_pos == len(target_hook_set)\n                                          == 0) or \\\n                   (0 <= insert_pos <= (len(target_hook_set))), \\\n                   f\"Got {insert_pos} as insert pos, you should specify a \" \\\n                   f\"integer (1) =-1 \" \\\n                   f\"or (2) =0 for null target_hook_set;\" \\\n                   f\"or (3) within [0, {(len(target_hook_set))}].\"\n        elif base_hook is not None:\n            base_hook_pos = target_hook_set.index(base_hook)\n            insert_pos = base_hook_pos - 1 if insert_mode == \"before\" else \\\n                base_hook_pos + 1\n            # bounding the insert_pos in rational range\n            insert_pos = 0 if insert_pos < 0 else insert_pos\n            insert_pos = -1 if insert_pos > len(\n                target_hook_set) else insert_pos\n        else:\n            insert_pos = -1  # By default, the new hook is called finally\n        # register the new hook\n        if insert_pos == -1:\n            hooks_dict[trigger].append(new_hook)\n        else:\n            hooks_dict[trigger].insert(insert_pos, new_hook)\n\n    @use_diff\n    def train(self, target_data_split_name=\"train\", hooks_set=None):\n        hooks_set = hooks_set or self.hooks_in_train\n\n        self.ctx.check_split(target_data_split_name)\n\n        num_samples = self._run_routine(MODE.TRAIN, hooks_set,\n                                        target_data_split_name)\n\n        return num_samples, self.get_model_para(), self.ctx.eval_metrics\n\n    def evaluate(self, target_data_split_name=\"test\", hooks_set=None):\n        hooks_set = hooks_set or self.hooks_in_eval\n\n        if self.ctx.check_split(target_data_split_name, skip=True):\n            self._run_routine(MODE.TEST, hooks_set, target_data_split_name)\n        else:\n            self.ctx.eval_metrics = dict()\n\n        return self.ctx.eval_metrics\n\n    def finetune(self, target_data_split_name=\"train\", hooks_set=None):\n        hooks_set = hooks_set or self.hooks_in_ft\n\n        self.ctx.check_split(target_data_split_name)\n\n        self._run_routine(MODE.FINETUNE, hooks_set, target_data_split_name)\n\n    @lifecycle(LIFECYCLE.ROUTINE)\n    def _run_routine(self, mode, hooks_set, dataset_name=None):\n        \"\"\"Run the hooks_set and maintain the mode\n        Arguments:\n            mode: running mode of client, chosen from train/val/test\n        Note:\n            Considering evaluation could be in ```hooks_set[\"on_epoch_end\"]```,\n            there could be two data loaders in self.ctx, we must tell the\n            running hooks which data_loader to call and which\n            num_samples to count\n        \"\"\"\n        for hook in hooks_set[\"on_fit_start\"]:\n            hook(self.ctx)\n\n        self._run_epoch(hooks_set)\n\n        for hook in hooks_set[\"on_fit_end\"]:\n            hook(self.ctx)\n\n        return self.ctx.num_samples\n\n    @lifecycle(LIFECYCLE.EPOCH)\n    def _run_epoch(self, hooks_set):\n        for epoch_i in range(\n                getattr(self.ctx, f\"num_{self.ctx.cur_split}_epoch\")):\n            self.ctx.cur_epoch_i = CtxVar(epoch_i, \"epoch\")\n\n            for hook in hooks_set[\"on_epoch_start\"]:\n                hook(self.ctx)\n\n            self._run_batch(hooks_set)\n\n            for hook in hooks_set[\"on_epoch_end\"]:\n                hook(self.ctx)\n\n    @lifecycle(LIFECYCLE.BATCH)\n    def _run_batch(self, hooks_set):\n        for batch_i in range(\n                getattr(self.ctx, f\"num_{self.ctx.cur_split}_batch\")):\n            self.ctx.cur_batch_i = CtxVar(batch_i, LIFECYCLE.BATCH)\n\n            for hook in hooks_set[\"on_batch_start\"]:\n                hook(self.ctx)\n\n            for hook in hooks_set[\"on_batch_forward\"]:\n                hook(self.ctx)\n\n            for hook in hooks_set[\"on_batch_backward\"]:\n                hook(self.ctx)\n\n            for hook in hooks_set[\"on_batch_end\"]:\n                hook(self.ctx)\n\n            # Break in the final epoch\n            if self.ctx.cur_mode in [\n                    MODE.TRAIN, MODE.FINETUNE\n            ] and self.ctx.cur_epoch_i == self.ctx.num_train_epoch - 1:\n                if batch_i >= self.ctx.num_train_batch_last_epoch - 1:\n                    break\n\n    def update(self, model_parameters, strict=False):\n        \"\"\"\n            Called by the FL client to update the model parameters\n        Arguments:\n            model_parameters (dict): {model_name: model_val}\n            strict (bool): ensure the k-v paris are strictly same\n        \"\"\"", "metadata": {"task_id": "alibaba_FederatedScope/52", "ground_truth": "        pass", "fpath_tuple": ["alibaba_FederatedScope", "federatedscope", "core", "trainers", "trainer.py"], "context_start_lineno": 154, "line_no": 326, "query_window": {"context": "            for hook in hooks_set[\"on_batch_backward\"]:\n                hook(self.ctx)\n\n            for hook in hooks_set[\"on_batch_end\"]:\n                hook(self.ctx)\n\n            # Break in the final epoch\n            if self.ctx.cur_mode in [\n                    MODE.TRAIN, MODE.FINETUNE\n            ] and self.ctx.cur_epoch_i == self.ctx.num_train_epoch - 1:\n                if batch_i >= self.ctx.num_train_batch_last_epoch - 1:\n                    break\n\n    def update(self, model_parameters, strict=False):\n        \"\"\"\n            Called by the FL client to update the model parameters\n        Arguments:\n            model_parameters (dict): {model_name: model_val}\n            strict (bool): ensure the k-v paris are strictly same\n        \"\"\"", "metadata": {"fpath_tuple": ["alibaba_FederatedScope", "federatedscope", "core", "trainers", "trainer.py"], "line_no": 326, "task_id": "alibaba_FederatedScope/52", "start_line_no": 306, "end_line_no": 326, "window_size": 20, "context_start_lineno": 154, "repo": "alibaba_FederatedScope"}}, "top_k_context": [{"context": "            for hook in hooks_set[\"on_batch_backward\"]:\n                hook(self.ctx)\n\n            for hook in hooks_set[\"on_batch_end\"]:\n                hook(self.ctx)\n\n            # Break in the final epoch\n            if self.ctx.cur_mode in [MODE.TRAIN, MODE.FINETUNE] and \\\n                self.ctx.cur_epoch_i == getattr(\n                    self.ctx, f'num_{self.ctx.cur_mode}_epoch', None) - 1:\n                if batch_i >= \\\n                        getattr(self.ctx,\n                                f'num_{self.ctx.cur_mode}_batch_last_epoch',\n                                None) - 1:\n                    break\n\n    def _hook_on_fit_start_init(self, ctx):\n        ctx.model.to(ctx.device)\n\n        if ctx.cur_mode in [MODE.TRAIN, MODE.FINETUNE]:", "metadata": [{"fpath_tuple": ["alibaba_FederatedScope", "federatedscope", "nlp", "hetero_tasks", "trainer", "trainer.py"], "line_no": 288, "start_line_no": 278, "end_line_no": 298, "window_size": 20, "repo": "alibaba_FederatedScope", "slice_size": 10}], "sim_score": 0.5344827586206896}, {"context": "                hook(self.ctx)\n\n            for hook in hooks_set[\"on_batch_backward\"]:\n                hook(self.ctx)\n\n            for hook in hooks_set[\"on_batch_end\"]:\n                hook(self.ctx)\n\n            # Break in the final epoch\n            if self.ctx.cur_mode in [MODE.TRAIN, MODE.FINETUNE] and \\\n                self.ctx.cur_epoch_i == getattr(\n                    self.ctx, f'num_{self.ctx.cur_mode}_epoch', None) - 1:\n                if batch_i >= \\\n                        getattr(self.ctx,\n                                f'num_{self.ctx.cur_mode}_batch_last_epoch',\n                                None) - 1:\n                    break\n\n    def _hook_on_fit_start_init(self, ctx):\n        ctx.model.to(ctx.device)", "metadata": [{"fpath_tuple": ["alibaba_FederatedScope", "federatedscope", "nlp", "hetero_tasks", "trainer", "trainer.py"], "line_no": 286, "start_line_no": 276, "end_line_no": 296, "window_size": 20, "repo": "alibaba_FederatedScope", "slice_size": 10}], "sim_score": 0.5344827586206896}, {"context": "\n            for hook in hooks_set[\"on_batch_forward\"]:\n                hook(self.ctx)\n\n            for hook in hooks_set[\"on_batch_backward\"]:\n                hook(self.ctx)\n\n            for hook in hooks_set[\"on_batch_end\"]:\n                hook(self.ctx)\n\n            # Break in the final epoch\n            if self.ctx.cur_mode in [MODE.TRAIN, MODE.FINETUNE] and \\\n                self.ctx.cur_epoch_i == getattr(\n                    self.ctx, f'num_{self.ctx.cur_mode}_epoch', None) - 1:\n                if batch_i >= \\\n                        getattr(self.ctx,\n                                f'num_{self.ctx.cur_mode}_batch_last_epoch',\n                                None) - 1:\n                    break\n", "metadata": [{"fpath_tuple": ["alibaba_FederatedScope", "federatedscope", "nlp", "hetero_tasks", "trainer", "trainer.py"], "line_no": 284, "start_line_no": 274, "end_line_no": 294, "window_size": 20, "repo": "alibaba_FederatedScope", "slice_size": 10}], "sim_score": 0.5277777777777778}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# tutorials/sphinx-tutorials/tensordict_module.py\n# --------------------------------------------------\n# \n# \n# class FFN(nn.Module):\n#     def __init__(self, to_dim, hidden_dim, dropout_rate=0.2):\n#         super().__init__()\n#         self.FFN = nn.Sequential(\n#             nn.Linear(to_dim, hidden_dim),\n#             nn.ReLU(),\n#             nn.Linear(hidden_dim, to_dim),\n#             nn.Dropout(dropout_rate),\n#         )\n# \n#     def forward(self, X):\n#         return self.FFN(X)\n# \n# \n# class AttentionBlock(nn.Module):\n#     def __init__(self, to_dim, to_len, from_dim, latent_dim, num_heads):\n#         super().__init__()\n#         self.tokens_to_qkv = TokensToQKV(to_dim, from_dim, latent_dim)\n# --------------------------------------------------\n# the below code fragment can be found in:\n# tutorials/sphinx-tutorials/tensordict_module.py\n# --------------------------------------------------\n#     def forward(self, X_to, X_from):\n#         Q, K, V = self.tokens_to_qkv(X_to, X_from)\n#         Q, K, V = self.split_heads(Q, K, V)\n#         out, attention = self.attention(Q, K, V)\n#         out = self.skip(X_to, out)\n#         return out\n# \n# \n# class EncoderTransformerBlock(nn.Module):\n#     def __init__(self, to_dim, to_len, latent_dim, num_heads):\n#         super().__init__()\n#         self.attention_block = AttentionBlock(\n#             to_dim, to_len, to_dim, latent_dim, num_heads\n#         )\n#         self.FFN = FFN(to_dim, 4 * to_dim)\n#         self.skip = SkipLayerNorm(to_len, to_dim)\n# \n#     def forward(self, X_to):\n#         X_to = self.attention_block(X_to, X_to)\n#         X_out = self.FFN(X_to)\n# --------------------------------------------------\n# the below code fragment can be found in:\n# torchrl/modules/tensordict_module/probabilistic.py\n# --------------------------------------------------\n#             sequentially.\n#          partial_tolerant (bool, optional): if True, the input tensordict can miss some\n#             of the input keys. If so, the only module that will be executed are those\n#             who can be executed given the keys that are present. Also, if the input\n#             tensordict is a lazy stack of tensordicts AND if partial_tolerant is\n#             :obj:`True` AND if the stack does not have the required keys, then\n#             TensorDictSequential will scan through the sub-tensordicts looking for those\n#             that have the required keys, if any.\n# \n#     \"\"\"\n# \n#     def __init__(\n#         self,\n#         *modules: Union[TensorDictModule, ProbabilisticTensorDictModule],\n#         partial_tolerant: bool = False,\n#     ) -> None:\n#         super().__init__(*modules, partial_tolerant=partial_tolerant)\n#         super(ProbabilisticTensorDictSequential, self).__init__(\n#             *modules, partial_tolerant=partial_tolerant\n#         )\n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\nX_out, X_to)\n\n\nclass DecoderTransformerBlock(nn.Module):\n    def __init__(self, to_dim, to_len, from_dim, latent_dim, num_heads):\n        super().__init__()\n        self.attention_block = AttentionBlock(\n            to_dim, to_len, from_dim, latent_dim, num_heads\n        )\n        self.encoder_block = EncoderTransformerBlock(\n            to_dim, to_len, latent_dim, num_heads\n        )\n\n    def forward(self, X_to, X_from):\n        X_to = self.attention_block(X_to, X_from)\n        X_to = self.encoder_block(X_to)\n        return X_to\n\n\nclass TransformerEncoder(nn.Module):\n    def __init__(self, num_blocks, to_dim, to_len, latent_dim, num_heads):\n        super().__init__()\n        self.encoder = nn.ModuleList(\n            [\n                EncoderTransformerBlock(to_dim, to_len, latent_dim, num_heads)\n                for i in range(num_blocks)\n            ]\n        )\n\n    def forward(self, X_to):\n        for i in range(len(self.encoder)):\n            X_to = self.encoder[i](X_to)\n        return X_to\n\n\nclass TransformerDecoder(nn.Module):\n    def __init__(self, num_blocks, to_dim, to_len, from_dim, latent_dim, num_heads):\n        super().__init__()\n        self.decoder = nn.ModuleList(\n            [\n                DecoderTransformerBlock(to_dim, to_len, from_dim, latent_dim, num_heads)\n                for i in range(num_blocks)\n            ]\n        )\n\n    def forward(self, X_to, X_from):\n        for i in range(len(self.decoder)):\n            X_to = self.decoder[i](X_to, X_from)\n        return X_to\n\n\nclass Transformer(nn.Module):\n    def __init__(\n        self, num_blocks, to_dim, to_len, from_dim, from_len, latent_dim, num_heads\n    ):\n        super().__init__()\n        self.encoder = TransformerEncoder(\n            num_blocks, to_dim, to_len, latent_dim, num_heads\n        )\n        self.decoder = TransformerDecoder(\n            num_blocks, from_dim, from_len, to_dim, latent_dim, num_heads\n        )\n\n    def forward(self, X_to, X_from):\n        X_to = self.encoder(X_to)\n        X_out = self.decoder(X_from, X_to)\n        return X_out\n\n\n###############################################################################\n# We first create the ``AttentionBlockTensorDict``, the attention block using\n# ``TensorDictModule`` and ``TensorDictSequential``.\n#\n# The wiring operation that connects the modules to each other requires us\n# to indicate which key each of them must read and write. Unlike\n# ``nn.Sequence``, a ``TensorDictSequential`` can read/write more than one\n# input/output. Moreover, its components inputs need not be identical to the\n# previous layers outputs, allowing us to code complicated neural architecture.\n\n\nclass AttentionBlockTensorDict(TensorDictSequential):\n    def __init__(\n        self,\n        to_name,\n        from_name,\n        to_dim,\n        to_len,\n        from_dim,\n        latent_dim,\n        num_heads,\n    ):\n        super().__init__(\n            TensorDictModule(\n                TokensToQKV(to_dim, from_dim, latent_dim),\n                in_keys=[to_name, from_name],\n                out_keys=[\"Q\", \"K\", \"V\"],\n            ),\n            TensorDictModule(\n                SplitHeads(num_heads),\n                in_keys=[\"Q\", \"K\", \"V\"],\n                out_keys=[\"Q\", \"K\", \"V\"],\n            ),\n            TensorDictModule(\n                Attention(latent_dim, to_dim),\n                in_keys=[\"Q\", \"K\", \"V\"],\n                out_keys=[\"X_out\", \"Attn\"],\n            ),\n            TensorDictModule(\n                SkipLayerNorm(to_len, to_dim),\n                in_keys=[to_name, \"X_out\"],\n                out_keys=[to_name],\n            ),\n        )\n\n\n###############################################################################\n# We build the encoder and decoder blocks that will be part of the transformer\n# thanks to ``TensorDictModule``.\n\n\nclass TransformerBlockEncoderTensorDict(TensorDictSequential):\n    def __init__(\n        self,\n        to_name,\n        from_name,\n        to_dim,\n        to_len,\n        from_dim,\n        latent_dim,\n        num_heads,\n    ):\n        super().__init__(\n            AttentionBlockTensorDict(\n                to_name,\n                from_name,\n                to_dim,\n                to_len,\n                from_dim,\n                latent_dim,\n                num_heads,\n            ),\n            TensorDictModule(\n                FFN(to_dim, 4 * to_dim),\n                in_keys=[to_name],\n                out_keys=[\"X_out\"],\n            ),\n            TensorDictModule(\n                SkipLayerNorm(to_len, to_dim),\n                in_keys=[to_name, \"X_out\"],\n                out_keys=[to_name],\n            ),\n        )\n\n\nclass TransformerBlockDecoderTensorDict(TensorDictSequential):\n    def __init__(\n        self,\n        to_name,\n        from_name,\n        to_dim,\n        to_len,\n        from_dim,\n        latent_dim,\n        num_heads,\n    ):\n        super().__init__(\n            AttentionBlockTensorDict(\n                to_name,\n                to_name,\n                to_dim,\n                to_len,\n                to_dim,\n                latent_dim,\n                num_heads,\n            ),\n            TransformerBlockEncoderTensorDict(\n                to_name,\n                from_name,\n                to_dim,\n                to_len,\n                from_dim,\n                latent_dim,\n                num_heads,\n            ),\n        )\n\n\n###############################################################################\n# We create the transformer encoder and decoder.\n#\n# For an encoder, we just need to take the same tokens for both queries,\n# keys and values.\n#\n# For a decoder, we now can extract info from ``X_from`` into ``X_to``.\n# ``X_from`` will map to queries whereas ``X_from`` will map to keys and values.\n\n\nclass TransformerEncoderTensorDict(TensorDictSequential):\n    def __init__(\n        self,\n        num_blocks,\n        to_name,\n        from_name,\n        to_dim,\n        to_len,\n        from_dim,\n        latent_dim,\n        num_heads,\n    ):\n        super().__init__(\n            *[\n                TransformerBlockEncoderTensorDict(\n                    to_name,\n                    from_name,\n                    to_dim,\n                    to_len,\n                    from_dim,\n                    latent_dim,\n                    num_heads,\n                )\n                for _ in range(num_blocks)\n            ]\n        )\n\n\nclass TransformerDecoderTensorDict(TensorDictSequential):\n    def __init__(\n        self,\n        num_blocks,\n        to_name,\n        from_name,\n        to_dim,\n        to_len,\n        from_dim,\n        latent_dim,\n        num_heads,\n    ):\n        super().__init__(\n            *[\n                TransformerBlockDecoderTensorDict(\n                    to_name,\n                    from_name,\n                    to_dim,\n                    to_len,", "metadata": {"task_id": "pytorch_rl/49", "ground_truth": "                    from_dim,", "fpath_tuple": ["pytorch_rl", "tutorials", "sphinx-tutorials", "tensordict_module.py"], "context_start_lineno": 432, "line_no": 676, "query_window": {"context": "\nclass TransformerDecoderTensorDict(TensorDictSequential):\n    def __init__(\n        self,\n        num_blocks,\n        to_name,\n        from_name,\n        to_dim,\n        to_len,\n        from_dim,\n        latent_dim,\n        num_heads,\n    ):\n        super().__init__(\n            *[\n                TransformerBlockDecoderTensorDict(\n                    to_name,\n                    from_name,\n                    to_dim,\n                    to_len,", "metadata": {"fpath_tuple": ["pytorch_rl", "tutorials", "sphinx-tutorials", "tensordict_module.py"], "line_no": 676, "task_id": "pytorch_rl/49", "start_line_no": 656, "end_line_no": 676, "window_size": 20, "context_start_lineno": 432, "repo": "pytorch_rl"}}, "top_k_context": [{"context": "    \"\"\"\n\n    def __init__(\n        self,\n        *modules: Union[TensorDictModule, ProbabilisticTensorDictModule],\n        partial_tolerant: bool = False,\n    ) -> None:\n        super().__init__(*modules, partial_tolerant=partial_tolerant)\n        super(ProbabilisticTensorDictSequential, self).__init__(\n            *modules, partial_tolerant=partial_tolerant\n        )", "metadata": [{"fpath_tuple": ["pytorch_rl", "torchrl", "modules", "tensordict_module", "probabilistic.py"], "line_no": 220, "start_line_no": 210, "end_line_no": 221, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}], "sim_score": 0.3235294117647059}, {"context": "        self.skip = SkipLayerNorm(to_len, to_dim)\n\n    def forward(self, X_to, X_from):\n        Q, K, V = self.tokens_to_qkv(X_to, X_from)\n        Q, K, V = self.split_heads(Q, K, V)\n        out, attention = self.attention(Q, K, V)\n        out = self.skip(X_to, out)\n        return out\n\n\nclass EncoderTransformerBlock(nn.Module):\n    def __init__(self, to_dim, to_len, latent_dim, num_heads):\n        super().__init__()\n        self.attention_block = AttentionBlock(\n            to_dim, to_len, to_dim, latent_dim, num_heads\n        )\n        self.FFN = FFN(to_dim, 4 * to_dim)\n        self.skip = SkipLayerNorm(to_len, to_dim)\n\n    def forward(self, X_to):", "metadata": [{"fpath_tuple": ["pytorch_rl", "tutorials", "sphinx-tutorials", "tensordict_module.py"], "line_no": 420, "start_line_no": 410, "end_line_no": 430, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}], "sim_score": 0.3132530120481928}, {"context": "    def forward(self, x_0, x_1):\n        return self.layer_norm(x_0 + x_1)\n\n\nclass FFN(nn.Module):\n    def __init__(self, to_dim, hidden_dim, dropout_rate=0.2):\n        super().__init__()\n        self.FFN = nn.Sequential(\n            nn.Linear(to_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, to_dim),\n            nn.Dropout(dropout_rate),\n        )\n\n    def forward(self, X):\n        return self.FFN(X)\n\n\nclass AttentionBlock(nn.Module):\n    def __init__(self, to_dim, to_len, from_dim, latent_dim, num_heads):", "metadata": [{"fpath_tuple": ["pytorch_rl", "tutorials", "sphinx-tutorials", "tensordict_module.py"], "line_no": 396, "start_line_no": 386, "end_line_no": 406, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}], "sim_score": 0.3132530120481928}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# test/test_cost.py\n# --------------------------------------------------\n#     )\n#     @pytest.mark.parametrize(\"n\", list(range(4)))\n#     @pytest.mark.parametrize(\"delay_value\", (True, False))\n#     @pytest.mark.parametrize(\"delay_actor\", (True, False))\n#     @pytest.mark.parametrize(\"delay_qvalue\", (True, False))\n#     @pytest.mark.parametrize(\"num_qvalue\", [1, 2, 4, 8])\n#     @pytest.mark.parametrize(\"device\", get_available_devices())\n#     def test_sac_batcher(\n#         self,\n#         n,\n#         delay_value,\n#         delay_actor,\n#         delay_qvalue,\n#         num_qvalue,\n#         device,\n#         version,\n#         gamma=0.9,\n#     ):\n#         if (delay_actor or delay_qvalue) and not delay_value:\n#             pytest.skip(\"incompatible config\")\n# --------------------------------------------------\n# the below code fragment can be found in:\n# test/test_cost.py\n# --------------------------------------------------\n#     @pytest.mark.skipif(\n#         not _has_functorch, reason=f\"functorch not installed: {FUNCTORCH_ERR}\"\n#     )\n#     @pytest.mark.parametrize(\"n\", list(range(4)))\n#     @pytest.mark.parametrize(\"delay_value\", (True, False))\n#     @pytest.mark.parametrize(\"delay_actor\", (True, False))\n#     @pytest.mark.parametrize(\"delay_qvalue\", (True, False))\n#     @pytest.mark.parametrize(\"num_qvalue\", [1, 2, 4, 8])\n#     @pytest.mark.parametrize(\"device\", get_available_devices())\n#     def test_sac_batcher(\n#         self,\n#         n,\n#         delay_value,\n#         delay_actor,\n#         delay_qvalue,\n#         num_qvalue,\n#         device,\n#         version,\n#         gamma=0.9,\n#     ):\n# --------------------------------------------------\n# the below code fragment can be found in:\n# test/test_cost.py\n# --------------------------------------------------\n#         if loss_fn.delay_value:\n#             assert all((p1 == p2).all() for p1, p2 in zip(target_value, target_value2))\n#         else:\n#             assert not any(\n#                 (p1 == p2).any() for p1, p2 in zip(target_value, target_value2)\n#             )\n# \n#         # check that policy is updated after parameter update\n#         parameters = [p.clone() for p in actor.parameters()]\n#         for p in loss_fn.parameters():\n#             p.data += torch.randn_like(p)\n#         assert all((p1 != p2).all() for p1, p2 in zip(parameters, actor.parameters()))\n# \n#     @pytest.mark.skipif(\n#         not _has_functorch, reason=f\"functorch not installed: {FUNCTORCH_ERR}\"\n#     )\n#     @pytest.mark.parametrize(\"n\", list(range(4)))\n#     @pytest.mark.parametrize(\"device\", get_available_devices())\n#     @pytest.mark.parametrize(\"delay_actor,delay_value\", [(False, False), (True, True)])\n#     def test_ddpg_batcher(self, n, delay_actor, delay_value, device, gamma=0.9):\n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\n\n            next_state_value,\n            reward,\n            done,\n            rolling_gamma,\n        )\n        v2a = vec_td_lambda_advantage_estimate(\n            gamma_tensor[..., : T // 2, :],\n            lmbda,\n            state_value[..., : T // 2, :],\n            next_state_value[..., : T // 2, :],\n            reward[..., : T // 2, :],\n            done[..., : T // 2, :],\n            rolling_gamma,\n        )\n        v2b = vec_td_lambda_advantage_estimate(\n            gamma_tensor[..., T // 2 :, :],\n            lmbda,\n            state_value[..., T // 2 :, :],\n            next_state_value[..., T // 2 :, :],\n            reward[..., T // 2 :, :],\n            done[..., T // 2 :, :],\n            rolling_gamma,\n        )\n\n        torch.testing.assert_close(v1, v2, rtol=1e-4, atol=1e-4)\n        torch.testing.assert_close(v1a, v2a, rtol=1e-4, atol=1e-4)\n\n        torch.testing.assert_close(v1b, v2b, rtol=1e-4, atol=1e-4)\n        torch.testing.assert_close(v2, torch.cat([v2a, v2b], -2), rtol=1e-4, atol=1e-4)\n\n    @pytest.mark.parametrize(\"device\", get_available_devices())\n    @pytest.mark.parametrize(\"N\", [(3,), (3, 7)])\n    @pytest.mark.parametrize(\"T\", [3, 5, 200])\n    def test_successive_traj_tdadv(\n        self,\n        device,\n        N,\n        T,\n    ):\n        \"\"\"Tests td_lambda_advantage_estimate against vec_td_lambda_advantage_estimate\n        with gamma being a random tensor\n\n        \"\"\"\n        torch.manual_seed(0)\n\n        lmbda = torch.rand([]).item()\n\n        done = torch.zeros(*N, T, 1, device=device, dtype=torch.bool)\n        done[..., T // 2 - 1, :] = 1\n\n        reward = torch.randn(*N, T, 1, device=device)\n        state_value = torch.randn(*N, T, 1, device=device)\n        next_state_value = torch.randn(*N, T, 1, device=device)\n\n        # avoid low values of gamma\n        gamma_tensor = 0.5 + torch.rand_like(next_state_value) / 2\n\n        v1 = td_advantage_estimate(\n            gamma_tensor,\n            state_value,\n            next_state_value,\n            reward,\n            done,\n        )\n        v1a = td_advantage_estimate(\n            gamma_tensor[..., : T // 2, :],\n            state_value[..., : T // 2, :],\n            next_state_value[..., : T // 2, :],\n            reward[..., : T // 2, :],\n            done[..., : T // 2, :],\n        )\n        v1b = td_advantage_estimate(\n            gamma_tensor[..., T // 2 :, :],\n            state_value[..., T // 2 :, :],\n            next_state_value[..., T // 2 :, :],\n            reward[..., T // 2 :, :],\n            done[..., T // 2 :, :],\n        )\n        torch.testing.assert_close(v1, torch.cat([v1a, v1b], -2), rtol=1e-4, atol=1e-4)\n\n    @pytest.mark.parametrize(\"device\", get_available_devices())\n    @pytest.mark.parametrize(\"N\", [(3,), (3, 7)])\n    @pytest.mark.parametrize(\"T\", [3, 5, 200])\n    def test_successive_traj_gae(\n        self,\n        device,\n        N,\n        T,\n    ):\n        \"\"\"Tests td_lambda_advantage_estimate against vec_td_lambda_advantage_estimate\n        with gamma being a random tensor\n\n        \"\"\"\n        torch.manual_seed(0)\n\n        lmbda = torch.rand([]).item()\n\n        done = torch.zeros(*N, T, 1, device=device, dtype=torch.bool)\n        done[..., T // 2 - 1, :] = 1\n\n        reward = torch.randn(*N, T, 1, device=device)\n        state_value = torch.randn(*N, T, 1, device=device)\n        next_state_value = torch.randn(*N, T, 1, device=device)\n\n        # avoid low values of gamma\n        gamma_tensor = 0.95\n\n        v1 = generalized_advantage_estimate(\n            gamma_tensor,\n            lmbda,\n            state_value,\n            next_state_value,\n            reward,\n            done,\n        )[0]\n        v1a = generalized_advantage_estimate(\n            gamma_tensor,\n            lmbda,\n            state_value[..., : T // 2, :],\n            next_state_value[..., : T // 2, :],\n            reward[..., : T // 2, :],\n            done[..., : T // 2, :],\n        )[0]\n        v1b = generalized_advantage_estimate(\n            gamma_tensor,\n            lmbda,\n            state_value[..., T // 2 :, :],\n            next_state_value[..., T // 2 :, :],\n            reward[..., T // 2 :, :],\n            done[..., T // 2 :, :],\n        )[0]\n        torch.testing.assert_close(v1, torch.cat([v1a, v1b], -2), rtol=1e-4, atol=1e-4)\n\n        v2 = vec_generalized_advantage_estimate(\n            gamma_tensor,\n            lmbda,\n            state_value,\n            next_state_value,\n            reward,\n            done,\n        )[0]\n        v2a = vec_generalized_advantage_estimate(\n            gamma_tensor,\n            lmbda,\n            state_value[..., : T // 2, :],\n            next_state_value[..., : T // 2, :],\n            reward[..., : T // 2, :],\n            done[..., : T // 2, :],\n        )[0]\n        v2b = vec_generalized_advantage_estimate(\n            gamma_tensor,\n            lmbda,\n            state_value[..., T // 2 :, :],\n            next_state_value[..., T // 2 :, :],\n            reward[..., T // 2 :, :],\n            done[..., T // 2 :, :],\n        )[0]\n        torch.testing.assert_close(v1, v2, rtol=1e-4, atol=1e-4)\n        torch.testing.assert_close(v2, torch.cat([v2a, v2b], -2), rtol=1e-4, atol=1e-4)\n\n\n@pytest.mark.skipif(\n    not _has_functorch,\n    reason=f\"no vmap allowed without functorch, error: {FUNCTORCH_ERR}\",\n)\n@pytest.mark.parametrize(\n    \"dest,expected_dtype,expected_device\",\n    list(\n        zip(\n            get_available_devices(),\n            [torch.float] * len(get_available_devices()),\n            get_available_devices(),\n        )\n    )\n    + [\n        [\"cuda\", torch.float, \"cuda:0\"],", "metadata": {"task_id": "pytorch_rl/71", "ground_truth": "        [\"double\", torch.double, \"cpu\"],", "fpath_tuple": ["pytorch_rl", "test", "test_cost.py"], "context_start_lineno": 3266, "line_no": 3443, "query_window": {"context": "        )[0]\n        torch.testing.assert_close(v1, v2, rtol=1e-4, atol=1e-4)\n        torch.testing.assert_close(v2, torch.cat([v2a, v2b], -2), rtol=1e-4, atol=1e-4)\n\n\n@pytest.mark.skipif(\n    not _has_functorch,\n    reason=f\"no vmap allowed without functorch, error: {FUNCTORCH_ERR}\",\n)\n@pytest.mark.parametrize(\n    \"dest,expected_dtype,expected_device\",\n    list(\n        zip(\n            get_available_devices(),\n            [torch.float] * len(get_available_devices()),\n            get_available_devices(),\n        )\n    )\n    + [\n        [\"cuda\", torch.float, \"cuda:0\"],", "metadata": {"fpath_tuple": ["pytorch_rl", "test", "test_cost.py"], "line_no": 3443, "task_id": "pytorch_rl/71", "start_line_no": 3423, "end_line_no": 3443, "window_size": 20, "context_start_lineno": 3266, "repo": "pytorch_rl"}}, "top_k_context": [{"context": "                (p1 == p2).any() for p1, p2 in zip(target_actor, target_actor2)\n            )\n        if loss_fn.delay_value:\n            assert all((p1 == p2).all() for p1, p2 in zip(target_value, target_value2))\n        else:\n            assert not any(\n                (p1 == p2).any() for p1, p2 in zip(target_value, target_value2)\n            )\n\n        # check that policy is updated after parameter update\n        parameters = [p.clone() for p in actor.parameters()]\n        for p in loss_fn.parameters():\n            p.data += torch.randn_like(p)\n        assert all((p1 != p2).all() for p1, p2 in zip(parameters, actor.parameters()))\n\n    @pytest.mark.skipif(\n        not _has_functorch, reason=f\"functorch not installed: {FUNCTORCH_ERR}\"\n    )\n    @pytest.mark.parametrize(\"n\", list(range(4)))\n    @pytest.mark.parametrize(\"device\", get_available_devices())", "metadata": [{"fpath_tuple": ["pytorch_rl", "test", "test_cost.py"], "line_no": 608, "start_line_no": 598, "end_line_no": 618, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}], "sim_score": 0.35135135135135137}, {"context": "            assert p.grad.norm() > 0.0, f\"parameter {name} has a null gradient\"\n\n    @pytest.mark.skipif(\n        not _has_functorch, reason=f\"functorch not installed: {FUNCTORCH_ERR}\"\n    )\n    @pytest.mark.parametrize(\"n\", list(range(4)))\n    @pytest.mark.parametrize(\"delay_value\", (True, False))\n    @pytest.mark.parametrize(\"delay_actor\", (True, False))\n    @pytest.mark.parametrize(\"delay_qvalue\", (True, False))\n    @pytest.mark.parametrize(\"num_qvalue\", [1, 2, 4, 8])\n    @pytest.mark.parametrize(\"device\", get_available_devices())\n    def test_sac_batcher(\n        self,\n        n,\n        delay_value,\n        delay_actor,\n        delay_qvalue,\n        num_qvalue,\n        device,\n        version,", "metadata": [{"fpath_tuple": ["pytorch_rl", "test", "test_cost.py"], "line_no": 1176, "start_line_no": 1166, "end_line_no": 1186, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}], "sim_score": 0.3475177304964539}, {"context": "    @pytest.mark.skipif(\n        not _has_functorch, reason=f\"functorch not installed: {FUNCTORCH_ERR}\"\n    )\n    @pytest.mark.parametrize(\"n\", list(range(4)))\n    @pytest.mark.parametrize(\"delay_value\", (True, False))\n    @pytest.mark.parametrize(\"delay_actor\", (True, False))\n    @pytest.mark.parametrize(\"delay_qvalue\", (True, False))\n    @pytest.mark.parametrize(\"num_qvalue\", [1, 2, 4, 8])\n    @pytest.mark.parametrize(\"device\", get_available_devices())\n    def test_sac_batcher(\n        self,\n        n,\n        delay_value,\n        delay_actor,\n        delay_qvalue,\n        num_qvalue,\n        device,\n        version,\n        gamma=0.9,\n    ):", "metadata": [{"fpath_tuple": ["pytorch_rl", "test", "test_cost.py"], "line_no": 1178, "start_line_no": 1168, "end_line_no": 1188, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}], "sim_score": 0.3435114503816794}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# federatedscope/core/auxiliaries/worker_builder.py\n# --------------------------------------------------\n# \n#     if cfg.federate.method.lower() in constants.SERVER_TYPE:\n#         server_type = constants.SERVER_TYPE[cfg.federate.method.lower()]\n#     else:\n#         server_type = \"normal\"\n#         logger.warning(\n#             'Server for method {} is not implemented. Will use default one'.\n#             format(cfg.federate.method))\n# \n#     if server_type == 'fedsageplus':\n#         from federatedscope.gfl.fedsageplus.worker import FedSagePlusServer\n#         server_class = FedSagePlusServer\n#     elif server_type == 'gcflplus':\n#         from federatedscope.gfl.gcflplus.worker import GCFLPlusServer\n#         server_class = GCFLPlusServer\n#     elif server_type == 'fedgc':\n#         from federatedscope.cl.fedgc.server import GlobalContrastFLServer\n#         server_class = GlobalContrastFLServer\n#     else:\n#         server_class = Server\n# --------------------------------------------------\n# the below code fragment can be found in:\n# federatedscope/core/auxiliaries/worker_builder.py\n# --------------------------------------------------\n#         from federatedscope.nlp.hetero_tasks.worker import ATCServer\n#         return ATCServer\n# \n#     if cfg.federate.method.lower() in constants.SERVER_TYPE:\n#         server_type = constants.SERVER_TYPE[cfg.federate.method.lower()]\n#     else:\n#         server_type = \"normal\"\n#         logger.warning(\n#             'Server for method {} is not implemented. Will use default one'.\n#             format(cfg.federate.method))\n# \n#     if server_type == 'fedsageplus':\n#         from federatedscope.gfl.fedsageplus.worker import FedSagePlusServer\n#         server_class = FedSagePlusServer\n#     elif server_type == 'gcflplus':\n#         from federatedscope.gfl.gcflplus.worker import GCFLPlusServer\n#         server_class = GCFLPlusServer\n#     elif server_type == 'fedgc':\n#         from federatedscope.cl.fedgc.server import GlobalContrastFLServer\n#         server_class = GlobalContrastFLServer\n# --------------------------------------------------\n# the below code fragment can be found in:\n# federatedscope/core/auxiliaries/worker_builder.py\n# --------------------------------------------------\n#         return ATCClient\n# \n#     if cfg.federate.method.lower() in constants.CLIENTS_TYPE:\n#         client_type = constants.CLIENTS_TYPE[cfg.federate.method.lower()]\n#     else:\n#         client_type = \"normal\"\n#         logger.warning(\n#             'Clients for method {} is not implemented. Will use default one'.\n#             format(cfg.federate.method))\n# \n#     if client_type == 'fedsageplus':\n#         from federatedscope.gfl.fedsageplus.worker import FedSagePlusClient\n#         client_class = FedSagePlusClient\n#     elif client_type == 'gcflplus':\n#         from federatedscope.gfl.gcflplus.worker import GCFLPlusClient\n#         client_class = GCFLPlusClient\n#     elif client_type == 'fedgc':\n#         from federatedscope.cl.fedgc.client import GlobalContrastFLClient\n#         client_class = GlobalContrastFLClient\n#     else:\n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\nimport logging\nfrom federatedscope.core.configs import constants\n\nlogger = logging.getLogger(__name__)\n\n\ndef get_aggregator(method, model=None, device=None, online=False, config=None):\n    \"\"\"\n    This function builds an aggregator, which is a protocol for aggregate \\\n    all clients' model(s).\n\n    Arguments:\n        method: key to determine which aggregator to use\n        model:  model to be aggregated\n        device: where to aggregate models (``cpu`` or ``gpu``)\n        online: ``True`` or ``False`` to use online aggregator.\n        config: configurations for FL, see ``federatedscope.core.configs``\n\n    Returns:\n        An instance of aggregator (see ``core.aggregator`` for details)\n\n    Note:\n      The key-value pairs of ``method`` and aggregators:\n        ==================================  ===========================\n        Method                              Aggregator\n        ==================================  ===========================\n        ``tensorflow``                      ``cross_backends.FedAvgAggregator``\n        ``local``                           \\\n        ``core.aggregators.NoCommunicationAggregator``\n        ``global``                          \\\n        ``core.aggregators.NoCommunicationAggregator``\n        ``fedavg``                          \\\n        ``core.aggregators.OnlineClientsAvgAggregator`` or \\\n        ``core.aggregators.AsynClientsAvgAggregator`` or \\\n        ``ClientsAvgAggregator``\n        ``pfedme``                          \\\n        ``core.aggregators.ServerClientsInterpolateAggregator``\n        ``ditto``                           \\\n        ``core.aggregators.OnlineClientsAvgAggregator`` or \\\n        ``core.aggregators.AsynClientsAvgAggregator`` or \\\n        ``ClientsAvgAggregator``\n        ``fedsageplus``                     \\\n        ``core.aggregators.OnlineClientsAvgAggregator`` or \\\n        ``core.aggregators.AsynClientsAvgAggregator`` or \\\n        ``ClientsAvgAggregator``\n        ``gcflplus``                        \\\n        ``core.aggregators.OnlineClientsAvgAggregator`` or \\\n        ``core.aggregators.AsynClientsAvgAggregator`` or \\\n        ``ClientsAvgAggregator``\n        ``fedopt``                          \\\n        ``core.aggregators.FedOptAggregator``\n        ==================================  ===========================\n    \"\"\"\n    if config.backend == 'tensorflow':\n        from federatedscope.cross_backends import FedAvgAggregator\n        return FedAvgAggregator(model=model, device=device)\n    else:\n        from federatedscope.core.aggregators import ClientsAvgAggregator, \\\n            OnlineClientsAvgAggregator, ServerClientsInterpolateAggregator, \\\n            FedOptAggregator, NoCommunicationAggregator, \\\n            AsynClientsAvgAggregator\n\n    if method.lower() in constants.AGGREGATOR_TYPE:\n        aggregator_type = constants.AGGREGATOR_TYPE[method.lower()]\n    else:\n        aggregator_type = \"clients_avg\"\n        logger.warning(\n            'Aggregator for method {} is not implemented. Will use default one'\n            .format(method))\n\n    if config.data.type.lower() == 'hetero_nlp_tasks' and \\\n            not config.federate.atc_vanilla:\n        from federatedscope.nlp.hetero_tasks.aggregator import ATCAggregator\n        return ATCAggregator(model=model, config=config, device=device)\n\n    if config.fedopt.use or aggregator_type == 'fedopt':\n        return FedOptAggregator(config=config, model=model, device=device)\n    elif aggregator_type == 'clients_avg':\n        if online:\n            return OnlineClientsAvgAggregator(\n                model=model,\n                device=device,\n                config=config,\n                src_device=device\n                if config.federate.share_local_model else 'cpu')", "metadata": {"task_id": "alibaba_FederatedScope/166", "ground_truth": "        elif config.asyn.use:", "fpath_tuple": ["alibaba_FederatedScope", "federatedscope", "core", "auxiliaries", "aggregator_builder.py"], "context_start_lineno": 0, "line_no": 85, "query_window": {"context": "        aggregator_type = \"clients_avg\"\n        logger.warning(\n            'Aggregator for method {} is not implemented. Will use default one'\n            .format(method))\n\n    if config.data.type.lower() == 'hetero_nlp_tasks' and \\\n            not config.federate.atc_vanilla:\n        from federatedscope.nlp.hetero_tasks.aggregator import ATCAggregator\n        return ATCAggregator(model=model, config=config, device=device)\n\n    if config.fedopt.use or aggregator_type == 'fedopt':\n        return FedOptAggregator(config=config, model=model, device=device)\n    elif aggregator_type == 'clients_avg':\n        if online:\n            return OnlineClientsAvgAggregator(\n                model=model,\n                device=device,\n                config=config,\n                src_device=device\n                if config.federate.share_local_model else 'cpu')", "metadata": {"fpath_tuple": ["alibaba_FederatedScope", "federatedscope", "core", "auxiliaries", "aggregator_builder.py"], "line_no": 85, "task_id": "alibaba_FederatedScope/166", "start_line_no": 65, "end_line_no": 85, "window_size": 20, "context_start_lineno": 0, "repo": "alibaba_FederatedScope"}}, "top_k_context": [{"context": "    if cfg.data.type.lower() == 'hetero_nlp_tasks':\n        from federatedscope.nlp.hetero_tasks.worker import ATCClient\n        return ATCClient\n\n    if cfg.federate.method.lower() in constants.CLIENTS_TYPE:\n        client_type = constants.CLIENTS_TYPE[cfg.federate.method.lower()]\n    else:\n        client_type = \"normal\"\n        logger.warning(\n            'Clients for method {} is not implemented. Will use default one'.\n            format(cfg.federate.method))\n\n    if client_type == 'fedsageplus':\n        from federatedscope.gfl.fedsageplus.worker import FedSagePlusClient\n        client_class = FedSagePlusClient\n    elif client_type == 'gcflplus':\n        from federatedscope.gfl.gcflplus.worker import GCFLPlusClient\n        client_class = GCFLPlusClient\n    elif client_type == 'fedgc':\n        from federatedscope.cl.fedgc.client import GlobalContrastFLClient", "metadata": [{"fpath_tuple": ["alibaba_FederatedScope", "federatedscope", "core", "auxiliaries", "worker_builder.py"], "line_no": 72, "start_line_no": 62, "end_line_no": 82, "window_size": 20, "repo": "alibaba_FederatedScope", "slice_size": 10}], "sim_score": 0.47244094488188976}, {"context": "\n    if cfg.data.type.lower() == 'hetero_nlp_tasks':\n        from federatedscope.nlp.hetero_tasks.worker import ATCServer\n        return ATCServer\n\n    if cfg.federate.method.lower() in constants.SERVER_TYPE:\n        server_type = constants.SERVER_TYPE[cfg.federate.method.lower()]\n    else:\n        server_type = \"normal\"\n        logger.warning(\n            'Server for method {} is not implemented. Will use default one'.\n            format(cfg.federate.method))\n\n    if server_type == 'fedsageplus':\n        from federatedscope.gfl.fedsageplus.worker import FedSagePlusServer\n        server_class = FedSagePlusServer\n    elif server_type == 'gcflplus':\n        from federatedscope.gfl.gcflplus.worker import GCFLPlusServer\n        server_class = GCFLPlusServer\n    elif server_type == 'fedgc':", "metadata": [{"fpath_tuple": ["alibaba_FederatedScope", "federatedscope", "core", "auxiliaries", "worker_builder.py"], "line_no": 174, "start_line_no": 164, "end_line_no": 184, "window_size": 20, "repo": "alibaba_FederatedScope", "slice_size": 10}], "sim_score": 0.4672131147540984}, {"context": "        from federatedscope.nlp.hetero_tasks.worker import ATCServer\n        return ATCServer\n\n    if cfg.federate.method.lower() in constants.SERVER_TYPE:\n        server_type = constants.SERVER_TYPE[cfg.federate.method.lower()]\n    else:\n        server_type = \"normal\"\n        logger.warning(\n            'Server for method {} is not implemented. Will use default one'.\n            format(cfg.federate.method))\n\n    if server_type == 'fedsageplus':\n        from federatedscope.gfl.fedsageplus.worker import FedSagePlusServer\n        server_class = FedSagePlusServer\n    elif server_type == 'gcflplus':\n        from federatedscope.gfl.gcflplus.worker import GCFLPlusServer\n        server_class = GCFLPlusServer\n    elif server_type == 'fedgc':\n        from federatedscope.cl.fedgc.server import GlobalContrastFLServer\n        server_class = GlobalContrastFLServer", "metadata": [{"fpath_tuple": ["alibaba_FederatedScope", "federatedscope", "core", "auxiliaries", "worker_builder.py"], "line_no": 176, "start_line_no": 166, "end_line_no": 186, "window_size": 20, "repo": "alibaba_FederatedScope", "slice_size": 10}], "sim_score": 0.4523809523809524}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# fortuna/conformal/classification/simple_prediction.py\n# --------------------------------------------------\n# \n#         Returns\n#         -------\n#         float\n#             The conformal quantiles.\n#         \"\"\"\n#         if error < 0 or error > 1:\n#             raise ValueError(\"\"\"`error` must be a scalar between 0 and 1.\"\"\")\n# \n#         if scores is None:\n#             scores = self.score(val_probs, val_targets)\n#         n = scores.shape[0]\n#         return jnp.quantile(scores, jnp.ceil((n + 1) * (1 - error)) / n)\n# \n#     def conformal_set(\n#         self,\n#         val_probs: Array,\n#         test_probs: Array,\n#         val_targets: Array,\n#         error: float = 0.05,\n# --------------------------------------------------\n# the below code fragment can be found in:\n# fortuna/conformal/classification/adaptive_prediction.py\n# --------------------------------------------------\n#         Returns\n#         -------\n#         float\n#             The conformal quantiles.\n#         \"\"\"\n#         if error < 0 or error > 1:\n#             raise ValueError(\"\"\"`error` must be a scalar between 0 and 1.\"\"\")\n# \n#         if scores is None:\n#             scores = self.score(val_probs, val_targets)\n#         n = scores.shape[0]\n#         return jnp.quantile(scores, jnp.ceil((n + 1) * (1 - error)) / n)\n# \n#     def conformal_set(\n#         self,\n#         val_probs: Array,\n#         test_probs: Array,\n#         val_targets: Array,\n#         error: float = 0.05,\n#         quantile: Optional[float] = None,\n# --------------------------------------------------\n# the below code fragment can be found in:\n# fortuna/conformal/regression/onedim_uncertainty.py\n# --------------------------------------------------\n# \n#         Returns\n#         -------\n#         float\n#             The conformal quantile.\n#         \"\"\"\n#         if error < 0 or error > 1:\n#             raise ValueError(\"\"\"`error` must be a scalar between 0 and 1.\"\"\")\n#         if scores is None:\n#             scores = self.score(val_preds, val_uncertainties, val_targets)\n#         n = scores.shape[0]\n#         return jnp.quantile(scores, jnp.ceil((n + 1) * (1 - error)) / n)\n# \n#     def conformal_interval(\n#         self,\n#         val_preds: Array,\n#         val_uncertainties: Array,\n#         test_preds: Array,\n#         test_uncertainties: Array,\n#         val_targets: Array,\n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\nfrom typing import Optional\n\nimport jax.numpy as jnp\n\nfrom fortuna.typing import Array\n\n\nclass QuantileConformalRegressor:\n    def score(\n        self, val_lower_bounds: Array, val_upper_bounds: Array, val_targets: Array,\n    ) -> jnp.ndarray:\n        \"\"\"\n        Compute score function.\n\n        Parameters\n        ----------\n        val_lower_bounds: Array\n            Interval lower bounds computed on a validation set.\n        val_upper_bounds: Array\n            Interval upper bounds computed on a validation set.\n        val_targets: Array\n            A two-dimensional array of validation target variables.\n\n        Returns\n        -------\n        jnp.ndarray\n            The conformal scores.\n        \"\"\"\n        if val_lower_bounds.shape != val_upper_bounds.shape:\n            raise ValueError(\n                \"\"\"The shapes of `val_lower_bounds` and `val_upper_bounds` must be the same, but shapes {} and {} \n                were given, respectively.\"\"\".format(\n                    val_lower_bounds.shape, val_upper_bounds.shape\n                )\n            )\n        if val_lower_bounds.ndim != 1:\n            raise ValueError(\n                \"`val_lower_bounds` and `val_upper_bounds` must be one-dimensional arrays.\"\n            )\n        if val_targets.shape[1] != 1:\n            raise ValueError(\n                \"\"\"The second dimension of `val_targets` must have only one component.\"\"\"\n            )\n        val_targets = val_targets.squeeze(1)\n        return jnp.maximum(\n            val_lower_bounds - val_targets, val_targets - val_upper_bounds\n        )\n\n    def quantile(\n        self,\n        val_lower_bounds: Array,\n        val_upper_bounds: Array,\n        val_targets: Array,\n        error: float,\n        scores: Optional[Array] = None,\n    ) -> Array:\n        \"\"\"\n        Compute a quantile of the scores.\n\n        Parameters\n        ----------\n        val_lower_bounds: Array\n            Interval lower bounds computed on a validation set.\n        val_upper_bounds: Array\n            Interval upper bounds computed on a validation set.\n        val_targets: Array\n            A two-dimensional array of validation target variables.\n        error: float\n            Coverage error. This must be a scalar between 0 and 1, extremes included. This should correspond to the\n            coverage error for which `val_lower_bounds`, `val_upper_bounds`, `test_lower_bounds` and\n            `test_upper_bounds` were computed.\n        scores: Optional[float]\n            Conformal scores. This should be the output of\n            :meth:`~fortuna.conformal.regression.quantile.QuantileConformalRegressor.score`.\n\n        Returns\n        -------\n        float\n            The conformal quantile.\n        \"\"\"\n        if error < 0 or error > 1:\n            raise ValueError(\"\"\"`error` must be a scalar between 0 and 1.\"\"\")\n        if scores is None:\n            scores = self.score(val_lower_bounds, val_upper_bounds, val_targets)\n        n = scores.shape[0]\n        return jnp.quantile(scores, jnp.ceil((n + 1) * (1 - error)) / n)\n\n    def conformal_interval(\n        self,\n        val_lower_bounds: Array,\n        val_upper_bounds: Array,\n        test_lower_bounds: Array,\n        test_upper_bounds: Array,", "metadata": {"task_id": "awslabs_fortuna/21", "ground_truth": "        val_targets: Array,", "fpath_tuple": ["awslabs_fortuna", "fortuna", "conformal", "regression", "quantile.py"], "context_start_lineno": 0, "line_no": 93, "query_window": {"context": "            :meth:`~fortuna.conformal.regression.quantile.QuantileConformalRegressor.score`.\n\n        Returns\n        -------\n        float\n            The conformal quantile.\n        \"\"\"\n        if error < 0 or error > 1:\n            raise ValueError(\"\"\"`error` must be a scalar between 0 and 1.\"\"\")\n        if scores is None:\n            scores = self.score(val_lower_bounds, val_upper_bounds, val_targets)\n        n = scores.shape[0]\n        return jnp.quantile(scores, jnp.ceil((n + 1) * (1 - error)) / n)\n\n    def conformal_interval(\n        self,\n        val_lower_bounds: Array,\n        val_upper_bounds: Array,\n        test_lower_bounds: Array,\n        test_upper_bounds: Array,", "metadata": {"fpath_tuple": ["awslabs_fortuna", "fortuna", "conformal", "regression", "quantile.py"], "line_no": 93, "task_id": "awslabs_fortuna/21", "start_line_no": 73, "end_line_no": 93, "window_size": 20, "context_start_lineno": 0, "repo": "awslabs_fortuna"}}, "top_k_context": [{"context": "            Conformal scores. This should be the output of\n            :meth:`~fortuna.conformal.regression.onedim_uncertainty.OneDimensionalUncertaintyConformalRegressor.score`.\n\n        Returns\n        -------\n        float\n            The conformal quantile.\n        \"\"\"\n        if error < 0 or error > 1:\n            raise ValueError(\"\"\"`error` must be a scalar between 0 and 1.\"\"\")\n        if scores is None:\n            scores = self.score(val_preds, val_uncertainties, val_targets)\n        n = scores.shape[0]\n        return jnp.quantile(scores, jnp.ceil((n + 1) * (1 - error)) / n)\n\n    def conformal_interval(\n        self,\n        val_preds: Array,\n        val_uncertainties: Array,\n        test_preds: Array,", "metadata": [{"fpath_tuple": ["awslabs_fortuna", "fortuna", "conformal", "regression", "onedim_uncertainty.py"], "line_no": 82, "start_line_no": 72, "end_line_no": 92, "window_size": 20, "repo": "awslabs_fortuna", "slice_size": 10}], "sim_score": 0.7983193277310925}, {"context": "            :meth:`~fortuna.conformal.classification.adaptive_prediction.AdaptivePredictionConformalClassifier.score`.\n\n        Returns\n        -------\n        float\n            The conformal quantiles.\n        \"\"\"\n        if error < 0 or error > 1:\n            raise ValueError(\"\"\"`error` must be a scalar between 0 and 1.\"\"\")\n\n        if scores is None:\n            scores = self.score(val_probs, val_targets)\n        n = scores.shape[0]\n        return jnp.quantile(scores, jnp.ceil((n + 1) * (1 - error)) / n)\n\n    def conformal_set(\n        self,\n        val_probs: Array,\n        test_probs: Array,\n        val_targets: Array,", "metadata": [{"fpath_tuple": ["awslabs_fortuna", "fortuna", "conformal", "classification", "adaptive_prediction.py"], "line_no": 72, "start_line_no": 62, "end_line_no": 82, "window_size": 20, "repo": "awslabs_fortuna", "slice_size": 10}], "sim_score": 0.7894736842105263}, {"context": "            The conformal scores. This should be the output of\n            :meth:`~fortuna.conformal.classification.simple_prediction.SimplePredictionConformalClassifier.score`.\n\n        Returns\n        -------\n        float\n            The conformal quantiles.\n        \"\"\"\n        if error < 0 or error > 1:\n            raise ValueError(\"\"\"`error` must be a scalar between 0 and 1.\"\"\")\n\n        if scores is None:\n            scores = self.score(val_probs, val_targets)\n        n = scores.shape[0]\n        return jnp.quantile(scores, jnp.ceil((n + 1) * (1 - error)) / n)\n\n    def conformal_set(\n        self,\n        val_probs: Array,\n        test_probs: Array,", "metadata": [{"fpath_tuple": ["awslabs_fortuna", "fortuna", "conformal", "classification", "simple_prediction.py"], "line_no": 66, "start_line_no": 56, "end_line_no": 76, "window_size": 20, "repo": "awslabs_fortuna", "slice_size": 10}], "sim_score": 0.7627118644067796}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# ding/utils/tests/test_scheduler_helper.py\n# --------------------------------------------------\n#         assert test_scheduler.in_cooldown is False\n# \n#     def test_step(self):\n# \n#         self.test_merged_scheduler_config.cooldown = 1\n# \n#         test_scheduler = Scheduler(self.test_merged_scheduler_config)\n#         assert test_scheduler.cooldown_counter == 1\n#         test_scheduler.last_metrics = 1.0\n# \n#         old_param = self.test_policy_config.learn.entropy_weight\n# \n#         # good epoch with maximum cooldown lenth is 1\n#         self.test_policy_config_param = test_scheduler.step(0.9, self.test_policy_config_param)\n#         assert self.test_policy_config_param == old_param\n#         assert test_scheduler.cooldown_counter == 0\n#         assert test_scheduler.last_metrics == 0.9\n#         assert test_scheduler.bad_epochs_num == 0\n# \n#         # first bad epoch in cooldown period\n# --------------------------------------------------\n# the below code fragment can be found in:\n# ding/torch_utils/checkpoint_helper.py\n# --------------------------------------------------\n#     @property\n#     def val(self) -> int:\n#         return self._val\n# \n#     def update(self, val: int) -> None:\n#         r\"\"\"\n#         Overview:\n#             Update the var counter\n#         Arguments:\n#             - val (:obj:`int`): the update value of the counter\n#         \"\"\"\n#         self._val = val\n# \n#     def add(self, add_num: int):\n#         r\"\"\"\n#         Overview:\n#             Add the number to counter\n#         Arguments:\n#             - add_num (:obj:`int`): the number added to the counter\n#         \"\"\"\n# --------------------------------------------------\n# the below code fragment can be found in:\n# ding/torch_utils/checkpoint_helper.py\n# --------------------------------------------------\n#         return self._val\n# \n#     def update(self, val: int) -> None:\n#         r\"\"\"\n#         Overview:\n#             Update the var counter\n#         Arguments:\n#             - val (:obj:`int`): the update value of the counter\n#         \"\"\"\n#         self._val = val\n# \n#     def add(self, add_num: int):\n#         r\"\"\"\n#         Overview:\n#             Add the number to counter\n#         Arguments:\n#             - add_num (:obj:`int`): the number added to the counter\n#         \"\"\"\n#         self._val += add_num\n# \n# --------------------------------------------------\n# the below code fragment can be found in:\n# ding/torch_utils/checkpoint_helper.py\n# --------------------------------------------------\n#     def update(self, val: int) -> None:\n#         r\"\"\"\n#         Overview:\n#             Update the var counter\n#         Arguments:\n#             - val (:obj:`int`): the update value of the counter\n#         \"\"\"\n#         self._val = val\n# \n#     def add(self, add_num: int):\n#         r\"\"\"\n#         Overview:\n#             Add the number to counter\n#         Arguments:\n#             - add_num (:obj:`int`): the number added to the counter\n#         \"\"\"\n#         self._val += add_num\n# \n# \n# def auto_checkpoint(func: Callable) -> Callable:\n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\nfrom .default_helper import deep_merge_dicts\nfrom easydict import EasyDict\n\n\nclass Scheduler(object):\n    \"\"\"\n    Overview:\n        Update learning parameters when the trueskill metrics has stopped improving.\n        For example, models often benefits from reducing entropy weight once the learning process stagnates.\n        This scheduler reads a metrics quantity and if no improvement is seen for a 'patience' number of epochs,\n        the corresponding parameter is increased or decreased, which decides on the 'schedule_mode'.\n    Args:\n        - schedule_flag (:obj:`bool`): Indicates whether to use scheduler in training pipeline.\n            Default: False\n        - schedule_mode (:obj:`str`): One of 'reduce', 'add','multi','div'. The schecule_mode\n            decides the way of updating the parameters.  Default:'reduce'.\n        - factor (:obj:`float`) : Amount (greater than 0) by which the parameter will be\n            increased/decreased. Default: 0.05\n        - change_range (:obj:`list`): Indicates the minimum and maximum value\n            the parameter can reach respectively. Default: [-1,1]\n        - threshold (:obj:`float`): Threshold for measuring the new optimum,\n            to only focus on significant changes. Default:  1e-4.\n        - optimize_mode (:obj:`str`): One of 'min', 'max', which indicates the sign of\n            optimization objective. Dynamic_threshold = last_metrics + threshold in `max`\n            mode or last_metrics - threshold in `min` mode. Default: 'min'\n        - patience (:obj:`int`): Number of epochs with no improvement after which\n            the parameter will be updated. For example, if `patience = 2`, then we\n            will ignore the first 2 epochs with no improvement, and will only update\n            the parameter after the 3rd epoch if the metrics still hasn't improved then.\n            Default: 10.\n        - cooldown (:obj:`int`): Number of epochs to wait before resuming\n            normal operation after the parameter has been updated. Default: 0.\n    Interfaces:\n        __init__, update_param, step\n    Property:\n        in_cooldown, is_better\n    \"\"\"\n\n    config = dict(\n        schedule_flag=False,\n        schedule_mode='reduce',\n        factor=0.05,\n        change_range=[-1, 1],\n        threshold=1e-4,\n        optimize_mode='min',\n        patience=10,\n        cooldown=0,\n    )\n\n    def __init__(self, merged_scheduler_config: EasyDict) -> None:\n        '''\n        Overview:\n            Initialize the scheduler.\n        Args:\n            - merged_scheduler_config (:obj:`EasyDict`): the scheduler config, which merges the user\n                config and defaul config\n        '''\n\n        schedule_mode = merged_scheduler_config.schedule_mode\n        factor = merged_scheduler_config.factor\n        change_range = merged_scheduler_config.change_range\n        threshold = merged_scheduler_config.threshold\n        optimize_mode = merged_scheduler_config.optimize_mode\n        patience = merged_scheduler_config.patience\n        cooldown = merged_scheduler_config.cooldown\n\n        assert schedule_mode in [\n            'reduce', 'add', 'multi', 'div'\n        ], 'The schedule mode should be one of [\\'reduce\\', \\'add\\', \\'multi\\',\\'div\\']'\n        self.schedule_mode = schedule_mode\n\n        assert isinstance(factor, (float, int)), 'The factor should be a float/int number '\n        assert factor > 0, 'The factor should be greater than 0'\n        self.factor = float(factor)\n\n        assert isinstance(change_range,\n                          list) and len(change_range) == 2, 'The change_range should be a list with 2 float numbers'\n        assert (isinstance(change_range[0], (float, int))) and (\n            isinstance(change_range[1], (float, int))\n        ), 'The change_range should be a list with 2 float/int numbers'\n        assert change_range[0] < change_range[1], 'The first num should be smaller than the second num'\n        self.change_range = change_range\n\n        assert isinstance(threshold, (float, int)), 'The threshold should be a float/int number'\n        self.threshold = threshold\n\n        assert optimize_mode in ['min', 'max'], 'The optimize_mode should be one of [\\'min\\', \\'max\\']'\n        self.optimize_mode = optimize_mode\n\n        assert isinstance(patience, int), 'The patience should be a integer greater than or equal to 0'\n        assert patience >= 0, 'The patience should be a integer greater than or equal to 0'\n        self.patience = patience\n\n        assert isinstance(cooldown, int), 'The cooldown_counter should be a integer greater than or equal to 0'\n        assert cooldown >= 0, 'The cooldown_counter should be a integer greater than or equal to 0'\n        self.cooldown = cooldown\n        self.cooldown_counter = cooldown\n\n        self.last_metrics = None\n        self.bad_epochs_num = 0\n\n    def step(self, metrics: float, param: float) -> float:\n        '''\n        Overview:\n            Decides whether to update the scheduled parameter\n        Args:\n            - metrics (:obj:`float`): current input metrics\n            - param (:obj:`float`): parameter need to be updated\n        Returns:\n            - step_param (:obj:`float`): parameter after one step\n        '''\n        assert isinstance(metrics, float), 'The metrics should be converted to a float number'", "metadata": {"task_id": "opendilab_ACE/73", "ground_truth": "        cur_metrics = metrics", "fpath_tuple": ["opendilab_ACE", "ding", "utils", "scheduler_helper.py"], "context_start_lineno": 0, "line_no": 112, "query_window": {"context": "\n        assert isinstance(cooldown, int), 'The cooldown_counter should be a integer greater than or equal to 0'\n        assert cooldown >= 0, 'The cooldown_counter should be a integer greater than or equal to 0'\n        self.cooldown = cooldown\n        self.cooldown_counter = cooldown\n\n        self.last_metrics = None\n        self.bad_epochs_num = 0\n\n    def step(self, metrics: float, param: float) -> float:\n        '''\n        Overview:\n            Decides whether to update the scheduled parameter\n        Args:\n            - metrics (:obj:`float`): current input metrics\n            - param (:obj:`float`): parameter need to be updated\n        Returns:\n            - step_param (:obj:`float`): parameter after one step\n        '''\n        assert isinstance(metrics, float), 'The metrics should be converted to a float number'", "metadata": {"fpath_tuple": ["opendilab_ACE", "ding", "utils", "scheduler_helper.py"], "line_no": 112, "task_id": "opendilab_ACE/73", "start_line_no": 92, "end_line_no": 112, "window_size": 20, "context_start_lineno": 0, "repo": "opendilab_ACE"}}, "top_k_context": [{"context": "        return self._val\n\n    def update(self, val: int) -> None:\n        r\"\"\"\n        Overview:\n            Update the var counter\n        Arguments:\n            - val (:obj:`int`): the update value of the counter\n        \"\"\"\n        self._val = val\n\n    def add(self, add_num: int):\n        r\"\"\"\n        Overview:\n            Add the number to counter\n        Arguments:\n            - add_num (:obj:`int`): the number added to the counter\n        \"\"\"\n        self._val += add_num\n", "metadata": [{"fpath_tuple": ["opendilab_ACE", "ding", "torch_utils", "checkpoint_helper.py"], "line_no": 276, "start_line_no": 266, "end_line_no": 286, "window_size": 20, "repo": "opendilab_ACE", "slice_size": 10}], "sim_score": 0.2857142857142857}, {"context": "    @property\n    def val(self) -> int:\n        return self._val\n\n    def update(self, val: int) -> None:\n        r\"\"\"\n        Overview:\n            Update the var counter\n        Arguments:\n            - val (:obj:`int`): the update value of the counter\n        \"\"\"\n        self._val = val\n\n    def add(self, add_num: int):\n        r\"\"\"\n        Overview:\n            Add the number to counter\n        Arguments:\n            - add_num (:obj:`int`): the number added to the counter\n        \"\"\"", "metadata": [{"fpath_tuple": ["opendilab_ACE", "ding", "torch_utils", "checkpoint_helper.py"], "line_no": 274, "start_line_no": 264, "end_line_no": 284, "window_size": 20, "repo": "opendilab_ACE", "slice_size": 10}], "sim_score": 0.2828282828282828}, {"context": "        self._val = init_val\n\n    @property\n    def val(self) -> int:\n        return self._val\n\n    def update(self, val: int) -> None:\n        r\"\"\"\n        Overview:\n            Update the var counter\n        Arguments:\n            - val (:obj:`int`): the update value of the counter\n        \"\"\"\n        self._val = val\n\n    def add(self, add_num: int):\n        r\"\"\"\n        Overview:\n            Add the number to counter\n        Arguments:", "metadata": [{"fpath_tuple": ["opendilab_ACE", "ding", "torch_utils", "checkpoint_helper.py"], "line_no": 272, "start_line_no": 262, "end_line_no": 282, "window_size": 20, "repo": "opendilab_ACE", "slice_size": 10}], "sim_score": 0.2828282828282828}, {"context": "        self.test_merged_scheduler_config.cooldown_counter = 0\n        test_scheduler = Scheduler(self.test_merged_scheduler_config)\n        assert test_scheduler.in_cooldown is False\n\n    def test_step(self):\n\n        self.test_merged_scheduler_config.cooldown = 1\n\n        test_scheduler = Scheduler(self.test_merged_scheduler_config)\n        assert test_scheduler.cooldown_counter == 1\n        test_scheduler.last_metrics = 1.0\n\n        old_param = self.test_policy_config.learn.entropy_weight\n\n        # good epoch with maximum cooldown lenth is 1\n        self.test_policy_config_param = test_scheduler.step(0.9, self.test_policy_config_param)\n        assert self.test_policy_config_param == old_param\n        assert test_scheduler.cooldown_counter == 0\n        assert test_scheduler.last_metrics == 0.9\n        assert test_scheduler.bad_epochs_num == 0", "metadata": [{"fpath_tuple": ["opendilab_ACE", "ding", "utils", "tests", "test_scheduler_helper.py"], "line_no": 88, "start_line_no": 78, "end_line_no": 98, "window_size": 20, "repo": "opendilab_ACE", "slice_size": 10}], "sim_score": 0.2777777777777778}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# test/mocking_classes.py\n# --------------------------------------------------\n#                 shape=batch_size,\n#             )\n#         cls._reward_spec = reward_spec\n#         cls._observation_spec = observation_spec\n#         cls._input_spec = input_spec\n#         cls.from_pixels = from_pixels\n#         return super().__new__(*args, **kwargs)\n# \n#     def _get_in_obs(self, obs):\n#         return obs\n# \n#     def _get_out_obs(self, obs):\n#         return obs\n# \n#     def _reset(self, tensordict: TensorDictBase) -> TensorDictBase:\n#         self.counter += 1\n#         self.step_count = 0\n#         # state = torch.zeros(self.size) + self.counter\n#         if tensordict is None:\n#             tensordict = TensorDict({}, self.batch_size, device=self.device)\n# --------------------------------------------------\n# the below code fragment can be found in:\n# test/mocking_classes.py\n# --------------------------------------------------\n#         done = torch.zeros_like(done).all(-1).unsqueeze(-1)\n#         tensordict.set(\"reward\", reward.to(torch.get_default_dtype()))\n#         tensordict.set(\"done\", done)\n#         return tensordict\n# \n# \n# class ContinuousActionVecMockEnv(_MockEnv):\n#     @classmethod\n#     def __new__(\n#         cls,\n#         *args,\n#         observation_spec=None,\n#         action_spec=None,\n#         input_spec=None,\n#         reward_spec=None,\n#         from_pixels=False,\n#         **kwargs,\n#     ):\n#         batch_size = kwargs.setdefault(\"batch_size\", torch.Size([]))\n#         size = cls.size = 7\n# --------------------------------------------------\n# the below code fragment can be found in:\n# test/mocking_classes.py\n# --------------------------------------------------\n#             cls._out_key = \"observation_orig\"\n#             input_spec = CompositeSpec(\n#                 **{\n#                     cls._out_key: observation_spec[\"observation\"],\n#                     \"action\": action_spec,\n#                 },\n#                 shape=batch_size,\n#             )\n#         cls._reward_spec = reward_spec\n#         cls._observation_spec = observation_spec\n#         cls._input_spec = input_spec\n#         cls.from_pixels = from_pixels\n#         return super().__new__(*args, **kwargs)\n# \n#     def _get_in_obs(self, obs):\n#         return obs\n# \n#     def _get_out_obs(self, obs):\n#         return obs\n# \n# --------------------------------------------------\n# the below code fragment can be found in:\n# test/mocking_classes.py\n# --------------------------------------------------\n#                 **{\n#                     cls._out_key: observation_spec[\"observation\"],\n#                     \"action\": action_spec,\n#                 },\n#                 shape=batch_size,\n#             )\n#         cls._reward_spec = reward_spec\n#         cls._observation_spec = observation_spec\n#         cls._input_spec = input_spec\n#         cls.from_pixels = from_pixels\n#         return super().__new__(*args, **kwargs)\n# \n#     def _get_in_obs(self, obs):\n#         return obs\n# \n#     def _get_out_obs(self, obs):\n#         return obs\n# \n#     def _reset(self, tensordict: TensorDictBase) -> TensorDictBase:\n#         self.counter += 1\n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\nset(\"done\", done)\n        return tensordict\n\n    def _obs_step(self, obs, a):\n        return obs + a / self.maxstep\n\n\nclass DiscreteActionVecPolicy:\n    in_keys = [\"observation\"]\n    out_keys = [\"action\"]\n\n    def _get_in_obs(self, tensordict):\n        obs = tensordict.get(*self.in_keys)\n        return obs\n\n    def __call__(self, tensordict):\n        obs = self._get_in_obs(tensordict)\n        max_obs = (obs == obs.max(dim=-1, keepdim=True)[0]).cumsum(-1).argmax(-1)\n        k = tensordict.get(*self.in_keys).shape[-1]\n        max_obs = (max_obs + 1) % k\n        action = torch.nn.functional.one_hot(max_obs, k)\n        tensordict.set(*self.out_keys, action)\n        return tensordict\n\n\nclass DiscreteActionConvMockEnv(DiscreteActionVecMockEnv):\n    @classmethod\n    def __new__(\n        cls,\n        *args,\n        observation_spec=None,\n        action_spec=None,\n        input_spec=None,\n        reward_spec=None,\n        from_pixels=True,\n        **kwargs,\n    ):\n        batch_size = kwargs.setdefault(\"batch_size\", torch.Size([]))\n        if observation_spec is None:\n            cls.out_key = \"pixels\"\n            observation_spec = CompositeSpec(\n                pixels=UnboundedContinuousTensorSpec(\n                    shape=torch.Size([*batch_size, 1, 7, 7])\n                ),\n                pixels_orig=UnboundedContinuousTensorSpec(\n                    shape=torch.Size([*batch_size, 1, 7, 7])\n                ),\n                shape=batch_size,\n            )\n        if action_spec is None:\n            action_spec = OneHotDiscreteTensorSpec(7, shape=(*batch_size, 7))\n        if reward_spec is None:\n            reward_spec = UnboundedContinuousTensorSpec(shape=(*batch_size, 1))\n\n        if input_spec is None:\n            cls._out_key = \"pixels_orig\"\n            input_spec = CompositeSpec(\n                **{\n                    cls._out_key: observation_spec[\"pixels_orig\"],\n                    \"action\": action_spec,\n                },\n                shape=batch_size,\n            )\n        return super().__new__(\n            *args,\n            observation_spec=observation_spec,\n            action_spec=action_spec,\n            reward_spec=reward_spec,\n            input_spec=input_spec,\n            from_pixels=from_pixels,\n            **kwargs,\n        )\n\n    def _get_out_obs(self, obs):\n        obs = torch.diag_embed(obs, 0, -2, -1).unsqueeze(0)\n        return obs\n\n    def _get_in_obs(self, obs):\n        return obs.diagonal(0, -1, -2).squeeze()\n\n\nclass DiscreteActionConvMockEnvNumpy(DiscreteActionConvMockEnv):\n    @classmethod\n    def __new__(\n        cls,\n        *args,\n        observation_spec=None,\n        action_spec=None,\n        input_spec=None,\n        reward_spec=None,\n        from_pixels=True,\n        categorical_action_encoding=False,\n        **kwargs,\n    ):\n        batch_size = kwargs.setdefault(\"batch_size\", torch.Size([]))\n        if observation_spec is None:\n            cls.out_key = \"pixels\"\n            observation_spec = CompositeSpec(\n                pixels=UnboundedContinuousTensorSpec(\n                    shape=torch.Size([*batch_size, 7, 7, 3])\n                ),\n                pixels_orig=UnboundedContinuousTensorSpec(\n                    shape=torch.Size([*batch_size, 7, 7, 3])\n                ),\n                shape=batch_size,\n            )\n        if action_spec is None:\n            action_spec_cls = (\n                DiscreteTensorSpec\n                if categorical_action_encoding\n                else OneHotDiscreteTensorSpec\n            )\n            action_spec = action_spec_cls(7, shape=(*batch_size, 7))\n        if input_spec is None:\n            cls._out_key = \"pixels_orig\"\n            input_spec = CompositeSpec(\n                **{\n                    cls._out_key: observation_spec[\"pixels_orig\"],\n                    \"action\": action_spec,\n                },\n                shape=batch_size,\n            )\n\n        return super().__new__(\n            *args,\n            observation_spec=observation_spec,\n            action_spec=action_spec,\n            reward_spec=reward_spec,\n            input_spec=input_spec,\n            from_pixels=from_pixels,\n            categorical_action_encoding=categorical_action_encoding,\n            **kwargs,\n        )\n\n    def _get_out_obs(self, obs):\n        obs = torch.diag_embed(obs, 0, -2, -1).unsqueeze(-1)\n        obs = obs.expand(*obs.shape[:-1], 3)\n        return obs\n\n    def _get_in_obs(self, obs):\n        return obs.diagonal(0, -2, -3)[..., 0, :]\n\n    def _obs_step(self, obs, a):\n        return obs + a.unsqueeze(-1) / self.maxstep\n\n\nclass ContinuousActionConvMockEnv(ContinuousActionVecMockEnv):\n    @classmethod\n    def __new__(\n        cls,\n        *args,\n        observation_spec=None,\n        action_spec=None,\n        input_spec=None,\n        reward_spec=None,\n        from_pixels=True,\n        pixel_shape=None,\n        **kwargs,\n    ):\n        batch_size = kwargs.setdefault(\"batch_size\", torch.Size([]))\n        if pixel_shape is None:\n            pixel_shape = [1, 7, 7]\n        if observation_spec is None:\n            cls.out_key = \"pixels\"\n            observation_spec = CompositeSpec(\n                pixels=UnboundedContinuousTensorSpec(\n                    shape=torch.Size([*batch_size, *pixel_shape])\n                ),\n                pixels_orig=UnboundedContinuousTensorSpec(\n                    shape=torch.Size([*batch_size, *pixel_shape])\n                ),\n                shape=batch_size,\n            )\n\n        if action_spec is None:\n            action_spec = BoundedTensorSpec(-1, 1, [*batch_size, pixel_shape[-1]])\n\n        if reward_spec is None:\n            reward_spec = UnboundedContinuousTensorSpec(shape=(*batch_size, 1))\n        if input_spec is None:\n            cls._out_key = \"pixels_orig\"\n            input_spec = CompositeSpec(\n                **{cls._out_key: observation_spec[\"pixels\"], \"action\": action_spec},\n                shape=batch_size,\n            )\n        return super().__new__(\n            *args,\n            observation_spec=observation_spec,\n            action_spec=action_spec,\n            reward_spec=reward_spec,\n            input_spec=input_spec,\n            from_pixels=from_pixels,\n            **kwargs,\n        )\n\n    def _get_out_obs(self, obs):\n        obs = torch.diag_embed(obs, 0, -2, -1)\n        return obs\n\n    def _get_in_obs(self, obs):\n        return obs.diagonal(0, -1, -2).squeeze()\n\n\nclass ContinuousActionConvMockEnvNumpy(ContinuousActionConvMockEnv):\n    @classmethod\n    def __new__(", "metadata": {"task_id": "pytorch_rl/147", "ground_truth": "        cls,", "fpath_tuple": ["pytorch_rl", "test", "mocking_classes.py"], "context_start_lineno": 512, "line_no": 718, "query_window": {"context": "            *args,\n            observation_spec=observation_spec,\n            action_spec=action_spec,\n            reward_spec=reward_spec,\n            input_spec=input_spec,\n            from_pixels=from_pixels,\n            **kwargs,\n        )\n\n    def _get_out_obs(self, obs):\n        obs = torch.diag_embed(obs, 0, -2, -1)\n        return obs\n\n    def _get_in_obs(self, obs):\n        return obs.diagonal(0, -1, -2).squeeze()\n\n\nclass ContinuousActionConvMockEnvNumpy(ContinuousActionConvMockEnv):\n    @classmethod\n    def __new__(", "metadata": {"fpath_tuple": ["pytorch_rl", "test", "mocking_classes.py"], "line_no": 718, "task_id": "pytorch_rl/147", "start_line_no": 698, "end_line_no": 718, "window_size": 20, "context_start_lineno": 512, "repo": "pytorch_rl"}}, "top_k_context": [{"context": "            cls._out_key = \"observation_orig\"\n            input_spec = CompositeSpec(\n                **{\n                    cls._out_key: observation_spec[\"observation\"],\n                    \"action\": action_spec,\n                },\n                shape=batch_size,\n            )\n        cls._reward_spec = reward_spec\n        cls._observation_spec = observation_spec\n        cls._input_spec = input_spec\n        cls.from_pixels = from_pixels\n        return super().__new__(*args, **kwargs)\n\n    def _get_in_obs(self, obs):\n        return obs\n\n    def _get_out_obs(self, obs):\n        return obs\n", "metadata": [{"fpath_tuple": ["pytorch_rl", "test", "mocking_classes.py"], "line_no": 470, "start_line_no": 460, "end_line_no": 480, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}], "sim_score": 0.4489795918367347}, {"context": "\n        if input_spec is None:\n            cls._out_key = \"observation_orig\"\n            input_spec = CompositeSpec(\n                **{\n                    cls._out_key: observation_spec[\"observation\"],\n                    \"action\": action_spec,\n                },\n                shape=batch_size,\n            )\n        cls._reward_spec = reward_spec\n        cls._observation_spec = observation_spec\n        cls._input_spec = input_spec\n        cls.from_pixels = from_pixels\n        return super().__new__(*args, **kwargs)\n\n    def _get_in_obs(self, obs):\n        return obs\n\n    def _get_out_obs(self, obs):", "metadata": [{"fpath_tuple": ["pytorch_rl", "test", "mocking_classes.py"], "line_no": 468, "start_line_no": 458, "end_line_no": 478, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}], "sim_score": 0.43564356435643564}, {"context": "        reward = done.any(-1).unsqueeze(-1)\n        # set done to False\n        done = torch.zeros_like(done).all(-1).unsqueeze(-1)\n        tensordict.set(\"reward\", reward.to(torch.get_default_dtype()))\n        tensordict.set(\"done\", done)\n        return tensordict\n\n\nclass ContinuousActionVecMockEnv(_MockEnv):\n    @classmethod\n    def __new__(\n        cls,\n        *args,\n        observation_spec=None,\n        action_spec=None,\n        input_spec=None,\n        reward_spec=None,\n        from_pixels=False,\n        **kwargs,\n    ):", "metadata": [{"fpath_tuple": ["pytorch_rl", "test", "mocking_classes.py"], "line_no": 424, "start_line_no": 414, "end_line_no": 434, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}], "sim_score": 0.42727272727272725}, {"context": "                    \"action\": action_spec,\n                },\n                shape=batch_size,\n            )\n        cls._reward_spec = reward_spec\n        cls._observation_spec = observation_spec\n        cls._input_spec = input_spec\n        cls.from_pixels = from_pixels\n        return super().__new__(*args, **kwargs)\n\n    def _get_in_obs(self, obs):\n        return obs\n\n    def _get_out_obs(self, obs):\n        return obs\n\n    def _reset(self, tensordict: TensorDictBase) -> TensorDictBase:\n        self.counter += 1\n        self.step_count = 0\n        # state = torch.zeros(self.size) + self.counter", "metadata": [{"fpath_tuple": ["pytorch_rl", "test", "mocking_classes.py"], "line_no": 474, "start_line_no": 464, "end_line_no": 484, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}], "sim_score": 0.41818181818181815}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# ding/worker/coordinator/solo_parallel_commander.py\n# --------------------------------------------------\n#         self._last_eval_time = 0\n#         return buffer_id\n# \n#     def notify_fail_collector_task(self, task: dict) -> None:\n#         r\"\"\"\n#         Overview:\n#             Release task space when collector task fails.\n#         \"\"\"\n#         self._collector_task_space.release_space()\n# \n#     def notify_fail_learner_task(self, task: dict) -> None:\n#         r\"\"\"\n#         Overview:\n#             Release task space when learner task fails.\n#         \"\"\"\n#         self._learner_task_space.release_space()\n# \n#     def update_learner_info(self, task_id: str, info: dict) -> None:\n#         r\"\"\"\n#         Overview:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# ding/worker/coordinator/solo_parallel_commander.py\n# --------------------------------------------------\n#         self._current_buffer_id = None\n#         self._current_policy_id = None\n#         self._learner_info = [{'learner_step': 0}]\n#         self._evaluator_info = []\n#         self._last_eval_time = 0\n#         return buffer_id\n# \n#     def notify_fail_collector_task(self, task: dict) -> None:\n#         r\"\"\"\n#         Overview:\n#             Release task space when collector task fails.\n#         \"\"\"\n#         self._collector_task_space.release_space()\n# \n#     def notify_fail_learner_task(self, task: dict) -> None:\n#         r\"\"\"\n#         Overview:\n#             Release task space when learner task fails.\n#         \"\"\"\n#         self._learner_task_space.release_space()\n# --------------------------------------------------\n# the below code fragment can be found in:\n# ding/worker/coordinator/solo_parallel_commander.py\n# --------------------------------------------------\n# \n#     def notify_fail_collector_task(self, task: dict) -> None:\n#         r\"\"\"\n#         Overview:\n#             Release task space when collector task fails.\n#         \"\"\"\n#         self._collector_task_space.release_space()\n# \n#     def notify_fail_learner_task(self, task: dict) -> None:\n#         r\"\"\"\n#         Overview:\n#             Release task space when learner task fails.\n#         \"\"\"\n#         self._learner_task_space.release_space()\n# \n#     def update_learner_info(self, task_id: str, info: dict) -> None:\n#         r\"\"\"\n#         Overview:\n#             Append the info to learner_info:\n#         Arguments:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# ding/worker/coordinator/solo_parallel_commander.py\n# --------------------------------------------------\n#         r\"\"\"\n#         Overview:\n#             Release task space when collector task fails.\n#         \"\"\"\n#         self._collector_task_space.release_space()\n# \n#     def notify_fail_learner_task(self, task: dict) -> None:\n#         r\"\"\"\n#         Overview:\n#             Release task space when learner task fails.\n#         \"\"\"\n#         self._learner_task_space.release_space()\n# \n#     def update_learner_info(self, task_id: str, info: dict) -> None:\n#         r\"\"\"\n#         Overview:\n#             Append the info to learner_info:\n#         Arguments:\n#             - task_id (:obj:`str`): Learner task_id\n#             - info (:obj:`dict`): Dict type learner info.\n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\nfrom abc import ABC, abstractmethod\nfrom collections import defaultdict\nfrom easydict import EasyDict\nimport copy\n\nfrom ding.utils import import_module, COMMANDER_REGISTRY, LimitedSpaceContainer\n\n\nclass BaseCommander(ABC):\n    r\"\"\"\n    Overview:\n        Base parallel commander abstract class.\n    Interface:\n        get_collector_task\n    \"\"\"\n\n    @classmethod\n    def default_config(cls: type) -> EasyDict:\n        cfg = EasyDict(copy.deepcopy(cls.config))\n        cfg.cfg_type = cls.__name__ + 'Dict'\n        return cfg\n\n    @abstractmethod\n    def get_collector_task(self) -> dict:\n        raise NotImplementedError\n\n    def judge_collector_finish(self, task_id: str, info: dict) -> bool:\n        collector_done = info.get('collector_done', False)\n        if collector_done:\n            return True\n        return False\n\n    def judge_learner_finish(self, task_id: str, info: dict) -> bool:\n        learner_done = info.get('learner_done', False)\n        if learner_done:\n            return True\n        return False\n\n\n@COMMANDER_REGISTRY.register('naive')\nclass NaiveCommander(BaseCommander):\n    r\"\"\"\n    Overview:\n        A naive implementation of parallel commander.\n    Interface:\n        __init__, get_collector_task, get_learner_task, finsh_collector_task, finish_learner_task,\n        notify_fail_collector_task, notify_fail_learner_task, update_learner_info\n    \"\"\"\n    config = dict(\n        collector_task_space=1,\n        learner_task_space=1,\n        eval_interval=60,\n    )\n\n    def __init__(self, cfg: dict) -> None:\n        r\"\"\"\n        Overview:\n            Init the naive commander according to config\n        Arguments:\n            - cfg (:obj:`dict`): The config to init commander. Should include \\\n                \"collector_task_space\" and \"learner_task_space\".\n        \"\"\"\n        self._cfg = cfg\n        self._exp_name = cfg.exp_name\n        commander_cfg = self._cfg.policy.other.commander\n        self._collector_task_space = LimitedSpaceContainer(0, commander_cfg.collector_task_space)\n        self._learner_task_space = LimitedSpaceContainer(0, commander_cfg.learner_task_space)\n\n        self._collector_env_cfg = copy.deepcopy(self._cfg.env)\n        self._collector_env_cfg.pop('collector_episode_num')\n        self._collector_env_cfg.pop('evaluator_episode_num')\n        self._collector_env_cfg.manager.episode_num = self._cfg.env.collector_episode_num\n\n        self._collector_task_count = 0\n        self._learner_task_count = 0\n        self._learner_info = defaultdict(list)\n        self._learner_task_finish_count = 0\n        self._collector_task_finish_count = 0\n\n    def get_collector_task(self) -> dict:\n        r\"\"\"\n        Overview:\n            Get a new collector task when ``collector_task_count`` is smaller than ``collector_task_space``.\n        Return:\n            - task (:obj:`dict`): New collector task.\n        \"\"\"\n        if self._collector_task_space.acquire_space():\n            self._collector_task_count += 1\n            collector_cfg = copy.deepcopy(self._cfg.policy.collect.collector)\n            collector_cfg.collect_setting = {'eps': 0.9}\n            collector_cfg.eval_flag = False\n            collector_cfg.policy = copy.deepcopy(self._cfg.policy)\n            collector_cfg.policy_update_path = 'test.pth'\n            collector_cfg.env = self._collector_env_cfg\n            collector_cfg.exp_name = self._exp_name\n            return {\n                'task_id': 'collector_task_id{}'.format(self._collector_task_count),\n                'buffer_id': 'test',\n                'collector_cfg': collector_cfg,\n            }\n        else:\n            return None\n\n    def get_learner_task(self) -> dict:\n        r\"\"\"\n        Overview:\n            Get the new learner task when task_count is less than task_space\n        Return:\n            - task (:obj:`dict`): the new learner task\n        \"\"\"\n        if self._learner_task_space.acquire_space():\n            self._learner_task_count += 1\n            learner_cfg = copy.deepcopy(self._cfg.policy.learn.learner)\n            learner_cfg.exp_name = self._exp_name\n            return {\n                'task_id': 'learner_task_id{}'.format(self._learner_task_count),\n                'policy_id': 'test.pth',\n                'buffer_id': 'test',\n                'learner_cfg': learner_cfg,\n                'replay_buffer_cfg': copy.deepcopy(self._cfg.policy.other.replay_buffer),\n                'policy': copy.deepcopy(self._cfg.policy),\n            }\n        else:\n            return None\n\n    def finish_collector_task(self, task_id: str, finished_task: dict) -> None:\n        r\"\"\"\n        Overview:\n            finish collector task will add the collector_task_finish_count\n        \"\"\"\n        self._collector_task_space.release_space()\n        self._collector_task_finish_count += 1\n\n    def finish_learner_task(self, task_id: str, finished_task: dict) -> str:\n        r\"\"\"\n        Overview:\n            finish learner task will add the learner_task_finish_count and get the buffer_id of task to close the buffer\n        Return:\n            the finished_task buffer_id\n        \"\"\"\n        self._learner_task_finish_count += 1\n        self._learner_task_space.release_space()\n        return finished_task['buffer_id']\n\n    def notify_fail_collector_task(self, task: dict) -> None:\n        r\"\"\"\n        Overview:\n            naive coordinator will pass when need to notify_fail_collector_task\n        \"\"\"\n        self._collector_task_space.release_space()\n\n    def notify_fail_learner_task(self, task: dict) -> None:\n        r\"\"\"\n        Overview:", "metadata": {"task_id": "opendilab_ACE/42", "ground_truth": "            naive coordinator will pass when need to notify_fail_learner_task", "fpath_tuple": ["opendilab_ACE", "ding", "worker", "coordinator", "base_parallel_commander.py"], "context_start_lineno": 0, "line_no": 154, "query_window": {"context": "        r\"\"\"\n        Overview:\n            finish learner task will add the learner_task_finish_count and get the buffer_id of task to close the buffer\n        Return:\n            the finished_task buffer_id\n        \"\"\"\n        self._learner_task_finish_count += 1\n        self._learner_task_space.release_space()\n        return finished_task['buffer_id']\n\n    def notify_fail_collector_task(self, task: dict) -> None:\n        r\"\"\"\n        Overview:\n            naive coordinator will pass when need to notify_fail_collector_task\n        \"\"\"\n        self._collector_task_space.release_space()\n\n    def notify_fail_learner_task(self, task: dict) -> None:\n        r\"\"\"\n        Overview:", "metadata": {"fpath_tuple": ["opendilab_ACE", "ding", "worker", "coordinator", "base_parallel_commander.py"], "line_no": 154, "task_id": "opendilab_ACE/42", "start_line_no": 134, "end_line_no": 154, "window_size": 20, "context_start_lineno": 0, "repo": "opendilab_ACE"}}, "top_k_context": [{"context": "\n    def notify_fail_collector_task(self, task: dict) -> None:\n        r\"\"\"\n        Overview:\n            Release task space when collector task fails.\n        \"\"\"\n        self._collector_task_space.release_space()\n\n    def notify_fail_learner_task(self, task: dict) -> None:\n        r\"\"\"\n        Overview:\n            Release task space when learner task fails.\n        \"\"\"\n        self._learner_task_space.release_space()\n\n    def update_learner_info(self, task_id: str, info: dict) -> None:\n        r\"\"\"\n        Overview:\n            Append the info to learner_info:\n        Arguments:", "metadata": [{"fpath_tuple": ["opendilab_ACE", "ding", "worker", "coordinator", "solo_parallel_commander.py"], "line_no": 214, "start_line_no": 204, "end_line_no": 224, "window_size": 20, "repo": "opendilab_ACE", "slice_size": 10}], "sim_score": 0.5205479452054794}, {"context": "        self._last_eval_time = 0\n        return buffer_id\n\n    def notify_fail_collector_task(self, task: dict) -> None:\n        r\"\"\"\n        Overview:\n            Release task space when collector task fails.\n        \"\"\"\n        self._collector_task_space.release_space()\n\n    def notify_fail_learner_task(self, task: dict) -> None:\n        r\"\"\"\n        Overview:\n            Release task space when learner task fails.\n        \"\"\"\n        self._learner_task_space.release_space()\n\n    def update_learner_info(self, task_id: str, info: dict) -> None:\n        r\"\"\"\n        Overview:", "metadata": [{"fpath_tuple": ["opendilab_ACE", "ding", "worker", "coordinator", "solo_parallel_commander.py"], "line_no": 212, "start_line_no": 202, "end_line_no": 222, "window_size": 20, "repo": "opendilab_ACE", "slice_size": 10}], "sim_score": 0.5135135135135135}, {"context": "        self._learner_task_space.release_space()\n        buffer_id = finished_task['buffer_id']\n        self._current_buffer_id = None\n        self._current_policy_id = None\n        self._learner_info = [{'learner_step': 0}]\n        self._evaluator_info = []\n        self._last_eval_time = 0\n        return buffer_id\n\n    def notify_fail_collector_task(self, task: dict) -> None:\n        r\"\"\"\n        Overview:\n            Release task space when collector task fails.\n        \"\"\"\n        self._collector_task_space.release_space()\n\n    def notify_fail_learner_task(self, task: dict) -> None:\n        r\"\"\"\n        Overview:\n            Release task space when learner task fails.", "metadata": [{"fpath_tuple": ["opendilab_ACE", "ding", "worker", "coordinator", "solo_parallel_commander.py"], "line_no": 206, "start_line_no": 196, "end_line_no": 216, "window_size": 20, "repo": "opendilab_ACE", "slice_size": 10}], "sim_score": 0.5060240963855421}, {"context": "        self._learner_info = [{'learner_step': 0}]\n        self._evaluator_info = []\n        self._last_eval_time = 0\n        return buffer_id\n\n    def notify_fail_collector_task(self, task: dict) -> None:\n        r\"\"\"\n        Overview:\n            Release task space when collector task fails.\n        \"\"\"\n        self._collector_task_space.release_space()\n\n    def notify_fail_learner_task(self, task: dict) -> None:\n        r\"\"\"\n        Overview:\n            Release task space when learner task fails.\n        \"\"\"\n        self._learner_task_space.release_space()\n\n    def update_learner_info(self, task_id: str, info: dict) -> None:", "metadata": [{"fpath_tuple": ["opendilab_ACE", "ding", "worker", "coordinator", "solo_parallel_commander.py"], "line_no": 210, "start_line_no": 200, "end_line_no": 220, "window_size": 20, "repo": "opendilab_ACE", "slice_size": 10}], "sim_score": 0.4523809523809524}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# vizier/pyvizier/converters/core.py\n# --------------------------------------------------\n#     self.shift_safe_metrics = shift_safe_metrics\n# \n#   @property\n#   def _should_flip_sign(self) -> bool:\n#     return (\n#         self._original_metric_information.goal\n#         == pyvizier.ObjectiveMetricGoal.MINIMIZE\n#         and self.flip_sign_for_minimization_metrics\n#     )\n# \n#   def convert(\n#       self, measurements: Sequence[Optional[pyvizier.Measurement]]\n#   ) -> np.ndarray:\n#     \"\"\"Returns a (len(measurements), 1) array.\"\"\"\n#     if not measurements:\n#       return np.zeros([0, 1], dtype=self.dtype)\n# \n#     all_metrics = [m.metrics if m is not None else dict() for m in measurements]\n#     if not self.raise_errors_for_missing_metrics:\n#       metricvalues = [\n# --------------------------------------------------\n# the below code fragment can be found in:\n# vizier/pyvizier/converters/core_test.py\n# --------------------------------------------------\n#     root = study_config.search_space.root\n#     root.add_float_param('x1', 0.0, 10.0)\n#     root.add_float_param('x2', 0.0, 10.0)\n# \n#     study_config.metric_information.extend([\n#         pyvizier.MetricInformation(\n#             name='y1', goal=pyvizier.ObjectiveMetricGoal.MAXIMIZE\n#         ),\n#         pyvizier.MetricInformation(\n#             name='y2', goal=pyvizier.ObjectiveMetricGoal.MINIMIZE\n#         ),\n#         pyvizier.MetricInformation(\n#             name='y3', goal=pyvizier.ObjectiveMetricGoal.MINIMIZE\n#         ),\n#     ])\n#     converter = core.DefaultTrialConverter.from_study_config(study_config)\n#     actual_features = {\n#         'x1': np.array([[1.0, 3.0]]).T,\n#         'x2': np.array([[2.0, 4.0]]).T,\n#     }\n# --------------------------------------------------\n# the below code fragment can be found in:\n# vizier/_src/benchmarks/analyzers/convergence_curve_test.py\n# --------------------------------------------------\n# \n# class ConvergenceCurveConverterTest(parameterized.TestCase):\n# \n#   @parameterized.named_parameters(\n#       ('maximize', pyvizier.ObjectiveMetricGoal.MAXIMIZE, [[2, 2, 3]]),\n#       ('minimize', pyvizier.ObjectiveMetricGoal.MINIMIZE, [[2, 1, 1]]))\n#   def test_convert_basic(self, goal, expected):\n#     trials = _gen_trials([2, 1, 3])\n#     generator = convergence.ConvergenceCurveConverter(\n#         pyvizier.MetricInformation(name='', goal=goal))\n#     curve = generator.convert(trials)\n#     np.testing.assert_array_equal(curve.xs, [1, 2, 3])\n#     np.testing.assert_array_equal(curve.ys, expected)\n# \n# \n# class ConvergenceComparatorTest(absltest.TestCase):\n# \n#   def setUp(self):\n#     super(ConvergenceComparatorTest, self).setUp()\n#     xs = np.array(range(0, 20))\n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\n 0, 20)\n    converter = core.DefaultTrialConverter.from_study_config(study_config)\n    trials = converter.to_trials({'x': features}, {'y': labels})\n    for t in trials:\n      if not t.final_measurement.metrics:\n        self.assertTrue(t.infeasible)\n    for label, t in zip(labels.flatten(), trials):\n      if np.isnan(label):\n        self.assertEmpty(t.final_measurement.metrics)\n\n  def test_metrics(self):\n    converter = core.DefaultTrialConverter.from_study_configs(\n        [],\n        [\n            pyvizier.MetricInformation(\n                name='metric1', goal=pyvizier.ObjectiveMetricGoal.MINIMIZE\n            )\n        ],\n        use_study_id_feature=False,\n    )\n\n    expected = {'metric1': (None, 1)}\n    self.assertDictEqual(converter.labels_shape, expected)\n\n\nclass DefaultModelOutputConverterTest(parameterized.TestCase):\n\n  @property\n  def _measurements(self):\n    return [\n        pyvizier.Measurement(\n            metrics={\n                'metric1': pyvizier.Metric(1.0),\n                'metric2': pyvizier.Metric(1.1),\n            }\n        ),\n        pyvizier.Measurement(\n            metrics={\n                'metric1': pyvizier.Metric(2.0),\n                'metric2': pyvizier.Metric(2.1),\n            }\n        ),\n        pyvizier.Measurement(\n            metrics={\n                'metric1': pyvizier.Metric(4.0),\n                'metric2': pyvizier.Metric(4.1),\n            }\n        ),\n    ]\n\n  def test_empty(self):\n    converter = core.DefaultModelOutputConverter(\n        pyvizier.MetricInformation(\n            name='metric1', goal=pyvizier.ObjectiveMetricGoal.MINIMIZE\n        ),\n        flip_sign_for_minimization_metrics=True,\n    )\n    np.testing.assert_array_equal(\n        converter.convert([]), np.zeros([0, 1], dtype=converter.dtype)\n    )\n\n  @parameterized.parameters([\n      dict(dtype=np.float32),\n      dict(dtype=np.float64),\n      dict(dtype='float32'),\n      dict(dtype='float64'),\n  ])\n  def test_sign_flips(self, dtype):\n    converter = core.DefaultModelOutputConverter(\n        pyvizier.MetricInformation(\n            name='metric1', goal=pyvizier.ObjectiveMetricGoal.MINIMIZE\n        ),\n        flip_sign_for_minimization_metrics=True,\n        dtype=dtype,\n    )\n    actual = converter.convert(self._measurements)\n\n    expected = -np.asarray([[1.0], [2.0], [4.0]], dtype)\n    np.testing.assert_allclose(expected, actual)\n    self.assertEqual(expected.dtype, actual.dtype)\n    self.assertEqual(\n        converter.metric_information,\n        pyvizier.MetricInformation(\n            name='metric1', goal=pyvizier.ObjectiveMetricGoal.MAXIMIZE\n        ),\n    )\n\n  @parameterized.parameters([\n      dict(dtype=np.float32),\n      dict(dtype=np.float64),\n      dict(dtype='float32'),\n      dict(dtype='float64'),\n  ])\n  def test_no_sign_flips(self, dtype):\n    converter = core.DefaultModelOutputConverter(\n        pyvizier.MetricInformation(\n            name='metric2', goal=pyvizier.ObjectiveMetricGoal.MINIMIZE\n        ),\n        flip_sign_for_minimization_metrics=False,\n        dtype=dtype,\n    )\n    actual = converter.convert(self._measurements)\n\n    expected = np.asarray([[1.1], [2.1], [4.1]], dtype)\n    np.testing.assert_allclose(expected, actual)\n    self.assertEqual(expected.dtype, actual.dtype)\n    self.assertEqual(\n        converter.metric_information,\n        pyvizier.MetricInformation(\n            name='metric2', goal=pyvizier.ObjectiveMetricGoal.MINIMIZE\n        ),\n    )\n\n  def test_shift_threshould(self):\n    converter = core.DefaultModelOutputConverter(\n        pyvizier.MetricInformation(\n            name='metric2',\n            goal=pyvizier.ObjectiveMetricGoal.MINIMIZE,\n            safety_threshold=5.0,\n        ),\n        flip_sign_for_minimization_metrics=False,\n        dtype=float,\n    )\n    converter.shift_safe_metrics = False\n    self.assertEqual(5.0, converter.metric_information.safety_threshold)\n    converter.shift_safe_metrics = True\n    self.assertEqual(0.0, converter.metric_information.safety_threshold)\n\n  def test_raise_errors_for_missing_metrics(self):\n    converter = core.DefaultModelOutputConverter(\n        pyvizier.MetricInformation(\n            name='metric3', goal=pyvizier.ObjectiveMetricGoal.MINIMIZE\n        ),\n        flip_sign_for_minimization_metrics=False,\n        raise_errors_for_missing_metrics=True,\n    )\n    with self.assertRaises(KeyError):\n      converter.convert(self._measurements)\n\n  def test_do_not_raise_errors_for_missing_metrics(self):\n    converter = core.DefaultModelOutputConverter(\n        pyvizier.MetricInformation(\n            name='metric3', goal=pyvizier.ObjectiveMetricGoal.MINIMIZE\n        ),\n        flip_sign_for_minimization_metrics=False,\n        raise_errors_for_missing_metrics=False,\n    )\n    np.testing.assert_equal(\n        converter.convert(self._measurements), np.asarray([[np.nan]] * 3)\n    )\n\n  @parameterized.parameters([dict(flip_sign=True), dict(flip_sign=False)])\n  def test_safe_maximize_metric(self, flip_sign):\n    converter = core.DefaultModelOutputConverter(\n        pyvizier.MetricInformation(\n            name='metric1',\n            goal=pyvizier.ObjectiveMetricGoal.MAXIMIZE,\n            safety_threshold=3.0,\n        ),\n        flip_sign_for_minimization_metrics=flip_sign,\n    )\n    actual = converter.convert(self._measurements)\n    np.testing.assert_equal(actual, [[-2.0], [-1.0], [1.0]])\n\n  @parameterized.parameters([dict(flip_sign=True), dict(flip_sign=False)])\n  def test_safe_minimize_metric(self, flip_sign):\n    converter = core.DefaultModelOutputConverter(\n        pyvizier.MetricInformation(\n            name='metric1',\n            goal=pyvizier.ObjectiveMetricGoal.MINIMIZE,\n            safety_threshold=3.0,\n        ),\n        flip_sign_for_minimization_metrics=flip_sign,\n    )\n    actual = converter.convert(self._measurements)\n    expected = np.array([[-2.0], [-1.0], [1.0]], dtype=np.float32) * (\n        -1 if flip_sign else 1\n    )", "metadata": {"task_id": "google_vizier/74", "ground_truth": "    np.testing.assert_equal(actual, expected)", "fpath_tuple": ["google_vizier", "vizier", "pyvizier", "converters", "core_test.py"], "context_start_lineno": 354, "line_no": 532, "query_window": {"context": "        ),\n        flip_sign_for_minimization_metrics=flip_sign,\n    )\n    actual = converter.convert(self._measurements)\n    np.testing.assert_equal(actual, [[-2.0], [-1.0], [1.0]])\n\n  @parameterized.parameters([dict(flip_sign=True), dict(flip_sign=False)])\n  def test_safe_minimize_metric(self, flip_sign):\n    converter = core.DefaultModelOutputConverter(\n        pyvizier.MetricInformation(\n            name='metric1',\n            goal=pyvizier.ObjectiveMetricGoal.MINIMIZE,\n            safety_threshold=3.0,\n        ),\n        flip_sign_for_minimization_metrics=flip_sign,\n    )\n    actual = converter.convert(self._measurements)\n    expected = np.array([[-2.0], [-1.0], [1.0]], dtype=np.float32) * (\n        -1 if flip_sign else 1\n    )", "metadata": {"fpath_tuple": ["google_vizier", "vizier", "pyvizier", "converters", "core_test.py"], "line_no": 532, "task_id": "google_vizier/74", "start_line_no": 512, "end_line_no": 532, "window_size": 20, "context_start_lineno": 354, "repo": "google_vizier"}}, "top_k_context": [{"context": "      convergence.ConvergenceCurve.align_xs([c1, c2])\n\n\nclass ConvergenceCurveConverterTest(parameterized.TestCase):\n\n  @parameterized.named_parameters(\n      ('maximize', pyvizier.ObjectiveMetricGoal.MAXIMIZE, [[2, 2, 3]]),\n      ('minimize', pyvizier.ObjectiveMetricGoal.MINIMIZE, [[2, 1, 1]]))\n  def test_convert_basic(self, goal, expected):\n    trials = _gen_trials([2, 1, 3])\n    generator = convergence.ConvergenceCurveConverter(\n        pyvizier.MetricInformation(name='', goal=goal))\n    curve = generator.convert(trials)\n    np.testing.assert_array_equal(curve.xs, [1, 2, 3])\n    np.testing.assert_array_equal(curve.ys, expected)\n\n\nclass ConvergenceComparatorTest(absltest.TestCase):\n\n  def setUp(self):", "metadata": [{"fpath_tuple": ["google_vizier", "vizier", "_src", "benchmarks", "analyzers", "convergence_curve_test.py"], "line_no": 114, "start_line_no": 104, "end_line_no": 124, "window_size": 20, "repo": "google_vizier", "slice_size": 10}], "sim_score": 0.39864864864864863}, {"context": "  def test_parameters_and_labels(self):\n    study_config = pyvizier.ProblemStatement()\n    root = study_config.search_space.root\n    root.add_float_param('x1', 0.0, 10.0)\n    root.add_float_param('x2', 0.0, 10.0)\n\n    study_config.metric_information.extend([\n        pyvizier.MetricInformation(\n            name='y1', goal=pyvizier.ObjectiveMetricGoal.MAXIMIZE\n        ),\n        pyvizier.MetricInformation(\n            name='y2', goal=pyvizier.ObjectiveMetricGoal.MINIMIZE\n        ),\n        pyvizier.MetricInformation(\n            name='y3', goal=pyvizier.ObjectiveMetricGoal.MINIMIZE\n        ),\n    ])\n    converter = core.DefaultTrialConverter.from_study_config(study_config)\n    actual_features = {\n        'x1': np.array([[1.0, 3.0]]).T,", "metadata": [{"fpath_tuple": ["google_vizier", "vizier", "pyvizier", "converters", "core_test.py"], "line_no": 278, "start_line_no": 268, "end_line_no": 288, "window_size": 20, "repo": "google_vizier", "slice_size": 10}], "sim_score": 0.39436619718309857}, {"context": "    self.raise_errors_for_missing_metrics = raise_errors_for_missing_metrics\n    self.dtype = dtype\n    self.shift_safe_metrics = shift_safe_metrics\n\n  @property\n  def _should_flip_sign(self) -> bool:\n    return (\n        self._original_metric_information.goal\n        == pyvizier.ObjectiveMetricGoal.MINIMIZE\n        and self.flip_sign_for_minimization_metrics\n    )\n\n  def convert(\n      self, measurements: Sequence[Optional[pyvizier.Measurement]]\n  ) -> np.ndarray:\n    \"\"\"Returns a (len(measurements), 1) array.\"\"\"\n    if not measurements:\n      return np.zeros([0, 1], dtype=self.dtype)\n\n    all_metrics = [m.metrics if m is not None else dict() for m in measurements]", "metadata": [{"fpath_tuple": ["google_vizier", "vizier", "pyvizier", "converters", "core.py"], "line_no": 780, "start_line_no": 770, "end_line_no": 790, "window_size": 20, "repo": "google_vizier", "slice_size": 10}], "sim_score": 0.3870967741935484}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# fortuna/prob_model/predictive/classification.py\n# --------------------------------------------------\n#         .. math::\n#             \\mathbb{E}_{W|D}[\\text{Var}_{\\tilde{Y}|W, x}[\\tilde{Y}]],\n# \n#         where:\n#          - :math:`x` is an observed input variable;\n#          - :math:`\\tilde{Y}` is a one-hot encoded random target variable;\n#          - :math:`\\mathcal{D}` is the observed training data set;\n#          - :math:`W` denotes the random model parameters.\n# \n#         Parameters\n#         ----------\n#         inputs_loader : InputsLoader\n#             A loader of input data points.\n#         n_posterior_samples : int\n#             Number of samples to draw from the posterior distribution for each input.\n#         rng : Optional[PRNGKeyArray]\n#             A random number generator. If not passed, this will be taken from the attributes of this class.\n#         distribute: bool\n#             Whether to distribute computation over multiple devices, if available.\n# \n# --------------------------------------------------\n# the below code fragment can be found in:\n# fortuna/prob_model/predictive/classification.py\n# --------------------------------------------------\n# \n#         .. math::\n#             -\\mathbb{E}_{W|\\mathcal{D}}[\\mathbb{E}_{Y|W, x}[\\log p(Y|W, x)]],\n# \n#         where:\n#          - :math:`x` is an observed input variable;\n#          - :math:`Y` is a random target variable;\n#          - :math:`\\mathcal{D}` is the observed training data set;\n#          - :math:`W` denotes the random model parameters.\n# \n#         Parameters\n#         ----------\n#         inputs_loader : InputsLoader\n#             A loader of input data points.\n#         n_posterior_samples : int\n#             Number of samples to draw from the posterior distribution for each input.\n#         rng : Optional[PRNGKeyArray]\n#             A random number generator. If not passed, this will be taken from the attributes of this class.\n#         distribute: bool\n#             Whether to distribute computation over multiple devices, if available.\n# --------------------------------------------------\n# the below code fragment can be found in:\n# fortuna/prob_model/predictive/classification.py\n# --------------------------------------------------\n#         .. math::\n#             \\text{Var}_{W|\\mathcal{D}}[\\mathbb{E}_{\\tilde{Y}|W, x}[\\tilde{Y}]],\n# \n#         where:\n#          - :math:`x` is an observed input variable;\n#          - :math:`\\tilde{Y}` is a one-hot encoded random target variable;\n#          - :math:`\\mathcal{D}` is the observed training data set;\n#          - :math:`W` denotes the random model parameters.\n# \n#         Parameters\n#         ----------\n#         inputs_loader : InputsLoader\n#             A loader of input data points.\n#         n_posterior_samples : int\n#             Number of samples to draw from the posterior distribution for each input.\n#         rng : Optional[PRNGKeyArray]\n#             A random number generator. If not passed, this will be taken from the attributes of this class.\n#         distribute: bool\n#             Whether to distribute computation over multiple devices, if available.\n# \n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\n Optional[PRNGKeyArray] = None,\n        distribute: bool = True,\n    ) -> jnp.ndarray:\n        if rng is None:\n            rng = self.rng.get()\n\n        return self._loop_fun_through_inputs_loader(\n            self._sample_batched_outputs,\n            inputs_loader,\n            n_output_samples,\n            rng,\n            distribute,\n        )\n\n    def _sample_batched_outputs(\n        self,\n        inputs: Array,\n        n_output_samples: int = 1,\n        rng: Optional[PRNGKeyArray] = None,\n    ) -> jnp.ndarray:\n        if rng is None:\n            rng = self.rng.get()\n        keys = random.split(rng, n_output_samples)\n\n        def _sample(key):\n            sample = self.posterior.sample(inputs=inputs, rng=key)\n            return self.likelihood.model_manager.apply(\n                params=sample.params, inputs=inputs, mutable=sample.mutable\n            )\n\n        return lax.map(_sample, keys)\n\n    def _sample_outputs_loader(\n        self,\n        inputs_loader: InputsLoader,\n        n_output_samples: int = 1,\n        rng: Optional[PRNGKeyArray] = None,\n        return_size: bool = False,\n        distribute: bool = True,\n    ) -> Union[TargetsLoader, Tuple[TargetsLoader, int]]:\n        if rng is None:\n            rng = self.rng.get()\n        keys = random.split(rng, n_output_samples)\n\n        if distribute and jax.local_device_count() <= 1:\n            distribute = False\n\n        if distribute:\n            inputs_loader = DeviceDimensionAugmentedInputsLoader(inputs_loader)\n\n        @jit\n        def _sample(key, _inputs):\n            sample = self.posterior.sample(inputs=_inputs, rng=key)\n            return self.likelihood.model_manager.apply(\n                params=sample.params, inputs=_inputs, mutable=sample.mutable\n            )\n\n        iterable = []\n        size = 0\n        for inputs in inputs_loader:\n            size += inputs.shape[0]\n            if distribute:\n                outputs = pmap(\n                    lambda _inputs: lax.map(lambda key: _sample(key, _inputs), keys)\n                )(inputs)\n                outputs = self._unshard_ensemble_arrays(outputs)\n            else:\n                outputs = lax.map(lambda key: _sample(key, inputs), keys)\n            iterable.append(outputs)\n        iterable = TargetsLoader.from_iterable(iterable=iterable)\n        if return_size:\n            return iterable, size\n        return iterable\n\n    def mean(\n        self,\n        inputs_loader: InputsLoader,\n        n_posterior_samples: int = 30,\n        rng: Optional[PRNGKeyArray] = None,\n        distribute: bool = True,\n    ) -> jnp.ndarray:\n        r\"\"\"\n        Estimate the predictive mean of the target variable, that is\n\n        .. math::\n            \\mathbb{E}_{Y|x, \\mathcal{D}}[Y],\n\n        where:\n         - :math:`x` is an observed input variable;\n         - :math:`Y` is a random target variable;\n         - :math:`\\mathcal{D}` is the observed training data set;\n         - :math:`W` denotes the random model parameters.\n\n        Parameters\n        ----------\n        inputs_loader : InputsLoader\n            A loader of input data points.\n        n_posterior_samples : int\n            Number of samples to draw from the posterior distribution for each input.\n        rng: Optional[PRNGKeyArray]\n            A random number generator. If not passed, this will be taken from the attributes of this class.\n        distribute: bool\n            Whether to distribute computation over multiple devices, if available.\n\n        Returns\n        -------\n        jnp.ndarray\n            An estimate of the predictive mean for each input.\n        \"\"\"\n        if rng is None:\n            rng = self.rng.get()\n\n        return self._loop_fun_through_inputs_loader(\n            self._batched_mean, inputs_loader, n_posterior_samples, rng, distribute\n        )\n\n    def _batched_mean(\n        self,\n        inputs: Array,\n        n_posterior_samples: int = 30,\n        rng: Optional[PRNGKeyArray] = None,\n    ) -> jnp.ndarray:\n        if rng is None:\n            rng = self.rng.get()\n        keys = random.split(rng, n_posterior_samples)\n\n        def fun(i, _curr_sum):\n            _sample = self.posterior.sample(inputs=inputs, rng=keys[i])\n            _curr_sum += self.likelihood._batched_mean(\n                _sample.params,\n                inputs,\n                _sample.mutable,\n                calib_params=_sample.calib_params,\n                calib_mutable=_sample.calib_mutable,\n            )\n            return _curr_sum\n\n        curr_sum = fun(0, 0.0)\n        curr_sum = lax.fori_loop(1, n_posterior_samples, fun, curr_sum)\n        return curr_sum / n_posterior_samples\n\n    @abc.abstractmethod\n    def mode(\n        self,\n        inputs_loader: InputsLoader,\n        n_posterior_samples: int = 30,\n        means: Optional[jnp.ndarray] = None,\n        rng: Optional[PRNGKeyArray] = None,\n        distribute: bool = True,\n    ) -> jnp.ndarray:\n        r\"\"\"\n        Estimate the predictive mode of the target variable, that is\n\n        .. math::\n            \\text{argmax}_y\\ p(y|x, \\mathcal{D}),\n\n        where:\n         - :math:`x` is an observed input variable;\n         - :math:`\\mathcal{D}` is the observed training data set;\n         - :math:`y` is the target variable to optimize upon.\n\n        Parameters\n        ----------\n        inputs_loader : InputsLoader\n            A loader of input data points.\n        n_posterior_samples : int\n            Number of samples to draw from the posterior distribution for each input.\n        means : Optional[jnp.ndarray] = None\n            An estimate of the predictive mean.\n        rng : Optional[PRNGKeyArray]\n            A random number generator. If not passed, this will be taken from the attributes of this class.\n        distribute: bool\n            Whether to distribute computation over multiple devices, if available.\n\n        Returns\n        -------\n        jnp.ndarray\n            An estimate of the predictive mode for each input.\n        \"\"\"\n        pass\n\n    def aleatoric_variance(\n        self,\n        inputs_loader: InputsLoader,\n        n_posterior_samples: int = 30,\n        rng: Optional[PRNGKeyArray] = None,\n        distribute: bool = True,\n    ) -> jnp.ndarray:\n        r\"\"\"\n        Estimate the predictive aleatoric variance of the target variable, that is\n\n        .. math::\n            \\text{Var}_{W|\\mathcal{D}}[\\mathbb{E}_{Y|W, x}[Y]],\n\n        where:\n         - :math:`x` is an observed input variable;\n         - :math:`Y` is a random target variable;\n         - :math:`\\mathcal{D}` is the observed training data set;\n         - :math:`W` denotes the random model parameters.\n\n        Parameters\n        ----------\n        inputs_loader : InputsLoader\n            A loader of input data points.\n        n_posterior_samples : int\n            Number of samples to draw from the posterior distribution for each input.\n        rng : Optional[PRNGKeyArray]\n            A random number generator. If not passed, this will be taken from the attributes of this class.", "metadata": {"task_id": "awslabs_fortuna/64", "ground_truth": "        distribute: bool", "fpath_tuple": ["awslabs_fortuna", "fortuna", "prob_model", "predictive", "base.py"], "context_start_lineno": 377, "line_no": 585, "query_window": {"context": "        r\"\"\"\n        Estimate the predictive aleatoric variance of the target variable, that is\n\n        .. math::\n            \\text{Var}_{W|\\mathcal{D}}[\\mathbb{E}_{Y|W, x}[Y]],\n\n        where:\n         - :math:`x` is an observed input variable;\n         - :math:`Y` is a random target variable;\n         - :math:`\\mathcal{D}` is the observed training data set;\n         - :math:`W` denotes the random model parameters.\n\n        Parameters\n        ----------\n        inputs_loader : InputsLoader\n            A loader of input data points.\n        n_posterior_samples : int\n            Number of samples to draw from the posterior distribution for each input.\n        rng : Optional[PRNGKeyArray]\n            A random number generator. If not passed, this will be taken from the attributes of this class.", "metadata": {"fpath_tuple": ["awslabs_fortuna", "fortuna", "prob_model", "predictive", "base.py"], "line_no": 585, "task_id": "awslabs_fortuna/64", "start_line_no": 565, "end_line_no": 585, "window_size": 20, "context_start_lineno": 377, "repo": "awslabs_fortuna"}}, "top_k_context": [{"context": "        Estimate the predictive aleatoric variance of the one-hot encoded target variable, that is\n\n        .. math::\n            \\text{Var}_{W|\\mathcal{D}}[\\mathbb{E}_{\\tilde{Y}|W, x}[\\tilde{Y}]],\n\n        where:\n         - :math:`x` is an observed input variable;\n         - :math:`\\tilde{Y}` is a one-hot encoded random target variable;\n         - :math:`\\mathcal{D}` is the observed training data set;\n         - :math:`W` denotes the random model parameters.\n\n        Parameters\n        ----------\n        inputs_loader : InputsLoader\n            A loader of input data points.\n        n_posterior_samples : int\n            Number of samples to draw from the posterior distribution for each input.\n        rng : Optional[PRNGKeyArray]\n            A random number generator. If not passed, this will be taken from the attributes of this class.\n        distribute: bool", "metadata": [{"fpath_tuple": ["awslabs_fortuna", "fortuna", "prob_model", "predictive", "classification.py"], "line_no": 96, "start_line_no": 86, "end_line_no": 106, "window_size": 20, "repo": "awslabs_fortuna", "slice_size": 10}], "sim_score": 0.9217391304347826}, {"context": "        r\"\"\"\n        Estimate the predictive aleatoric entropy, that is\n\n        .. math::\n            -\\mathbb{E}_{W|\\mathcal{D}}[\\mathbb{E}_{Y|W, x}[\\log p(Y|W, x)]],\n\n        where:\n         - :math:`x` is an observed input variable;\n         - :math:`Y` is a random target variable;\n         - :math:`\\mathcal{D}` is the observed training data set;\n         - :math:`W` denotes the random model parameters.\n\n        Parameters\n        ----------\n        inputs_loader : InputsLoader\n            A loader of input data points.\n        n_posterior_samples : int\n            Number of samples to draw from the posterior distribution for each input.\n        rng : Optional[PRNGKeyArray]\n            A random number generator. If not passed, this will be taken from the attributes of this class.", "metadata": [{"fpath_tuple": ["awslabs_fortuna", "fortuna", "prob_model", "predictive", "classification.py"], "line_no": 264, "start_line_no": 254, "end_line_no": 274, "window_size": 20, "repo": "awslabs_fortuna", "slice_size": 10}], "sim_score": 0.9196428571428571}, {"context": "        Estimate the predictive epistemic variance of the one-hot encoded target variable, that is\n\n        .. math::\n            \\mathbb{E}_{W|D}[\\text{Var}_{\\tilde{Y}|W, x}[\\tilde{Y}]],\n\n        where:\n         - :math:`x` is an observed input variable;\n         - :math:`\\tilde{Y}` is a one-hot encoded random target variable;\n         - :math:`\\mathcal{D}` is the observed training data set;\n         - :math:`W` denotes the random model parameters.\n\n        Parameters\n        ----------\n        inputs_loader : InputsLoader\n            A loader of input data points.\n        n_posterior_samples : int\n            Number of samples to draw from the posterior distribution for each input.\n        rng : Optional[PRNGKeyArray]\n            A random number generator. If not passed, this will be taken from the attributes of this class.\n        distribute: bool", "metadata": [{"fpath_tuple": ["awslabs_fortuna", "fortuna", "prob_model", "predictive", "classification.py"], "line_no": 136, "start_line_no": 126, "end_line_no": 146, "window_size": 20, "repo": "awslabs_fortuna", "slice_size": 10}], "sim_score": 0.8717948717948718}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# vizier/service/vizier_client.py\n# --------------------------------------------------\n# \n# from google.longrunning import operations_pb2\n# from google.protobuf import duration_pb2\n# from google.protobuf import json_format\n# \n# flags.DEFINE_integer(\n#     'vizier_new_suggestion_polling_secs',\n#     1,\n#     (\n#         'The period to wait between polling for the status of long-running '\n#         'SuggestOperations. Vizier may increase this period if multiple polls '\n#         'are needed. (You may use zero for interactive demos, but it is only '\n#         'appropriate for very small Studies.)'\n#     ),\n# )\n# FLAGS = flags.FLAGS\n# \n# Metadata = Mapping[Tuple[str, str], Any]\n# \n# \n# --------------------------------------------------\n# the below code fragment can be found in:\n# demos/run_vizier_server.py\n# --------------------------------------------------\n# \n# import time\n# from typing import Sequence\n# \n# from absl import app\n# from absl import flags\n# from absl import logging\n# \n# from vizier.service import vizier_server\n# \n# flags.DEFINE_string(\n#     'host',\n#     'localhost',\n#     'Host location for the server. For distributed cases, use the IP address.',\n# )\n# \n# FLAGS = flags.FLAGS\n# \n# _ONE_DAY_IN_SECONDS = 60 * 60 * 24\n# \n# --------------------------------------------------\n# the below code fragment can be found in:\n# demos/run_vizier_server.py\n# --------------------------------------------------\n# After running the command, the address of the server, formatted as:\n# \"localhost:[PORT]\" will be logged to stdout.\n# This address should be used as a command line argument to run_vizier_client.py\n# \"\"\"\n# \n# import time\n# from typing import Sequence\n# \n# from absl import app\n# from absl import flags\n# from absl import logging\n# \n# from vizier.service import vizier_server\n# \n# flags.DEFINE_string(\n#     'host',\n#     'localhost',\n#     'Host location for the server. For distributed cases, use the IP address.',\n# )\n# \n# --------------------------------------------------\n# the below code fragment can be found in:\n# demos/run_vizier_server.py\n# --------------------------------------------------\n# This address should be used as a command line argument to run_vizier_client.py\n# \"\"\"\n# \n# import time\n# from typing import Sequence\n# \n# from absl import app\n# from absl import flags\n# from absl import logging\n# \n# from vizier.service import vizier_server\n# \n# flags.DEFINE_string(\n#     'host',\n#     'localhost',\n#     'Host location for the server. For distributed cases, use the IP address.',\n# )\n# \n# FLAGS = flags.FLAGS\n# \n# --------------------------------------------------\n# the below code fragment can be found in:\n# demos/run_vizier_server.py\n# --------------------------------------------------\n# ```\n# \n# After running the command, the address of the server, formatted as:\n# \"localhost:[PORT]\" will be logged to stdout.\n# This address should be used as a command line argument to run_vizier_client.py\n# \"\"\"\n# \n# import time\n# from typing import Sequence\n# \n# from absl import app\n# from absl import flags\n# from absl import logging\n# \n# from vizier.service import vizier_server\n# \n# flags.DEFINE_string(\n#     'host',\n#     'localhost',\n#     'Host location for the server. For distributed cases, use the IP address.',\n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\n# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import annotations\n\n\"\"\"Example of a Vizier Client, which can be run on multiple machines.\n\nFor distributed cases, this is meant to be used after the Vizier Server (see\nrun_vizier_server.py`) has been launched and provided an address to connect to.\nExample of a launch command:\n\n```\npython run_vizier_client.py --address=\"localhost:[PORT]\"\n```\n\nwhere `address` was provided by the server.\n\nIf not provided, the Vizier Server will be created locally, which still allows\nparallelization via multithreading, but will not be able to coordinate jobs\nacross different machines.\n\"\"\"\n\nfrom typing import Sequence\n\nfrom absl import app\nfrom absl import flags\nfrom absl import logging\n\nfrom vizier.service import clients\nfrom vizier.service import pyvizier as vz\n\nflags.DEFINE_string(\n    'address',\n    clients.NO_ENDPOINT,\n    (\n        'Address of the Vizier Server which will be used by this demo. Should'\n        \" be of the form e.g. 'localhost:6006' if running on the same machine,\"\n        ' or `[IP]:[PORT]` if running on a remote machine. If unset, a local'\n        ' Vizier server will be created inside this process.'\n    ),\n)\nflags.DEFINE_integer(\n    'max_num_iterations',\n    10,\n    'Maximum number of possible iterations / calls to get suggestions.',\n)\nflags.DEFINE_integer(\n    'suggestion_count',", "metadata": {"task_id": "google_vizier/134", "ground_truth": "    5,", "fpath_tuple": ["google_vizier", "demos", "run_vizier_client.py"], "context_start_lineno": 0, "line_no": 59, "query_window": {"context": "from vizier.service import clients\nfrom vizier.service import pyvizier as vz\n\nflags.DEFINE_string(\n    'address',\n    clients.NO_ENDPOINT,\n    (\n        'Address of the Vizier Server which will be used by this demo. Should'\n        \" be of the form e.g. 'localhost:6006' if running on the same machine,\"\n        ' or `[IP]:[PORT]` if running on a remote machine. If unset, a local'\n        ' Vizier server will be created inside this process.'\n    ),\n)\nflags.DEFINE_integer(\n    'max_num_iterations',\n    10,\n    'Maximum number of possible iterations / calls to get suggestions.',\n)\nflags.DEFINE_integer(\n    'suggestion_count',", "metadata": {"fpath_tuple": ["google_vizier", "demos", "run_vizier_client.py"], "line_no": 59, "task_id": "google_vizier/134", "start_line_no": 39, "end_line_no": 59, "window_size": 20, "context_start_lineno": 0, "repo": "google_vizier"}}, "top_k_context": [{"context": "```\npython run_vizier_server.py\n```\n\nAfter running the command, the address of the server, formatted as:\n\"localhost:[PORT]\" will be logged to stdout.\nThis address should be used as a command line argument to run_vizier_client.py\n\"\"\"\n\nimport time\nfrom typing import Sequence\n\nfrom absl import app\nfrom absl import flags\nfrom absl import logging\n\nfrom vizier.service import vizier_server\n\nflags.DEFINE_string(\n    'host',", "metadata": [{"fpath_tuple": ["google_vizier", "demos", "run_vizier_server.py"], "line_no": 30, "start_line_no": 20, "end_line_no": 40, "window_size": 20, "repo": "google_vizier", "slice_size": 10}], "sim_score": 0.25384615384615383}, {"context": "After running the command, the address of the server, formatted as:\n\"localhost:[PORT]\" will be logged to stdout.\nThis address should be used as a command line argument to run_vizier_client.py\n\"\"\"\n\nimport time\nfrom typing import Sequence\n\nfrom absl import app\nfrom absl import flags\nfrom absl import logging\n\nfrom vizier.service import vizier_server\n\nflags.DEFINE_string(\n    'host',\n    'localhost',\n    'Host location for the server. For distributed cases, use the IP address.',\n)\n", "metadata": [{"fpath_tuple": ["google_vizier", "demos", "run_vizier_server.py"], "line_no": 34, "start_line_no": 24, "end_line_no": 44, "window_size": 20, "repo": "google_vizier", "slice_size": 10}], "sim_score": 0.25}, {"context": "```\n\nAfter running the command, the address of the server, formatted as:\n\"localhost:[PORT]\" will be logged to stdout.\nThis address should be used as a command line argument to run_vizier_client.py\n\"\"\"\n\nimport time\nfrom typing import Sequence\n\nfrom absl import app\nfrom absl import flags\nfrom absl import logging\n\nfrom vizier.service import vizier_server\n\nflags.DEFINE_string(\n    'host',\n    'localhost',\n    'Host location for the server. For distributed cases, use the IP address.',", "metadata": [{"fpath_tuple": ["google_vizier", "demos", "run_vizier_server.py"], "line_no": 32, "start_line_no": 22, "end_line_no": 42, "window_size": 20, "repo": "google_vizier", "slice_size": 10}], "sim_score": 0.24817518248175183}, {"context": "This address should be used as a command line argument to run_vizier_client.py\n\"\"\"\n\nimport time\nfrom typing import Sequence\n\nfrom absl import app\nfrom absl import flags\nfrom absl import logging\n\nfrom vizier.service import vizier_server\n\nflags.DEFINE_string(\n    'host',\n    'localhost',\n    'Host location for the server. For distributed cases, use the IP address.',\n)\n\nFLAGS = flags.FLAGS\n", "metadata": [{"fpath_tuple": ["google_vizier", "demos", "run_vizier_server.py"], "line_no": 36, "start_line_no": 26, "end_line_no": 46, "window_size": 20, "repo": "google_vizier", "slice_size": 10}], "sim_score": 0.22137404580152673}, {"context": "from vizier.service import vizier_service_pb2_grpc\nfrom vizier.utils import attrs_utils\n\nfrom google.longrunning import operations_pb2\nfrom google.protobuf import duration_pb2\nfrom google.protobuf import json_format\n\nflags.DEFINE_integer(\n    'vizier_new_suggestion_polling_secs',\n    1,\n    (\n        'The period to wait between polling for the status of long-running '\n        'SuggestOperations. Vizier may increase this period if multiple polls '\n        'are needed. (You may use zero for interactive demos, but it is only '\n        'appropriate for very small Studies.)'\n    ),\n)\nFLAGS = flags.FLAGS\n\nMetadata = Mapping[Tuple[str, str], Any]", "metadata": [{"fpath_tuple": ["google_vizier", "vizier", "service", "vizier_client.py"], "line_no": 48, "start_line_no": 38, "end_line_no": 58, "window_size": 20, "repo": "google_vizier", "slice_size": 10}], "sim_score": 0.20958083832335328}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# test/test_tensordictmodules.py\n# --------------------------------------------------\n#             assert ((td.get(\"out\") > 0.1) | (td.get(\"out\") < -0.1)).any()\n#         elif safe and spec_type == \"bounded\":\n#             assert ((td.get(\"out\") < 0.1) | (td.get(\"out\") > -0.1)).all()\n# \n#     @pytest.mark.parametrize(\"safe\", [True, False])\n#     @pytest.mark.parametrize(\"spec_type\", [None, \"bounded\", \"unbounded\"])\n#     @pytest.mark.parametrize(\"out_keys\", [[\"loc\", \"scale\"], [\"loc_1\", \"scale_1\"]])\n#     @pytest.mark.parametrize(\"lazy\", [True, False])\n#     @pytest.mark.parametrize(\"exp_mode\", [\"mode\", \"random\", None])\n#     def test_stateful_probabilistic(self, safe, spec_type, lazy, exp_mode, out_keys):\n#         torch.manual_seed(0)\n#         param_multiplier = 2\n#         if lazy:\n#             net = nn.LazyLinear(4 * param_multiplier)\n#         else:\n#             net = nn.Linear(3, 4 * param_multiplier)\n# \n#         in_keys = [\"in\"]\n#         net = SafeModule(\n#             module=NormalParamWrapper(net),\n# --------------------------------------------------\n# the below code fragment can be found in:\n# test/test_tensordictmodules.py\n# --------------------------------------------------\n#             assert ((td.get(\"out\") < 0.1) | (td.get(\"out\") > -0.1)).all()\n# \n#     @pytest.mark.parametrize(\"safe\", [True, False])\n#     @pytest.mark.parametrize(\"spec_type\", [None, \"bounded\", \"unbounded\"])\n#     @pytest.mark.parametrize(\"out_keys\", [[\"loc\", \"scale\"], [\"loc_1\", \"scale_1\"]])\n#     @pytest.mark.parametrize(\"lazy\", [True, False])\n#     @pytest.mark.parametrize(\"exp_mode\", [\"mode\", \"random\", None])\n#     def test_stateful_probabilistic(self, safe, spec_type, lazy, exp_mode, out_keys):\n#         torch.manual_seed(0)\n#         param_multiplier = 2\n#         if lazy:\n#             net = nn.LazyLinear(4 * param_multiplier)\n#         else:\n#             net = nn.Linear(3, 4 * param_multiplier)\n# \n#         in_keys = [\"in\"]\n#         net = SafeModule(\n#             module=NormalParamWrapper(net),\n#             spec=None,\n#             in_keys=in_keys,\n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\n is None:\n            spec = None\n        elif spec_type == \"bounded\":\n            spec = BoundedTensorSpec(-0.1, 0.1, 4)\n        elif spec_type == \"unbounded\":\n            spec = UnboundedContinuousTensorSpec(4)\n        else:\n            raise NotImplementedError\n\n        kwargs = {\"distribution_class\": TanhNormal}\n\n        if safe and spec is None:\n            with pytest.raises(\n                RuntimeError,\n                match=\"is not a valid configuration as the tensor specs are not \"\n                \"specified\",\n            ):\n                prob_module = SafeProbabilisticModule(\n                    in_keys=[\"loc\", \"scale\"],\n                    out_keys=[\"out\"],\n                    spec=spec,\n                    safe=safe,\n                    **kwargs,\n                )\n            return\n        else:\n            prob_module = SafeProbabilisticModule(\n                in_keys=[\"loc\", \"scale\"],\n                out_keys=[\"out\"],\n                spec=spec,\n                safe=safe,\n                **kwargs,\n            )\n\n        tdmodule = SafeProbabilisticSequential(tdnet, prob_module)\n        params = make_functional(tdmodule)\n\n        # vmap = True\n        params = params.expand(10)\n        td = TensorDict({\"in\": torch.randn(3, 3)}, [3])\n        if safe and spec_type == \"bounded\":\n            with pytest.raises(\n                RuntimeError, match=\"vmap cannot be used with safe=True\"\n            ):\n                td_out = vmap(tdmodule, (None, 0))(td, params)\n            return\n        else:\n            td_out = vmap(tdmodule, (None, 0))(td, params)\n        assert td_out is not td\n        assert td_out.shape == torch.Size([10, 3])\n        assert td_out.get(\"out\").shape == torch.Size([10, 3, 4])\n        # test bounds\n        if not safe and spec_type == \"bounded\":\n            assert ((td_out.get(\"out\") > 0.1) | (td_out.get(\"out\") < -0.1)).any()\n        elif safe and spec_type == \"bounded\":\n            assert ((td_out.get(\"out\") < 0.1) | (td_out.get(\"out\") > -0.1)).all()\n\n        # vmap = (0, 0)\n        td = TensorDict({\"in\": torch.randn(3, 3)}, [3])\n        td_repeat = td.expand(10, *td.batch_size)\n        td_out = vmap(tdmodule, (0, 0))(td_repeat, params)\n        assert td_out is not td\n        assert td_out.shape == torch.Size([10, 3])\n        assert td_out.get(\"out\").shape == torch.Size([10, 3, 4])\n        # test bounds\n        if not safe and spec_type == \"bounded\":\n            assert ((td_out.get(\"out\") > 0.1) | (td_out.get(\"out\") < -0.1)).any()\n        elif safe and spec_type == \"bounded\":\n            assert ((td_out.get(\"out\") < 0.1) | (td_out.get(\"out\") > -0.1)).all()\n\n\nclass TestTDSequence:\n    def test_in_key_warning(self):\n        with pytest.warns(UserWarning, match='key \"_\" is for ignoring output'):\n            tensordict_module = SafeModule(\n                nn.Linear(3, 4), in_keys=[\"_\"], out_keys=[\"out1\"]\n            )\n        with pytest.warns(UserWarning, match='key \"_\" is for ignoring output'):\n            tensordict_module = SafeModule(\n                nn.Linear(3, 4), in_keys=[\"_\", \"key2\"], out_keys=[\"out1\"]\n            )\n\n    @pytest.mark.parametrize(\"safe\", [True, False])\n    @pytest.mark.parametrize(\"spec_type\", [None, \"bounded\", \"unbounded\"])\n    @pytest.mark.parametrize(\"lazy\", [True, False])\n    def test_stateful(self, safe, spec_type, lazy):\n        torch.manual_seed(0)\n        param_multiplier = 1\n        if lazy:\n            net1 = nn.LazyLinear(4)\n            dummy_net = nn.LazyLinear(4)\n            net2 = nn.LazyLinear(4 * param_multiplier)\n        else:\n            net1 = nn.Linear(3, 4)\n            dummy_net = nn.Linear(4, 4)\n            net2 = nn.Linear(4, 4 * param_multiplier)\n\n        if spec_type is None:\n            spec = None\n        elif spec_type == \"bounded\":\n            spec = BoundedTensorSpec(-0.1, 0.1, 4)\n        elif spec_type == \"unbounded\":\n            spec = UnboundedContinuousTensorSpec(4)\n\n        kwargs = {}\n\n        if safe and spec is None:\n            pytest.skip(\"safe and spec is None is checked elsewhere\")\n        else:\n            tdmodule1 = SafeModule(\n                net1,\n                spec=None,\n                in_keys=[\"in\"],\n                out_keys=[\"hidden\"],\n                safe=False,\n            )\n            dummy_tdmodule = SafeModule(\n                dummy_net,\n                spec=None,\n                in_keys=[\"hidden\"],\n                out_keys=[\"hidden\"],\n                safe=False,\n            )\n            tdmodule2 = SafeModule(\n                spec=spec,\n                module=net2,\n                in_keys=[\"hidden\"],\n                out_keys=[\"out\"],\n                safe=False,\n                **kwargs,\n            )\n            tdmodule = SafeSequential(tdmodule1, dummy_tdmodule, tdmodule2)\n\n        assert hasattr(tdmodule, \"__setitem__\")\n        assert len(tdmodule) == 3\n        tdmodule[1] = tdmodule2\n        assert len(tdmodule) == 3\n\n        assert hasattr(tdmodule, \"__delitem__\")\n        assert len(tdmodule) == 3\n        del tdmodule[2]\n        assert len(tdmodule) == 2\n\n        assert hasattr(tdmodule, \"__getitem__\")\n        assert tdmodule[0] is tdmodule1\n        assert tdmodule[1] is tdmodule2\n\n        td = TensorDict({\"in\": torch.randn(3, 3)}, [3])\n        tdmodule(td)\n        assert td.shape == torch.Size([3])\n        assert td.get(\"out\").shape == torch.Size([3, 4])\n\n        # test bounds\n        if not safe and spec_type == \"bounded\":\n            assert ((td.get(\"out\") > 0.1) | (td.get(\"out\") < -0.1)).any()\n        elif safe and spec_type == \"bounded\":\n            assert ((td.get(\"out\") < 0.1) | (td.get(\"out\") > -0.1)).all()\n\n    @pytest.mark.parametrize(\"safe\", [True, False])\n    @pytest.mark.parametrize(\"spec_type\", [None, \"bounded\", \"unbounded\"])\n    @pytest.mark.parametrize(\"lazy\", [True, False])\n    def test_stateful_probabilistic(self, safe, spec_type, lazy):\n        torch.manual_seed(0)\n        param_multiplier = 2\n        if lazy:\n            net1 = nn.LazyLinear(4)\n            dummy_net = nn.LazyLinear(4)\n            net2 = nn.LazyLinear(4 * param_multiplier)\n        else:\n            net1 = nn.Linear(3, 4)\n            dummy_net = nn.Linear(4, 4)\n            net2 = nn.Linear(4, 4 * param_multiplier)\n        net2 = NormalParamWrapper(net2)\n\n        if spec_type is None:", "metadata": {"task_id": "pytorch_rl/23", "ground_truth": "            spec = None", "fpath_tuple": ["pytorch_rl", "test", "test_tensordictmodules.py"], "context_start_lineno": 542, "line_no": 717, "query_window": {"context": "        elif safe and spec_type == \"bounded\":\n            assert ((td.get(\"out\") < 0.1) | (td.get(\"out\") > -0.1)).all()\n\n    @pytest.mark.parametrize(\"safe\", [True, False])\n    @pytest.mark.parametrize(\"spec_type\", [None, \"bounded\", \"unbounded\"])\n    @pytest.mark.parametrize(\"lazy\", [True, False])\n    def test_stateful_probabilistic(self, safe, spec_type, lazy):\n        torch.manual_seed(0)\n        param_multiplier = 2\n        if lazy:\n            net1 = nn.LazyLinear(4)\n            dummy_net = nn.LazyLinear(4)\n            net2 = nn.LazyLinear(4 * param_multiplier)\n        else:\n            net1 = nn.Linear(3, 4)\n            dummy_net = nn.Linear(4, 4)\n            net2 = nn.Linear(4, 4 * param_multiplier)\n        net2 = NormalParamWrapper(net2)\n\n        if spec_type is None:", "metadata": {"fpath_tuple": ["pytorch_rl", "test", "test_tensordictmodules.py"], "line_no": 717, "task_id": "pytorch_rl/23", "start_line_no": 697, "end_line_no": 717, "window_size": 20, "context_start_lineno": 542, "repo": "pytorch_rl"}}, "top_k_context": [{"context": "            assert ((td.get(\"out\") > 0.1) | (td.get(\"out\") < -0.1)).any()\n        elif safe and spec_type == \"bounded\":\n            assert ((td.get(\"out\") < 0.1) | (td.get(\"out\") > -0.1)).all()\n\n    @pytest.mark.parametrize(\"safe\", [True, False])\n    @pytest.mark.parametrize(\"spec_type\", [None, \"bounded\", \"unbounded\"])\n    @pytest.mark.parametrize(\"out_keys\", [[\"loc\", \"scale\"], [\"loc_1\", \"scale_1\"]])\n    @pytest.mark.parametrize(\"lazy\", [True, False])\n    @pytest.mark.parametrize(\"exp_mode\", [\"mode\", \"random\", None])\n    def test_stateful_probabilistic(self, safe, spec_type, lazy, exp_mode, out_keys):\n        torch.manual_seed(0)\n        param_multiplier = 2\n        if lazy:\n            net = nn.LazyLinear(4 * param_multiplier)\n        else:\n            net = nn.Linear(3, 4 * param_multiplier)\n\n        in_keys = [\"in\"]\n        net = SafeModule(\n            module=NormalParamWrapper(net),", "metadata": [{"fpath_tuple": ["pytorch_rl", "test", "test_tensordictmodules.py"], "line_no": 160, "start_line_no": 150, "end_line_no": 170, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}], "sim_score": 0.8}, {"context": "        # test bounds\n        if not safe and spec_type == \"bounded\":\n            assert ((td.get(\"out\") > 0.1) | (td.get(\"out\") < -0.1)).any()\n        elif safe and spec_type == \"bounded\":\n            assert ((td.get(\"out\") < 0.1) | (td.get(\"out\") > -0.1)).all()\n\n    @pytest.mark.parametrize(\"safe\", [True, False])\n    @pytest.mark.parametrize(\"spec_type\", [None, \"bounded\", \"unbounded\"])\n    @pytest.mark.parametrize(\"out_keys\", [[\"loc\", \"scale\"], [\"loc_1\", \"scale_1\"]])\n    @pytest.mark.parametrize(\"lazy\", [True, False])\n    @pytest.mark.parametrize(\"exp_mode\", [\"mode\", \"random\", None])\n    def test_stateful_probabilistic(self, safe, spec_type, lazy, exp_mode, out_keys):\n        torch.manual_seed(0)\n        param_multiplier = 2\n        if lazy:\n            net = nn.LazyLinear(4 * param_multiplier)\n        else:\n            net = nn.Linear(3, 4 * param_multiplier)\n\n        in_keys = [\"in\"]", "metadata": [{"fpath_tuple": ["pytorch_rl", "test", "test_tensordictmodules.py"], "line_no": 158, "start_line_no": 148, "end_line_no": 168, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}], "sim_score": 0.7863247863247863}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# fortuna/calib_model/regression.py\n# --------------------------------------------------\n# \n# class CalibRegressor(CalibModel):\n#     def __init__(\n#         self,\n#         output_calibrator: Optional[nn.Module] = RegressionTemperatureScaler(),\n#         seed: int = 0,\n#     ) -> None:\n#         r\"\"\"\n#         A calibration regressor class.\n# \n#         Parameters\n#         ----------\n#         output_calibrator : Optional[nn.Module]\n#             An output calibrator object. The default is temperature scaling for regression, which inflates the variance\n#             of the likelihood with a scalar temperature parameter. Given outputs :math:`o` of the model manager, the\n#             output calibrator is described by a function :math:`g(\\phi, o)`, where `phi` are\n#             calibration parameters.\n#         seed: int\n#             A random seed.\n# \n# --------------------------------------------------\n# the below code fragment can be found in:\n# fortuna/calib_model/regression.py\n# --------------------------------------------------\n# from fortuna.typing import Array, Status\n# \n# \n# class CalibRegressor(CalibModel):\n#     def __init__(\n#         self,\n#         output_calibrator: Optional[nn.Module] = RegressionTemperatureScaler(),\n#         seed: int = 0,\n#     ) -> None:\n#         r\"\"\"\n#         A calibration regressor class.\n# \n#         Parameters\n#         ----------\n#         output_calibrator : Optional[nn.Module]\n#             An output calibrator object. The default is temperature scaling for regression, which inflates the variance\n#             of the likelihood with a scalar temperature parameter. Given outputs :math:`o` of the model manager, the\n#             output calibrator is described by a function :math:`g(\\phi, o)`, where `phi` are\n#             calibration parameters.\n#         seed: int\n# --------------------------------------------------\n# the below code fragment can be found in:\n# fortuna/calib_model/regression.py\n# --------------------------------------------------\n# from fortuna.output_calibrator.regression import RegressionTemperatureScaler\n# from fortuna.prob_output_layer.regression import RegressionProbOutputLayer\n# from fortuna.typing import Array, Status\n# \n# \n# class CalibRegressor(CalibModel):\n#     def __init__(\n#         self,\n#         output_calibrator: Optional[nn.Module] = RegressionTemperatureScaler(),\n#         seed: int = 0,\n#     ) -> None:\n#         r\"\"\"\n#         A calibration regressor class.\n# \n#         Parameters\n#         ----------\n#         output_calibrator : Optional[nn.Module]\n#             An output calibrator object. The default is temperature scaling for regression, which inflates the variance\n#             of the likelihood with a scalar temperature parameter. Given outputs :math:`o` of the model manager, the\n#             output calibrator is described by a function :math:`g(\\phi, o)`, where `phi` are\n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\nfrom typing import Optional\n\nimport flax.linen as nn\nimport jax.numpy as jnp\nimport numpy as np\n\nfrom fortuna.calib_model.base import CalibModel\nfrom fortuna.calib_model.calib_config.base import CalibConfig\nfrom fortuna.calib_model.predictive.classification import \\\n    ClassificationPredictive\nfrom fortuna.output_calibrator.classification import \\\n    ClassificationTemperatureScaler\nfrom fortuna.output_calibrator.output_calib_manager.base import \\\n    OutputCalibManager\nfrom fortuna.prob_output_layer.classification import \\\n    ClassificationProbOutputLayer\nfrom fortuna.typing import Array, Status\n\n\nclass CalibClassifier(CalibModel):\n    def __init__(\n        self,\n        output_calibrator: Optional[nn.Module] = ClassificationTemperatureScaler(),\n        seed: int = 0,\n    ) -> None:\n        r\"\"\"\n        A calibration classifier class.\n\n        Parameters\n        ----------\n        output_calibrator : Optional[nn.Module]\n            An output calibrator object. The default is temperature scaling for classification, which rescales the\n            logits with a scalar temperature parameter. Given outputs :math:`o`,", "metadata": {"task_id": "awslabs_fortuna/181", "ground_truth": "            the output calibrator is described by a function :math:`g(\\phi, o)`,", "fpath_tuple": ["awslabs_fortuna", "fortuna", "calib_model", "classification.py"], "context_start_lineno": 0, "line_no": 33, "query_window": {"context": "    OutputCalibManager\nfrom fortuna.prob_output_layer.classification import \\\n    ClassificationProbOutputLayer\nfrom fortuna.typing import Array, Status\n\n\nclass CalibClassifier(CalibModel):\n    def __init__(\n        self,\n        output_calibrator: Optional[nn.Module] = ClassificationTemperatureScaler(),\n        seed: int = 0,\n    ) -> None:\n        r\"\"\"\n        A calibration classifier class.\n\n        Parameters\n        ----------\n        output_calibrator : Optional[nn.Module]\n            An output calibrator object. The default is temperature scaling for classification, which rescales the\n            logits with a scalar temperature parameter. Given outputs :math:`o`,", "metadata": {"fpath_tuple": ["awslabs_fortuna", "fortuna", "calib_model", "classification.py"], "line_no": 33, "task_id": "awslabs_fortuna/181", "start_line_no": 13, "end_line_no": 33, "window_size": 20, "context_start_lineno": 0, "repo": "awslabs_fortuna"}}, "top_k_context": [{"context": "from fortuna.output_calibrator.output_calib_manager.base import \\\n    OutputCalibManager\nfrom fortuna.output_calibrator.regression import RegressionTemperatureScaler\nfrom fortuna.prob_output_layer.regression import RegressionProbOutputLayer\nfrom fortuna.typing import Array, Status\n\n\nclass CalibRegressor(CalibModel):\n    def __init__(\n        self,\n        output_calibrator: Optional[nn.Module] = RegressionTemperatureScaler(),\n        seed: int = 0,\n    ) -> None:\n        r\"\"\"\n        A calibration regressor class.\n\n        Parameters\n        ----------\n        output_calibrator : Optional[nn.Module]\n            An output calibrator object. The default is temperature scaling for regression, which inflates the variance", "metadata": [{"fpath_tuple": ["awslabs_fortuna", "fortuna", "calib_model", "regression.py"], "line_no": 18, "start_line_no": 8, "end_line_no": 28, "window_size": 20, "repo": "awslabs_fortuna", "slice_size": 10}], "sim_score": 0.7117117117117117}, {"context": "from fortuna.output_calibrator.regression import RegressionTemperatureScaler\nfrom fortuna.prob_output_layer.regression import RegressionProbOutputLayer\nfrom fortuna.typing import Array, Status\n\n\nclass CalibRegressor(CalibModel):\n    def __init__(\n        self,\n        output_calibrator: Optional[nn.Module] = RegressionTemperatureScaler(),\n        seed: int = 0,\n    ) -> None:\n        r\"\"\"\n        A calibration regressor class.\n\n        Parameters\n        ----------\n        output_calibrator : Optional[nn.Module]\n            An output calibrator object. The default is temperature scaling for regression, which inflates the variance\n            of the likelihood with a scalar temperature parameter. Given outputs :math:`o` of the model manager, the\n            output calibrator is described by a function :math:`g(\\phi, o)`, where `phi` are", "metadata": [{"fpath_tuple": ["awslabs_fortuna", "fortuna", "calib_model", "regression.py"], "line_no": 20, "start_line_no": 10, "end_line_no": 30, "window_size": 20, "repo": "awslabs_fortuna", "slice_size": 10}], "sim_score": 0.7016129032258065}, {"context": "from fortuna.typing import Array, Status\n\n\nclass CalibRegressor(CalibModel):\n    def __init__(\n        self,\n        output_calibrator: Optional[nn.Module] = RegressionTemperatureScaler(),\n        seed: int = 0,\n    ) -> None:\n        r\"\"\"\n        A calibration regressor class.\n\n        Parameters\n        ----------\n        output_calibrator : Optional[nn.Module]\n            An output calibrator object. The default is temperature scaling for regression, which inflates the variance\n            of the likelihood with a scalar temperature parameter. Given outputs :math:`o` of the model manager, the\n            output calibrator is described by a function :math:`g(\\phi, o)`, where `phi` are\n            calibration parameters.\n        seed: int", "metadata": [{"fpath_tuple": ["awslabs_fortuna", "fortuna", "calib_model", "regression.py"], "line_no": 22, "start_line_no": 12, "end_line_no": 32, "window_size": 20, "repo": "awslabs_fortuna", "slice_size": 10}], "sim_score": 0.6451612903225806}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# torchrl/objectives/reinforce.py\n# --------------------------------------------------\n# \n#     def __init__(\n#         self,\n#         actor_network: SafeProbabilisticSequential,\n#         critic: Optional[SafeModule] = None,\n#         delay_value: bool = False,\n#         gamma: float = 0.99,\n#         advantage_key: str = \"advantage\",\n#         value_target_key: str = \"value_target\",\n#         loss_critic_type: str = \"smooth_l1\",\n#     ) -> None:\n#         super().__init__()\n# \n#         self.delay_value = delay_value\n#         self.advantage_key = advantage_key\n#         self.value_target_key = value_target_key\n#         self.loss_critic_type = loss_critic_type\n#         self.register_buffer(\"gamma\", torch.tensor(gamma))\n# \n#         # Actor\n# --------------------------------------------------\n# the below code fragment can be found in:\n# torchrl/objectives/dreamer.py\n# --------------------------------------------------\n#         value_model (SafeModule): the value model.\n#         value_loss (str, optional): the loss to use for the value loss. Default: \"l2\".\n#         gamma (float, optional): the gamma discount factor. Default: 0.99.\n#         discount_loss (bool, optional): if True, the loss is discounted with a\n#             gamma discount factor. Default: False.\n# \n#     \"\"\"\n# \n#     def __init__(\n#         self,\n#         value_model: SafeModule,\n#         value_loss: Optional[str] = None,\n#         gamma: int = 0.99,\n#         discount_loss: bool = False,  # for consistency with paper\n#     ):\n#         super().__init__()\n#         self.value_model = value_model\n#         self.value_loss = value_loss if value_loss is not None else \"l2\"\n#         self.gamma = gamma\n#         self.discount_loss = discount_loss\n# --------------------------------------------------\n# the below code fragment can be found in:\n# torchrl/objectives/ddpg.py\n# --------------------------------------------------\n#             data collection. Default is :obj:`False`.\n#         delay_value (bool, optional): whether to separate the target value networks from the value networks used for\n#             data collection. Default is :obj:`False`.\n#     \"\"\"\n# \n#     def __init__(\n#         self,\n#         actor_network: SafeModule,\n#         value_network: SafeModule,\n#         gamma: float,\n#         loss_function: str = \"l2\",\n#         delay_actor: bool = False,\n#         delay_value: bool = False,\n#     ) -> None:\n#         super().__init__()\n#         self.delay_actor = delay_actor\n#         self.delay_value = delay_value\n# \n#         actor_critic = ActorCriticWrapper(actor_network, value_network)\n#         params = make_functional(actor_critic)\n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\n# Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom functools import wraps\nfrom typing import List, Optional, Tuple, Union\n\nimport torch\nfrom tensordict.nn import dispatch_kwargs\nfrom tensordict.tensordict import TensorDictBase\nfrom torch import nn, Tensor\n\nfrom torchrl.envs.utils import step_mdp\nfrom torchrl.modules import SafeModule\n\nfrom torchrl.objectives.utils import hold_out_net\nfrom torchrl.objectives.value.functional import (\n    td_advantage_estimate,\n    td_lambda_advantage_estimate,\n    vec_generalized_advantage_estimate,\n    vec_td_lambda_advantage_estimate,\n)\n\n\ndef _self_set_grad_enabled(fun):\n    @wraps(fun)\n    def new_fun(self, *args, **kwargs):\n        with torch.set_grad_enabled(self.differentiable):\n            return fun(self, *args, **kwargs)\n\n    return new_fun\n\n\nclass TDEstimate(nn.Module):\n    \"\"\"Temporal Difference estimate of advantage function.\n\n    Args:\n        gamma (scalar): exponential mean discount.\n        value_network (SafeModule): value operator used to retrieve the value estimates.\n        average_rewards (bool, optional): if True, rewards will be standardized\n            before the TD is computed.\n        differentiable (bool, optional): if True, gradients are propagated throught\n            the computation of the value function. Default is :obj:`False`.\n        advantage_key (str or tuple of str, optional): the key of the advantage entry.\n            Defaults to \"advantage\".\n        value_target_key (str or tuple of str, optional): the key of the advantage entry.\n            Defaults to \"value_target\".\n        value_key (str or tuple of str, optional): the value key to read from the input tensordict.\n            Defaults to \"state_value\".\n\n    \"\"\"\n\n    def __init__(\n        self,\n        gamma: Union[float, torch.Tensor],\n        value_network: SafeModule,\n        average_rewards: bool = False,\n        differentiable: bool = False,\n        advantage_key: Union[str, Tuple] = \"advantage\",\n        value_target_key: Union[str, Tuple] = \"value_target\",\n        value_key: Union[str, Tuple] = \"state_value\",\n    ):\n        super().__init__()\n        try:\n            device = next(value_network.parameters()).device", "metadata": {"task_id": "pytorch_rl/30", "ground_truth": "        except StopIteration:", "fpath_tuple": ["pytorch_rl", "torchrl", "objectives", "value", "advantages.py"], "context_start_lineno": 0, "line_no": 66, "query_window": {"context": "        value_target_key (str or tuple of str, optional): the key of the advantage entry.\n            Defaults to \"value_target\".\n        value_key (str or tuple of str, optional): the value key to read from the input tensordict.\n            Defaults to \"state_value\".\n\n    \"\"\"\n\n    def __init__(\n        self,\n        gamma: Union[float, torch.Tensor],\n        value_network: SafeModule,\n        average_rewards: bool = False,\n        differentiable: bool = False,\n        advantage_key: Union[str, Tuple] = \"advantage\",\n        value_target_key: Union[str, Tuple] = \"value_target\",\n        value_key: Union[str, Tuple] = \"state_value\",\n    ):\n        super().__init__()\n        try:\n            device = next(value_network.parameters()).device", "metadata": {"fpath_tuple": ["pytorch_rl", "torchrl", "objectives", "value", "advantages.py"], "line_no": 66, "task_id": "pytorch_rl/30", "start_line_no": 46, "end_line_no": 66, "window_size": 20, "context_start_lineno": 0, "repo": "pytorch_rl"}}, "top_k_context": [{"context": "        loss_function (str): loss function for the value discrepancy. Can be one of \"l1\", \"l2\" or \"smooth_l1\".\n        delay_actor (bool, optional): whether to separate the target actor networks from the actor networks used for\n            data collection. Default is :obj:`False`.\n        delay_value (bool, optional): whether to separate the target value networks from the value networks used for\n            data collection. Default is :obj:`False`.\n    \"\"\"\n\n    def __init__(\n        self,\n        actor_network: SafeModule,\n        value_network: SafeModule,\n        gamma: float,\n        loss_function: str = \"l2\",\n        delay_actor: bool = False,\n        delay_value: bool = False,\n    ) -> None:\n        super().__init__()\n        self.delay_actor = delay_actor\n        self.delay_value = delay_value\n", "metadata": [{"fpath_tuple": ["pytorch_rl", "torchrl", "objectives", "ddpg.py"], "line_no": 42, "start_line_no": 32, "end_line_no": 52, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}], "sim_score": 0.3504273504273504}, {"context": "\n    Args:\n        value_model (SafeModule): the value model.\n        value_loss (str, optional): the loss to use for the value loss. Default: \"l2\".\n        gamma (float, optional): the gamma discount factor. Default: 0.99.\n        discount_loss (bool, optional): if True, the loss is discounted with a\n            gamma discount factor. Default: False.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        value_model: SafeModule,\n        value_loss: Optional[str] = None,\n        gamma: int = 0.99,\n        discount_loss: bool = False,  # for consistency with paper\n    ):\n        super().__init__()\n        self.value_model = value_model\n        self.value_loss = value_loss if value_loss is not None else \"l2\"", "metadata": [{"fpath_tuple": ["pytorch_rl", "torchrl", "objectives", "dreamer.py"], "line_no": 228, "start_line_no": 218, "end_line_no": 238, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}], "sim_score": 0.34513274336283184}, {"context": "\n    \"\"\"\n\n    def __init__(\n        self,\n        actor_network: SafeProbabilisticSequential,\n        critic: Optional[SafeModule] = None,\n        delay_value: bool = False,\n        gamma: float = 0.99,\n        advantage_key: str = \"advantage\",\n        value_target_key: str = \"value_target\",\n        loss_critic_type: str = \"smooth_l1\",\n    ) -> None:\n        super().__init__()\n\n        self.delay_value = delay_value\n        self.advantage_key = advantage_key\n        self.value_target_key = value_target_key\n        self.loss_critic_type = loss_critic_type\n        self.register_buffer(\"gamma\", torch.tensor(gamma))", "metadata": [{"fpath_tuple": ["pytorch_rl", "torchrl", "objectives", "reinforce.py"], "line_no": 30, "start_line_no": 20, "end_line_no": 40, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}], "sim_score": 0.34210526315789475}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# ding/utils/tests/test_system_helper.py\n# --------------------------------------------------\n# import pytest\n# \n# from ding.utils.system_helper import get_ip, get_pid, get_task_uid\n# \n# \n# @pytest.mark.unittest\n# class TestSystemHelper():\n# \n#     def test_get(self):\n#         try:\n#             get_ip()\n#         except:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# ding/utils/tests/test_default_helper.py\n# --------------------------------------------------\n# \n#     def test_deep_merge_dicts(self):\n#         dict1 = {\n#             'a': 3,\n#             'b': {\n#                 'c': 3,\n#                 'd': {\n#                     'e': 6,\n#                     'f': 5,\n#                 }\n#             }\n#         }\n#         dict2 = {\n#             'b': {\n#                 'c': 5,\n#                 'd': 6,\n#                 'g': 4,\n#             }\n#         }\n#         new_dict = deep_merge_dicts(dict1, dict2)\n# --------------------------------------------------\n# the below code fragment can be found in:\n# ding/worker/coordinator/tests/test_one_vs_one_commander.py\n# --------------------------------------------------\n# import time\n# import pytest\n# import os\n# \n# \n# @pytest.mark.unittest\n# class Test1v1Commander:\n# \n#     def test_init(self, setup_1v1commander):\n#         # basic\n#         assert not setup_1v1commander._end_flag\n#         # task space\n#         assert setup_1v1commander._collector_task_space.cur == setup_1v1commander._collector_task_space.min_val == 0\n#         assert setup_1v1commander._collector_task_space.max_val == 2\n# --------------------------------------------------\n# the below code fragment can be found in:\n# ding/utils/loader/tests/loader/test_base.py\n# --------------------------------------------------\n# import pytest\n# \n# from ding.utils.loader import Loader\n# \n# \n# @pytest.mark.unittest\n# class TestConfigLoaderBase:\n# \n#     def test_load(self):\n#         _loader = Loader(int)\n#         assert _loader.load(1) == 1\n#         with pytest.raises(TypeError):\n# --------------------------------------------------\n# the below code fragment can be found in:\n# ding/utils/tests/test_default_helper.py\n# --------------------------------------------------\n# @pytest.mark.unittest\n# class TestDict:\n# \n#     def test_deep_merge_dicts(self):\n#         dict1 = {\n#             'a': 3,\n#             'b': {\n#                 'c': 3,\n#                 'd': {\n#                     'e': 6,\n#                     'f': 5,\n#                 }\n#             }\n#         }\n#         dict2 = {\n#             'b': {\n#                 'c': 5,\n#                 'd': 6,\n#                 'g': 4,\n#             }\n# --------------------------------------------------\n# the below code fragment can be found in:\n# ding/worker/coordinator/tests/test_one_vs_one_commander.py\n# --------------------------------------------------\n# import time\n# import pytest\n# import os\n# \n# \n# @pytest.mark.unittest\n# class Test1v1Commander:\n# \n#     def test_init(self, setup_1v1commander):\n#         # basic\n#         assert not setup_1v1commander._end_flag\n#         # task space\n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\nimport os\nimport time\n\nimport pytest\nimport torch\nimport torch.nn as nn\nimport uuid\n\nfrom ding.torch_utils.checkpoint_helper import auto_checkpoint, build_checkpoint_helper, CountVar\nfrom ding.utils import read_file, save_file\n\n\nclass DstModel(nn.Module):\n\n    def __init__(self):\n        super(DstModel, self).__init__()\n        self.fc1 = nn.Linear(3, 3)\n        self.fc2 = nn.Linear(3, 8)\n        self.fc_dst = nn.Linear(3, 6)\n\n\nclass SrcModel(nn.Module):\n\n    def __init__(self):\n        super(SrcModel, self).__init__()\n        self.fc1 = nn.Linear(3, 3)\n        self.fc2 = nn.Linear(3, 8)\n        self.fc_src = nn.Linear(3, 7)\n\n\nclass HasStateDict(object):\n\n    def __init__(self, name):\n        self._name = name\n        self._state_dict = name + str(uuid.uuid4())\n\n    def state_dict(self):\n        old = self._state_dict\n        self._state_dict = self._name + str(uuid.uuid4())\n        return old\n\n    def load_state_dict(self, state_dict):\n        self._state_dict = state_dict\n\n\n@pytest.mark.unittest\nclass TestCkptHelper:\n\n    def test_load_model(self):", "metadata": {"task_id": "opendilab_ACE/161", "ground_truth": "        path = 'model.pt'", "fpath_tuple": ["opendilab_ACE", "ding", "torch_utils", "tests", "test_ckpt_helper.py"], "context_start_lineno": 0, "line_no": 49, "query_window": {"context": "\nclass HasStateDict(object):\n\n    def __init__(self, name):\n        self._name = name\n        self._state_dict = name + str(uuid.uuid4())\n\n    def state_dict(self):\n        old = self._state_dict\n        self._state_dict = self._name + str(uuid.uuid4())\n        return old\n\n    def load_state_dict(self, state_dict):\n        self._state_dict = state_dict\n\n\n@pytest.mark.unittest\nclass TestCkptHelper:\n\n    def test_load_model(self):", "metadata": {"fpath_tuple": ["opendilab_ACE", "ding", "torch_utils", "tests", "test_ckpt_helper.py"], "line_no": 49, "task_id": "opendilab_ACE/161", "start_line_no": 29, "end_line_no": 49, "window_size": 20, "context_start_lineno": 0, "repo": "opendilab_ACE"}}, "top_k_context": [{"context": "import time\nimport pytest\nimport os\n\n\n@pytest.mark.unittest\nclass Test1v1Commander:\n\n    def test_init(self, setup_1v1commander):\n        # basic", "metadata": [{"fpath_tuple": ["opendilab_ACE", "ding", "worker", "coordinator", "tests", "test_one_vs_one_commander.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 10, "window_size": 20, "repo": "opendilab_ACE", "slice_size": 10}], "sim_score": 0.35384615384615387}, {"context": "\n\n@pytest.mark.unittest\nclass TestDict:\n\n    def test_deep_merge_dicts(self):\n        dict1 = {\n            'a': 3,\n            'b': {\n                'c': 3,\n                'd': {\n                    'e': 6,\n                    'f': 5,\n                }\n            }\n        }\n        dict2 = {\n            'b': {\n                'c': 5,\n                'd': 6,", "metadata": [{"fpath_tuple": ["opendilab_ACE", "ding", "utils", "tests", "test_default_helper.py"], "line_no": 130, "start_line_no": 120, "end_line_no": 140, "window_size": 20, "repo": "opendilab_ACE", "slice_size": 10}], "sim_score": 0.34210526315789475}, {"context": "import pytest\n\nfrom ding.utils.loader import Loader\n\n\n@pytest.mark.unittest\nclass TestConfigLoaderBase:\n\n    def test_load(self):\n        _loader = Loader(int)", "metadata": [{"fpath_tuple": ["opendilab_ACE", "ding", "utils", "loader", "tests", "loader", "test_base.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 10, "window_size": 20, "repo": "opendilab_ACE", "slice_size": 10}], "sim_score": 0.3382352941176471}, {"context": "import time\nimport pytest\nimport os\n\n\n@pytest.mark.unittest\nclass Test1v1Commander:\n\n    def test_init(self, setup_1v1commander):\n        # basic\n        assert not setup_1v1commander._end_flag\n        # task space", "metadata": [{"fpath_tuple": ["opendilab_ACE", "ding", "worker", "coordinator", "tests", "test_one_vs_one_commander.py"], "line_no": 2, "start_line_no": 0, "end_line_no": 12, "window_size": 20, "repo": "opendilab_ACE", "slice_size": 10}], "sim_score": 0.3380281690140845}, {"context": "@pytest.mark.unittest\nclass TestDict:\n\n    def test_deep_merge_dicts(self):\n        dict1 = {\n            'a': 3,\n            'b': {\n                'c': 3,\n                'd': {\n                    'e': 6,\n                    'f': 5,\n                }\n            }\n        }\n        dict2 = {\n            'b': {\n                'c': 5,\n                'd': 6,\n                'g': 4,\n            }", "metadata": [{"fpath_tuple": ["opendilab_ACE", "ding", "utils", "tests", "test_default_helper.py"], "line_no": 132, "start_line_no": 122, "end_line_no": 142, "window_size": 20, "repo": "opendilab_ACE", "slice_size": 10}], "sim_score": 0.3333333333333333}, {"context": "import pytest\n\nfrom ding.utils.system_helper import get_ip, get_pid, get_task_uid\n\n\n@pytest.mark.unittest\nclass TestSystemHelper():\n\n    def test_get(self):\n        try:", "metadata": [{"fpath_tuple": ["opendilab_ACE", "ding", "utils", "tests", "test_system_helper.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 10, "window_size": 20, "repo": "opendilab_ACE", "slice_size": 10}], "sim_score": 0.323943661971831}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# src/diffusers/models/resnet_flax.py\n# --------------------------------------------------\n# # Licensed under the Apache License, Version 2.0 (the \"License\");\n# # you may not use this file except in compliance with the License.\n# # You may obtain a copy of the License at\n# #\n# #     http://www.apache.org/licenses/LICENSE-2.0\n# #\n# # Unless required by applicable law or agreed to in writing, software\n# # distributed under the License is distributed on an \"AS IS\" BASIS,\n# # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# # See the License for the specific language governing permissions and\n# # limitations under the License.\n# import flax.linen as nn\n# import jax\n# import jax.numpy as jnp\n# \n# \n# class FlaxUpsample2D(nn.Module):\n#     out_channels: int\n#     dtype: jnp.dtype = jnp.float32\n# \n# --------------------------------------------------\n# the below code fragment can be found in:\n# src/diffusers/models/resnet_flax.py\n# --------------------------------------------------\n# # You may obtain a copy of the License at\n# #\n# #     http://www.apache.org/licenses/LICENSE-2.0\n# #\n# # Unless required by applicable law or agreed to in writing, software\n# # distributed under the License is distributed on an \"AS IS\" BASIS,\n# # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# # See the License for the specific language governing permissions and\n# # limitations under the License.\n# import flax.linen as nn\n# import jax\n# import jax.numpy as jnp\n# \n# \n# class FlaxUpsample2D(nn.Module):\n#     out_channels: int\n#     dtype: jnp.dtype = jnp.float32\n# \n#     def setup(self):\n#         self.conv = nn.Conv(\n# --------------------------------------------------\n# the below code fragment can be found in:\n# src/diffusers/models/attention_flax.py\n# --------------------------------------------------\n# # Licensed under the Apache License, Version 2.0 (the \"License\");\n# # you may not use this file except in compliance with the License.\n# # You may obtain a copy of the License at\n# #\n# #     http://www.apache.org/licenses/LICENSE-2.0\n# #\n# # Unless required by applicable law or agreed to in writing, software\n# # distributed under the License is distributed on an \"AS IS\" BASIS,\n# # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# # See the License for the specific language governing permissions and\n# # limitations under the License.\n# \n# import flax.linen as nn\n# import jax.numpy as jnp\n# \n# \n# class FlaxAttentionBlock(nn.Module):\n#     r\"\"\"\n#     A Flax multi-head attention module as described in: https://arxiv.org/abs/1706.03762\n# \n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\n# Copyright 2022 The HuggingFace Team. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport flax.linen as nn\nimport jax.numpy as jnp\n\nfrom .attention_flax import FlaxTransformer2DModel\nfrom .resnet_flax import FlaxDownsample2D, FlaxResnetBlock2D, FlaxUpsample2D\n\n\nclass FlaxCrossAttnDownBlock2D(nn.Module):\n    r\"\"\"", "metadata": {"task_id": "huggingface_diffusers/138", "ground_truth": "    Cross Attention 2D Downsizing block - original architecture from Unet transformers:", "fpath_tuple": ["huggingface_diffusers", "src", "diffusers", "models", "unet_2d_blocks_flax.py"], "context_start_lineno": 0, "line_no": 23, "query_window": {"context": "# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport flax.linen as nn\nimport jax.numpy as jnp\n\nfrom .attention_flax import FlaxTransformer2DModel\nfrom .resnet_flax import FlaxDownsample2D, FlaxResnetBlock2D, FlaxUpsample2D\n\n\nclass FlaxCrossAttnDownBlock2D(nn.Module):\n    r\"\"\"", "metadata": {"fpath_tuple": ["huggingface_diffusers", "src", "diffusers", "models", "unet_2d_blocks_flax.py"], "line_no": 23, "task_id": "huggingface_diffusers/138", "start_line_no": 3, "end_line_no": 23, "window_size": 20, "context_start_lineno": 0, "repo": "huggingface_diffusers"}}, "top_k_context": [{"context": "# Copyright 2022 The HuggingFace Team. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport flax.linen as nn\nimport jax.numpy as jnp\n\n\nclass FlaxAttentionBlock(nn.Module):\n    r\"\"\"", "metadata": [{"fpath_tuple": ["huggingface_diffusers", "src", "diffusers", "models", "attention_flax.py"], "line_no": 10, "start_line_no": 0, "end_line_no": 20, "window_size": 20, "repo": "huggingface_diffusers", "slice_size": 10}], "sim_score": 0.7375886524822695}, {"context": "# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport flax.linen as nn\nimport jax\nimport jax.numpy as jnp\n\n\nclass FlaxUpsample2D(nn.Module):\n    out_channels: int\n    dtype: jnp.dtype = jnp.float32\n", "metadata": [{"fpath_tuple": ["huggingface_diffusers", "src", "diffusers", "models", "resnet_flax.py"], "line_no": 12, "start_line_no": 2, "end_line_no": 22, "window_size": 20, "repo": "huggingface_diffusers", "slice_size": 10}], "sim_score": 0.7323943661971831}, {"context": "# Copyright 2022 The HuggingFace Team. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport flax.linen as nn\nimport jax\nimport jax.numpy as jnp\n\n\nclass FlaxUpsample2D(nn.Module):\n    out_channels: int", "metadata": [{"fpath_tuple": ["huggingface_diffusers", "src", "diffusers", "models", "resnet_flax.py"], "line_no": 10, "start_line_no": 0, "end_line_no": 20, "window_size": 20, "repo": "huggingface_diffusers", "slice_size": 10}], "sim_score": 0.7123287671232876}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# ding/policy/ddpg.py\n# --------------------------------------------------\n#            | ``update_freq``                                | once, how many times will actor   | 2 for TD3. Delayed\n#            |                                                | network update.                   | Policy Updates method\n#            |                                                |                                   | in TD3 paper.\n#         8  | ``learn.noise``     bool        False          | Whether to add noise on target    | Default False for\n#            |                                                | network's action.                 | DDPG, True for TD3.\n#            |                                                |                                   | Target Policy Smoo-\n#            |                                                |                                   | thing Regularization\n#            |                                                |                                   | in TD3 paper.\n#         9  | ``learn.-``         bool        False          | Determine whether to ignore       | Use ignore_done only\n#            | ``ignore_done``                                | done flag.                        | in halfcheetah env.\n#         10 | ``learn.-``         float       0.005          | Used for soft update of the       | aka. Interpolation\n#            | ``target_theta``                               | target network.                   | factor in polyak aver\n#            |                                                |                                   | aging for target\n#            |                                                |                                   | networks.\n#         11 | ``collect.-``       float       0.1            | Used for add noise during co-     | Sample noise from dis\n#            | ``noise_sigma``                                | llection, through controlling     | tribution, Ornstein-\n#            |                                                | the sigma of distribution         | Uhlenbeck process in\n#            |                                                |                                   | DDPG paper, Guassian\n#            |                                                |                                   | process in ours.\n#         == ====================  ========    =============  =================================   =======================\n# --------------------------------------------------\n# the below code fragment can be found in:\n# ding/policy/ddpg.py\n# --------------------------------------------------\n#            |                                                |                                   | in TD3 paper.\n#         8  | ``learn.noise``     bool        False          | Whether to add noise on target    | Default False for\n#            |                                                | network's action.                 | DDPG, True for TD3.\n#            |                                                |                                   | Target Policy Smoo-\n#            |                                                |                                   | thing Regularization\n#            |                                                |                                   | in TD3 paper.\n#         9  | ``learn.-``         bool        False          | Determine whether to ignore       | Use ignore_done only\n#            | ``ignore_done``                                | done flag.                        | in halfcheetah env.\n#         10 | ``learn.-``         float       0.005          | Used for soft update of the       | aka. Interpolation\n#            | ``target_theta``                               | target network.                   | factor in polyak aver\n#            |                                                |                                   | aging for target\n#            |                                                |                                   | networks.\n#         11 | ``collect.-``       float       0.1            | Used for add noise during co-     | Sample noise from dis\n#            | ``noise_sigma``                                | llection, through controlling     | tribution, Ornstein-\n#            |                                                | the sigma of distribution         | Uhlenbeck process in\n#            |                                                |                                   | DDPG paper, Guassian\n#            |                                                |                                   | process in ours.\n#         == ====================  ========    =============  =================================   =======================\n#     \"\"\"\n# \n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\nfrom ding.utils import POLICY_REGISTRY\nfrom .ddpg import DDPGPolicy\n\n\n@POLICY_REGISTRY.register('td3')\nclass TD3Policy(DDPGPolicy):\n    r\"\"\"\n    Overview:\n        Policy class of TD3 algorithm. Since DDPG and TD3 share many common things, we can easily derive this TD3\n        class from DDPG class by changing ``_actor_update_freq``, ``_twin_critic`` and noise in model wrapper.\n    Property:\n        learn_mode, collect_mode, eval_mode\n\n    Config:\n\n    == ====================  ========    ==================  =================================   =======================\n    ID Symbol                Type        Default Value       Description                         Other(Shape)\n    == ====================  ========    ==================  =================================   =======================\n    1  ``type``              str         td3                 | RL policy register name, refer    | this arg is optional,\n                                                             | to registry ``POLICY_REGISTRY``   | a placeholder\n    2  ``cuda``              bool        True                | Whether to use cuda for network   |\n    3  | ``random_``         int         25000               | Number of randomly collected      | Default to 25000 for\n       | ``collect_size``                                    | training samples in replay        | DDPG/TD3, 10000 for\n       |                                                     | buffer when training starts.      | sac.\n    4  | ``model.twin_``     bool        True                | Whether to use two critic         | Default True for TD3,\n       | ``critic``                                          | networks or only one.             | Clipped Double\n       |                                                     |                                   | Q-learning method in\n       |                                                     |                                   | TD3 paper.\n    5  | ``learn.learning``  float       1e-3                | Learning rate for actor           |\n       | ``_rate_actor``                                     | network(aka. policy).             |\n    6  | ``learn.learning``  float       1e-3                | Learning rates for critic         |\n       | ``_rate_critic``                                    | network (aka. Q-network).         |\n    7  | ``learn.actor_``    int         2                   | When critic network updates       | Default 2 for TD3, 1\n       | ``update_freq``                                     | once, how many times will actor   | for DDPG. Delayed\n       |                                                     | network update.                   | Policy Updates method\n       |                                                     |                                   | in TD3 paper.\n    8  | ``learn.noise``     bool        True                | Whether to add noise on target    | Default True for TD3,\n       |                                                     | network's action.                 | False for DDPG.\n       |                                                     |                                   | Target Policy Smoo-\n       |                                                     |                                   | thing Regularization\n       |                                                     |                                   | in TD3 paper.\n    9  | ``learn.noise_``    dict        | dict(min=-0.5,    | Limit for range of target         |\n       | ``range``                       |      max=0.5,)    | policy smoothing noise,           |\n       |                                 |                   | aka. noise_clip.                  |\n    10 | ``learn.-``         bool        False               | Determine whether to ignore       | Use ignore_done only\n       | ``ignore_done``                                     | done flag.                        | in halfcheetah env.\n    11 | ``learn.-``         float       0.005               | Used for soft update of the       | aka. Interpolation\n       | ``target_theta``                                    | target network.                   | factor in polyak aver\n       |                                                     |                                   | aging for target\n       |                                                     |                                   | networks.\n    12 | ``collect.-``       float       0.1                 | Used for add noise during co-     | Sample noise from dis\n       | ``noise_sigma``                                     | llection, through controlling     | tribution, Ornstein-\n       |                                                     | the sigma of distribution         | Uhlenbeck process in", "metadata": {"task_id": "opendilab_ACE/35", "ground_truth": "       |                                                     |                                   | DDPG paper, Guassian", "fpath_tuple": ["opendilab_ACE", "ding", "policy", "td3.py"], "context_start_lineno": 0, "line_no": 53, "query_window": {"context": "       | ``update_freq``                                     | once, how many times will actor   | for DDPG. Delayed\n       |                                                     | network update.                   | Policy Updates method\n       |                                                     |                                   | in TD3 paper.\n    8  | ``learn.noise``     bool        True                | Whether to add noise on target    | Default True for TD3,\n       |                                                     | network's action.                 | False for DDPG.\n       |                                                     |                                   | Target Policy Smoo-\n       |                                                     |                                   | thing Regularization\n       |                                                     |                                   | in TD3 paper.\n    9  | ``learn.noise_``    dict        | dict(min=-0.5,    | Limit for range of target         |\n       | ``range``                       |      max=0.5,)    | policy smoothing noise,           |\n       |                                 |                   | aka. noise_clip.                  |\n    10 | ``learn.-``         bool        False               | Determine whether to ignore       | Use ignore_done only\n       | ``ignore_done``                                     | done flag.                        | in halfcheetah env.\n    11 | ``learn.-``         float       0.005               | Used for soft update of the       | aka. Interpolation\n       | ``target_theta``                                    | target network.                   | factor in polyak aver\n       |                                                     |                                   | aging for target\n       |                                                     |                                   | networks.\n    12 | ``collect.-``       float       0.1                 | Used for add noise during co-     | Sample noise from dis\n       | ``noise_sigma``                                     | llection, through controlling     | tribution, Ornstein-\n       |                                                     | the sigma of distribution         | Uhlenbeck process in", "metadata": {"fpath_tuple": ["opendilab_ACE", "ding", "policy", "td3.py"], "line_no": 53, "task_id": "opendilab_ACE/35", "start_line_no": 33, "end_line_no": 53, "window_size": 20, "context_start_lineno": 0, "repo": "opendilab_ACE"}}, "top_k_context": [{"context": "           | ``update_freq``                                | once, how many times will actor   | 2 for TD3. Delayed\n           |                                                | network update.                   | Policy Updates method\n           |                                                |                                   | in TD3 paper.\n        8  | ``learn.noise``     bool        False          | Whether to add noise on target    | Default False for\n           |                                                | network's action.                 | DDPG, True for TD3.\n           |                                                |                                   | Target Policy Smoo-\n           |                                                |                                   | thing Regularization\n           |                                                |                                   | in TD3 paper.\n        9  | ``learn.-``         bool        False          | Determine whether to ignore       | Use ignore_done only\n           | ``ignore_done``                                | done flag.                        | in halfcheetah env.\n        10 | ``learn.-``         float       0.005          | Used for soft update of the       | aka. Interpolation\n           | ``target_theta``                               | target network.                   | factor in polyak aver\n           |                                                |                                   | aging for target\n           |                                                |                                   | networks.\n        11 | ``collect.-``       float       0.1            | Used for add noise during co-     | Sample noise from dis\n           | ``noise_sigma``                                | llection, through controlling     | tribution, Ornstein-\n           |                                                | the sigma of distribution         | Uhlenbeck process in\n           |                                                |                                   | DDPG paper, Guassian\n           |                                                |                                   | process in ours.\n        == ====================  ========    =============  =================================   =======================", "metadata": [{"fpath_tuple": ["opendilab_ACE", "ding", "policy", "ddpg.py"], "line_no": 52, "start_line_no": 42, "end_line_no": 62, "window_size": 20, "repo": "opendilab_ACE", "slice_size": 10}], "sim_score": 0.7818181818181819}, {"context": "           | ``_rate_critic``                               | network (aka. Q-network).         |\n        7  | ``learn.actor_``    int         2              | When critic network updates       | Default 1 for DDPG,\n           | ``update_freq``                                | once, how many times will actor   | 2 for TD3. Delayed\n           |                                                | network update.                   | Policy Updates method\n           |                                                |                                   | in TD3 paper.\n        8  | ``learn.noise``     bool        False          | Whether to add noise on target    | Default False for\n           |                                                | network's action.                 | DDPG, True for TD3.\n           |                                                |                                   | Target Policy Smoo-\n           |                                                |                                   | thing Regularization\n           |                                                |                                   | in TD3 paper.\n        9  | ``learn.-``         bool        False          | Determine whether to ignore       | Use ignore_done only\n           | ``ignore_done``                                | done flag.                        | in halfcheetah env.\n        10 | ``learn.-``         float       0.005          | Used for soft update of the       | aka. Interpolation\n           | ``target_theta``                               | target network.                   | factor in polyak aver\n           |                                                |                                   | aging for target\n           |                                                |                                   | networks.\n        11 | ``collect.-``       float       0.1            | Used for add noise during co-     | Sample noise from dis\n           | ``noise_sigma``                                | llection, through controlling     | tribution, Ornstein-\n           |                                                | the sigma of distribution         | Uhlenbeck process in\n           |                                                |                                   | DDPG paper, Guassian", "metadata": [{"fpath_tuple": ["opendilab_ACE", "ding", "policy", "ddpg.py"], "line_no": 50, "start_line_no": 40, "end_line_no": 60, "window_size": 20, "repo": "opendilab_ACE", "slice_size": 10}], "sim_score": 0.75}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# vizier/_src/pyglove/client.py\n# --------------------------------------------------\n#       problem: vz.ProblemStatement,\n#       converter: converters.VizierConverter,\n#       owner: str,\n#       name: str,\n#       algorithm: pg.DNAGenerator,\n#   ) -> client_abc.StudyInterface:\n#     \"\"\"Creates a new study.\"\"\"\n# \n#   @abc.abstractmethod\n#   def get_group_id(self, group_id: Union[None, int, str] = None) -> str:\n#     \"\"\"Get worker ID.\"\"\"\n# \n#   @abc.abstractmethod\n#   def ping_tuner(self, tuner_id: str) -> bool:\n#     \"\"\"See if the tuner is alive.\"\"\"\n# \n#   @abc.abstractmethod\n#   def pythia_supporter(\n#       self, study: client_abc.StudyInterface\n#   ) -> pythia.PolicySupporter:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# vizier/_src/pyglove/client.py\n# --------------------------------------------------\n#   ) -> client_abc.StudyInterface:\n#     \"\"\"Creates a new study.\"\"\"\n# \n#   @abc.abstractmethod\n#   def get_group_id(self, group_id: Union[None, int, str] = None) -> str:\n#     \"\"\"Get worker ID.\"\"\"\n# \n#   @abc.abstractmethod\n#   def ping_tuner(self, tuner_id: str) -> bool:\n#     \"\"\"See if the tuner is alive.\"\"\"\n# \n#   @abc.abstractmethod\n#   def pythia_supporter(\n#       self, study: client_abc.StudyInterface\n#   ) -> pythia.PolicySupporter:\n#     \"\"\"Creates a pythia policy supporter for this study.\"\"\"\n# \n#   @abc.abstractmethod\n#   def use_pythia_for_study(self, study: client_abc.StudyInterface) -> None:\n#     \"\"\"Uses current Pythia service to serve the input study.\"\"\"\n# --------------------------------------------------\n# the below code fragment can be found in:\n# vizier/_src/pyglove/client.py\n# --------------------------------------------------\n#   ) -> client_abc.StudyInterface:\n#     \"\"\"Creates a new study.\"\"\"\n# \n#   @abc.abstractmethod\n#   def get_group_id(self, group_id: Union[None, int, str] = None) -> str:\n#     \"\"\"Get worker ID.\"\"\"\n# \n#   @abc.abstractmethod\n#   def ping_tuner(self, tuner_id: str) -> bool:\n#     \"\"\"See if the tuner is alive.\"\"\"\n# \n#   @abc.abstractmethod\n#   def pythia_supporter(\n#       self, study: client_abc.StudyInterface\n#   ) -> pythia.PolicySupporter:\n#     \"\"\"Creates a pythia policy supporter for this study.\"\"\"\n# \n#   @abc.abstractmethod\n#   def use_pythia_for_study(self, study: client_abc.StudyInterface) -> None:\n#     \"\"\"Uses current Pythia service to serve the input study.\"\"\"\n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\n specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import annotations\n\n# Copyright 2022 The PyGlove Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Tuner implementation based on Open Source Vizier.\"\"\"\n\nfrom concurrent import futures\nimport os\nimport threading\nfrom typing import Optional, Union\n\nfrom absl import logging\nimport attr\nimport grpc\nimport portpicker\nimport pyglove as pg\nfrom vizier import pythia\nfrom vizier import pyvizier as vz\nfrom vizier._src.pyglove import algorithms\nfrom vizier._src.pyglove import backend\nfrom vizier._src.pyglove import client\nfrom vizier._src.pyglove import converters\nfrom vizier._src.pyglove import pythia as pyglove_pythia\nfrom vizier.client import client_abc\nfrom vizier.service import clients as pyvizier_clients\nfrom vizier.service import constants\nfrom vizier.service import pythia_service\nfrom vizier.service import pythia_service_pb2_grpc\nfrom vizier.service import pyvizier as svz\nfrom vizier.service import resources\nfrom vizier.service import service_policy_supporter\nfrom vizier.service import stubs_util\nfrom vizier.service import types as vizier_types\nfrom vizier.service import vizier_client\n\nfrom google.protobuf import empty_pb2\n\nTunerPolicy = pyglove_pythia.TunerPolicy\nBuiltinAlgorithm = algorithms.BuiltinAlgorithm\n\nExpandedStudyName = client.ExpandedStudyName\nStudyKey = client.StudyKey\n\n\n@attr.define\nclass _GlobalStates:\n  \"\"\"Global settings for all backends.\"\"\"\n\n  vizier_tuner: Optional[client.VizierTuner] = attr.field(default=None)\n\n\n_global_states = _GlobalStates()\n\n\nclass _OSSVizierTuner(client.VizierTuner):\n  \"\"\"OSS Vizier tuner for pyglove.\"\"\"\n\n  _vizier_service: vizier_types.VizierService\n  _pythia_port: int = 9999\n  _pythia_servicer: Optional[pythia_service.PythiaServicer] = None\n  _pythia_server: Optional[grpc.Server] = None\n\n  def __init__(\n      self, endpoint: Optional[str] = None, pythia_port: Optional[int] = None\n  ):\n    super().__init__()\n    if endpoint:\n      pyvizier_clients.environment_variables.server_endpoint = endpoint\n    else:\n      endpoint = constants.NO_ENDPOINT\n\n    self._vizier_service = vizier_client.create_vizier_servicer_or_stub(\n        endpoint\n    )\n    self._pythia_port = pythia_port or portpicker.pick_unused_port()\n\n  def get_tuner_id(self, algorithm: pg.DNAGenerator) -> str:\n    \"\"\"See parent class.\"\"\"\n    del algorithm\n    return self._get_pythia_endpoint()\n\n  def _start_pythia_service(\n      self, policy_cache: dict[StudyKey, TunerPolicy]\n  ) -> bool:\n    \"\"\"See parent class.\"\"\"\n\n    if not self._pythia_server:\n      def policy_factory(\n          problem_statement, algorithm, policy_supporter, study_name  # pylint:disable=unused-argument\n      ):\n        study_resource = resources.StudyResource.from_name(study_name)\n        study_key = StudyKey(\n            study_resource.owner_id, ExpandedStudyName(study_resource.study_id)\n        )\n        if study_key in policy_cache:\n          return policy_cache[study_key]\n\n        # Use default Vizier algorithms if not using PyGlove poliices.\n        return pythia_service.default_policy_factory(\n            problem_statement, algorithm, policy_supporter, study_name\n        )\n\n      self._pythia_servicer = pythia_service.PythiaServicer(\n          self._vizier_service, policy_factory\n      )\n      self._pythia_server = grpc.server(\n          futures.ThreadPoolExecutor(max_workers=1)\n      )\n      pythia_service_pb2_grpc.add_PythiaServiceServicer_to_server(\n          self._pythia_servicer, self._pythia_server\n      )\n      self._pythia_server.add_insecure_port(self._get_pythia_endpoint())\n      self._pythia_server.start()\n      return True\n    else:\n      return False\n\n  def _get_pythia_endpoint(self) -> str:\n    return f'{os.uname()[1]}:{self._pythia_port}'\n\n  def load_prior_study(self, resource_name: str) -> client_abc.StudyInterface:\n    \"\"\"See parent class.\"\"\"\n    return pyvizier_clients.Study.from_resource_name(resource_name)\n\n  def load_study(\n      self, owner: str, name: ExpandedStudyName\n  ) -> client_abc.StudyInterface:\n    \"\"\"See parent class.\"\"\"\n    return pyvizier_clients.Study.from_owner_and_id(owner, name)\n\n  def _configure_algorithm(\n      self, study_config: svz.StudyConfig, algorithm: pg.DNAGenerator\n  ) -> None:\n    \"\"\"Configure algorithm for a study.\"\"\"\n    if isinstance(algorithm, algorithms.BuiltinAlgorithm):\n      study_config.algorithm = algorithm.name\n    else:\n      study_config.algorithm = 'EXTERNAL_PYTHIA_SERVICE'\n    study_config.pythia_endpoint = self.get_tuner_id(algorithm)\n\n  def create_study(\n      self,\n      problem: vz.ProblemStatement,\n      converter: converters.VizierConverter,\n      owner: str,\n      name: str,\n      algorithm: pg.DNAGenerator,\n  ) -> client_abc.StudyInterface:\n    \"\"\"See parent class.\"\"\"\n    study_config = svz.StudyConfig.from_problem(problem)\n    if converter.vizier_conversion_error:\n      study_config.observation_noise = svz.ObservationNoise.HIGH\n    self._configure_algorithm(study_config, algorithm)\n    logging.info(\n        'Created OSS Vizier study with owner: %s, name: %s', owner, name\n    )\n    return pyvizier_clients.Study.from_study_config(\n        study_config, owner=owner, study_id=name\n    )\n\n  def get_group_id(self, group_id: Union[None, int, str] = None) -> str:\n    \"\"\"See parent class.\"\"\"\n    if group_id is None:\n      hostname = os.uname()[1]\n      thread_id = threading.get_ident()\n      return f'{thread_id}@{hostname}'\n    elif isinstance(group_id, int):\n      return f'group:{group_id}'\n    elif isinstance(group_id, str):\n      return group_id  # pytype: disable=bad-return-type\n\n  def ping_tuner(self, tuner_id: str) -> bool:\n    # We treat `tuner_id` as the Pythia endpoint.\n    try:\n      stubs_util.create_pythia_server_stub(tuner_id, timeout=3).Ping(", "metadata": {"task_id": "google_vizier/80", "ground_truth": "          empty_pb2.Empty()", "fpath_tuple": ["google_vizier", "vizier", "_src", "pyglove", "oss_vizier.py"], "context_start_lineno": 11, "line_no": 200, "query_window": {"context": "    )\n    return pyvizier_clients.Study.from_study_config(\n        study_config, owner=owner, study_id=name\n    )\n\n  def get_group_id(self, group_id: Union[None, int, str] = None) -> str:\n    \"\"\"See parent class.\"\"\"\n    if group_id is None:\n      hostname = os.uname()[1]\n      thread_id = threading.get_ident()\n      return f'{thread_id}@{hostname}'\n    elif isinstance(group_id, int):\n      return f'group:{group_id}'\n    elif isinstance(group_id, str):\n      return group_id  # pytype: disable=bad-return-type\n\n  def ping_tuner(self, tuner_id: str) -> bool:\n    # We treat `tuner_id` as the Pythia endpoint.\n    try:\n      stubs_util.create_pythia_server_stub(tuner_id, timeout=3).Ping(", "metadata": {"fpath_tuple": ["google_vizier", "vizier", "_src", "pyglove", "oss_vizier.py"], "line_no": 200, "task_id": "google_vizier/80", "start_line_no": 180, "end_line_no": 200, "window_size": 20, "context_start_lineno": 11, "repo": "google_vizier"}}, "top_k_context": [{"context": "  @abc.abstractmethod\n  def get_group_id(self, group_id: Union[None, int, str] = None) -> str:\n    \"\"\"Get worker ID.\"\"\"\n\n  @abc.abstractmethod\n  def ping_tuner(self, tuner_id: str) -> bool:\n    \"\"\"See if the tuner is alive.\"\"\"\n\n  @abc.abstractmethod\n  def pythia_supporter(\n      self, study: client_abc.StudyInterface\n  ) -> pythia.PolicySupporter:\n    \"\"\"Creates a pythia policy supporter for this study.\"\"\"\n\n  @abc.abstractmethod\n  def use_pythia_for_study(self, study: client_abc.StudyInterface) -> None:\n    \"\"\"Uses current Pythia service to serve the input study.\"\"\"", "metadata": [{"fpath_tuple": ["google_vizier", "vizier", "_src", "pyglove", "client.py"], "line_no": 112, "start_line_no": 102, "end_line_no": 119, "window_size": 20, "repo": "google_vizier", "slice_size": 10}], "sim_score": 0.32857142857142857}, {"context": "    \"\"\"Creates a new study.\"\"\"\n\n  @abc.abstractmethod\n  def get_group_id(self, group_id: Union[None, int, str] = None) -> str:\n    \"\"\"Get worker ID.\"\"\"\n\n  @abc.abstractmethod\n  def ping_tuner(self, tuner_id: str) -> bool:\n    \"\"\"See if the tuner is alive.\"\"\"\n\n  @abc.abstractmethod\n  def pythia_supporter(\n      self, study: client_abc.StudyInterface\n  ) -> pythia.PolicySupporter:\n    \"\"\"Creates a pythia policy supporter for this study.\"\"\"\n\n  @abc.abstractmethod\n  def use_pythia_for_study(self, study: client_abc.StudyInterface) -> None:\n    \"\"\"Uses current Pythia service to serve the input study.\"\"\"", "metadata": [{"fpath_tuple": ["google_vizier", "vizier", "_src", "pyglove", "client.py"], "line_no": 110, "start_line_no": 100, "end_line_no": 119, "window_size": 20, "repo": "google_vizier", "slice_size": 10}], "sim_score": 0.3262411347517731}, {"context": "  def create_study(\n      self,\n      problem: vz.ProblemStatement,\n      converter: converters.VizierConverter,\n      owner: str,\n      name: str,\n      algorithm: pg.DNAGenerator,\n  ) -> client_abc.StudyInterface:\n    \"\"\"Creates a new study.\"\"\"\n\n  @abc.abstractmethod\n  def get_group_id(self, group_id: Union[None, int, str] = None) -> str:\n    \"\"\"Get worker ID.\"\"\"\n\n  @abc.abstractmethod\n  def ping_tuner(self, tuner_id: str) -> bool:\n    \"\"\"See if the tuner is alive.\"\"\"\n\n  @abc.abstractmethod\n  def pythia_supporter(", "metadata": [{"fpath_tuple": ["google_vizier", "vizier", "_src", "pyglove", "client.py"], "line_no": 102, "start_line_no": 92, "end_line_no": 112, "window_size": 20, "repo": "google_vizier", "slice_size": 10}], "sim_score": 0.3219178082191781}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# metrics/wiki_split/wiki_split.py\n# --------------------------------------------------\n# # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# # See the License for the specific language governing permissions and\n# # limitations under the License.\n# \"\"\" WIKI_SPLIT metric.\"\"\"\n# \n# import re\n# import string\n# from collections import Counter\n# \n# import datasets\n# import sacrebleu\n# import sacremoses\n# from packaging import version\n# \n# import evaluate\n# \n# \n# _CITATION = \"\"\"\n# @inproceedings{xu-etal-2016-optimizing,\n#     title = {Optimizing Statistical Machine Translation for Text Simplification},\n# --------------------------------------------------\n# the below code fragment can be found in:\n# metrics/cer/cer.py\n# --------------------------------------------------\n# # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# # See the License for the specific language governing permissions and\n# # limitations under the License.\n# \"\"\" Character Error Ratio (CER) metric. \"\"\"\n# \n# from typing import List\n# \n# import datasets\n# import jiwer\n# import jiwer.transforms as tr\n# from datasets.config import PY_VERSION\n# from packaging import version\n# \n# import evaluate\n# \n# \n# if PY_VERSION < version.parse(\"3.8\"):\n#     import importlib_metadata\n# else:\n#     import importlib.metadata as importlib_metadata\n# --------------------------------------------------\n# the below code fragment can be found in:\n# metrics/bertscore/bertscore.py\n# --------------------------------------------------\n# # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# # See the License for the specific language governing permissions and\n# # limitations under the License.\n# \"\"\" BERTScore metric. \"\"\"\n# \n# import functools\n# from contextlib import contextmanager\n# \n# import bert_score\n# import datasets\n# from packaging import version\n# \n# import evaluate\n# \n# \n# @contextmanager\n# def filter_logging_context():\n#     def filter_log(record):\n#         return False if \"This IS expected if you are initializing\" in record.msg else True\n# \n# --------------------------------------------------\n# the below code fragment can be found in:\n# measurements/text_duplicates/text_duplicates.py\n# --------------------------------------------------\n# # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# # See the License for the specific language governing permissions and\n# # limitations under the License.\n# \n# import hashlib\n# from collections import Counter\n# \n# import datasets\n# \n# import evaluate\n# \n# \n# logger = evaluate.logging.get_logger(__name__)\n# \n# _DESCRIPTION = \"\"\"\n# Returns the duplicate fraction of duplicate strings in the input.\n# \"\"\"\n# \n# _KWARGS_DESCRIPTION = \"\"\"\n# Args:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# metrics/bleu/tokenizer_13a.py\n# metrics/google_bleu/tokenizer_13a.py\n# --------------------------------------------------\n# # distributed under the License is distributed on an \"AS IS\" BASIS,\n# # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# # See the License for the specific language governing permissions and\n# # limitations under the License.\n# \n# import re\n# from functools import lru_cache\n# \n# \n# class BaseTokenizer:\n#     \"\"\"A base dummy tokenizer to derive from.\"\"\"\n# \n#     def signature(self):\n#         \"\"\"\n#         Returns a signature for the tokenizer.\n#         :return: signature string\n#         \"\"\"\n#         return \"none\"\n# \n#     def __call__(self, line):\n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\n# Copyright 2021 The HuggingFace Evaluate Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport unittest\n\nfrom cer import CER\n\n\ncer = CER()\n\n\nclass TestCER(unittest.TestCase):\n    def test_cer_case_sensitive(self):\n        refs = [\"White House\"]\n        preds = [\"white house\"]\n        # S = 2, D = 0, I = 0, N = 11, CER = 2 / 11\n        char_error_rate = cer.compute(predictions=preds, references=refs)", "metadata": {"task_id": "huggingface_evaluate/105", "ground_truth": "        self.assertTrue(abs(char_error_rate - 0.1818181818) < 1e-6)", "fpath_tuple": ["huggingface_evaluate", "metrics", "cer", "test_cer.py"], "context_start_lineno": 0, "line_no": 27, "query_window": {"context": "#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport unittest\n\nfrom cer import CER\n\n\ncer = CER()\n\n\nclass TestCER(unittest.TestCase):\n    def test_cer_case_sensitive(self):\n        refs = [\"White House\"]\n        preds = [\"white house\"]\n        # S = 2, D = 0, I = 0, N = 11, CER = 2 / 11\n        char_error_rate = cer.compute(predictions=preds, references=refs)", "metadata": {"fpath_tuple": ["huggingface_evaluate", "metrics", "cer", "test_cer.py"], "line_no": 27, "task_id": "huggingface_evaluate/105", "start_line_no": 7, "end_line_no": 27, "window_size": 20, "context_start_lineno": 0, "repo": "huggingface_evaluate"}}, "top_k_context": [{"context": "#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport re\nfrom functools import lru_cache\n\n\nclass BaseTokenizer:\n    \"\"\"A base dummy tokenizer to derive from.\"\"\"\n\n    def signature(self):\n        \"\"\"\n        Returns a signature for the tokenizer.\n        :return: signature string\n        \"\"\"\n        return \"none\"", "metadata": [{"fpath_tuple": ["huggingface_evaluate", "metrics", "bleu", "tokenizer_13a.py"], "line_no": 18, "start_line_no": 8, "end_line_no": 28, "window_size": 20, "repo": "huggingface_evaluate", "slice_size": 10}, {"fpath_tuple": ["huggingface_evaluate", "metrics", "google_bleu", "tokenizer_13a.py"], "line_no": 18, "start_line_no": 8, "end_line_no": 28, "window_size": 20, "repo": "huggingface_evaluate", "slice_size": 10}], "sim_score": 0.45588235294117646}, {"context": "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport hashlib\nfrom collections import Counter\n\nimport datasets\n\nimport evaluate\n\n\nlogger = evaluate.logging.get_logger(__name__)\n\n_DESCRIPTION = \"\"\"\nReturns the duplicate fraction of duplicate strings in the input.\n\"\"\"\n", "metadata": [{"fpath_tuple": ["huggingface_evaluate", "measurements", "text_duplicates", "text_duplicates.py"], "line_no": 18, "start_line_no": 8, "end_line_no": 28, "window_size": 20, "repo": "huggingface_evaluate", "slice_size": 10}], "sim_score": 0.4461538461538462}, {"context": "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\" BERTScore metric. \"\"\"\n\nimport functools\nfrom contextlib import contextmanager\n\nimport bert_score\nimport datasets\nfrom packaging import version\n\nimport evaluate\n\n\n@contextmanager\ndef filter_logging_context():\n    def filter_log(record):", "metadata": [{"fpath_tuple": ["huggingface_evaluate", "metrics", "bertscore", "bertscore.py"], "line_no": 18, "start_line_no": 8, "end_line_no": 28, "window_size": 20, "repo": "huggingface_evaluate", "slice_size": 10}], "sim_score": 0.43703703703703706}, {"context": "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\" Character Error Ratio (CER) metric. \"\"\"\n\nfrom typing import List\n\nimport datasets\nimport jiwer\nimport jiwer.transforms as tr\nfrom datasets.config import PY_VERSION\nfrom packaging import version\n\nimport evaluate\n\n\nif PY_VERSION < version.parse(\"3.8\"):\n    import importlib_metadata", "metadata": [{"fpath_tuple": ["huggingface_evaluate", "metrics", "cer", "cer.py"], "line_no": 18, "start_line_no": 8, "end_line_no": 28, "window_size": 20, "repo": "huggingface_evaluate", "slice_size": 10}], "sim_score": 0.42857142857142855}, {"context": "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\" WIKI_SPLIT metric.\"\"\"\n\nimport re\nimport string\nfrom collections import Counter\n\nimport datasets\nimport sacrebleu\nimport sacremoses\nfrom packaging import version\n\nimport evaluate\n\n\n_CITATION = \"\"\"", "metadata": [{"fpath_tuple": ["huggingface_evaluate", "metrics", "wiki_split", "wiki_split.py"], "line_no": 18, "start_line_no": 8, "end_line_no": 28, "window_size": 20, "repo": "huggingface_evaluate", "slice_size": 10}], "sim_score": 0.4253731343283582}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_upscale.py\n# --------------------------------------------------\n#                 The prompt or prompts to guide the image generation. If not defined, one has to pass `prompt_embeds`.\n#                 instead.\n#             image (`PIL.Image.Image` or List[`PIL.Image.Image`] or `torch.FloatTensor`):\n#                 `Image`, or tensor representing an image batch which will be upscaled. *\n#             num_inference_steps (`int`, *optional*, defaults to 50):\n#                 The number of denoising steps. More denoising steps usually lead to a higher quality image at the\n#                 expense of slower inference.\n#             guidance_scale (`float`, *optional*, defaults to 7.5):\n#                 Guidance scale as defined in [Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598).\n#                 `guidance_scale` is defined as `w` of equation 2. of [Imagen\n#                 Paper](https://arxiv.org/pdf/2205.11487.pdf). Guidance scale is enabled by setting `guidance_scale >\n#                 1`. Higher guidance scale encourages to generate images that are closely linked to the text `prompt`,\n#                 usually at the expense of lower image quality.\n#             negative_prompt (`str` or `List[str]`, *optional*):\n#                 The prompt or prompts not to guide the image generation. If not defined, one has to pass\n#                 `negative_prompt_embeds`. instead. Ignored when not using guidance (i.e., ignored if `guidance_scale`\n#                 is less than `1`).\n#             num_images_per_prompt (`int`, *optional*, defaults to 1):\n#                 The number of images to generate per prompt.\n#             eta (`float`, *optional*, defaults to 0.0):\n# --------------------------------------------------\n# the below code fragment can be found in:\n# src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_upscale.py\n# --------------------------------------------------\n#         Function invoked when calling the pipeline for generation.\n# \n#         Args:\n#             prompt (`str` or `List[str]`, *optional*):\n#                 The prompt or prompts to guide the image generation. If not defined, one has to pass `prompt_embeds`.\n#                 instead.\n#             image (`PIL.Image.Image` or List[`PIL.Image.Image`] or `torch.FloatTensor`):\n#                 `Image`, or tensor representing an image batch which will be upscaled. *\n#             num_inference_steps (`int`, *optional*, defaults to 50):\n#                 The number of denoising steps. More denoising steps usually lead to a higher quality image at the\n#                 expense of slower inference.\n#             guidance_scale (`float`, *optional*, defaults to 7.5):\n#                 Guidance scale as defined in [Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598).\n#                 `guidance_scale` is defined as `w` of equation 2. of [Imagen\n#                 Paper](https://arxiv.org/pdf/2205.11487.pdf). Guidance scale is enabled by setting `guidance_scale >\n#                 1`. Higher guidance scale encourages to generate images that are closely linked to the text `prompt`,\n#                 usually at the expense of lower image quality.\n#             negative_prompt (`str` or `List[str]`, *optional*):\n#                 The prompt or prompts not to guide the image generation. If not defined, one has to pass\n#                 `negative_prompt_embeds`. instead. Ignored when not using guidance (i.e., ignored if `guidance_scale`\n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\n isinstance(image, PIL.Image.Image):\n        image = [image]\n\n    if isinstance(image[0], PIL.Image.Image):\n        w, h = image[0].size\n        w, h = map(lambda x: x - x % 8, (w, h))  # resize to integer multiple of 8\n\n        image = [np.array(i.resize((w, h), resample=PIL_INTERPOLATION[\"lanczos\"]))[None, :] for i in image]\n        image = np.concatenate(image, axis=0)\n        image = np.array(image).astype(np.float32) / 255.0\n        image = image.transpose(0, 3, 1, 2)\n        image = 2.0 * image - 1.0\n        image = torch.from_numpy(image)\n    elif isinstance(image[0], torch.Tensor):\n        image = torch.cat(image, dim=0)\n    return image\n\n\nclass StableDiffusionInstructPix2PixPipeline(DiffusionPipeline):\n    r\"\"\"\n    Pipeline for pixel-level image editing by following text instructions. Based on Stable Diffusion.\n\n    This model inherits from [`DiffusionPipeline`]. Check the superclass documentation for the generic methods the\n    library implements for all the pipelines (such as downloading or saving, running on a particular device, etc.)\n\n    Args:\n        vae ([`AutoencoderKL`]):\n            Variational Auto-Encoder (VAE) Model to encode and decode images to and from latent representations.\n        text_encoder ([`CLIPTextModel`]):\n            Frozen text-encoder. Stable Diffusion uses the text portion of\n            [CLIP](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModel), specifically\n            the [clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14) variant.\n        tokenizer (`CLIPTokenizer`):\n            Tokenizer of class\n            [CLIPTokenizer](https://huggingface.co/docs/transformers/v4.21.0/en/model_doc/clip#transformers.CLIPTokenizer).\n        unet ([`UNet2DConditionModel`]): Conditional U-Net architecture to denoise the encoded image latents.\n        scheduler ([`SchedulerMixin`]):\n            A scheduler to be used in combination with `unet` to denoise the encoded image latents. Can be one of\n            [`DDIMScheduler`], [`LMSDiscreteScheduler`], or [`PNDMScheduler`].\n        safety_checker ([`StableDiffusionSafetyChecker`]):\n            Classification module that estimates whether generated images could be considered offensive or harmful.\n            Please, refer to the [model card](https://huggingface.co/runwayml/stable-diffusion-v1-5) for details.\n        feature_extractor ([`CLIPFeatureExtractor`]):\n            Model that extracts features from generated images to be used as inputs for the `safety_checker`.\n    \"\"\"\n    _optional_components = [\"safety_checker\", \"feature_extractor\"]\n\n    def __init__(\n        self,\n        vae: AutoencoderKL,\n        text_encoder: CLIPTextModel,\n        tokenizer: CLIPTokenizer,\n        unet: UNet2DConditionModel,\n        scheduler: KarrasDiffusionSchedulers,\n        safety_checker: StableDiffusionSafetyChecker,\n        feature_extractor: CLIPFeatureExtractor,\n        requires_safety_checker: bool = True,\n    ):\n        super().__init__()\n\n        if safety_checker is None and requires_safety_checker:\n            logger.warning(\n                f\"You have disabled the safety checker for {self.__class__} by passing `safety_checker=None`. Ensure\"\n                \" that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered\"\n                \" results in services or applications open to the public. Both the diffusers team and Hugging Face\"\n                \" strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling\"\n                \" it only for use-cases that involve analyzing network behavior or auditing its results. For more\"\n                \" information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\"\n            )\n\n        if safety_checker is not None and feature_extractor is None:\n            raise ValueError(\n                \"Make sure to define a feature extractor when loading {self.__class__} if you want to use the safety\"\n                \" checker. If you do not want to use the safety checker, you can pass `'safety_checker=None'` instead.\"\n            )\n\n        self.register_modules(\n            vae=vae,\n            text_encoder=text_encoder,\n            tokenizer=tokenizer,\n            unet=unet,\n            scheduler=scheduler,\n            safety_checker=safety_checker,\n            feature_extractor=feature_extractor,\n        )\n        self.vae_scale_factor = 2 ** (len(self.vae.config.block_out_channels) - 1)\n        self.register_to_config(requires_safety_checker=requires_safety_checker)\n\n    @torch.no_grad()\n    def __call__(\n        self,\n        prompt: Union[str, List[str]] = None,\n        image: Union[torch.FloatTensor, PIL.Image.Image] = None,\n        num_inference_steps: int = 100,\n        guidance_scale: float = 7.5,\n        image_guidance_scale: float = 1.5,\n        negative_prompt: Optional[Union[str, List[str]]] = None,\n        num_images_per_prompt: Optional[int] = 1,\n        eta: float = 0.0,\n        generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,\n        latents: Optional[torch.FloatTensor] = None,\n        prompt_embeds: Optional[torch.FloatTensor] = None,\n        negative_prompt_embeds: Optional[torch.FloatTensor] = None,\n        output_type: Optional[str] = \"pil\",\n        return_dict: bool = True,\n        callback: Optional[Callable[[int, int, torch.FloatTensor], None]] = None,\n        callback_steps: Optional[int] = 1,\n    ):\n        r\"\"\"\n        Function invoked when calling the pipeline for generation.\n\n        Args:\n             prompt (`str` or `List[str]`, *optional*):\n                The prompt or prompts to guide the image generation. If not defined, one has to pass `prompt_embeds`.\n                instead.\n            image (`PIL.Image.Image`):\n                `Image`, or tensor representing an image batch which will be repainted according to `prompt`.\n            num_inference_steps (`int`, *optional*, defaults to 100):\n                The number of denoising steps. More denoising steps usually lead to a higher quality image at the\n                expense of slower inference.\n            guidance_scale (`float`, *optional*, defaults to 7.5):\n                Guidance scale as defined in [Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598).\n                `guidance_scale` is defined as `w` of equation 2. of [Imagen\n                Paper](https://arxiv.org/pdf/2205.11487.pdf). Guidance scale is enabled by setting `guidance_scale >\n                1`. Higher guidance scale encourages to generate images that are closely linked to the text `prompt`,\n                usually at the expense of lower image quality. This pipeline requires a value of at least `1`.\n            image_guidance_scale (`float`, *optional*, defaults to 1.5):\n                Image guidance scale is to push the generated image towards the inital image `image`. Image guidance\n                scale is enabled by setting `image_guidance_scale > 1`. Higher image guidance scale encourages to\n                generate images that are closely linked to the source image `image`, usually at the expense of lower", "metadata": {"task_id": "huggingface_diffusers/130", "ground_truth": "                image quality. This pipeline requires a value of at least `1`.", "fpath_tuple": ["huggingface_diffusers", "src", "diffusers", "pipelines", "stable_diffusion", "pipeline_stable_diffusion_instruct_pix2pix.py"], "context_start_lineno": 38, "line_no": 168, "query_window": {"context": "\n        Args:\n             prompt (`str` or `List[str]`, *optional*):\n                The prompt or prompts to guide the image generation. If not defined, one has to pass `prompt_embeds`.\n                instead.\n            image (`PIL.Image.Image`):\n                `Image`, or tensor representing an image batch which will be repainted according to `prompt`.\n            num_inference_steps (`int`, *optional*, defaults to 100):\n                The number of denoising steps. More denoising steps usually lead to a higher quality image at the\n                expense of slower inference.\n            guidance_scale (`float`, *optional*, defaults to 7.5):\n                Guidance scale as defined in [Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598).\n                `guidance_scale` is defined as `w` of equation 2. of [Imagen\n                Paper](https://arxiv.org/pdf/2205.11487.pdf). Guidance scale is enabled by setting `guidance_scale >\n                1`. Higher guidance scale encourages to generate images that are closely linked to the text `prompt`,\n                usually at the expense of lower image quality. This pipeline requires a value of at least `1`.\n            image_guidance_scale (`float`, *optional*, defaults to 1.5):\n                Image guidance scale is to push the generated image towards the inital image `image`. Image guidance\n                scale is enabled by setting `image_guidance_scale > 1`. Higher image guidance scale encourages to\n                generate images that are closely linked to the source image `image`, usually at the expense of lower", "metadata": {"fpath_tuple": ["huggingface_diffusers", "src", "diffusers", "pipelines", "stable_diffusion", "pipeline_stable_diffusion_instruct_pix2pix.py"], "line_no": 168, "task_id": "huggingface_diffusers/130", "start_line_no": 148, "end_line_no": 168, "window_size": 20, "context_start_lineno": 38, "repo": "huggingface_diffusers"}}, "top_k_context": [{"context": "    ):\n        r\"\"\"\n        Function invoked when calling the pipeline for generation.\n\n        Args:\n            prompt (`str` or `List[str]`, *optional*):\n                The prompt or prompts to guide the image generation. If not defined, one has to pass `prompt_embeds`.\n                instead.\n            image (`PIL.Image.Image` or List[`PIL.Image.Image`] or `torch.FloatTensor`):\n                `Image`, or tensor representing an image batch which will be upscaled. *\n            num_inference_steps (`int`, *optional*, defaults to 50):\n                The number of denoising steps. More denoising steps usually lead to a higher quality image at the\n                expense of slower inference.\n            guidance_scale (`float`, *optional*, defaults to 7.5):\n                Guidance scale as defined in [Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598).\n                `guidance_scale` is defined as `w` of equation 2. of [Imagen\n                Paper](https://arxiv.org/pdf/2205.11487.pdf). Guidance scale is enabled by setting `guidance_scale >\n                1`. Higher guidance scale encourages to generate images that are closely linked to the text `prompt`,\n                usually at the expense of lower image quality.\n            negative_prompt (`str` or `List[str]`, *optional*):", "metadata": [{"fpath_tuple": ["huggingface_diffusers", "src", "diffusers", "pipelines", "stable_diffusion", "pipeline_stable_diffusion_upscale.py"], "line_no": 402, "start_line_no": 392, "end_line_no": 412, "window_size": 20, "repo": "huggingface_diffusers", "slice_size": 10}], "sim_score": 0.7816091954022989}, {"context": "        Args:\n            prompt (`str` or `List[str]`, *optional*):\n                The prompt or prompts to guide the image generation. If not defined, one has to pass `prompt_embeds`.\n                instead.\n            image (`PIL.Image.Image` or List[`PIL.Image.Image`] or `torch.FloatTensor`):\n                `Image`, or tensor representing an image batch which will be upscaled. *\n            num_inference_steps (`int`, *optional*, defaults to 50):\n                The number of denoising steps. More denoising steps usually lead to a higher quality image at the\n                expense of slower inference.\n            guidance_scale (`float`, *optional*, defaults to 7.5):\n                Guidance scale as defined in [Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598).\n                `guidance_scale` is defined as `w` of equation 2. of [Imagen\n                Paper](https://arxiv.org/pdf/2205.11487.pdf). Guidance scale is enabled by setting `guidance_scale >\n                1`. Higher guidance scale encourages to generate images that are closely linked to the text `prompt`,\n                usually at the expense of lower image quality.\n            negative_prompt (`str` or `List[str]`, *optional*):\n                The prompt or prompts not to guide the image generation. If not defined, one has to pass\n                `negative_prompt_embeds`. instead. Ignored when not using guidance (i.e., ignored if `guidance_scale`\n                is less than `1`).\n            num_images_per_prompt (`int`, *optional*, defaults to 1):", "metadata": [{"fpath_tuple": ["huggingface_diffusers", "src", "diffusers", "pipelines", "stable_diffusion", "pipeline_stable_diffusion_upscale.py"], "line_no": 406, "start_line_no": 396, "end_line_no": 416, "window_size": 20, "repo": "huggingface_diffusers", "slice_size": 10}], "sim_score": 0.7640449438202247}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# fortuna/calib_model/calib_model_calibrator.py\n# --------------------------------------------------\n#         aux: Dict[str, Any],\n#         targets: Array,\n#         metrics: Optional[Tuple[Callable[[jnp.ndarray, Array], Array], ...]],\n#     ) -> Dict[str, jnp.ndarray]:\n#         training_losses_and_metrics = super(MultiDeviceMixin, self).training_step_end(\n#             current_epoch, state, aux, targets, metrics\n#         )\n#         return tree_map(lambda x: x.mean(), training_losses_and_metrics)\n# \n#     def on_val_start(self, state: CalibState) -> CalibState:\n#         state = super(MultiDeviceMixin, self).on_val_start(state)\n#         if state.mutable[\"output_calibrator\"] is not None:\n#             state = self.sync_mutable(state)\n#         return state\n# \n#     @partial(jax.pmap, axis_name=\"batch\", static_broadcasted_argnums=(0, 4))\n#     def val_loss_step(\n#         self,\n#         state: CalibState,\n#         targets: Array,\n# --------------------------------------------------\n# the below code fragment can be found in:\n# fortuna/calib_model/calib_model_calibrator.py\n# --------------------------------------------------\n#         metrics: Optional[Tuple[Callable[[jnp.ndarray, Array], Array], ...]],\n#     ) -> Dict[str, jnp.ndarray]:\n#         training_losses_and_metrics = super(MultiDeviceMixin, self).training_step_end(\n#             current_epoch, state, aux, targets, metrics\n#         )\n#         return tree_map(lambda x: x.mean(), training_losses_and_metrics)\n# \n#     def on_val_start(self, state: CalibState) -> CalibState:\n#         state = super(MultiDeviceMixin, self).on_val_start(state)\n#         if state.mutable[\"output_calibrator\"] is not None:\n#             state = self.sync_mutable(state)\n#         return state\n# \n#     @partial(jax.pmap, axis_name=\"batch\", static_broadcasted_argnums=(0, 4))\n#     def val_loss_step(\n#         self,\n#         state: CalibState,\n#         targets: Array,\n#         outputs: Array,\n#         fun: Callable,\n# --------------------------------------------------\n# the below code fragment can be found in:\n# fortuna/calib_model/calib_model_calibrator.py\n# --------------------------------------------------\n#         training_losses_and_metrics = super(MultiDeviceMixin, self).training_step_end(\n#             current_epoch, state, aux, targets, metrics\n#         )\n#         return tree_map(lambda x: x.mean(), training_losses_and_metrics)\n# \n#     def on_val_start(self, state: CalibState) -> CalibState:\n#         state = super(MultiDeviceMixin, self).on_val_start(state)\n#         if state.mutable[\"output_calibrator\"] is not None:\n#             state = self.sync_mutable(state)\n#         return state\n# \n#     @partial(jax.pmap, axis_name=\"batch\", static_broadcasted_argnums=(0, 4))\n#     def val_loss_step(\n#         self,\n#         state: CalibState,\n#         targets: Array,\n#         outputs: Array,\n#         fun: Callable,\n#         rng: PRNGKeyArray,\n#     ) -> Dict[str, jnp.ndarray]:\n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\n\n        outputs: Array,\n        fun: Callable,\n        rng: PRNGKeyArray,\n        n_data: int,\n    ) -> Tuple[CalibState, Dict[str, Any]]:\n        return super().training_step(state, batch, outputs, fun, rng, n_data)\n\n    @partial(jax.jit, static_argnums=(0, 4, 6))\n    def val_loss_step(\n        self,\n        state: CalibState,\n        batch: Batch,\n        outputs: Array,\n        fun: Callable,\n        rng: PRNGKeyArray,\n        n_data: int,\n    ) -> Dict[str, jnp.ndarray]:\n        return super().val_loss_step(state, batch, outputs, fun, rng, n_data)\n\n\nclass MultiDeviceMixin:\n    all_reduce_mean = jax.pmap(lambda x: lax.pmean(x, \"x\"), \"x\")\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.multi_device = True\n\n    @staticmethod\n    def _add_device_dim_to_data_loader(data_loader: DataLoader) -> DataLoader:\n        def _reshape_batch(batch):\n            n_devices = jax.local_device_count()\n            if batch.shape[0] % n_devices != 0:\n                raise ValueError(\n                    f\"The size of all batches must be a multiple of {n_devices}, that is the number of \"\n                    f\"available devices. However, a batch with shape {batch.shape[0]} was found. \"\n                    f\"Please set an appropriate batch size.\"\n                )\n            return batch.reshape((n_devices, -1) + batch.shape[1:])\n\n        class DataLoaderWrapper:\n            def __init__(self, data_loader: DataLoader):\n                self._data_loader = data_loader\n\n            def __iter__(self):\n                data_loader = map(\n                    lambda batch: tree_map(_reshape_batch, batch), self._data_loader\n                )\n                data_loader = jax_utils.prefetch_to_device(data_loader, 2)\n                yield from data_loader\n\n        return (\n            DataLoaderWrapper(data_loader) if data_loader is not None else data_loader\n        )\n\n    @staticmethod\n    def _add_device_dim_to_outputs_loader(\n        outputs_loader: TargetsLoader,\n    ) -> TargetsLoader:\n        def _reshape_batch(batch):\n            n_devices = jax.local_device_count()\n            if batch.shape[0] % n_devices != 0:\n                raise ValueError(\n                    f\"The size of all output batches must be a multiple of {n_devices}, that is the number of \"\n                    f\"available devices. However, a batch of outputs with shape {batch.shape[0]} was found. \"\n                    f\"Please set an appropriate batch size.\"\n                )\n            return batch.reshape((n_devices, -1) + batch.shape[1:])\n\n        class TargetsLoaderWrapper:\n            def __init__(self, outputs_loader: TargetsLoader):\n                self._outputs_loader = outputs_loader\n\n            def __iter__(self):\n                outputs_loader = map(\n                    lambda batch: tree_map(_reshape_batch, batch), self._outputs_loader\n                )\n                outputs_loader = jax_utils.prefetch_to_device(outputs_loader, 2)\n                yield from outputs_loader\n\n        return (\n            TargetsLoaderWrapper(outputs_loader)\n            if outputs_loader is not None\n            else outputs_loader\n        )\n\n    @staticmethod\n    def sync_mutable(state: CalibState) -> CalibState:\n        return (\n            state.replace(mutable=MultiDeviceMixin.all_reduce_mean(state.mutable))\n            if state.mutable[\"output_calibrator\"] is not None\n            else state\n        )\n\n    @staticmethod\n    def sync_gradients_and_loss(\n        grads: jnp.ndarray, loss: jnp.ndarray\n    ) -> Tuple[jnp.ndarray, jnp.ndarray]:\n        grad = lax.pmean(grads, axis_name=\"batch\")\n        loss = lax.pmean(loss, axis_name=\"batch\")\n        return grad, loss\n\n    def save_checkpoint(\n        self,\n        state: CalibState,\n        save_checkpoint_dir: Path,\n        keep: int = 1,\n        force_save: bool = False,\n        prefix: str = \"checkpoint_\",\n    ) -> None:\n        state = self.sync_mutable(state)\n        state = jax.device_get(tree_map(lambda x: x[0], state))\n        return super(MultiDeviceMixin, self).save_checkpoint(\n            state, save_checkpoint_dir, keep, force_save, prefix\n        )\n\n    def on_train_start(\n        self,\n        state: CalibState,\n        data_loaders: List[DataLoader],\n        outputs_loaders: List[TargetsLoader],\n        rng: PRNGKeyArray,\n    ) -> Tuple[CalibState, List[DataLoader], List[TargetsLoader], PRNGKeyArray]:\n        state, data_loaders, outputs_loaders, rng = super(\n            MultiDeviceMixin, self\n        ).on_train_start(state, data_loaders, outputs_loaders, rng)\n        state = jax_utils.replicate(state)\n        data_loaders = [\n            self._add_device_dim_to_data_loader(dl) if dl is not None else dl\n            for dl in data_loaders\n        ]\n        outputs_loaders = [\n            self._add_device_dim_to_outputs_loader(ol) if ol is not None else ol\n            for ol in outputs_loaders\n        ]\n        model_key = random.split(rng, jax.local_device_count())\n        return state, data_loaders, outputs_loaders, model_key\n\n    def on_train_end(self, state: CalibState) -> CalibState:\n        state = super(MultiDeviceMixin, self).on_train_end(state)\n        return jax.device_get(tree_map(lambda x: x[0], state))\n\n    @partial(jax.pmap, axis_name=\"batch\", static_broadcasted_argnums=(0, 4, 6))\n    def training_step(\n        self,\n        state: CalibState,\n        batch: Batch,\n        outputs: Array,\n        fun: Callable,\n        rng: PRNGKeyArray,\n        n_data: int,\n    ) -> Tuple[CalibState, Dict[str, Any]]:\n        return super().training_step(state, batch, outputs, fun, rng, n_data)\n\n    def training_step_end(\n        self,\n        current_epoch: int,\n        state: CalibState,\n        aux: Dict[str, Any],\n        batch: Batch,\n        metrics: Optional[Tuple[Callable[[jnp.ndarray, Array], float], ...]],\n    ) -> Dict[str, jnp.ndarray]:\n        training_losses_and_metrics = super(MultiDeviceMixin, self).training_step_end(\n            current_epoch, state, aux, batch, metrics\n        )\n        return tree_map(lambda x: x.mean(), training_losses_and_metrics)\n\n    def on_val_start(self, state: CalibState) -> CalibState:\n        state = super(MultiDeviceMixin, self).on_val_start(state)\n        if state.mutable[\"output_calibrator\"] is not None:\n            state = self.sync_mutable(state)\n        return state\n\n    @partial(jax.pmap, axis_name=\"batch\", static_broadcasted_argnums=(0, 4, 6))\n    def val_loss_step(\n        self,\n        state: CalibState,\n        batch: Batch,\n        outputs: Array,", "metadata": {"task_id": "awslabs_fortuna/53", "ground_truth": "        fun: Callable,", "fpath_tuple": ["awslabs_fortuna", "fortuna", "calibration", "calibrator.py"], "context_start_lineno": 468, "line_no": 647, "query_window": {"context": "        batch: Batch,\n        metrics: Optional[Tuple[Callable[[jnp.ndarray, Array], float], ...]],\n    ) -> Dict[str, jnp.ndarray]:\n        training_losses_and_metrics = super(MultiDeviceMixin, self).training_step_end(\n            current_epoch, state, aux, batch, metrics\n        )\n        return tree_map(lambda x: x.mean(), training_losses_and_metrics)\n\n    def on_val_start(self, state: CalibState) -> CalibState:\n        state = super(MultiDeviceMixin, self).on_val_start(state)\n        if state.mutable[\"output_calibrator\"] is not None:\n            state = self.sync_mutable(state)\n        return state\n\n    @partial(jax.pmap, axis_name=\"batch\", static_broadcasted_argnums=(0, 4, 6))\n    def val_loss_step(\n        self,\n        state: CalibState,\n        batch: Batch,\n        outputs: Array,", "metadata": {"fpath_tuple": ["awslabs_fortuna", "fortuna", "calibration", "calibrator.py"], "line_no": 647, "task_id": "awslabs_fortuna/53", "start_line_no": 627, "end_line_no": 647, "window_size": 20, "context_start_lineno": 468, "repo": "awslabs_fortuna"}}, "top_k_context": [{"context": "        metrics: Optional[Tuple[Callable[[jnp.ndarray, Array], Array], ...]],\n    ) -> Dict[str, jnp.ndarray]:\n        training_losses_and_metrics = super(MultiDeviceMixin, self).training_step_end(\n            current_epoch, state, aux, targets, metrics\n        )\n        return tree_map(lambda x: x.mean(), training_losses_and_metrics)\n\n    def on_val_start(self, state: CalibState) -> CalibState:\n        state = super(MultiDeviceMixin, self).on_val_start(state)\n        if state.mutable[\"output_calibrator\"] is not None:\n            state = self.sync_mutable(state)\n        return state\n\n    @partial(jax.pmap, axis_name=\"batch\", static_broadcasted_argnums=(0, 4))\n    def val_loss_step(\n        self,\n        state: CalibState,\n        targets: Array,\n        outputs: Array,\n        fun: Callable,", "metadata": [{"fpath_tuple": ["awslabs_fortuna", "fortuna", "calib_model", "calib_model_calibrator.py"], "line_no": 598, "start_line_no": 588, "end_line_no": 608, "window_size": 20, "repo": "awslabs_fortuna", "slice_size": 10}], "sim_score": 0.9316239316239316}, {"context": "        aux: Dict[str, Any],\n        targets: Array,\n        metrics: Optional[Tuple[Callable[[jnp.ndarray, Array], Array], ...]],\n    ) -> Dict[str, jnp.ndarray]:\n        training_losses_and_metrics = super(MultiDeviceMixin, self).training_step_end(\n            current_epoch, state, aux, targets, metrics\n        )\n        return tree_map(lambda x: x.mean(), training_losses_and_metrics)\n\n    def on_val_start(self, state: CalibState) -> CalibState:\n        state = super(MultiDeviceMixin, self).on_val_start(state)\n        if state.mutable[\"output_calibrator\"] is not None:\n            state = self.sync_mutable(state)\n        return state\n\n    @partial(jax.pmap, axis_name=\"batch\", static_broadcasted_argnums=(0, 4))\n    def val_loss_step(\n        self,\n        state: CalibState,\n        targets: Array,", "metadata": [{"fpath_tuple": ["awslabs_fortuna", "fortuna", "calib_model", "calib_model_calibrator.py"], "line_no": 596, "start_line_no": 586, "end_line_no": 606, "window_size": 20, "repo": "awslabs_fortuna", "slice_size": 10}], "sim_score": 0.9310344827586207}, {"context": "        current_epoch: int,\n        state: CalibState,\n        aux: Dict[str, Any],\n        targets: Array,\n        metrics: Optional[Tuple[Callable[[jnp.ndarray, Array], Array], ...]],\n    ) -> Dict[str, jnp.ndarray]:\n        training_losses_and_metrics = super(MultiDeviceMixin, self).training_step_end(\n            current_epoch, state, aux, targets, metrics\n        )\n        return tree_map(lambda x: x.mean(), training_losses_and_metrics)\n\n    def on_val_start(self, state: CalibState) -> CalibState:\n        state = super(MultiDeviceMixin, self).on_val_start(state)\n        if state.mutable[\"output_calibrator\"] is not None:\n            state = self.sync_mutable(state)\n        return state\n\n    @partial(jax.pmap, axis_name=\"batch\", static_broadcasted_argnums=(0, 4))\n    def val_loss_step(\n        self,", "metadata": [{"fpath_tuple": ["awslabs_fortuna", "fortuna", "calib_model", "calib_model_calibrator.py"], "line_no": 594, "start_line_no": 584, "end_line_no": 604, "window_size": 20, "repo": "awslabs_fortuna", "slice_size": 10}], "sim_score": 0.9230769230769231}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# ding/torch_utils/tests/test_data_helper.py\n# --------------------------------------------------\n#         tlist = [torch.randn(3, 5), torch.randn(4, 5)]\n#         assert not same_shape(tlist)\n# \n#     def test_get_tensor_data(self):\n#         a = {\n#             'tensor': torch.tensor([1, 2, 3.], requires_grad=True),\n#             'list': [torch.tensor([1, 2, 3.], requires_grad=True) for _ in range(2)],\n#             'none': None\n#         }\n#         tensor_a = get_tensor_data(a)\n#         assert not tensor_a['tensor'].requires_grad\n#         for t in tensor_a['list']:\n#             assert not t.requires_grad\n#         with pytest.raises(TypeError):\n#             get_tensor_data(EasyTimer())\n# \n# \n# @pytest.mark.unittest\n# def test_log_dict():\n#     log_buffer = build_log_buffer()\n# --------------------------------------------------\n# the below code fragment can be found in:\n# ding/torch_utils/tests/test_data_helper.py\n# --------------------------------------------------\n#         a = {\n#             'tensor': torch.tensor([1, 2, 3.], requires_grad=True),\n#             'list': [torch.tensor([1, 2, 3.], requires_grad=True) for _ in range(2)],\n#             'none': None\n#         }\n#         tensor_a = get_tensor_data(a)\n#         assert not tensor_a['tensor'].requires_grad\n#         for t in tensor_a['list']:\n#             assert not t.requires_grad\n#         with pytest.raises(TypeError):\n#             get_tensor_data(EasyTimer())\n# \n# \n# @pytest.mark.unittest\n# def test_log_dict():\n#     log_buffer = build_log_buffer()\n#     log_buffer['not_tensor'] = torch.randn(3)\n#     assert isinstance(log_buffer['not_tensor'], list)\n#     assert len(log_buffer['not_tensor']) == 3\n#     log_buffer.update({'not_tensor': 4, 'a': 5})\n# --------------------------------------------------\n# the below code fragment can be found in:\n# ding/torch_utils/tests/test_data_helper.py\n# --------------------------------------------------\n#             'list': [torch.tensor([1, 2, 3.], requires_grad=True) for _ in range(2)],\n#             'none': None\n#         }\n#         tensor_a = get_tensor_data(a)\n#         assert not tensor_a['tensor'].requires_grad\n#         for t in tensor_a['list']:\n#             assert not t.requires_grad\n#         with pytest.raises(TypeError):\n#             get_tensor_data(EasyTimer())\n# \n# \n# @pytest.mark.unittest\n# def test_log_dict():\n#     log_buffer = build_log_buffer()\n#     log_buffer['not_tensor'] = torch.randn(3)\n#     assert isinstance(log_buffer['not_tensor'], list)\n#     assert len(log_buffer['not_tensor']) == 3\n#     log_buffer.update({'not_tensor': 4, 'a': 5})\n#     assert log_buffer['not_tensor'] == 4\n# \n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\nimport pytest\nfrom collections import namedtuple\nimport random\nimport numpy as np\nimport torch\nfrom ding.utils.data import timestep_collate, default_collate, default_decollate, diff_shape_collate\n\nB, T = 4, 3\n\n\n@pytest.mark.unittest\nclass TestTimestepCollate:\n\n    def get_data(self):\n        data = {\n            'obs': [torch.randn(4) for _ in range(T)],\n            'reward': [torch.FloatTensor([0]) for _ in range(T)],\n            'done': [False for _ in range(T)],\n            'prev_state': [(torch.randn(3), torch.randn(3)) for _ in range(T)],\n            'action': [[torch.randn(3), torch.randn(5)] for _ in range(T)],\n        }\n        return data\n\n    def get_multi_shape_state_data(self):\n        data = {\n            'obs': [torch.randn(4) for _ in range(T)],\n            'reward': [torch.FloatTensor([0]) for _ in range(T)],\n            'done': [False for _ in range(T)],\n            'prev_state': [\n                [(torch.randn(3), torch.randn(5)), (torch.randn(4), ), (torch.randn(5), torch.randn(6))]\n                for _ in range(T)\n            ],\n            'action': [[torch.randn(3), torch.randn(5)] for _ in range(T)],\n        }\n        return data\n\n    def test(self):\n        batch = timestep_collate([self.get_data() for _ in range(B)])\n        assert isinstance(batch, dict)\n        assert set(batch.keys()) == set(['obs', 'reward', 'done', 'prev_state', 'action'])\n        assert batch['obs'].shape == (T, B, 4)\n        assert batch['reward'].shape == (T, B)\n        assert batch['done'].shape == (T, B) and batch['done'].dtype == torch.bool\n        assert isinstance(batch['prev_state'], list)\n        assert len(batch['prev_state']) == T and len(batch['prev_state'][0]) == B\n        assert isinstance(batch['action'], list) and len(batch['action']) == T\n        assert batch['action'][0][0].shape == (B, 3)\n        assert batch['action'][0][1].shape == (B, 5)\n\n        # hidden_state might contain multi prev_states with different shapes\n        batch = timestep_collate([self.get_multi_shape_state_data() for _ in range(B)])\n        assert isinstance(batch, dict)\n        assert set(batch.keys()) == set(['obs', 'reward', 'done', 'prev_state', 'action'])\n        assert batch['obs'].shape == (T, B, 4)\n        assert batch['reward'].shape == (T, B)\n        assert batch['done'].shape == (T, B) and batch['done'].dtype == torch.bool\n        assert isinstance(batch['prev_state'], list)\n        print(batch['prev_state'][0][0])\n        assert len(batch['prev_state']) == T and len(batch['prev_state'][0]\n                                                     ) == B and len(batch['prev_state'][0][0]) == 3\n        assert isinstance(batch['action'], list) and len(batch['action']) == T\n        assert batch['action'][0][0].shape == (B, 3)\n        assert batch['action'][0][1].shape == (B, 5)\n\n\n@pytest.mark.unittest\nclass TestDefaultCollate:\n\n    def test_numpy(self):\n        data = [np.random.randn(4, 3).astype(np.float64) for _ in range(5)]\n        data = default_collate(data)\n        assert data.shape == (5, 4, 3)\n        assert data.dtype == torch.float64\n        data = [float(np.random.randn(1)[0]) for _ in range(6)]\n        data = default_collate(data)\n        assert data.shape == (6, )\n        assert data.dtype == torch.float32\n        with pytest.raises(TypeError):\n            default_collate([np.array(['str']) for _ in range(3)])\n\n    def test_basic(self):\n        data = [random.random() for _ in range(3)]\n        data = default_collate(data)\n        assert data.shape == (3, )\n        assert data.dtype == torch.float32\n        data = [random.randint(0, 10) for _ in range(3)]\n        data = default_collate(data)\n        assert data.shape == (3, )\n        assert data.dtype == torch.int64\n        data = ['str' for _ in range(4)]\n        data = default_collate(data)\n        assert len(data) == 4\n        assert all([s == 'str' for s in data])\n        T = namedtuple('T', ['x', 'y'])\n        data = [T(1, 2) for _ in range(4)]\n        data = default_collate(data)\n        assert isinstance(data, T)\n        assert data.x.shape == (4, ) and data.x.eq(1).sum() == 4\n        assert data.y.shape == (4, ) and data.y.eq(2).sum() == 4\n        with pytest.raises(TypeError):\n            default_collate([object() for _ in range(4)])\n\n\n@pytest.mark.unittest\nclass TestDefaultDecollate:\n\n    def test(self):\n        with pytest.raises(TypeError):\n            default_decollate([object() for _ in range(4)])\n        data = torch.randn(4, 3, 5)\n        data = default_decollate(data)\n        print([d.shape for d in data])\n        assert len(data) == 4 and all([d.shape == (3, 5) for d in data])\n        data = [torch.randn(8, 2, 4), torch.randn(8, 5)]\n        data = default_decollate(data)\n        assert len(data) == 8 and all([d[0].shape == (2, 4) and d[1].shape == (5, ) for d in data])\n        data = {\n            'logit': torch.randn(4, 13),\n            'action': torch.randint(0, 13, size=(4, )),\n            'prev_state': [(torch.zeros(3, 1, 12), torch.zeros(3, 1, 12)) for _ in range(4)],\n        }\n        data = default_decollate(data)\n        assert len(data) == 4 and isinstance(data, list)\n        assert all([d['logit'].shape == (13, ) for d in data])\n        assert all([d['action'].shape == (1, ) for d in data])\n        assert all([len(d['prev_state']) == 2 and d['prev_state'][0].shape == (3, 1, 12) for d in data])\n\n\n@pytest.mark.unittest\nclass TestDiffShapeCollate:\n\n    def test(self):\n        with pytest.raises(TypeError):\n            diff_shape_collate([object() for _ in range(4)])\n        data = [\n            {\n                'item1': torch.randn(4),\n                'item2': None,\n                'item3': torch.randn(3),\n                'item4': np.random.randn(5, 6)", "metadata": {"task_id": "opendilab_ACE/170", "ground_truth": "            },", "fpath_tuple": ["opendilab_ACE", "ding", "utils", "data", "tests", "test_collate_fn.py"], "context_start_lineno": 0, "line_no": 140, "query_window": {"context": "        }\n        data = default_decollate(data)\n        assert len(data) == 4 and isinstance(data, list)\n        assert all([d['logit'].shape == (13, ) for d in data])\n        assert all([d['action'].shape == (1, ) for d in data])\n        assert all([len(d['prev_state']) == 2 and d['prev_state'][0].shape == (3, 1, 12) for d in data])\n\n\n@pytest.mark.unittest\nclass TestDiffShapeCollate:\n\n    def test(self):\n        with pytest.raises(TypeError):\n            diff_shape_collate([object() for _ in range(4)])\n        data = [\n            {\n                'item1': torch.randn(4),\n                'item2': None,\n                'item3': torch.randn(3),\n                'item4': np.random.randn(5, 6)", "metadata": {"fpath_tuple": ["opendilab_ACE", "ding", "utils", "data", "tests", "test_collate_fn.py"], "line_no": 140, "task_id": "opendilab_ACE/170", "start_line_no": 120, "end_line_no": 140, "window_size": 20, "context_start_lineno": 0, "repo": "opendilab_ACE"}}, "top_k_context": [{"context": "        a = {\n            'tensor': torch.tensor([1, 2, 3.], requires_grad=True),\n            'list': [torch.tensor([1, 2, 3.], requires_grad=True) for _ in range(2)],\n            'none': None\n        }\n        tensor_a = get_tensor_data(a)\n        assert not tensor_a['tensor'].requires_grad\n        for t in tensor_a['list']:\n            assert not t.requires_grad\n        with pytest.raises(TypeError):\n            get_tensor_data(EasyTimer())\n\n\n@pytest.mark.unittest\ndef test_log_dict():\n    log_buffer = build_log_buffer()\n    log_buffer['not_tensor'] = torch.randn(3)\n    assert isinstance(log_buffer['not_tensor'], list)\n    assert len(log_buffer['not_tensor']) == 3\n    log_buffer.update({'not_tensor': 4, 'a': 5})", "metadata": [{"fpath_tuple": ["opendilab_ACE", "ding", "torch_utils", "tests", "test_data_helper.py"], "line_no": 132, "start_line_no": 122, "end_line_no": 142, "window_size": 20, "repo": "opendilab_ACE", "slice_size": 10}], "sim_score": 0.43703703703703706}, {"context": "\n    def test_get_tensor_data(self):\n        a = {\n            'tensor': torch.tensor([1, 2, 3.], requires_grad=True),\n            'list': [torch.tensor([1, 2, 3.], requires_grad=True) for _ in range(2)],\n            'none': None\n        }\n        tensor_a = get_tensor_data(a)\n        assert not tensor_a['tensor'].requires_grad\n        for t in tensor_a['list']:\n            assert not t.requires_grad\n        with pytest.raises(TypeError):\n            get_tensor_data(EasyTimer())\n\n\n@pytest.mark.unittest\ndef test_log_dict():\n    log_buffer = build_log_buffer()\n    log_buffer['not_tensor'] = torch.randn(3)\n    assert isinstance(log_buffer['not_tensor'], list)", "metadata": [{"fpath_tuple": ["opendilab_ACE", "ding", "torch_utils", "tests", "test_data_helper.py"], "line_no": 130, "start_line_no": 120, "end_line_no": 140, "window_size": 20, "repo": "opendilab_ACE", "slice_size": 10}], "sim_score": 0.4318181818181818}, {"context": "        tlist = [torch.randn(3, 5) for i in range(5)]\n        assert same_shape(tlist)\n        tlist = [torch.randn(3, 5), torch.randn(4, 5)]\n        assert not same_shape(tlist)\n\n    def test_get_tensor_data(self):\n        a = {\n            'tensor': torch.tensor([1, 2, 3.], requires_grad=True),\n            'list': [torch.tensor([1, 2, 3.], requires_grad=True) for _ in range(2)],\n            'none': None\n        }\n        tensor_a = get_tensor_data(a)\n        assert not tensor_a['tensor'].requires_grad\n        for t in tensor_a['list']:\n            assert not t.requires_grad\n        with pytest.raises(TypeError):\n            get_tensor_data(EasyTimer())\n\n\n@pytest.mark.unittest", "metadata": [{"fpath_tuple": ["opendilab_ACE", "ding", "torch_utils", "tests", "test_data_helper.py"], "line_no": 126, "start_line_no": 116, "end_line_no": 136, "window_size": 20, "repo": "opendilab_ACE", "slice_size": 10}], "sim_score": 0.4296875}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# metrics/mean_iou/mean_iou.py\n# --------------------------------------------------\n#     total_area_pred_label = np.zeros((num_labels,), dtype=np.float64)\n#     total_area_label = np.zeros((num_labels,), dtype=np.float64)\n#     for result, gt_seg_map in zip(results, gt_seg_maps):\n#         area_intersect, area_union, area_pred_label, area_label = intersect_and_union(\n#             result, gt_seg_map, num_labels, ignore_index, label_map, reduce_labels\n#         )\n#         total_area_intersect += area_intersect\n#         total_area_union += area_union\n#         total_area_pred_label += area_pred_label\n#         total_area_label += area_label\n#     return total_area_intersect, total_area_union, total_area_pred_label, total_area_label\n# \n# \n# def mean_iou(\n#     results,\n#     gt_seg_maps,\n#     num_labels,\n#     ignore_index: bool,\n#     nan_to_num: Optional[int] = None,\n#     label_map: Optional[Dict[int, int]] = None,\n# --------------------------------------------------\n# the below code fragment can be found in:\n# metrics/cuad/compute_score.py\n# --------------------------------------------------\n# def get_prec_at_recall(precisions, recalls, recall_thresh):\n#     \"\"\"Assumes recalls are sorted in increasing order\"\"\"\n#     processed_precisions = process_precisions(precisions)\n#     prec_at_recall = 0\n#     for prec, recall in zip(processed_precisions, recalls):\n#         if recall >= recall_thresh:\n#             prec_at_recall = prec\n#             break\n#     return prec_at_recall\n# \n# \n# def exact_match_score(prediction, ground_truth):\n#     return normalize_answer(prediction) == normalize_answer(ground_truth)\n# \n# \n# def metric_max_over_ground_truths(metric_fn, predictions, ground_truths):\n#     score = 0\n#     for pred in predictions:\n#         for ground_truth in ground_truths:\n#             score = metric_fn(pred, ground_truth)\n# --------------------------------------------------\n# the below code fragment can be found in:\n# metrics/cuad/compute_score.py\n# --------------------------------------------------\n#     processed_precisions = process_precisions(precisions)\n#     prec_at_recall = 0\n#     for prec, recall in zip(processed_precisions, recalls):\n#         if recall >= recall_thresh:\n#             prec_at_recall = prec\n#             break\n#     return prec_at_recall\n# \n# \n# def exact_match_score(prediction, ground_truth):\n#     return normalize_answer(prediction) == normalize_answer(ground_truth)\n# \n# \n# def metric_max_over_ground_truths(metric_fn, predictions, ground_truths):\n#     score = 0\n#     for pred in predictions:\n#         for ground_truth in ground_truths:\n#             score = metric_fn(pred, ground_truth)\n#             if score == 1:  # break the loop when one prediction matches the ground truth\n#                 break\n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\nimport os\nimport pickle\nimport tempfile\nimport time\nfrom multiprocessing import Pool\nfrom unittest import TestCase, mock\n\nimport pytest\nfrom datasets.features import Features, Sequence, Value\n\nfrom evaluate.module import EvaluationModule, EvaluationModuleInfo, combine\n\nfrom .utils import require_tf, require_torch\n\n\nclass DummyMetric(EvaluationModule):\n    def _info(self):\n        return EvaluationModuleInfo(\n            description=\"dummy metric for tests\",\n            citation=\"insert citation here\",\n            features=Features({\"predictions\": Value(\"int64\"), \"references\": Value(\"int64\")}),\n        )\n\n    def _compute(self, predictions, references):\n        result = {}\n        if not predictions:\n            return result\n        else:\n            result[\"accuracy\"] = sum(i == j for i, j in zip(predictions, references)) / len(predictions)\n            try:\n                result[\"set_equality\"] = set(predictions) == set(references)\n            except TypeError:\n                result[\"set_equality\"] = None\n        return result\n\n    @classmethod\n    def predictions_and_references(cls):\n        return ([1, 2, 3, 4], [1, 2, 4, 3])\n\n    @classmethod\n    def predictions_and_references_strings(cls):\n        return ([\"a\", \"b\", \"c\", \"d\"], [\"a\", \"b\", \"d\", \"c\"])\n\n    @classmethod\n    def expected_results(cls):\n        return {\"accuracy\": 0.5, \"set_equality\": True}\n\n    @classmethod\n    def other_predictions_and_references(cls):\n        return ([1, 3, 4, 5], [1, 2, 3, 4])\n\n    @classmethod\n    def other_expected_results(cls):\n        return {\"accuracy\": 0.25, \"set_equality\": False}\n\n    @classmethod\n    def distributed_predictions_and_references(cls):\n        return ([1, 2, 3, 4], [1, 2, 3, 4]), ([1, 2, 4, 5], [1, 2, 3, 4])\n\n    @classmethod\n    def distributed_expected_results(cls):\n        return {\"accuracy\": 0.75, \"set_equality\": False}\n\n    @classmethod\n    def separate_predictions_and_references(cls):\n        return ([1, 2, 3, 4], [1, 2, 3, 4]), ([1, 2, 4, 5], [1, 2, 3, 4])\n\n    @classmethod\n    def separate_expected_results(cls):\n        return [{\"accuracy\": 1.0, \"set_equality\": True}, {\"accuracy\": 0.5, \"set_equality\": False}]\n\n\nclass AnotherDummyMetric(EvaluationModule):\n    def _info(self):\n        return EvaluationModuleInfo(\n            description=\"another dummy metric for tests\",\n            citation=\"insert citation here\",\n            features=Features({\"predictions\": Value(\"int64\"), \"references\": Value(\"int64\")}),\n        )\n\n    def _compute(self, predictions, references):\n        return {\"set_equality\": False}\n\n    @classmethod\n    def expected_results(cls):\n        return {\"set_equality\": False}\n\n\ndef properly_del_metric(metric):\n    \"\"\"properly delete a metric on windows if the process is killed during multiprocessing\"\"\"\n    if metric is not None:\n        if metric.filelock is not None:\n            metric.filelock.release()\n        if metric.rendez_vous_lock is not None:\n            metric.rendez_vous_lock.release()\n        del metric.writer\n        del metric.data\n        del metric\n\n\ndef metric_compute(arg):\n    \"\"\"Thread worker function for distributed evaluation testing.\n    On base level to be pickable.\n    \"\"\"\n    metric = None\n    try:\n        num_process, process_id, preds, refs, exp_id, cache_dir, wait = arg\n        metric = DummyMetric(\n            num_process=num_process, process_id=process_id, experiment_id=exp_id, cache_dir=cache_dir, timeout=5\n        )\n        time.sleep(wait)\n        results = metric.compute(predictions=preds, references=refs)\n        return results\n    finally:\n        properly_del_metric(metric)\n\n\ndef metric_add_batch_and_compute(arg):\n    \"\"\"Thread worker function for distributed evaluation testing.\n    On base level to be pickable.\n    \"\"\"\n    metric = None\n    try:\n        num_process, process_id, preds, refs, exp_id, cache_dir, wait = arg\n        metric = DummyMetric(\n            num_process=num_process, process_id=process_id, experiment_id=exp_id, cache_dir=cache_dir, timeout=5\n        )\n        metric.add_batch(predictions=preds, references=refs)\n        time.sleep(wait)\n        results = metric.compute()\n        return results\n    finally:\n        properly_del_metric(metric)\n\n\ndef metric_add_and_compute(arg):\n    \"\"\"Thread worker function for distributed evaluation testing.\n    On base level to be pickable.\n    \"\"\"\n    metric = None\n    try:\n        num_process, process_id, preds, refs, exp_id, cache_dir, wait = arg\n        metric = DummyMetric(\n            num_process=num_process, process_id=process_id, experiment_id=exp_id, cache_dir=cache_dir, timeout=5\n        )\n        for pred, ref in zip(preds, refs):\n            metric.add(prediction=pred, reference=ref)\n        time.sleep(wait)\n        results = metric.compute()\n        return results", "metadata": {"task_id": "huggingface_evaluate/98", "ground_truth": "    finally:", "fpath_tuple": ["huggingface_evaluate", "tests", "test_metric.py"], "context_start_lineno": 0, "line_no": 150, "query_window": {"context": "        return results\n    finally:\n        properly_del_metric(metric)\n\n\ndef metric_add_and_compute(arg):\n    \"\"\"Thread worker function for distributed evaluation testing.\n    On base level to be pickable.\n    \"\"\"\n    metric = None\n    try:\n        num_process, process_id, preds, refs, exp_id, cache_dir, wait = arg\n        metric = DummyMetric(\n            num_process=num_process, process_id=process_id, experiment_id=exp_id, cache_dir=cache_dir, timeout=5\n        )\n        for pred, ref in zip(preds, refs):\n            metric.add(prediction=pred, reference=ref)\n        time.sleep(wait)\n        results = metric.compute()\n        return results", "metadata": {"fpath_tuple": ["huggingface_evaluate", "tests", "test_metric.py"], "line_no": 150, "task_id": "huggingface_evaluate/98", "start_line_no": 130, "end_line_no": 150, "window_size": 20, "context_start_lineno": 0, "repo": "huggingface_evaluate"}}, "top_k_context": [{"context": "def get_prec_at_recall(precisions, recalls, recall_thresh):\n    \"\"\"Assumes recalls are sorted in increasing order\"\"\"\n    processed_precisions = process_precisions(precisions)\n    prec_at_recall = 0\n    for prec, recall in zip(processed_precisions, recalls):\n        if recall >= recall_thresh:\n            prec_at_recall = prec\n            break\n    return prec_at_recall\n\n\ndef exact_match_score(prediction, ground_truth):\n    return normalize_answer(prediction) == normalize_answer(ground_truth)\n\n\ndef metric_max_over_ground_truths(metric_fn, predictions, ground_truths):\n    score = 0\n    for pred in predictions:\n        for ground_truth in ground_truths:\n            score = metric_fn(pred, ground_truth)", "metadata": [{"fpath_tuple": ["huggingface_evaluate", "metrics", "cuad", "compute_score.py"], "line_no": 132, "start_line_no": 122, "end_line_no": 142, "window_size": 20, "repo": "huggingface_evaluate", "slice_size": 10}], "sim_score": 0.23076923076923078}, {"context": "\n\ndef get_prec_at_recall(precisions, recalls, recall_thresh):\n    \"\"\"Assumes recalls are sorted in increasing order\"\"\"\n    processed_precisions = process_precisions(precisions)\n    prec_at_recall = 0\n    for prec, recall in zip(processed_precisions, recalls):\n        if recall >= recall_thresh:\n            prec_at_recall = prec\n            break\n    return prec_at_recall\n\n\ndef exact_match_score(prediction, ground_truth):\n    return normalize_answer(prediction) == normalize_answer(ground_truth)\n\n\ndef metric_max_over_ground_truths(metric_fn, predictions, ground_truths):\n    score = 0\n    for pred in predictions:", "metadata": [{"fpath_tuple": ["huggingface_evaluate", "metrics", "cuad", "compute_score.py"], "line_no": 130, "start_line_no": 120, "end_line_no": 140, "window_size": 20, "repo": "huggingface_evaluate", "slice_size": 10}], "sim_score": 0.23076923076923078}, {"context": "    total_area_intersect = np.zeros((num_labels,), dtype=np.float64)\n    total_area_union = np.zeros((num_labels,), dtype=np.float64)\n    total_area_pred_label = np.zeros((num_labels,), dtype=np.float64)\n    total_area_label = np.zeros((num_labels,), dtype=np.float64)\n    for result, gt_seg_map in zip(results, gt_seg_maps):\n        area_intersect, area_union, area_pred_label, area_label = intersect_and_union(\n            result, gt_seg_map, num_labels, ignore_index, label_map, reduce_labels\n        )\n        total_area_intersect += area_intersect\n        total_area_union += area_union\n        total_area_pred_label += area_pred_label\n        total_area_label += area_label\n    return total_area_intersect, total_area_union, total_area_pred_label, total_area_label\n\n\ndef mean_iou(\n    results,\n    gt_seg_maps,\n    num_labels,\n    ignore_index: bool,", "metadata": [{"fpath_tuple": ["huggingface_evaluate", "metrics", "mean_iou", "mean_iou.py"], "line_no": 204, "start_line_no": 194, "end_line_no": 214, "window_size": 20, "repo": "huggingface_evaluate", "slice_size": 10}], "sim_score": 0.21929824561403508}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# torchrl/trainers/helpers/envs.py\n# --------------------------------------------------\n#                 \"cfg\": cfg,\n#                 \"use_env_creator\": False,\n#                 \"custom_env\": parallel_env,\n#                 \"batch_dims\": 1,\n#             }\n#         )\n#         env = transformed_env_constructor(**kwargs)()\n#         return env\n#     return parallel_env\n# \n# \n# @torch.no_grad()\n# def get_stats_random_rollout(\n#     cfg: \"DictConfig\",  # noqa: F821\n#     proof_environment: EnvBase = None,\n#     key: Optional[str] = None,\n# ):\n#     \"\"\"Gathers stas (loc and scale) from an environment using random rollouts.\n# \n#     Args:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# tutorials/sphinx-tutorials/coding_dqn.py\n# --------------------------------------------------\n# \n# \n# def make_env(parallel=False, m=0, s=1):\n# \n#     if parallel:\n#         base_env = ParallelEnv(\n#             n_workers,\n#             EnvCreator(\n#                 lambda: GymEnv(\n#                     \"CartPole-v1\", from_pixels=True, pixels_only=True, device=device\n#                 )\n#             ),\n#         )\n#     else:\n#         base_env = GymEnv(\n#             \"CartPole-v1\", from_pixels=True, pixels_only=True, device=device\n#         )\n# \n#     env = TransformedEnv(\n#         base_env,\n# --------------------------------------------------\n# the below code fragment can be found in:\n# torchrl/trainers/helpers/envs.py\n# --------------------------------------------------\n#         env = transformed_env_constructor(**kwargs)()\n#         return env\n#     return parallel_env\n# \n# \n# @torch.no_grad()\n# def get_stats_random_rollout(\n#     cfg: \"DictConfig\",  # noqa: F821\n#     proof_environment: EnvBase = None,\n#     key: Optional[str] = None,\n# ):\n#     \"\"\"Gathers stas (loc and scale) from an environment using random rollouts.\n# \n#     Args:\n#         cfg (DictConfig): a config object with `init_env_steps` field, indicating\n#             the total number of frames to be collected to compute the stats.\n#         proof_environment (EnvBase instance, optional): if provided, this env will\n#             be used ot execute the rollouts. If not, it will be created using\n#             the cfg object.\n#         key (str, optional): if provided, the stats of this key will be gathered.\n# --------------------------------------------------\n# the below code fragment can be found in:\n# torchrl/trainers/helpers/envs.py\n# --------------------------------------------------\n#             }\n#         )\n#         env = transformed_env_constructor(**kwargs)()\n#         return env\n#     return parallel_env\n# \n# \n# @torch.no_grad()\n# def get_stats_random_rollout(\n#     cfg: \"DictConfig\",  # noqa: F821\n#     proof_environment: EnvBase = None,\n#     key: Optional[str] = None,\n# ):\n#     \"\"\"Gathers stas (loc and scale) from an environment using random rollouts.\n# \n#     Args:\n#         cfg (DictConfig): a config object with `init_env_steps` field, indicating\n#             the total number of frames to be collected to compute the stats.\n#         proof_environment (EnvBase instance, optional): if provided, this env will\n#             be used ot execute the rollouts. If not, it will be created using\n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\n value network;\n# - how to collect data from your environment efficiently and store them\n#   in a replay buffer;\n# - how to store trajectories (and not transitions) in your replay buffer);\n# - and finally how to evaluate your model.\n#\n# This tutorial assumes the reader is familiar with some of TorchRL primitives,\n# such as ``TensorDict`` and ``TensorDictModules``, although it should be\n# sufficiently transparent to be understood without a deep understanding of\n# these classes.\n#\n# We do not aim at giving a SOTA implementation of the algorithm, but rather\n# to provide a high-level illustration of TorchRL features in the context of\n# this algorithm.\n\n# Make all the necessary imports for training\n\n# sphinx_gallery_start_ignore\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n# sphinx_gallery_end_ignore\n\nfrom copy import deepcopy\nfrom typing import Optional\n\nimport numpy as np\nimport torch\nimport torch.cuda\nimport tqdm\nfrom matplotlib import pyplot as plt\nfrom tensordict.nn import TensorDictModule\nfrom torch import nn, optim\nfrom torchrl.collectors import MultiaSyncDataCollector\nfrom torchrl.data import CompositeSpec, TensorDictReplayBuffer\nfrom torchrl.data.postprocs import MultiStep\nfrom torchrl.data.replay_buffers.samplers import PrioritizedSampler, RandomSampler\nfrom torchrl.data.replay_buffers.storages import LazyMemmapStorage\nfrom torchrl.envs import (\n    CatTensors,\n    DoubleToFloat,\n    EnvCreator,\n    ObservationNorm,\n    ParallelEnv,\n)\nfrom torchrl.envs.libs.dm_control import DMControlEnv\nfrom torchrl.envs.libs.gym import GymEnv\nfrom torchrl.envs.transforms import RewardScaling, TransformedEnv\nfrom torchrl.envs.utils import set_exploration_mode, step_mdp\nfrom torchrl.modules import (\n    MLP,\n    OrnsteinUhlenbeckProcessWrapper,\n    ProbabilisticActor,\n    ValueOperator,\n)\nfrom torchrl.modules.distributions.continuous import TanhDelta\nfrom torchrl.objectives.utils import hold_out_net\nfrom torchrl.trainers import Recorder\n\n###############################################################################\n# Environment\n# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n# Let us start by building the environment.\n#\n# For this example, we will be using the cheetah task. The goal is to make\n# a half-cheetah run as fast as possible.\n#\n# In TorchRL, one can create such a task by relying on dm_control or gym:\n#\n#   env = GymEnv(\"HalfCheetah-v4\")\n#\n# or\n#\n#   env = DMControlEnv(\"cheetah\", \"run\")\n#\n# We only consider the state-based environment, but if one wishes to use a\n# pixel-based environment, this can be done via the keyword argument\n# ``from_pixels=True`` which is passed when calling ``GymEnv`` or\n# ``DMControlEnv``.\n\n\ndef make_env():\n    \"\"\"\n    Create a base env\n    \"\"\"\n    global env_library\n    global env_name\n\n    if backend == \"dm_control\":\n        env_name = \"cheetah\"\n        env_task = \"run\"\n        env_args = (env_name, env_task)\n        env_library = DMControlEnv\n    elif backend == \"gym\":\n        env_name = \"HalfCheetah-v4\"\n        env_args = (env_name,)\n        env_library = GymEnv\n    else:\n        raise NotImplementedError\n\n    env_kwargs = {\n        \"device\": device,\n        \"frame_skip\": frame_skip,\n        \"from_pixels\": from_pixels,\n        \"pixels_only\": from_pixels,\n    }\n    env = env_library(*env_args, **env_kwargs)\n    return env\n\n\n###############################################################################\n# Transforms\n# ------------------------------\n# Now that we have a base environment, we may want to modify its representation\n# to make it more policy-friendly.\n#\n# It is common in DDPG to rescale the reward using some heuristic value. We\n# will multiply the reward by 5 in this example.\n#\n# If we are using dm_control, it is important also to transform the actions\n# to double precision numbers as this is the dtype expected by the library.\n#\n# We also leave the possibility to normalize the states: we will take care of\n# computing the normalizing constants later on.\n\n\ndef make_transformed_env(\n    env,\n    stats=None,\n):\n    \"\"\"\n    Apply transforms to the env (such as reward scaling and state normalization)\n    \"\"\"\n\n    env = TransformedEnv(env)\n\n    # we append transforms one by one, although we might as well create the transformed environment using the `env = TransformedEnv(base_env, transforms)` syntax.\n    env.append_transform(RewardScaling(loc=0.0, scale=reward_scaling))\n\n    double_to_float_list = []\n    double_to_float_inv_list = []\n    if env_library is DMControlEnv:\n        # DMControl requires double-precision\n        double_to_float_list += [\n            \"reward\",\n            \"action\",\n        ]\n        double_to_float_inv_list += [\"action\"]\n\n    # We concatenate all states into a single \"observation_vector\"\n    # even if there is a single tensor, it'll be renamed in \"observation_vector\".\n    # This facilitates the downstream operations as we know the name of the output tensor.\n    # In some environments (not half-cheetah), there may be more than one observation vector: in this case this code snippet will concatenate them all.\n    selected_keys = list(env.observation_spec.keys())\n    out_key = \"observation_vector\"\n    env.append_transform(CatTensors(in_keys=selected_keys, out_key=out_key))\n\n    #  we normalize the states\n    if stats is None:\n        _stats = {\"loc\": 0.0, \"scale\": 1.0}\n    else:\n        _stats = stats\n    env.append_transform(\n        ObservationNorm(**_stats, in_keys=[out_key], standard_normal=True)\n    )\n\n    double_to_float_list.append(out_key)\n    env.append_transform(\n        DoubleToFloat(\n            in_keys=double_to_float_list, in_keys_inv=double_to_float_inv_list\n        )\n    )\n\n    return env\n\n\n###############################################################################\n# Parallel execution\n# ------------------------------\n# The following helper function allows us to run environments in parallel.\n# One can choose between running each base env in a separate process and\n# execute the transform in the main process, or execute the transforms in\n# parallel. To leverage the vectorization capabilities of PyTorch, we adopt\n# the first method:\n\n\ndef parallel_env_constructor(\n    stats,\n    **env_kwargs,\n):\n    if env_per_collector == 1:\n        env_creator = EnvCreator(\n            lambda: make_transformed_env(make_env(), stats, **env_kwargs)\n        )\n        return env_creator\n\n    parallel_env = ParallelEnv(\n        num_workers=env_per_collector,\n        create_env_fn=EnvCreator(lambda: make_env()),\n        create_env_kwargs=None,\n        pin_memory=False,\n    )\n    env = make_transformed_env(parallel_env, stats, **env_kwargs)\n    return env\n\n\n###############################################################################\n# Normalization of the observations\n# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n# To compute the normalizing statistics, we run an arbitrary number of random\n# steps in the environment and compute the mean and standard deviation of the\n# collected observations:\n\n\ndef get_stats_random_rollout(proof_environment, key: Optional[str] = None):\n    print(\"computing state stats\")\n    n = 0\n    td_stats = []", "metadata": {"task_id": "pytorch_rl/12", "ground_truth": "    while n < init_env_steps:", "fpath_tuple": ["pytorch_rl", "tutorials", "sphinx-tutorials", "coding_ddpg.py"], "context_start_lineno": 17, "line_no": 235, "query_window": {"context": "        create_env_fn=EnvCreator(lambda: make_env()),\n        create_env_kwargs=None,\n        pin_memory=False,\n    )\n    env = make_transformed_env(parallel_env, stats, **env_kwargs)\n    return env\n\n\n###############################################################################\n# Normalization of the observations\n# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n# To compute the normalizing statistics, we run an arbitrary number of random\n# steps in the environment and compute the mean and standard deviation of the\n# collected observations:\n\n\ndef get_stats_random_rollout(proof_environment, key: Optional[str] = None):\n    print(\"computing state stats\")\n    n = 0\n    td_stats = []", "metadata": {"fpath_tuple": ["pytorch_rl", "tutorials", "sphinx-tutorials", "coding_ddpg.py"], "line_no": 235, "task_id": "pytorch_rl/12", "start_line_no": 215, "end_line_no": 235, "window_size": 20, "context_start_lineno": 17, "repo": "pytorch_rl"}}, "top_k_context": [{"context": "                \"custom_env\": parallel_env,\n                \"batch_dims\": 1,\n            }\n        )\n        env = transformed_env_constructor(**kwargs)()\n        return env\n    return parallel_env\n\n\n@torch.no_grad()\ndef get_stats_random_rollout(\n    cfg: \"DictConfig\",  # noqa: F821\n    proof_environment: EnvBase = None,\n    key: Optional[str] = None,\n):\n    \"\"\"Gathers stas (loc and scale) from an environment using random rollouts.\n\n    Args:\n        cfg (DictConfig): a config object with `init_env_steps` field, indicating\n            the total number of frames to be collected to compute the stats.", "metadata": [{"fpath_tuple": ["pytorch_rl", "torchrl", "trainers", "helpers", "envs.py"], "line_no": 370, "start_line_no": 360, "end_line_no": 380, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}], "sim_score": 0.2721518987341772}, {"context": "            }\n        )\n        env = transformed_env_constructor(**kwargs)()\n        return env\n    return parallel_env\n\n\n@torch.no_grad()\ndef get_stats_random_rollout(\n    cfg: \"DictConfig\",  # noqa: F821\n    proof_environment: EnvBase = None,\n    key: Optional[str] = None,\n):\n    \"\"\"Gathers stas (loc and scale) from an environment using random rollouts.\n\n    Args:\n        cfg (DictConfig): a config object with `init_env_steps` field, indicating\n            the total number of frames to be collected to compute the stats.\n        proof_environment (EnvBase instance, optional): if provided, this env will\n            be used ot execute the rollouts. If not, it will be created using", "metadata": [{"fpath_tuple": ["pytorch_rl", "torchrl", "trainers", "helpers", "envs.py"], "line_no": 372, "start_line_no": 362, "end_line_no": 382, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}], "sim_score": 0.2682926829268293}, {"context": "# - mean and standard deviation: we normalize the observations (images)\n#   with two parameters computed from a random rollout in the environment.\n\n\ndef make_env(parallel=False, m=0, s=1):\n\n    if parallel:\n        base_env = ParallelEnv(\n            n_workers,\n            EnvCreator(\n                lambda: GymEnv(\n                    \"CartPole-v1\", from_pixels=True, pixels_only=True, device=device\n                )\n            ),\n        )\n    else:\n        base_env = GymEnv(\n            \"CartPole-v1\", from_pixels=True, pixels_only=True, device=device\n        )\n", "metadata": [{"fpath_tuple": ["pytorch_rl", "tutorials", "sphinx-tutorials", "coding_dqn.py"], "line_no": 160, "start_line_no": 150, "end_line_no": 170, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}], "sim_score": 0.2647058823529412}, {"context": "        kwargs.update(\n            {\n                \"cfg\": cfg,\n                \"use_env_creator\": False,\n                \"custom_env\": parallel_env,\n                \"batch_dims\": 1,\n            }\n        )\n        env = transformed_env_constructor(**kwargs)()\n        return env\n    return parallel_env\n\n\n@torch.no_grad()\ndef get_stats_random_rollout(\n    cfg: \"DictConfig\",  # noqa: F821\n    proof_environment: EnvBase = None,\n    key: Optional[str] = None,\n):\n    \"\"\"Gathers stas (loc and scale) from an environment using random rollouts.", "metadata": [{"fpath_tuple": ["pytorch_rl", "torchrl", "trainers", "helpers", "envs.py"], "line_no": 366, "start_line_no": 356, "end_line_no": 376, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}], "sim_score": 0.24666666666666667}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# src/evaluate/inspect.py\n# --------------------------------------------------\n# \n# \n# class SplitsNotFoundError(ValueError):\n#     pass\n# \n# \n# def list_evaluation_modules(module_type=None, include_community=True, with_details=False):\n#     \"\"\"List all evaluation modules available on the Hugging Face Hub.\n# \n#     Args:\n#         module_type (`str`, *optional*, defaults to `None`):\n#             Type of evaluation modules to list. Has to be one of `'metric'`, `'comparison'`, or `'measurement'`. If `None`, all types are listed.\n#         include_community (`bool`, *optional*, defaults to `True`):\n#             Include community modules in the list.\n#         with_details (`bool`, *optional*, defaults to `False`):\n#             Return the full details on the metrics instead of only the ID.\n# \n#     Returns:\n#         `List[Union[str, dict]]`\n# \n# --------------------------------------------------\n# the below code fragment can be found in:\n# src/evaluate/inspect.py\n# --------------------------------------------------\n# from .utils.logging import get_logger\n# \n# \n# logger = get_logger(__name__)\n# \n# \n# class SplitsNotFoundError(ValueError):\n#     pass\n# \n# \n# def list_evaluation_modules(module_type=None, include_community=True, with_details=False):\n#     \"\"\"List all evaluation modules available on the Hugging Face Hub.\n# \n#     Args:\n#         module_type (`str`, *optional*, defaults to `None`):\n#             Type of evaluation modules to list. Has to be one of `'metric'`, `'comparison'`, or `'measurement'`. If `None`, all types are listed.\n#         include_community (`bool`, *optional*, defaults to `True`):\n#             Include community modules in the list.\n#         with_details (`bool`, *optional*, defaults to `False`):\n#             Return the full details on the metrics instead of only the ID.\n# --------------------------------------------------\n# the below code fragment can be found in:\n# src/evaluate/inspect.py\n# --------------------------------------------------\n# \n# logger = get_logger(__name__)\n# \n# \n# class SplitsNotFoundError(ValueError):\n#     pass\n# \n# \n# def list_evaluation_modules(module_type=None, include_community=True, with_details=False):\n#     \"\"\"List all evaluation modules available on the Hugging Face Hub.\n# \n#     Args:\n#         module_type (`str`, *optional*, defaults to `None`):\n#             Type of evaluation modules to list. Has to be one of `'metric'`, `'comparison'`, or `'measurement'`. If `None`, all types are listed.\n#         include_community (`bool`, *optional*, defaults to `True`):\n#             Include community modules in the list.\n#         with_details (`bool`, *optional*, defaults to `False`):\n#             Return the full details on the metrics instead of only the ID.\n# \n#     Returns:\n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\n# Copyright 2020 The HuggingFace Datasets Authors and the TensorFlow Datasets Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python3\n\"\"\" EvaluationModuleInfo records information we know about a dataset and a metric.\n\"\"\"\n\nimport dataclasses\nimport json\nimport os\nfrom dataclasses import asdict, dataclass, field\nfrom typing import List, Optional, Union\n\nfrom datasets.features import Features, Value\n\nfrom . import config\nfrom .utils.logging import get_logger\n\n\nlogger = get_logger(__name__)\n\n\n@dataclass\nclass EvaluationModuleInfo:\n    \"\"\"Base class to store information about an evaluation used for `MetricInfo`, `ComparisonInfo`,\n    and `MeasurementInfo`.\n\n    `EvaluationModuleInfo` documents an evaluation, including its name, version, and features.\n    See the constructor arguments and properties for a full list.\n\n    Note: Not all fields are known on construction and may be updated later.\n    \"\"\"\n\n    # Set in the dataset scripts", "metadata": {"task_id": "huggingface_evaluate/189", "ground_truth": "    description: str", "fpath_tuple": ["huggingface_evaluate", "src", "evaluate", "info.py"], "context_start_lineno": 0, "line_no": 45, "query_window": {"context": "\nfrom . import config\nfrom .utils.logging import get_logger\n\n\nlogger = get_logger(__name__)\n\n\n@dataclass\nclass EvaluationModuleInfo:\n    \"\"\"Base class to store information about an evaluation used for `MetricInfo`, `ComparisonInfo`,\n    and `MeasurementInfo`.\n\n    `EvaluationModuleInfo` documents an evaluation, including its name, version, and features.\n    See the constructor arguments and properties for a full list.\n\n    Note: Not all fields are known on construction and may be updated later.\n    \"\"\"\n\n    # Set in the dataset scripts", "metadata": {"fpath_tuple": ["huggingface_evaluate", "src", "evaluate", "info.py"], "line_no": 45, "task_id": "huggingface_evaluate/189", "start_line_no": 25, "end_line_no": 45, "window_size": 20, "context_start_lineno": 0, "repo": "huggingface_evaluate"}}, "top_k_context": [{"context": "from .utils.logging import get_logger\n\n\nlogger = get_logger(__name__)\n\n\nclass SplitsNotFoundError(ValueError):\n    pass\n\n\ndef list_evaluation_modules(module_type=None, include_community=True, with_details=False):\n    \"\"\"List all evaluation modules available on the Hugging Face Hub.\n\n    Args:\n        module_type (`str`, *optional*, defaults to `None`):\n            Type of evaluation modules to list. Has to be one of `'metric'`, `'comparison'`, or `'measurement'`. If `None`, all types are listed.\n        include_community (`bool`, *optional*, defaults to `True`):\n            Include community modules in the list.\n        with_details (`bool`, *optional*, defaults to `False`):\n            Return the full details on the metrics instead of only the ID.", "metadata": [{"fpath_tuple": ["huggingface_evaluate", "src", "evaluate", "inspect.py"], "line_no": 34, "start_line_no": 24, "end_line_no": 44, "window_size": 20, "repo": "huggingface_evaluate", "slice_size": 10}], "sim_score": 0.273972602739726}, {"context": "from .config import EVALUATION_MODULE_TYPES, HF_LIST_ENDPOINT\nfrom .loading import evaluation_module_factory\nfrom .utils.logging import get_logger\n\n\nlogger = get_logger(__name__)\n\n\nclass SplitsNotFoundError(ValueError):\n    pass\n\n\ndef list_evaluation_modules(module_type=None, include_community=True, with_details=False):\n    \"\"\"List all evaluation modules available on the Hugging Face Hub.\n\n    Args:\n        module_type (`str`, *optional*, defaults to `None`):\n            Type of evaluation modules to list. Has to be one of `'metric'`, `'comparison'`, or `'measurement'`. If `None`, all types are listed.\n        include_community (`bool`, *optional*, defaults to `True`):\n            Include community modules in the list.", "metadata": [{"fpath_tuple": ["huggingface_evaluate", "src", "evaluate", "inspect.py"], "line_no": 32, "start_line_no": 22, "end_line_no": 42, "window_size": 20, "repo": "huggingface_evaluate", "slice_size": 10}], "sim_score": 0.24528301886792453}, {"context": "\nlogger = get_logger(__name__)\n\n\nclass SplitsNotFoundError(ValueError):\n    pass\n\n\ndef list_evaluation_modules(module_type=None, include_community=True, with_details=False):\n    \"\"\"List all evaluation modules available on the Hugging Face Hub.\n\n    Args:\n        module_type (`str`, *optional*, defaults to `None`):\n            Type of evaluation modules to list. Has to be one of `'metric'`, `'comparison'`, or `'measurement'`. If `None`, all types are listed.\n        include_community (`bool`, *optional*, defaults to `True`):\n            Include community modules in the list.\n        with_details (`bool`, *optional*, defaults to `False`):\n            Return the full details on the metrics instead of only the ID.\n\n    Returns:", "metadata": [{"fpath_tuple": ["huggingface_evaluate", "src", "evaluate", "inspect.py"], "line_no": 36, "start_line_no": 26, "end_line_no": 46, "window_size": 20, "repo": "huggingface_evaluate", "slice_size": 10}], "sim_score": 0.24489795918367346}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# examples/distributed/distributed_replay_buffer.py\n# --------------------------------------------------\n# import torch.distributed.rpc as rpc\n# from tensordict import TensorDict\n# from torchrl.data.replay_buffers import RemoteTensorDictReplayBuffer\n# from torchrl.data.replay_buffers.samplers import RandomSampler\n# from torchrl.data.replay_buffers.storages import LazyMemmapStorage\n# from torchrl.data.replay_buffers.utils import accept_remote_rref_invocation\n# from torchrl.data.replay_buffers.writers import RoundRobinWriter\n# \n# RETRY_LIMIT = 2\n# RETRY_DELAY_SECS = 3\n# REPLAY_BUFFER_NODE = \"ReplayBuffer\"\n# TRAINER_NODE = \"Trainer\"\n# \n# parser = argparse.ArgumentParser(\n#     description=\"RPC Replay Buffer Example\",\n#     formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n# )\n# \n# parser.add_argument(\n#     \"--rank\",\n# --------------------------------------------------\n# the below code fragment can be found in:\n# examples/distributed/distributed_replay_buffer.py\n# --------------------------------------------------\n# from torchrl.data.replay_buffers.storages import LazyMemmapStorage\n# from torchrl.data.replay_buffers.utils import accept_remote_rref_invocation\n# from torchrl.data.replay_buffers.writers import RoundRobinWriter\n# \n# RETRY_LIMIT = 2\n# RETRY_DELAY_SECS = 3\n# REPLAY_BUFFER_NODE = \"ReplayBuffer\"\n# TRAINER_NODE = \"Trainer\"\n# \n# parser = argparse.ArgumentParser(\n#     description=\"RPC Replay Buffer Example\",\n#     formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n# )\n# \n# parser.add_argument(\n#     \"--rank\",\n#     type=int,\n#     default=-1,\n#     help=\"Node Rank [0 = Replay Buffer, 1 = Dummy Trainer, 2+ = Dummy Data Collector]\",\n# )\n# --------------------------------------------------\n# the below code fragment can be found in:\n# examples/distributed/distributed_replay_buffer.py\n# --------------------------------------------------\n# import os\n# import random\n# import sys\n# import time\n# \n# import torch\n# import torch.distributed.rpc as rpc\n# from tensordict import TensorDict\n# from torchrl.data.replay_buffers import RemoteTensorDictReplayBuffer\n# from torchrl.data.replay_buffers.samplers import RandomSampler\n# from torchrl.data.replay_buffers.storages import LazyMemmapStorage\n# from torchrl.data.replay_buffers.utils import accept_remote_rref_invocation\n# from torchrl.data.replay_buffers.writers import RoundRobinWriter\n# \n# RETRY_LIMIT = 2\n# RETRY_DELAY_SECS = 3\n# REPLAY_BUFFER_NODE = \"ReplayBuffer\"\n# TRAINER_NODE = \"Trainer\"\n# \n# parser = argparse.ArgumentParser(\n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\n# Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\n\"\"\"\nSample latency benchmarking (using RPC)\n======================================\nA rough benchmark of sample latency using different storage types over the network using `torch.rpc`.\nRun this script with --rank=0 and --rank=1 flags set in separate processes - these ranks correspond to the trainer worker and buffer worker respectively, and both need to be initialised.\ne.g. to benchmark LazyMemmapStorage, run the following commands using either two separate shells or multiprocessing.\n    - python3 benchmark_sample_latency_over_rpc.py --rank=0 --storage=LazyMemmapStorage\n    - python3 benchmark_sample_latency_over_rpc.py --rank=1 --storage=LazyMemmapStorage\nThis code is based on examples/distributed/distributed_replay_buffer.py.\n\"\"\"\nimport argparse\nimport os\nimport pickle\nimport sys\nimport time\nimport timeit\nfrom datetime import datetime\n\nimport torch\nimport torch.distributed.rpc as rpc\nfrom tensordict import TensorDict\nfrom torchrl.data.replay_buffers import RemoteTensorDictReplayBuffer\nfrom torchrl.data.replay_buffers.samplers import RandomSampler\nfrom torchrl.data.replay_buffers.storages import (\n    LazyMemmapStorage,\n    LazyTensorStorage,\n    ListStorage,\n)\nfrom torchrl.data.replay_buffers.writers import RoundRobinWriter\n\nRETRY_LIMIT = 2\nRETRY_DELAY_SECS = 3\nREPLAY_BUFFER_NODE = \"ReplayBuffer\"\nTRAINER_NODE = \"Trainer\"\nTENSOR_SIZE = 3 * 86 * 86\nBUFFER_SIZE = 1001\nBATCH_SIZE = 256\nREPEATS = 1000\n\nstorage_options = {\n    \"LazyMemmapStorage\": LazyMemmapStorage,\n    \"LazyTensorStorage\": LazyTensorStorage,\n    \"ListStorage\": ListStorage,\n}\n\nstorage_arg_options = {\n    \"LazyMemmapStorage\": {\"scratch_dir\": \"/tmp/\", \"device\": torch.device(\"cpu\")},\n    \"LazyTensorStorage\": {},\n    \"ListStorage\": {},\n}", "metadata": {"task_id": "pytorch_rl/17", "ground_truth": "parser = argparse.ArgumentParser(", "fpath_tuple": ["pytorch_rl", "benchmarks", "storage", "benchmark_sample_latency_over_rpc.py"], "context_start_lineno": 0, "line_no": 55, "query_window": {"context": "RETRY_LIMIT = 2\nRETRY_DELAY_SECS = 3\nREPLAY_BUFFER_NODE = \"ReplayBuffer\"\nTRAINER_NODE = \"Trainer\"\nTENSOR_SIZE = 3 * 86 * 86\nBUFFER_SIZE = 1001\nBATCH_SIZE = 256\nREPEATS = 1000\n\nstorage_options = {\n    \"LazyMemmapStorage\": LazyMemmapStorage,\n    \"LazyTensorStorage\": LazyTensorStorage,\n    \"ListStorage\": ListStorage,\n}\n\nstorage_arg_options = {\n    \"LazyMemmapStorage\": {\"scratch_dir\": \"/tmp/\", \"device\": torch.device(\"cpu\")},\n    \"LazyTensorStorage\": {},\n    \"ListStorage\": {},\n}", "metadata": {"fpath_tuple": ["pytorch_rl", "benchmarks", "storage", "benchmark_sample_latency_over_rpc.py"], "line_no": 55, "task_id": "pytorch_rl/17", "start_line_no": 35, "end_line_no": 55, "window_size": 20, "context_start_lineno": 0, "repo": "pytorch_rl"}}, "top_k_context": [{"context": "\nimport argparse\nimport os\nimport random\nimport sys\nimport time\n\nimport torch\nimport torch.distributed.rpc as rpc\nfrom tensordict import TensorDict\nfrom torchrl.data.replay_buffers import RemoteTensorDictReplayBuffer\nfrom torchrl.data.replay_buffers.samplers import RandomSampler\nfrom torchrl.data.replay_buffers.storages import LazyMemmapStorage\nfrom torchrl.data.replay_buffers.utils import accept_remote_rref_invocation\nfrom torchrl.data.replay_buffers.writers import RoundRobinWriter\n\nRETRY_LIMIT = 2\nRETRY_DELAY_SECS = 3\nREPLAY_BUFFER_NODE = \"ReplayBuffer\"\nTRAINER_NODE = \"Trainer\"", "metadata": [{"fpath_tuple": ["pytorch_rl", "examples", "distributed", "distributed_replay_buffer.py"], "line_no": 18, "start_line_no": 8, "end_line_no": 28, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}], "sim_score": 0.3305084745762712}, {"context": "from torchrl.data.replay_buffers import RemoteTensorDictReplayBuffer\nfrom torchrl.data.replay_buffers.samplers import RandomSampler\nfrom torchrl.data.replay_buffers.storages import LazyMemmapStorage\nfrom torchrl.data.replay_buffers.utils import accept_remote_rref_invocation\nfrom torchrl.data.replay_buffers.writers import RoundRobinWriter\n\nRETRY_LIMIT = 2\nRETRY_DELAY_SECS = 3\nREPLAY_BUFFER_NODE = \"ReplayBuffer\"\nTRAINER_NODE = \"Trainer\"\n\nparser = argparse.ArgumentParser(\n    description=\"RPC Replay Buffer Example\",\n    formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n)\n\nparser.add_argument(\n    \"--rank\",\n    type=int,\n    default=-1,", "metadata": [{"fpath_tuple": ["pytorch_rl", "examples", "distributed", "distributed_replay_buffer.py"], "line_no": 28, "start_line_no": 18, "end_line_no": 38, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}], "sim_score": 0.3283582089552239}, {"context": "\nimport torch\nimport torch.distributed.rpc as rpc\nfrom tensordict import TensorDict\nfrom torchrl.data.replay_buffers import RemoteTensorDictReplayBuffer\nfrom torchrl.data.replay_buffers.samplers import RandomSampler\nfrom torchrl.data.replay_buffers.storages import LazyMemmapStorage\nfrom torchrl.data.replay_buffers.utils import accept_remote_rref_invocation\nfrom torchrl.data.replay_buffers.writers import RoundRobinWriter\n\nRETRY_LIMIT = 2\nRETRY_DELAY_SECS = 3\nREPLAY_BUFFER_NODE = \"ReplayBuffer\"\nTRAINER_NODE = \"Trainer\"\n\nparser = argparse.ArgumentParser(\n    description=\"RPC Replay Buffer Example\",\n    formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n)\n", "metadata": [{"fpath_tuple": ["pytorch_rl", "examples", "distributed", "distributed_replay_buffer.py"], "line_no": 24, "start_line_no": 14, "end_line_no": 34, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}], "sim_score": 0.31851851851851853}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# comparisons/exact_match/exact_match.py\n# --------------------------------------------------\n# \n# _DESCRIPTION = \"\"\"\n# Returns the rate at which the predictions of one model exactly match those of another model.\n# \"\"\"\n# \n# \n# _KWARGS_DESCRIPTION = \"\"\"\n# Args:\n#     predictions1 (`list` of `int`): Predicted labels for model 1.\n#     predictions2 (`list` of `int`): Predicted labels for model 2.\n# \n# Returns:\n#     exact_match (`float`): Dictionary containing exact_match rate. Possible values are between 0.0 and 1.0, inclusive.\n# \n# Examples:\n#     >>> exact_match = evaluate.load(\"exact_match\", module_type=\"comparison\")\n#     >>> results = exact_match.compute(predictions1=[1, 1, 1], predictions2=[1, 1, 1])\n#     >>> print(results)\n#     {'exact_match': 1.0}\n# \"\"\"\n# --------------------------------------------------\n# the below code fragment can be found in:\n# measurements/text_duplicates/text_duplicates.py\n# --------------------------------------------------\n# _KWARGS_DESCRIPTION = \"\"\"\n# Args:\n#     `data`: a list of `str` to be checked for duplicates.\n# \n# Returns:\n#     `duplicate_fraction` (`float`) : the fraction of strings that are duplicated.\n#     `duplicates_dict` (`dict`) (optional) : a dictionary containing tuples with the duplicate strings and the number of times they are repeated.\n# \n# Examples:\n#     >>> data = [\"hello sun\",\"hello moon\", \"hello sun\"]\n#     >>> duplicates = evaluate.load(\"text_duplicates\")\n#     >>> results = duplicates.compute(data=data)\n#     >>> print(results)\n#     {'duplicate_fraction': 0.33333333333333337}\n# \n#     >>> data = [\"hello sun\",\"hello moon\", \"hello sun\"]\n#     >>> duplicates = evaluate.load(\"text_duplicates\")\n#     >>> results =  duplicates.compute(data=data, list_duplicates=True)\n#     >>> print(results)\n#     {'duplicate_fraction': 0.33333333333333337, 'duplicates_dict': {'hello sun': 2}}\n# --------------------------------------------------\n# the below code fragment can be found in:\n# metrics/competition_math/competition_math.py\n# --------------------------------------------------\n# \n# \n# _KWARGS_DESCRIPTION = r\"\"\"\n# Calculates accuracy after canonicalizing inputs.\n# \n# Args:\n#     predictions: list of predictions to score. Each prediction\n#         is a string that contains natural language and LaTex.\n#     references: list of reference for each prediction. Each\n#         reference is a string that contains natural language\n#         and LaTex.\n# Returns:\n#     accuracy: accuracy after canonicalizing inputs\n#         (e.g., converting \"1/2\" to \"\\\\frac{1}{2}\")\n# \n# Examples:\n#     >>> metric = evaluate.load(\"competition_math\")\n#     >>> results = metric.compute(references=[\"\\\\frac{1}{2}\"], predictions=[\"1/2\"])\n#     >>> print(results)\n#     {'accuracy': 1.0}\n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\n# Copyright 2020 The HuggingFace Datasets Authors and the current dataset script contributor.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Computes the RL Reliability Metrics.\"\"\"\n\nimport datasets\nimport numpy as np\nfrom rl_reliability_metrics.evaluation import eval_metrics\nfrom rl_reliability_metrics.metrics import metrics_offline, metrics_online\n\nimport evaluate\n\n\nlogger = evaluate.logging.get_logger(__name__)\n\nDEFAULT_EVAL_POINTS = [\n    50000,\n    150000,\n    250000,\n    350000,\n    450000,\n    550000,\n    650000,\n    750000,\n    850000,\n    950000,\n    1050000,\n    1150000,\n    1250000,\n    1350000,\n    1450000,\n    1550000,\n    1650000,\n    1750000,\n    1850000,\n    1950000,\n]\n\nN_RUNS_RECOMMENDED = 10\n\n_CITATION = \"\"\"\\\n@conference{rl_reliability_metrics,\n  title = {Measuring the Reliability of Reinforcement Learning Algorithms},\n  author = {Stephanie CY Chan, Sam Fishman, John Canny, Anoop Korattikara, and Sergio Guadarrama},\n  booktitle = {International Conference on Learning Representations, Addis Ababa, Ethiopia},\n  year = 2020,\n}\n\"\"\"\n\n_DESCRIPTION = \"\"\"\\\nComputes the RL reliability metrics from a set of experiments. There is an `\"online\"` and `\"offline\"` configuration for evaluation.\n\"\"\"\n\n\n_KWARGS_DESCRIPTION = \"\"\"\nComputes the RL reliability metrics from a set of experiments. There is an `\"online\"` and `\"offline\"` configuration for evaluation.\nArgs:\n    timestamps: list of timestep lists/arrays that serve as index.\n    rewards: list of reward lists/arrays of each experiment.\nReturns:\n    dictionary: a set of reliability metrics\nExamples:\n    >>> import numpy as np\n    >>> rl_reliability = evaluate.load(\"rl_reliability\", \"online\")\n    >>> results = rl_reliability.compute(\n    ...     timesteps=[np.linspace(0, 2000000, 1000)],\n    ...     rewards=[np.linspace(0, 100, 1000)]", "metadata": {"task_id": "huggingface_evaluate/145", "ground_truth": "    ...     )", "fpath_tuple": ["huggingface_evaluate", "metrics", "rl_reliability", "rl_reliability.py"], "context_start_lineno": 0, "line_no": 77, "query_window": {"context": "\"\"\"\n\n_DESCRIPTION = \"\"\"\\\nComputes the RL reliability metrics from a set of experiments. There is an `\"online\"` and `\"offline\"` configuration for evaluation.\n\"\"\"\n\n\n_KWARGS_DESCRIPTION = \"\"\"\nComputes the RL reliability metrics from a set of experiments. There is an `\"online\"` and `\"offline\"` configuration for evaluation.\nArgs:\n    timestamps: list of timestep lists/arrays that serve as index.\n    rewards: list of reward lists/arrays of each experiment.\nReturns:\n    dictionary: a set of reliability metrics\nExamples:\n    >>> import numpy as np\n    >>> rl_reliability = evaluate.load(\"rl_reliability\", \"online\")\n    >>> results = rl_reliability.compute(\n    ...     timesteps=[np.linspace(0, 2000000, 1000)],\n    ...     rewards=[np.linspace(0, 100, 1000)]", "metadata": {"fpath_tuple": ["huggingface_evaluate", "metrics", "rl_reliability", "rl_reliability.py"], "line_no": 77, "task_id": "huggingface_evaluate/145", "start_line_no": 57, "end_line_no": 77, "window_size": 20, "context_start_lineno": 0, "repo": "huggingface_evaluate"}}, "top_k_context": [{"context": "It first canonicalizes the inputs (e.g., converting \"1/2\" to \"\\\\frac{1}{2}\") and then computes accuracy.\n\"\"\"\n\n\n_KWARGS_DESCRIPTION = r\"\"\"\nCalculates accuracy after canonicalizing inputs.\n\nArgs:\n    predictions: list of predictions to score. Each prediction\n        is a string that contains natural language and LaTex.\n    references: list of reference for each prediction. Each\n        reference is a string that contains natural language\n        and LaTex.\nReturns:\n    accuracy: accuracy after canonicalizing inputs\n        (e.g., converting \"1/2\" to \"\\\\frac{1}{2}\")\n\nExamples:\n    >>> metric = evaluate.load(\"competition_math\")\n    >>> results = metric.compute(references=[\"\\\\frac{1}{2}\"], predictions=[\"1/2\"])", "metadata": [{"fpath_tuple": ["huggingface_evaluate", "metrics", "competition_math", "competition_math.py"], "line_no": 50, "start_line_no": 40, "end_line_no": 60, "window_size": 20, "repo": "huggingface_evaluate", "slice_size": 10}], "sim_score": 0.2777777777777778}, {"context": "\"\"\"\n\n_KWARGS_DESCRIPTION = \"\"\"\nArgs:\n    `data`: a list of `str` to be checked for duplicates.\n\nReturns:\n    `duplicate_fraction` (`float`) : the fraction of strings that are duplicated.\n    `duplicates_dict` (`dict`) (optional) : a dictionary containing tuples with the duplicate strings and the number of times they are repeated.\n\nExamples:\n    >>> data = [\"hello sun\",\"hello moon\", \"hello sun\"]\n    >>> duplicates = evaluate.load(\"text_duplicates\")\n    >>> results = duplicates.compute(data=data)\n    >>> print(results)\n    {'duplicate_fraction': 0.33333333333333337}\n\n    >>> data = [\"hello sun\",\"hello moon\", \"hello sun\"]\n    >>> duplicates = evaluate.load(\"text_duplicates\")\n    >>> results =  duplicates.compute(data=data, list_duplicates=True)", "metadata": [{"fpath_tuple": ["huggingface_evaluate", "measurements", "text_duplicates", "text_duplicates.py"], "line_no": 36, "start_line_no": 26, "end_line_no": 46, "window_size": 20, "repo": "huggingface_evaluate", "slice_size": 10}], "sim_score": 0.273972602739726}, {"context": "import evaluate\n\n\n_DESCRIPTION = \"\"\"\nReturns the rate at which the predictions of one model exactly match those of another model.\n\"\"\"\n\n\n_KWARGS_DESCRIPTION = \"\"\"\nArgs:\n    predictions1 (`list` of `int`): Predicted labels for model 1.\n    predictions2 (`list` of `int`): Predicted labels for model 2.\n\nReturns:\n    exact_match (`float`): Dictionary containing exact_match rate. Possible values are between 0.0 and 1.0, inclusive.\n\nExamples:\n    >>> exact_match = evaluate.load(\"exact_match\", module_type=\"comparison\")\n    >>> results = exact_match.compute(predictions1=[1, 1, 1], predictions2=[1, 1, 1])\n    >>> print(results)", "metadata": [{"fpath_tuple": ["huggingface_evaluate", "comparisons", "exact_match", "exact_match.py"], "line_no": 28, "start_line_no": 18, "end_line_no": 38, "window_size": 20, "repo": "huggingface_evaluate", "slice_size": 10}], "sim_score": 0.2676056338028169}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# src/evaluate/loading.py\n# --------------------------------------------------\n#     head_hf_s3,\n#     hf_hub_url,\n#     init_hf_modules,\n#     is_relative_path,\n#     relative_to_absolute_path,\n#     url_or_path_join,\n# )\n# from .utils.logging import get_logger\n# \n# \n# logger = get_logger(__name__)\n# \n# \n# ALL_ALLOWED_EXTENSIONS = list(_EXTENSION_TO_MODULE.keys()) + [\"zip\"]\n# \n# \n# def init_dynamic_modules(\n#     name: str = config.MODULE_NAME_FOR_DYNAMIC_MODULES, hf_modules_cache: Optional[Union[Path, str]] = None\n# ):\n#     \"\"\"\n# --------------------------------------------------\n# the below code fragment can be found in:\n# src/evaluate/commands/evaluate_cli.py\n# --------------------------------------------------\n# import argparse\n# import os\n# import subprocess\n# from pathlib import Path\n# \n# from cookiecutter.main import cookiecutter\n# from huggingface_hub import HfApi, Repository, create_repo\n# \n# from evaluate.utils.logging import get_logger\n# \n# \n# logger = get_logger(__name__)\n# \n# INSTRUCTIONS = \"\"\"\\\n# --------------------------------------------------\n# the below code fragment can be found in:\n# src/evaluate/loading.py\n# --------------------------------------------------\n#     init_hf_modules,\n#     is_relative_path,\n#     relative_to_absolute_path,\n#     url_or_path_join,\n# )\n# from .utils.logging import get_logger\n# \n# \n# logger = get_logger(__name__)\n# \n# \n# ALL_ALLOWED_EXTENSIONS = list(_EXTENSION_TO_MODULE.keys()) + [\"zip\"]\n# \n# \n# def init_dynamic_modules(\n#     name: str = config.MODULE_NAME_FOR_DYNAMIC_MODULES, hf_modules_cache: Optional[Union[Path, str]] = None\n# ):\n#     \"\"\"\n#     Create a module with name `name` in which you can add dynamic modules\n#     such as metrics or datasets. The module can be imported using its name.\n# --------------------------------------------------\n# the below code fragment can be found in:\n# src/evaluate/config.py\n# --------------------------------------------------\n# import importlib\n# import os\n# import platform\n# from pathlib import Path\n# \n# from packaging import version\n# \n# from .utils.logging import get_logger\n# \n# \n# logger = get_logger(__name__)\n# \n# \n# # Metrics\n# S3_METRICS_BUCKET_PREFIX = \"https://s3.amazonaws.com/datasets.huggingface.co/datasets/metrics\"\n# CLOUDFRONT_METRICS_DISTRIB_PREFIX = \"https://cdn-datasets.huggingface.co/datasets/metric\"\n# --------------------------------------------------\n# the below code fragment can be found in:\n# src/evaluate/config.py\n# --------------------------------------------------\n# import importlib\n# import os\n# import platform\n# from pathlib import Path\n# \n# from packaging import version\n# \n# from .utils.logging import get_logger\n# \n# \n# logger = get_logger(__name__)\n# \n# \n# # Metrics\n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\nfrom typing import Dict\n\nimport requests\nfrom huggingface_hub import dataset_info, model_info\nfrom huggingface_hub.repocard import metadata_update\n\nfrom .config import HF_HUB_ALLOWED_TASKS\nfrom .utils.logging import get_logger\n\n\nlogger = get_logger(__name__)\n\n\ndef push_to_hub(\n    model_id: str,\n    task_type: str,\n    dataset_type: str,\n    dataset_name: str,\n    metric_type: str,\n    metric_name: str,\n    metric_value: float,\n    task_name: str = None,\n    dataset_config: str = None,", "metadata": {"task_id": "huggingface_evaluate/114", "ground_truth": "    dataset_split: str = None,", "fpath_tuple": ["huggingface_evaluate", "src", "evaluate", "hub.py"], "context_start_lineno": 0, "line_no": 23, "query_window": {"context": "from huggingface_hub import dataset_info, model_info\nfrom huggingface_hub.repocard import metadata_update\n\nfrom .config import HF_HUB_ALLOWED_TASKS\nfrom .utils.logging import get_logger\n\n\nlogger = get_logger(__name__)\n\n\ndef push_to_hub(\n    model_id: str,\n    task_type: str,\n    dataset_type: str,\n    dataset_name: str,\n    metric_type: str,\n    metric_name: str,\n    metric_value: float,\n    task_name: str = None,\n    dataset_config: str = None,", "metadata": {"fpath_tuple": ["huggingface_evaluate", "src", "evaluate", "hub.py"], "line_no": 23, "task_id": "huggingface_evaluate/114", "start_line_no": 3, "end_line_no": 23, "window_size": 20, "context_start_lineno": 0, "repo": "huggingface_evaluate"}}, "top_k_context": [{"context": "import importlib\nimport os\nimport platform\nfrom pathlib import Path\n\nfrom packaging import version\n\nfrom .utils.logging import get_logger\n\n\nlogger = get_logger(__name__)\n", "metadata": [{"fpath_tuple": ["huggingface_evaluate", "src", "evaluate", "config.py"], "line_no": 2, "start_line_no": 0, "end_line_no": 12, "window_size": 20, "repo": "huggingface_evaluate", "slice_size": 10}], "sim_score": 0.288135593220339}, {"context": "import importlib\nimport os\nimport platform\nfrom pathlib import Path\n\nfrom packaging import version\n\nfrom .utils.logging import get_logger\n\n\nlogger = get_logger(__name__)\n\n\n# Metrics", "metadata": [{"fpath_tuple": ["huggingface_evaluate", "src", "evaluate", "config.py"], "line_no": 4, "start_line_no": 0, "end_line_no": 14, "window_size": 20, "repo": "huggingface_evaluate", "slice_size": 10}], "sim_score": 0.27419354838709675}, {"context": "    head_hf_s3,\n    hf_hub_url,\n    init_hf_modules,\n    is_relative_path,\n    relative_to_absolute_path,\n    url_or_path_join,\n)\nfrom .utils.logging import get_logger\n\n\nlogger = get_logger(__name__)\n\n\nALL_ALLOWED_EXTENSIONS = list(_EXTENSION_TO_MODULE.keys()) + [\"zip\"]\n\n\ndef init_dynamic_modules(\n    name: str = config.MODULE_NAME_FOR_DYNAMIC_MODULES, hf_modules_cache: Optional[Union[Path, str]] = None\n):\n    \"\"\"", "metadata": [{"fpath_tuple": ["huggingface_evaluate", "src", "evaluate", "loading.py"], "line_no": 50, "start_line_no": 40, "end_line_no": 60, "window_size": 20, "repo": "huggingface_evaluate", "slice_size": 10}], "sim_score": 0.27184466019417475}, {"context": "import argparse\nimport os\nimport subprocess\nfrom pathlib import Path\n\nfrom cookiecutter.main import cookiecutter\nfrom huggingface_hub import HfApi, Repository, create_repo\n\nfrom evaluate.utils.logging import get_logger\n\n\nlogger = get_logger(__name__)", "metadata": [{"fpath_tuple": ["huggingface_evaluate", "src", "evaluate", "commands", "evaluate_cli.py"], "line_no": 2, "start_line_no": 0, "end_line_no": 12, "window_size": 20, "repo": "huggingface_evaluate", "slice_size": 10}], "sim_score": 0.2702702702702703}, {"context": "    DownloadConfig,\n    cached_path,\n    head_hf_s3,\n    hf_hub_url,\n    init_hf_modules,\n    is_relative_path,\n    relative_to_absolute_path,\n    url_or_path_join,\n)\nfrom .utils.logging import get_logger\n\n\nlogger = get_logger(__name__)\n\n\nALL_ALLOWED_EXTENSIONS = list(_EXTENSION_TO_MODULE.keys()) + [\"zip\"]\n\n\ndef init_dynamic_modules(\n    name: str = config.MODULE_NAME_FOR_DYNAMIC_MODULES, hf_modules_cache: Optional[Union[Path, str]] = None", "metadata": [{"fpath_tuple": ["huggingface_evaluate", "src", "evaluate", "loading.py"], "line_no": 48, "start_line_no": 38, "end_line_no": 58, "window_size": 20, "repo": "huggingface_evaluate", "slice_size": 10}], "sim_score": 0.2692307692307692}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# torchrl/envs/model_based/common.py\n# --------------------------------------------------\n#         world_model: SafeModule,\n#         params: Optional[List[torch.Tensor]] = None,\n#         buffers: Optional[List[torch.Tensor]] = None,\n#         device: DEVICE_TYPING = \"cpu\",\n#         dtype: Optional[Union[torch.dtype, np.dtype]] = None,\n#         batch_size: Optional[torch.Size] = None,\n#         run_type_checks: bool = False,\n#     ):\n#         super(ModelBasedEnvBase, self).__init__(\n#             device=device,\n#             dtype=dtype,\n#             batch_size=batch_size,\n#             run_type_checks=run_type_checks,\n#         )\n#         self.world_model = world_model.to(self.device)\n#         self.world_model_params = params\n#         self.world_model_buffers = buffers\n# \n#     @classmethod\n#     def __new__(cls, *args, **kwargs):\n# --------------------------------------------------\n# the below code fragment can be found in:\n# torchrl/envs/model_based/common.py\n# --------------------------------------------------\n#     def __init__(\n#         self,\n#         world_model: SafeModule,\n#         params: Optional[List[torch.Tensor]] = None,\n#         buffers: Optional[List[torch.Tensor]] = None,\n#         device: DEVICE_TYPING = \"cpu\",\n#         dtype: Optional[Union[torch.dtype, np.dtype]] = None,\n#         batch_size: Optional[torch.Size] = None,\n#         run_type_checks: bool = False,\n#     ):\n#         super(ModelBasedEnvBase, self).__init__(\n#             device=device,\n#             dtype=dtype,\n#             batch_size=batch_size,\n#             run_type_checks=run_type_checks,\n#         )\n#         self.world_model = world_model.to(self.device)\n#         self.world_model_params = params\n#         self.world_model_buffers = buffers\n# \n# --------------------------------------------------\n# the below code fragment can be found in:\n# torchrl/envs/model_based/common.py\n# --------------------------------------------------\n#     \"\"\"\n# \n#     def __init__(\n#         self,\n#         world_model: SafeModule,\n#         params: Optional[List[torch.Tensor]] = None,\n#         buffers: Optional[List[torch.Tensor]] = None,\n#         device: DEVICE_TYPING = \"cpu\",\n#         dtype: Optional[Union[torch.dtype, np.dtype]] = None,\n#         batch_size: Optional[torch.Size] = None,\n#         run_type_checks: bool = False,\n#     ):\n#         super(ModelBasedEnvBase, self).__init__(\n#             device=device,\n#             dtype=dtype,\n#             batch_size=batch_size,\n#             run_type_checks=run_type_checks,\n#         )\n#         self.world_model = world_model.to(self.device)\n#         self.world_model_params = params\n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\n,\n        batch_locked: bool = True,\n    ):\n        self.device = device\n        self.tensordict = tensordict\n        self.specs = specs\n        self.batch_size = batch_size\n        self.env_str = env_str\n        self.batch_locked = batch_locked\n\n    @property\n    def tensordict(self):\n        return self._tensordict.to(self.device)\n\n    @property\n    def specs(self):\n        return self._specs.to(self.device)\n\n    @tensordict.setter\n    def tensordict(self, value: TensorDictBase):\n        self._tensordict = value.to(\"cpu\")\n\n    @specs.setter\n    def specs(self, value: CompositeSpec):\n        self._specs = value.to(\"cpu\")\n\n    @staticmethod\n    def build_metadata_from_env(env) -> EnvMetaData:\n        tensordict = env.fake_tensordict().clone()\n        specs = {\n            \"input_spec\": env.input_spec,\n            \"observation_spec\": env.observation_spec,\n            \"reward_spec\": env.reward_spec,\n        }\n        specs = CompositeSpec(**specs, shape=env.batch_size).to(\"cpu\")\n\n        batch_size = env.batch_size\n        env_str = str(env)\n        device = env.device\n        specs = specs.to(\"cpu\")\n        batch_locked = env.batch_locked\n        return EnvMetaData(tensordict, specs, batch_size, env_str, device, batch_locked)\n\n    def expand(self, *size: int) -> EnvMetaData:\n        tensordict = self.tensordict.expand(*size).to_tensordict()\n        batch_size = torch.Size(list(size))\n        return EnvMetaData(\n            tensordict,\n            self.specs.expand(*size),\n            batch_size,\n            self.env_str,\n            self.device,\n            self.batch_locked,\n        )\n\n    def clone(self):\n        return EnvMetaData(\n            self.tensordict.clone(),\n            self.specs.clone(),\n            torch.Size([*self.batch_size]),\n            deepcopy(self.env_str),\n            self.device,\n            self.batch_locked,\n        )\n\n    def to(self, device: DEVICE_TYPING) -> EnvMetaData:\n        tensordict = self.tensordict.contiguous().to(device)\n        specs = self.specs.to(device)\n        return EnvMetaData(\n            tensordict, specs, self.batch_size, self.env_str, device, self.batch_locked\n        )\n\n\nclass Specs:\n    \"\"\"Container for action, observation and reward specs.\n\n    This class allows one to create an environment, retrieve all of the specs\n    in a single data container (and access them in one place) before erasing\n    the environment from the workspace.\n\n    Args:\n        env (EnvBase): environment from which the specs have to be read.\n\n    \"\"\"\n\n    _keys = {\n        \"action_spec\",\n        \"observation_spec\",\n        \"reward_spec\",\n        \"input_spec\",\n        \"from_pixels\",\n    }\n\n    def __init__(self, env: EnvBase):\n        self.env = env\n\n    def __getitem__(self, item: str) -> Any:\n        if item not in self._keys:\n            raise KeyError(f\"item must be one of {self._keys}\")\n        return getattr(self.env, item)\n\n    def keys(self) -> Sequence[str]:\n        return self._keys\n\n    def build_tensordict(\n        self, next_observation: bool = True, log_prob: bool = False\n    ) -> TensorDictBase:\n        \"\"\"Returns a TensorDict with empty tensors of the desired shape.\n\n        Args:\n            next_observation (bool, optional): if False, the observation returned\n                will be of the current step only (no :obj:`\"next\"` nested tensordict will be present).\n                Default is True.\n            log_prob (bool, optional): If True, a log_prob key-value pair will be added\n                to the tensordict.\n\n        Returns: A tensordict populated according to the env specs.\n\n        \"\"\"\n        # build a tensordict from specs\n        td = TensorDict({}, batch_size=torch.Size([]), _run_checks=False)\n        action_placeholder = torch.zeros(\n            self[\"action_spec\"].shape, dtype=self[\"action_spec\"].dtype\n        )\n        if not isinstance(self[\"observation_spec\"], CompositeSpec):\n            raise RuntimeError(\"observation_spec is expected to be of Composite type.\")\n        else:\n            for (key, item) in self[\"observation_spec\"].items():\n                observation_placeholder = torch.zeros(item.shape, dtype=item.dtype)\n                if next_observation:\n                    td.update({\"next\": {key: observation_placeholder}})\n                td.set(\n                    key,\n                    observation_placeholder.clone(),\n                )\n\n        reward_placeholder = torch.zeros(\n            self[\"reward_spec\"].shape, dtype=self[\"reward_spec\"].dtype\n        )\n        done_placeholder = torch.zeros_like(reward_placeholder, dtype=torch.bool)\n\n        td.set(\"action\", action_placeholder)\n        td.set(\"reward\", reward_placeholder)\n\n        if log_prob:\n            td.set(\n                \"log_prob\",\n                torch.zeros_like(reward_placeholder, dtype=torch.float32),\n            )  # we assume log_prob to be of type float32\n        td.set(\"done\", done_placeholder)\n        return td\n\n\nclass EnvBase(nn.Module, metaclass=abc.ABCMeta):\n    \"\"\"Abstract environment parent class.\n\n    Properties:\n        - observation_spec (CompositeSpec): sampling spec of the observations;\n        - action_spec (TensorSpec): sampling spec of the actions;\n        - input_spec (CompositeSpec): sampling spec of the actions and/or other inputs;\n        - reward_spec (TensorSpec): sampling spec of the rewards;\n        - batch_size (torch.Size): number of environments contained in the instance;\n        - device (torch.device): device where the env input and output are expected to live\n        - run_type_checks (bool): if True, the observation and reward dtypes\n            will be compared against their respective spec and an exception\n            will be raised if they don't match.\n\n    Methods:\n        step (TensorDictBase -> TensorDictBase): step in the environment\n        reset (TensorDictBase, optional -> TensorDictBase): reset the environment\n        set_seed (int -> int): sets the seed of the environment\n        rand_step (TensorDictBase, optional -> TensorDictBase): random step given the action spec\n        rollout (Callable, ... -> TensorDictBase): executes a rollout in the environment with the given policy (or random\n            steps if no policy is provided)\n\n    \"\"\"\n\n    def __init__(\n        self,\n        device: DEVICE_TYPING = \"cpu\",\n        dtype: Optional[Union[torch.dtype, np.dtype]] = None,\n        batch_size: Optional[torch.Size] = None,\n        run_type_checks: bool = True,\n    ):\n        super().__init__()\n        if device is not None:\n            self.device = torch.device(device)\n        self.dtype = dtype_map.get(dtype, dtype)\n        if \"is_closed\" not in self.__dir__():\n            self.is_closed = True\n        if \"_input_spec\" not in self.__dir__():\n            self.__dict__[\"_input_spec\"] = None\n        if \"_reward_spec\" not in self.__dir__():\n            self.__dict__[\"_reward_spec\"] = None", "metadata": {"task_id": "pytorch_rl/53", "ground_truth": "        if \"_observation_spec\" not in self.__dir__():", "fpath_tuple": ["pytorch_rl", "torchrl", "envs", "common.py"], "context_start_lineno": 46, "line_no": 240, "query_window": {"context": "\n    \"\"\"\n\n    def __init__(\n        self,\n        device: DEVICE_TYPING = \"cpu\",\n        dtype: Optional[Union[torch.dtype, np.dtype]] = None,\n        batch_size: Optional[torch.Size] = None,\n        run_type_checks: bool = True,\n    ):\n        super().__init__()\n        if device is not None:\n            self.device = torch.device(device)\n        self.dtype = dtype_map.get(dtype, dtype)\n        if \"is_closed\" not in self.__dir__():\n            self.is_closed = True\n        if \"_input_spec\" not in self.__dir__():\n            self.__dict__[\"_input_spec\"] = None\n        if \"_reward_spec\" not in self.__dir__():\n            self.__dict__[\"_reward_spec\"] = None", "metadata": {"fpath_tuple": ["pytorch_rl", "torchrl", "envs", "common.py"], "line_no": 240, "task_id": "pytorch_rl/53", "start_line_no": 220, "end_line_no": 240, "window_size": 20, "context_start_lineno": 46, "repo": "pytorch_rl"}}, "top_k_context": [{"context": "            steps if no policy is provided)\n\n    \"\"\"\n\n    def __init__(\n        self,\n        world_model: SafeModule,\n        params: Optional[List[torch.Tensor]] = None,\n        buffers: Optional[List[torch.Tensor]] = None,\n        device: DEVICE_TYPING = \"cpu\",\n        dtype: Optional[Union[torch.dtype, np.dtype]] = None,\n        batch_size: Optional[torch.Size] = None,\n        run_type_checks: bool = False,\n    ):\n        super(ModelBasedEnvBase, self).__init__(\n            device=device,\n            dtype=dtype,\n            batch_size=batch_size,\n            run_type_checks=run_type_checks,\n        )", "metadata": [{"fpath_tuple": ["pytorch_rl", "torchrl", "envs", "model_based", "common.py"], "line_no": 118, "start_line_no": 108, "end_line_no": 128, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}], "sim_score": 0.5263157894736842}, {"context": "    \"\"\"\n\n    def __init__(\n        self,\n        world_model: SafeModule,\n        params: Optional[List[torch.Tensor]] = None,\n        buffers: Optional[List[torch.Tensor]] = None,\n        device: DEVICE_TYPING = \"cpu\",\n        dtype: Optional[Union[torch.dtype, np.dtype]] = None,\n        batch_size: Optional[torch.Size] = None,\n        run_type_checks: bool = False,\n    ):\n        super(ModelBasedEnvBase, self).__init__(\n            device=device,\n            dtype=dtype,\n            batch_size=batch_size,\n            run_type_checks=run_type_checks,\n        )\n        self.world_model = world_model.to(self.device)\n        self.world_model_params = params", "metadata": [{"fpath_tuple": ["pytorch_rl", "torchrl", "envs", "model_based", "common.py"], "line_no": 120, "start_line_no": 110, "end_line_no": 130, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}], "sim_score": 0.5052631578947369}, {"context": "    def __init__(\n        self,\n        world_model: SafeModule,\n        params: Optional[List[torch.Tensor]] = None,\n        buffers: Optional[List[torch.Tensor]] = None,\n        device: DEVICE_TYPING = \"cpu\",\n        dtype: Optional[Union[torch.dtype, np.dtype]] = None,\n        batch_size: Optional[torch.Size] = None,\n        run_type_checks: bool = False,\n    ):\n        super(ModelBasedEnvBase, self).__init__(\n            device=device,\n            dtype=dtype,\n            batch_size=batch_size,\n            run_type_checks=run_type_checks,\n        )\n        self.world_model = world_model.to(self.device)\n        self.world_model_params = params\n        self.world_model_buffers = buffers\n", "metadata": [{"fpath_tuple": ["pytorch_rl", "torchrl", "envs", "model_based", "common.py"], "line_no": 122, "start_line_no": 112, "end_line_no": 132, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}], "sim_score": 0.4742268041237113}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# fortuna/conformal/regression/jackknifeplus.py\n# --------------------------------------------------\n#                 \"This method is supported only for scalar model outputs only. However, `loo_val_outputs` has second \"\n#                 \"dimension greater than 1.\")\n#         if loo_val_targets.ndim == 1:\n#             loo_val_targets = loo_val_targets[:, None]\n#         elif loo_val_targets.shape[1] != 1:\n#             raise ValueError(\"This method is supported only for scalar target variables. However, `loo_val_targets` \"\n#                              \"has second dimension greater than 1.\")\n#         if loo_test_outputs.ndim < 2:\n#             raise ValueError(\"`loo_test_outputs` must have at least two dimensions.\")\n#         elif loo_test_outputs.ndim == 2:\n#             loo_test_outputs = loo_test_outputs[:, :, None]\n#         elif loo_test_outputs.shape[2] != 1:\n#             raise ValueError(\"This method is supported only for scalar model outputs only. However, `loo_test_outputs` \"\n#                              \"has last dimension greater than 1.\")\n# \n#         r = jnp.abs(loo_val_targets - loo_val_outputs)\n#         left = loo_test_outputs - r[:, None]\n#         right = loo_test_outputs + r[:, None]\n# \n#         qleft = jnp.quantile(left, q=error, axis=0)\n# --------------------------------------------------\n# the below code fragment can be found in:\n# fortuna/conformal/regression/jackknifeplus.py\n# --------------------------------------------------\n#         elif loo_val_outputs.shape[1] != 1:\n#             raise ValueError(\n#                 \"This method is supported only for scalar model outputs only. However, `loo_val_outputs` has second \"\n#                 \"dimension greater than 1.\")\n#         if loo_val_targets.ndim == 1:\n#             loo_val_targets = loo_val_targets[:, None]\n#         elif loo_val_targets.shape[1] != 1:\n#             raise ValueError(\"This method is supported only for scalar target variables. However, `loo_val_targets` \"\n#                              \"has second dimension greater than 1.\")\n#         if loo_test_outputs.ndim < 2:\n#             raise ValueError(\"`loo_test_outputs` must have at least two dimensions.\")\n#         elif loo_test_outputs.ndim == 2:\n#             loo_test_outputs = loo_test_outputs[:, :, None]\n#         elif loo_test_outputs.shape[2] != 1:\n#             raise ValueError(\"This method is supported only for scalar model outputs only. However, `loo_test_outputs` \"\n#                              \"has last dimension greater than 1.\")\n# \n#         r = jnp.abs(loo_val_targets - loo_val_outputs)\n#         left = loo_test_outputs - r[:, None]\n#         right = loo_test_outputs + r[:, None]\n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\nimport jax.numpy as jnp\nfrom fortuna.typing import Array\nfrom typing import List\n\n\nclass CVPlusConformalRegressor:\n    \"\"\"\n    This class implements the CV+ method introduced in\n    `Barber et al., 2021 <https://www.stat.cmu.edu/~ryantibs/papers/jackknife.pdf>`__. It is an extension of the\n    jackknife+ method, introduced in the same work, that consider a K-Fold instead of a leave-one-out strategy. If\n    :code:`K=n`, where :code:`n` is the total number of training data, then CV+ reduces to jackknife+.\n    \"\"\"\n    def conformal_interval(\n            self,\n            cross_val_outputs: List[Array],\n            cross_val_targets: List[Array],\n            cross_test_outputs: List[Array],\n            error: float\n    ) -> jnp.ndarray:\n        \"\"\"\n        Coverage interval of each of the test inputs, at the desired coverage error. This is supported only for\n        one-dimensional target variables.\n\n        Parameters\n        ----------\n        cross_val_outputs: List[Array]\n            Outputs of the models used during cross validation evaluated at their respective validation inputs. More\n            precisely, we assume the training data has been jointly partitioned in :code:`K` subsets. The i-th element\n            of the list of :code: `cross_val_outputs` is a model trained on all data but the i-th partition, and has\n            been evaluated at the inputs of the partition i-th itself, for :code:`i=1, 2, ..., K`.\n        cross_val_targets: List[Array]\n            Target variables organized in the same partitions used for `cross_val_outputs`. More precisely, the i-th\n            element of :code:`cross_val_targets` includes the array of target variables of the i-th partition of the\n            training data, for :code:`i=1, 2, ..., K`.\n        cross_test_outputs: List[Array]\n            Outputs of the models used during cross validation evaluated at the test inputs. More precisely, consider\n            the same partition of data as the one used for :code:`cross_val_outputs`. Then the i-th element of\n            :code:`cross_test_outputs` represents the outputs of the model that has been trained upon all the training\n            data but the i-th partition, and evaluated at the test inputs, for :code:`i=1, 2, ..., K`.\n        error: float\n            The desired coverage error. This must be a scalar between 0 and 1, extremes included.\n        Returns\n        -------\n        jnp.ndarray\n            The conformal intervals. The two components of the second axis correspond to the left and right interval\n            bounds.\n        \"\"\"\n        if type(cross_val_outputs) != list:\n            raise TypeError(\"`cross_val_outputs` must be a list of arrays.\")\n        if type(cross_val_targets) != list:\n            raise TypeError(\"`cross_val_targets` must be a list of arrays.\")\n        if type(cross_test_outputs) != list:\n            raise TypeError(\"`cross_test_outputs` must be a list of arrays.\")\n        for i, (mu, y, mu_test) in enumerate(zip(cross_val_outputs, cross_val_targets, cross_test_outputs)):\n            if mu.shape[0] != y.shape[0]:\n                raise ValueError(\"The first dimension of the i-th element in `cross_val_outputs` must be the same as \"\n                                 \"the one of the i-th element in `cross_val_targets`.\")\n            if mu.ndim == 1:\n                cross_val_outputs[i] = mu[:, None]\n            elif mu.shape[1] != 1:\n                raise ValueError(\n                    \"This method is supported only for scalar model outputs only. However, an element of \"\n                    \"`cross_val_outputs` has second dimension greater than 1.\")\n            if y.ndim == 1:\n                cross_val_targets[i] = y[:, None]\n            elif y.shape[1] != 1:\n                raise ValueError(\"This method is supported only for scalar target variables. However, an element of \"\n                                 \"`cross_val_targets` has second dimension greater than 1.\")\n            if mu_test.ndim == 1:\n                cross_test_outputs[i] = mu_test[:, None]\n            elif mu_test.shape[1] != 1:\n                raise ValueError(\"This method is supported only for scalar model outputs only. However, an element of \"\n                                 \"`cross_test_outputs` has second dimension greater than 1.\")\n\n        r = [jnp.abs(y - mu) for y, mu in zip(cross_val_targets, cross_val_outputs)]\n        left = jnp.concatenate([mu[None] - ri[:, None] for mu, ri in zip(cross_test_outputs, r)], 0)", "metadata": {"task_id": "awslabs_fortuna/154", "ground_truth": "        right = jnp.concatenate([mu[None] + ri[:, None] for mu, ri in zip(cross_test_outputs, r)])", "fpath_tuple": ["awslabs_fortuna", "fortuna", "conformal", "regression", "cvplus.py"], "context_start_lineno": 0, "line_no": 76, "query_window": {"context": "                                 \"the one of the i-th element in `cross_val_targets`.\")\n            if mu.ndim == 1:\n                cross_val_outputs[i] = mu[:, None]\n            elif mu.shape[1] != 1:\n                raise ValueError(\n                    \"This method is supported only for scalar model outputs only. However, an element of \"\n                    \"`cross_val_outputs` has second dimension greater than 1.\")\n            if y.ndim == 1:\n                cross_val_targets[i] = y[:, None]\n            elif y.shape[1] != 1:\n                raise ValueError(\"This method is supported only for scalar target variables. However, an element of \"\n                                 \"`cross_val_targets` has second dimension greater than 1.\")\n            if mu_test.ndim == 1:\n                cross_test_outputs[i] = mu_test[:, None]\n            elif mu_test.shape[1] != 1:\n                raise ValueError(\"This method is supported only for scalar model outputs only. However, an element of \"\n                                 \"`cross_test_outputs` has second dimension greater than 1.\")\n\n        r = [jnp.abs(y - mu) for y, mu in zip(cross_val_targets, cross_val_outputs)]\n        left = jnp.concatenate([mu[None] - ri[:, None] for mu, ri in zip(cross_test_outputs, r)], 0)", "metadata": {"fpath_tuple": ["awslabs_fortuna", "fortuna", "conformal", "regression", "cvplus.py"], "line_no": 76, "task_id": "awslabs_fortuna/154", "start_line_no": 56, "end_line_no": 76, "window_size": 20, "context_start_lineno": 0, "repo": "awslabs_fortuna"}}, "top_k_context": [{"context": "        if loo_val_outputs.ndim == 1:\n            loo_val_outputs = loo_val_outputs[:, None]\n        elif loo_val_outputs.shape[1] != 1:\n            raise ValueError(\n                \"This method is supported only for scalar model outputs only. However, `loo_val_outputs` has second \"\n                \"dimension greater than 1.\")\n        if loo_val_targets.ndim == 1:\n            loo_val_targets = loo_val_targets[:, None]\n        elif loo_val_targets.shape[1] != 1:\n            raise ValueError(\"This method is supported only for scalar target variables. However, `loo_val_targets` \"\n                             \"has second dimension greater than 1.\")\n        if loo_test_outputs.ndim < 2:\n            raise ValueError(\"`loo_test_outputs` must have at least two dimensions.\")\n        elif loo_test_outputs.ndim == 2:\n            loo_test_outputs = loo_test_outputs[:, :, None]\n        elif loo_test_outputs.shape[2] != 1:\n            raise ValueError(\"This method is supported only for scalar model outputs only. However, `loo_test_outputs` \"\n                             \"has last dimension greater than 1.\")\n\n        r = jnp.abs(loo_val_targets - loo_val_outputs)", "metadata": [{"fpath_tuple": ["awslabs_fortuna", "fortuna", "conformal", "regression", "jackknifeplus.py"], "line_no": 56, "start_line_no": 46, "end_line_no": 66, "window_size": 20, "repo": "awslabs_fortuna", "slice_size": 10}], "sim_score": 0.5752212389380531}, {"context": "        elif loo_val_outputs.shape[1] != 1:\n            raise ValueError(\n                \"This method is supported only for scalar model outputs only. However, `loo_val_outputs` has second \"\n                \"dimension greater than 1.\")\n        if loo_val_targets.ndim == 1:\n            loo_val_targets = loo_val_targets[:, None]\n        elif loo_val_targets.shape[1] != 1:\n            raise ValueError(\"This method is supported only for scalar target variables. However, `loo_val_targets` \"\n                             \"has second dimension greater than 1.\")\n        if loo_test_outputs.ndim < 2:\n            raise ValueError(\"`loo_test_outputs` must have at least two dimensions.\")\n        elif loo_test_outputs.ndim == 2:\n            loo_test_outputs = loo_test_outputs[:, :, None]\n        elif loo_test_outputs.shape[2] != 1:\n            raise ValueError(\"This method is supported only for scalar model outputs only. However, `loo_test_outputs` \"\n                             \"has last dimension greater than 1.\")\n\n        r = jnp.abs(loo_val_targets - loo_val_outputs)\n        left = loo_test_outputs - r[:, None]\n        right = loo_test_outputs + r[:, None]", "metadata": [{"fpath_tuple": ["awslabs_fortuna", "fortuna", "conformal", "regression", "jackknifeplus.py"], "line_no": 58, "start_line_no": 48, "end_line_no": 68, "window_size": 20, "repo": "awslabs_fortuna", "slice_size": 10}], "sim_score": 0.5739130434782609}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# test/test_cost.py\n# --------------------------------------------------\n# \n# class TestDDPG:\n#     seed = 0\n# \n#     def _create_mock_actor(self, batch=2, obs_dim=3, action_dim=4, device=\"cpu\"):\n#         # Actor\n#         action_spec = BoundedTensorSpec(\n#             -torch.ones(action_dim), torch.ones(action_dim), (action_dim,)\n#         )\n#         module = nn.Linear(obs_dim, action_dim)\n#         actor = Actor(\n#             spec=action_spec,\n#             module=module,\n#         )\n#         return actor.to(device)\n# \n#     def _create_mock_value(self, batch=2, obs_dim=3, action_dim=4, device=\"cpu\"):\n#         # Actor\n#         class ValueClass(nn.Module):\n#             def __init__(self):\n# --------------------------------------------------\n# the below code fragment can be found in:\n# test/test_cost.py\n# --------------------------------------------------\n#         with _check_td_steady(ms_td):\n#             loss_ms = loss_fn(ms_td)\n#         assert loss_fn.priority_key in ms_td.keys()\n# \n#         with torch.no_grad():\n#             loss = loss_fn(td)\n#         if n == 0:\n#             assert_allclose_td(td, ms_td.select(*list(td.keys())))\n#             _loss = sum([item for _, item in loss.items()])\n#             _loss_ms = sum([item for _, item in loss_ms.items()])\n#             assert (\n#                 abs(_loss - _loss_ms) < 1e-3\n#             ), f\"found abs(loss-loss_ms) = {abs(loss - loss_ms):4.5f} for n=0\"\n#         else:\n#             with pytest.raises(AssertionError):\n#                 assert_allclose_td(loss, loss_ms)\n#         sum([item for _, item in loss_ms.items()]).backward()\n#         assert torch.nn.utils.clip_grad.clip_grad_norm_(actor.parameters(), 1.0) > 0.0\n# \n#         # Check param update effect on targets\n# --------------------------------------------------\n# the below code fragment can be found in:\n# test/test_cost.py\n# --------------------------------------------------\n#             assert (\n#                 abs(_loss - _loss_ms) < 1e-3\n#             ), f\"found abs(loss-loss_ms) = {abs(loss - loss_ms):4.5f} for n=0\"\n#         else:\n#             with pytest.raises(AssertionError):\n#                 assert_allclose_td(loss, loss_ms)\n#         sum([item for _, item in loss_ms.items()]).backward()\n#         assert torch.nn.utils.clip_grad.clip_grad_norm_(actor.parameters(), 1.0) > 0.0\n# \n#         # Check param update effect on targets\n#         target_value = loss_fn.target_value_network_params.clone()\n#         for p in loss_fn.parameters():\n#             p.data += torch.randn_like(p)\n#         target_value2 = loss_fn.target_value_network_params.clone()\n#         if loss_fn.delay_value:\n#             assert_allclose_td(target_value, target_value2)\n#         else:\n#             assert not (target_value == target_value2).any()\n# \n#         # check that policy is updated after parameter update\n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\nbatch, T),\n            source={\n                \"observation\": obs.masked_fill_(~mask.unsqueeze(-1), 0.0),\n                \"next\": {\n                    \"observation\": next_obs.masked_fill_(~mask.unsqueeze(-1), 0.0)\n                },\n                \"done\": done,\n                \"collector\": {\"mask\": mask},\n                \"reward\": reward.masked_fill_(~mask.unsqueeze(-1), 0.0),\n                \"action\": action.masked_fill_(~mask.unsqueeze(-1), 0.0),\n            },\n            device=device,\n        )\n        return td\n\n    @pytest.mark.skipif(\n        not _has_functorch, reason=f\"functorch not installed: {FUNCTORCH_ERR}\"\n    )\n    @pytest.mark.parametrize(\"device\", get_available_devices())\n    @pytest.mark.parametrize(\"delay_actor,delay_value\", [(False, False), (True, True)])\n    def test_ddpg(self, delay_actor, delay_value, device):\n        torch.manual_seed(self.seed)\n        actor = self._create_mock_actor(device=device)\n        value = self._create_mock_value(device=device)\n        td = self._create_mock_data_ddpg(device=device)\n        loss_fn = DDPGLoss(\n            actor,\n            value,\n            gamma=0.9,\n            loss_function=\"l2\",\n            delay_actor=delay_actor,\n            delay_value=delay_value,\n        )\n        with _check_td_steady(td):\n            loss = loss_fn(td)\n\n        assert all(\n            (p.grad is None) or (p.grad == 0).all()\n            for p in loss_fn.value_network_params.values(True, True)\n        )\n        assert all(\n            (p.grad is None) or (p.grad == 0).all()\n            for p in loss_fn.actor_network_params.values(True, True)\n        )\n        # check that losses are independent\n        for k in loss.keys():\n            if not k.startswith(\"loss\"):\n                continue\n            loss[k].sum().backward(retain_graph=True)\n            if k == \"loss_actor\":\n                assert all(\n                    (p.grad is None) or (p.grad == 0).all()\n                    for p in loss_fn.value_network_params.values(True, True)\n                )\n                assert not any(\n                    (p.grad is None) or (p.grad == 0).all()\n                    for p in loss_fn.actor_network_params.values(True, True)\n                )\n            elif k == \"loss_value\":\n                assert all(\n                    (p.grad is None) or (p.grad == 0).all()\n                    for p in loss_fn.actor_network_params.values(True, True)\n                )\n                assert not any(\n                    (p.grad is None) or (p.grad == 0).all()\n                    for p in loss_fn.value_network_params.values(True, True)\n                )\n            else:\n                raise NotImplementedError(k)\n            loss_fn.zero_grad()\n\n        # check overall grad\n        sum([item for _, item in loss.items()]).backward()\n        parameters = list(actor.parameters()) + list(value.parameters())\n        for p in parameters:\n            assert p.grad.norm() > 0.0\n\n        # Check param update effect on targets\n        target_actor = [p.clone() for p in loss_fn.target_actor_network_params.values()]\n        target_value = [p.clone() for p in loss_fn.target_value_network_params.values()]\n        _i = -1\n        for _i, p in enumerate(loss_fn.parameters()):\n            p.data += torch.randn_like(p)\n        assert _i >= 0\n        target_actor2 = [\n            p.clone() for p in loss_fn.target_actor_network_params.values()\n        ]\n        target_value2 = [\n            p.clone() for p in loss_fn.target_value_network_params.values()\n        ]\n        if loss_fn.delay_actor:\n            assert all((p1 == p2).all() for p1, p2 in zip(target_actor, target_actor2))\n        else:\n            assert not any(\n                (p1 == p2).any() for p1, p2 in zip(target_actor, target_actor2)\n            )\n        if loss_fn.delay_value:\n            assert all((p1 == p2).all() for p1, p2 in zip(target_value, target_value2))\n        else:\n            assert not any(\n                (p1 == p2).any() for p1, p2 in zip(target_value, target_value2)\n            )\n\n        # check that policy is updated after parameter update\n        parameters = [p.clone() for p in actor.parameters()]\n        for p in loss_fn.parameters():\n            p.data += torch.randn_like(p)\n        assert all((p1 != p2).all() for p1, p2 in zip(parameters, actor.parameters()))\n\n    @pytest.mark.skipif(\n        not _has_functorch, reason=f\"functorch not installed: {FUNCTORCH_ERR}\"\n    )\n    @pytest.mark.parametrize(\"n\", list(range(4)))\n    @pytest.mark.parametrize(\"device\", get_available_devices())\n    @pytest.mark.parametrize(\"delay_actor,delay_value\", [(False, False), (True, True)])\n    def test_ddpg_batcher(self, n, delay_actor, delay_value, device, gamma=0.9):\n        torch.manual_seed(self.seed)\n        actor = self._create_mock_actor(device=device)\n        value = self._create_mock_value(device=device)\n        td = self._create_seq_mock_data_ddpg(device=device)\n        loss_fn = DDPGLoss(\n            actor,\n            value,\n            gamma=gamma,\n            loss_function=\"l2\",\n            delay_actor=delay_actor,\n            delay_value=delay_value,\n        )\n\n        ms = MultiStep(gamma=gamma, n_steps_max=n).to(device)\n        ms_td = ms(td.clone())\n        with _check_td_steady(ms_td):\n            loss_ms = loss_fn(ms_td)\n        with torch.no_grad():\n            loss = loss_fn(td)\n        if n == 0:\n            assert_allclose_td(td, ms_td.select(*list(td.keys())))\n            _loss = sum([item for _, item in loss.items()])\n            _loss_ms = sum([item for _, item in loss_ms.items()])\n            assert (\n                abs(_loss - _loss_ms) < 1e-3\n            ), f\"found abs(loss-loss_ms) = {abs(loss - loss_ms):4.5f} for n=0\"\n        else:\n            with pytest.raises(AssertionError):\n                assert_allclose_td(loss, loss_ms)\n        sum([item for _, item in loss_ms.items()]).backward()\n        parameters = list(actor.parameters()) + list(value.parameters())\n        for p in parameters:\n            assert p.grad.norm() > 0.0\n\n\nclass TestTD3:\n    seed = 0\n\n    def _create_mock_actor(self, batch=2, obs_dim=3, action_dim=4, device=\"cpu\"):\n        # Actor\n        action_spec = BoundedTensorSpec(\n            -torch.ones(action_dim), torch.ones(action_dim), (action_dim,)\n        )\n        module = nn.Linear(obs_dim, action_dim)", "metadata": {"task_id": "pytorch_rl/105", "ground_truth": "        actor = Actor(", "fpath_tuple": ["pytorch_rl", "test", "test_cost.py"], "context_start_lineno": 504, "line_no": 664, "query_window": {"context": "                abs(_loss - _loss_ms) < 1e-3\n            ), f\"found abs(loss-loss_ms) = {abs(loss - loss_ms):4.5f} for n=0\"\n        else:\n            with pytest.raises(AssertionError):\n                assert_allclose_td(loss, loss_ms)\n        sum([item for _, item in loss_ms.items()]).backward()\n        parameters = list(actor.parameters()) + list(value.parameters())\n        for p in parameters:\n            assert p.grad.norm() > 0.0\n\n\nclass TestTD3:\n    seed = 0\n\n    def _create_mock_actor(self, batch=2, obs_dim=3, action_dim=4, device=\"cpu\"):\n        # Actor\n        action_spec = BoundedTensorSpec(\n            -torch.ones(action_dim), torch.ones(action_dim), (action_dim,)\n        )\n        module = nn.Linear(obs_dim, action_dim)", "metadata": {"fpath_tuple": ["pytorch_rl", "test", "test_cost.py"], "line_no": 664, "task_id": "pytorch_rl/105", "start_line_no": 644, "end_line_no": 664, "window_size": 20, "context_start_lineno": 504, "repo": "pytorch_rl"}}, "top_k_context": [{"context": "            _loss = sum([item for _, item in loss.items()])\n            _loss_ms = sum([item for _, item in loss_ms.items()])\n            assert (\n                abs(_loss - _loss_ms) < 1e-3\n            ), f\"found abs(loss-loss_ms) = {abs(loss - loss_ms):4.5f} for n=0\"\n        else:\n            with pytest.raises(AssertionError):\n                assert_allclose_td(loss, loss_ms)\n        sum([item for _, item in loss_ms.items()]).backward()\n        assert torch.nn.utils.clip_grad.clip_grad_norm_(actor.parameters(), 1.0) > 0.0\n\n        # Check param update effect on targets\n        target_value = loss_fn.target_value_network_params.clone()\n        for p in loss_fn.parameters():\n            p.data += torch.randn_like(p)\n        target_value2 = loss_fn.target_value_network_params.clone()\n        if loss_fn.delay_value:\n            assert_allclose_td(target_value, target_value2)\n        else:\n            assert not (target_value == target_value2).any()", "metadata": [{"fpath_tuple": ["pytorch_rl", "test", "test_cost.py"], "line_no": 362, "start_line_no": 352, "end_line_no": 372, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}], "sim_score": 0.5273972602739726}, {"context": "        ms_td = ms(td.clone())\n\n        with _check_td_steady(ms_td):\n            loss_ms = loss_fn(ms_td)\n        assert loss_fn.priority_key in ms_td.keys()\n\n        with torch.no_grad():\n            loss = loss_fn(td)\n        if n == 0:\n            assert_allclose_td(td, ms_td.select(*list(td.keys())))\n            _loss = sum([item for _, item in loss.items()])\n            _loss_ms = sum([item for _, item in loss_ms.items()])\n            assert (\n                abs(_loss - _loss_ms) < 1e-3\n            ), f\"found abs(loss-loss_ms) = {abs(loss - loss_ms):4.5f} for n=0\"\n        else:\n            with pytest.raises(AssertionError):\n                assert_allclose_td(loss, loss_ms)\n        sum([item for _, item in loss_ms.items()]).backward()\n        assert torch.nn.utils.clip_grad.clip_grad_norm_(actor.parameters(), 1.0) > 0.0", "metadata": [{"fpath_tuple": ["pytorch_rl", "test", "test_cost.py"], "line_no": 352, "start_line_no": 342, "end_line_no": 362, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}], "sim_score": 0.5251798561151079}, {"context": "        assert all((p1 != p2).all() for p1, p2 in zip(parameters, actor.parameters()))\n\n\nclass TestDDPG:\n    seed = 0\n\n    def _create_mock_actor(self, batch=2, obs_dim=3, action_dim=4, device=\"cpu\"):\n        # Actor\n        action_spec = BoundedTensorSpec(\n            -torch.ones(action_dim), torch.ones(action_dim), (action_dim,)\n        )\n        module = nn.Linear(obs_dim, action_dim)\n        actor = Actor(\n            spec=action_spec,\n            module=module,\n        )\n        return actor.to(device)\n\n    def _create_mock_value(self, batch=2, obs_dim=3, action_dim=4, device=\"cpu\"):\n        # Actor", "metadata": [{"fpath_tuple": ["pytorch_rl", "test", "test_cost.py"], "line_no": 432, "start_line_no": 422, "end_line_no": 442, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}], "sim_score": 0.518796992481203}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# ding/worker/coordinator/tests/test_coordinator.py\n# --------------------------------------------------\n#     cfg = setup_config.system.coordinator.learner\n#     learner = {}\n#     for _, (name, host, port) in cfg.items():\n#         learner[name] = NaiveLearner(host, port, prefix=DATA_PREFIX)\n#         learner[name].start()\n#     yield learner\n#     for l in learner.values():\n#         l.close()\n# \n# \n# @pytest.mark.unittest(rerun=5)\n# class TestCoordinator:\n# \n#     def test_naive(self, setup_config, setup_collector, setup_learner):\n#         os.popen('rm -rf {}*'.format(DATA_PREFIX))\n#         assert len(setup_collector) == len(setup_config.system.coordinator.collector)\n#         assert len(setup_learner) == len(setup_config.system.coordinator.learner)\n#         try:\n#             coordinator = Coordinator(setup_config)\n#             coordinator.start()\n# --------------------------------------------------\n# the below code fragment can be found in:\n# ding/worker/collector/comm/tests/test_collector_with_coordinator.py\n# --------------------------------------------------\n#     yield learner\n#     time.sleep(1)\n#     for l in learner.values():\n#         l.close()\n# \n# \n# @pytest.mark.unittest\n# class TestCollectorWithCoordinator:\n# \n#     def test_naive(self, setup_config, setup_collector, setup_learner):\n#         os.popen('rm -rf {}*'.format(DATA_PREFIX))\n#         os.popen('rm -rf env_*_*')\n#         os.popen('rm -rf test.pth')\n#         assert len(setup_collector) == len(setup_config.system.coordinator.collector)\n#         try:\n#             coordinator = Coordinator(setup_config)\n#             coordinator.start()\n#             while True:\n#                 if setup_collector['collector0']._collector is not None:\n#                     break\n# --------------------------------------------------\n# the below code fragment can be found in:\n# ding/worker/coordinator/tests/test_coordinator.py\n# --------------------------------------------------\n#     for _, (name, host, port) in cfg.items():\n#         learner[name] = NaiveLearner(host, port, prefix=DATA_PREFIX)\n#         learner[name].start()\n#     yield learner\n#     for l in learner.values():\n#         l.close()\n# \n# \n# @pytest.mark.unittest(rerun=5)\n# class TestCoordinator:\n# \n#     def test_naive(self, setup_config, setup_collector, setup_learner):\n#         os.popen('rm -rf {}*'.format(DATA_PREFIX))\n#         assert len(setup_collector) == len(setup_config.system.coordinator.collector)\n#         assert len(setup_learner) == len(setup_config.system.coordinator.learner)\n#         try:\n#             coordinator = Coordinator(setup_config)\n#             coordinator.start()\n#             while True:\n#                 if coordinator._commander._learner_task_finish_count == 1:\n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\nimport pytest\nimport os\nimport copy\nimport time\nfrom threading import Thread\nimport json\nfrom queue import Queue\nfrom flask import Flask, request\n\nfrom ding.worker import Coordinator\nfrom ding.worker.learner.comm import NaiveLearner\nfrom ding.worker.collector.comm import NaiveCollector\nfrom ding.utils import find_free_port\nfrom ding.config import compile_config_parallel\nfrom ding.config.utils import parallel_test_main_config, parallel_test_create_config, parallel_test_system_config\n\nDATA_PREFIX = 'SLAVE_COLLECTOR_DATA_FAKE_OPERATOR_TEST'\ninit_replicas_request = {\n    \"collectors\": {\n        \"cpu\": \"0.5\",\n        \"memory\": \"200Mi\",\n        \"replicas\": 2,\n    },\n    \"learners\": {\n        \"cpu\": \"0.5\",\n        \"memory\": \"200Mi\",\n        \"gpu\": \"0\",\n        \"replicas\": 1,\n    },\n}\napi_version = 'v1alpha1'\nsystem_addr = 'https://0.0.0.0:14502'\n\n\ndef create_app(creator):\n    app = Flask(__name__)\n\n    @app.route('/{}/replicas'.format(api_version), methods=['POST'])\n    def post_replicas():\n        data = json.loads(request.data.decode())\n        collectors = data['collectors'][\"replicas\"]\n        learners = data['learners'][\"replicas\"]\n        creator.set_target_source(learners, collectors)\n        return {'success': True, 'code': 0, 'message': '', 'data': ''}\n\n    @app.route('/{}/replicas'.format(api_version), methods=['GET'])\n    def get_replicas():\n        data = json.loads(request.data.decode())\n        return {'success': True, 'code': 0, 'message': '', 'data': creator.current_resource}\n\n    return app\n\n\n@pytest.fixture(scope='function')\ndef setup_config():\n    cfg = compile_config_parallel(\n        parallel_test_main_config, create_cfg=parallel_test_create_config, system_cfg=parallel_test_system_config\n    )\n    cfg.system.coordinator.operator_server = dict(\n        system_addr=system_addr,\n        api_version=api_version,\n        init_replicas_request=init_replicas_request,\n        collector_target_num=len(cfg.system.coordinator.collector),\n        learner_target_num=len(cfg.system.coordinator.learner),\n    )\n    return cfg\n\n\nclass Creator:\n\n    def __init__(self, learner_addr, collector_addr):\n        self.learner_addr = learner_addr\n        self.collector_addr = collector_addr\n        self.collector_demand = Queue()\n        self.learner_demand = Queue()\n        self.learners = {}\n        self.collectors = {}\n        self.end_flag = False\n\n    def set_target_source(self, learner_target, collector_target):\n        print('set_target_source', learner_target, collector_target)\n        time.sleep(3)  # simulate\n        self.collector_demand.put(collector_target)\n        self.learner_demand.put(learner_target)\n\n    def start(self):\n        while not self.end_flag:\n            if self.learner_demand.empty() and self.collector_demand.empty():\n                time.sleep(0.1)\n                continue\n            else:\n                learner_demand, collector_demand = None, None\n                if not self.learner_demand.empty():\n                    learner_demand = self.learner_demand.get()\n                if not self.collector_demand.empty():\n                    collector_demand = self.collector_demand.get()\n\n                for i in range(collector_demand):\n                    name, host, port = self.collector_addr[i]\n                    self.collectors[name] = NaiveCollector(host, port, prefix=DATA_PREFIX)\n                    self.collectors[name].start()\n                for i in range(learner_demand):\n                    name, host, port = self.learner_addr[i]\n                    self.learners[name] = NaiveLearner(host, port, prefix=DATA_PREFIX)\n                    self.learners[name].start()\n\n    def close(self):\n        self.end_flag = True\n        time.sleep(1)\n        for t in self.learners.values():\n            t.close()\n        for t in self.collectors.values():\n            t.close()\n\n    @property\n    def current_resource(self):\n        collectors = {k: {} for k in self.collectors}\n        learners = {k: {} for k in self.learners}\n        return {\"collectors\": collectors, 'learners': learners}\n\n\n@pytest.fixture(scope='function')\ndef setup_operator_server(setup_config):\n    host, port = system_addr.split(\"https://\")[1].split(\":\")\n    port = int(port)\n    learner_addr = copy.deepcopy(setup_config.system.coordinator.learner)\n    learner_addr = list(learner_addr.values())\n    for i in range(len(learner_addr)):\n        learner_addr[i][0] = '{}:{}'.format(learner_addr[i][1], learner_addr[i][2])\n    collector_addr = copy.deepcopy(setup_config.system.coordinator.collector)\n    collector_addr = list(collector_addr.values())\n    for i in range(len(collector_addr)):\n        collector_addr[i][0] = '{}:{}'.format(collector_addr[i][1], collector_addr[i][2])\n    print(learner_addr, collector_addr)\n\n    creator = Creator(learner_addr, collector_addr)\n    creator_start_thread = Thread(target=creator.start, args=(), daemon=True)\n    creator_start_thread.start()\n\n    app = create_app(creator)\n    app_run_thread = Thread(target=app.run, args=(host, port), daemon=True)\n    app_run_thread.start()\n    yield app\n    creator.close()\n    print('end')\n\n\n@pytest.mark.unittest\nclass TestCoordinatorFakeOperator:\n\n    def test_naive(self, setup_config, setup_operator_server):\n        os.popen('rm -rf {}*'.format(DATA_PREFIX))\n        # learner/collector is created by operator-server\n        setup_config.system.coordinator.learner = {}\n        setup_config.system.coordinator.collector = {}\n\n        try:", "metadata": {"task_id": "opendilab_ACE/101", "ground_truth": "            coordinator = Coordinator(setup_config)", "fpath_tuple": ["opendilab_ACE", "ding", "worker", "coordinator", "tests", "test_fake_operator_server.py"], "context_start_lineno": 0, "line_no": 157, "query_window": {"context": "    creator_start_thread.start()\n\n    app = create_app(creator)\n    app_run_thread = Thread(target=app.run, args=(host, port), daemon=True)\n    app_run_thread.start()\n    yield app\n    creator.close()\n    print('end')\n\n\n@pytest.mark.unittest\nclass TestCoordinatorFakeOperator:\n\n    def test_naive(self, setup_config, setup_operator_server):\n        os.popen('rm -rf {}*'.format(DATA_PREFIX))\n        # learner/collector is created by operator-server\n        setup_config.system.coordinator.learner = {}\n        setup_config.system.coordinator.collector = {}\n\n        try:", "metadata": {"fpath_tuple": ["opendilab_ACE", "ding", "worker", "coordinator", "tests", "test_fake_operator_server.py"], "line_no": 157, "task_id": "opendilab_ACE/101", "start_line_no": 137, "end_line_no": 157, "window_size": 20, "context_start_lineno": 0, "repo": "opendilab_ACE"}}, "top_k_context": [{"context": "    cfg = setup_config.system.coordinator.learner\n    learner = {}\n    for _, (name, host, port) in cfg.items():\n        learner[name] = NaiveLearner(host, port, prefix=DATA_PREFIX)\n        learner[name].start()\n    yield learner\n    for l in learner.values():\n        l.close()\n\n\n@pytest.mark.unittest(rerun=5)\nclass TestCoordinator:\n\n    def test_naive(self, setup_config, setup_collector, setup_learner):\n        os.popen('rm -rf {}*'.format(DATA_PREFIX))\n        assert len(setup_collector) == len(setup_config.system.coordinator.collector)\n        assert len(setup_learner) == len(setup_config.system.coordinator.learner)\n        try:\n            coordinator = Coordinator(setup_config)\n            coordinator.start()", "metadata": [{"fpath_tuple": ["opendilab_ACE", "ding", "worker", "coordinator", "tests", "test_coordinator.py"], "line_no": 44, "start_line_no": 34, "end_line_no": 54, "window_size": 20, "repo": "opendilab_ACE", "slice_size": 10}], "sim_score": 0.5338983050847458}, {"context": "        learner[name] = NaiveLearner(host, port, prefix=DATA_PREFIX)\n        learner[name].start()\n    yield learner\n    time.sleep(1)\n    for l in learner.values():\n        l.close()\n\n\n@pytest.mark.unittest\nclass TestCollectorWithCoordinator:\n\n    def test_naive(self, setup_config, setup_collector, setup_learner):\n        os.popen('rm -rf {}*'.format(DATA_PREFIX))\n        os.popen('rm -rf env_*_*')\n        os.popen('rm -rf test.pth')\n        assert len(setup_collector) == len(setup_config.system.coordinator.collector)\n        try:\n            coordinator = Coordinator(setup_config)\n            coordinator.start()\n            while True:", "metadata": [{"fpath_tuple": ["opendilab_ACE", "ding", "worker", "collector", "comm", "tests", "test_collector_with_coordinator.py"], "line_no": 52, "start_line_no": 42, "end_line_no": 62, "window_size": 20, "repo": "opendilab_ACE", "slice_size": 10}], "sim_score": 0.5294117647058824}, {"context": "@pytest.fixture(scope='function')\ndef setup_learner(setup_config):\n    cfg = setup_config.system.coordinator.learner\n    learner = {}\n    for _, (name, host, port) in cfg.items():\n        learner[name] = NaiveLearner(host, port, prefix=DATA_PREFIX)\n        learner[name].start()\n    yield learner\n    for l in learner.values():\n        l.close()\n\n\n@pytest.mark.unittest(rerun=5)\nclass TestCoordinator:\n\n    def test_naive(self, setup_config, setup_collector, setup_learner):\n        os.popen('rm -rf {}*'.format(DATA_PREFIX))\n        assert len(setup_collector) == len(setup_config.system.coordinator.collector)\n        assert len(setup_learner) == len(setup_config.system.coordinator.learner)\n        try:", "metadata": [{"fpath_tuple": ["opendilab_ACE", "ding", "worker", "coordinator", "tests", "test_coordinator.py"], "line_no": 42, "start_line_no": 32, "end_line_no": 52, "window_size": 20, "repo": "opendilab_ACE", "slice_size": 10}], "sim_score": 0.5289256198347108}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# vizier/_src/pyglove/converters.py\n# --------------------------------------------------\n#   for key, value in pg_metadata.items():\n#     if key not in constants.TRIAL_METADATA_KEYS and value is not None:\n#       metadata[key] = pg.from_json_str(value)\n#   return metadata\n# \n# \n# def get_pyglove_study_metadata(problem: vz.ProblemStatement) -> pg.Dict:\n#   \"\"\"Extracts only the pyglove-related metadata into a simple dict.\"\"\"\n#   metadata = pg.Dict()\n#   pg_metadata = problem.metadata.ns(constants.METADATA_NAMESPACE)\n#   for key, value in pg_metadata.items():\n#     if key not in constants.STUDY_METADATA_KEYS and value is not None:\n#       metadata[key] = pg.from_json_str(value)\n#   return metadata\n# \n# \n# @attr.frozen\n# class VizierConverter:\n#   \"\"\"Converts between PyGlove DNA and Vizier Trial.\n# \n# --------------------------------------------------\n# the below code fragment can be found in:\n# vizier/_src/pyglove/converters.py\n# --------------------------------------------------\n# def get_pyglove_metadata(trial: vz.Trial) -> dict[str, Any]:\n#   \"\"\"Extracts only the pyglove-related metadata into a simple dict.\"\"\"\n#   metadata = dict()\n#   pg_metadata = trial.metadata.ns(constants.METADATA_NAMESPACE)\n#   for key, value in pg_metadata.items():\n#     if key not in constants.TRIAL_METADATA_KEYS and value is not None:\n#       metadata[key] = pg.from_json_str(value)\n#   return metadata\n# \n# \n# def get_pyglove_study_metadata(problem: vz.ProblemStatement) -> pg.Dict:\n#   \"\"\"Extracts only the pyglove-related metadata into a simple dict.\"\"\"\n#   metadata = pg.Dict()\n#   pg_metadata = problem.metadata.ns(constants.METADATA_NAMESPACE)\n#   for key, value in pg_metadata.items():\n#     if key not in constants.STUDY_METADATA_KEYS and value is not None:\n#       metadata[key] = pg.from_json_str(value)\n#   return metadata\n# \n# \n# --------------------------------------------------\n# the below code fragment can be found in:\n# vizier/_src/pyglove/converters.py\n# --------------------------------------------------\n#   metadata = dict()\n#   pg_metadata = trial.metadata.ns(constants.METADATA_NAMESPACE)\n#   for key, value in pg_metadata.items():\n#     if key not in constants.TRIAL_METADATA_KEYS and value is not None:\n#       metadata[key] = pg.from_json_str(value)\n#   return metadata\n# \n# \n# def get_pyglove_study_metadata(problem: vz.ProblemStatement) -> pg.Dict:\n#   \"\"\"Extracts only the pyglove-related metadata into a simple dict.\"\"\"\n#   metadata = pg.Dict()\n#   pg_metadata = problem.metadata.ns(constants.METADATA_NAMESPACE)\n#   for key, value in pg_metadata.items():\n#     if key not in constants.STUDY_METADATA_KEYS and value is not None:\n#       metadata[key] = pg.from_json_str(value)\n#   return metadata\n# \n# \n# @attr.frozen\n# class VizierConverter:\n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\n        final_measurement=converter.to_tuner_measurement(\n            trial.final_measurement),\n        status=_trial_status_legacy_value(trial.status),\n        created_time=int(trial.creation_time.timestamp()),\n        completed_time=int((trial.completion_time or\n                            datetime.datetime.fromtimestamp(0)).timestamp()),\n        infeasible=trial.infeasible,\n        **kwargs)\n    self._converter = converter\n    self._trial = trial\n\n  @property\n  def dna(self) -> pg.DNA:\n    \"\"\"Returns lazy loaded DNA.\"\"\"\n    if (self.sym_init_args.dna.value is None and\n        not self.sym_init_args.dna.children):\n      self.sym_init_args.dna = self._converter.to_dna(self._trial)\n    return self.sym_init_args.dna\n\n  @property\n  def metadata(self) -> dict[str, Any]:\n    \"\"\"Returns lazy loaded metadata.\"\"\"\n    if not self.sym_init_args.metadata and self._trial:\n      self.sym_init_args.metadata = converters.get_pyglove_metadata(self._trial)\n    return self.sym_init_args.metadata\n\n  @property\n  def related_links(self) -> dict[str, str]:\n    \"\"\"Returns lazy loaded related links.\"\"\"\n    if not self.sym_init_args.related_links and self._trial:\n      self.sym_init_args.related_links = dict(\n          self._trial.metadata.ns(constants.METADATA_NAMESPACE).ns(\n              constants.RELATED_LINKS_NAMESPACE))\n    return self.sym_init_args.related_links\n\n  @property\n  def measurements(self) -> list[pg.tuning.Measurement]:\n    \"\"\"Returns lazy loaded measurements.\"\"\"\n    if not self.sym_init_args.measurements:\n      self.sym_init_args.measurements = [\n          self._converter.to_tuner_measurement(m)\n          for m in self._trial.measurements\n      ]\n    return self.sym_init_args.measurements\n\n  def format(self, *args, **kwargs):\n    \"\"\"Fetch lazy bound properties before print.\"\"\"\n    # NOTE(daiyip): `format` depends on the symbolic attributes to generate\n    # the string representation. Since the following symbolic attributes are\n    # lazily assigned upon property accesses, we prefetch them before calling\n    # the `format`. Otherwise, the symbolic attributes are just default values\n    # set at __init__ time.\n    _, _, _, _ = self.dna, self.measurements, self.metadata, self.related_links\n    return super().format(*args, **kwargs)\n\n\nclass Feedback(pg.tuning.Feedback):\n  \"\"\"Tuning feedback for a vizier trial.\"\"\"\n\n  def __init__(self, vizier_trial: client_abc.TrialInterface,\n               converter: converters.VizierConverter):\n    \"\"\"Constructor.\n\n    Args:\n      vizier_trial: Vizier trial (cross-platform).\n      converter: Vizier-Pyglove converter.\n    \"\"\"\n    super().__init__(converter.metrics_to_optimize)\n    self._converter = converter\n    self._trial_client = vizier_trial\n    self._trial = self._trial_client.materialize()\n    self._dna_spec = converter.dna_spec\n    self._discard_reward = 'reward' not in converter.metrics_to_optimize\n\n  @property\n  def id(self) -> int:\n    \"\"\"Gets Trial ID as ID.\"\"\"\n    return self._trial_client.id\n\n  @property\n  def dna(self) -> pg.DNA:\n    \"\"\"Gets DNA of current trial.\"\"\"\n    return self._converter.to_dna(self._trial)\n\n  def get_trial(self) -> pg.tuning.Trial:\n    \"\"\"Gets current trial with all fields up-to-date.\"\"\"\n    self._trial = self._trial_client.materialize()\n    return VizierTrial(self._converter, self._trial)\n\n  @property\n  def checkpoint_to_warm_start_from(self) -> Optional[str]:\n    \"\"\"Gets checkpoint path to warm start from. Refreshes `_trial`.\"\"\"\n    # TODO: Add official support.\n    self._trial = self._trial_client.materialize()\n    return self._trial.metadata.ns(constants.METADATA_NAMESPACE).get(\n        constants.WARMSTART_CHECKPOINT_PATH_KEY, None)\n\n  @contextlib.contextmanager\n  def _maybe_race_condition(self, message: str):\n    \"\"\"Raise race condition error when error message matches with regex.\"\"\"\n    try:\n      yield\n    # TODO: once pyvizier expose common error types, we should\n    # change `Exception` to a narrower error type.\n    except Exception as e:  # pylint:disable=broad-except\n      if message in str(e):\n        raise pg.tuning.RaceConditionError(str(e)) from e\n      else:\n        raise\n\n  def _add_measurement(self, reward: Optional[float], metrics: dict[str, float],\n                       step: int, checkpoint_path: Optional[str],\n                       elapse_secs: float) -> None:\n    \"\"\"Reports tuning measurement to the pg.tuning.\"\"\"\n    if checkpoint_path:\n      # TODO: Add official support.\n      mu = vz.Metadata()\n      mu.ns(constants.METADATA_NAMESPACE)[\n          constants.WARMSTART_CHECKPOINT_PATH_KEY] = checkpoint_path\n      self._trial_client.update_metadata(mu)\n    if reward is not None and not self._discard_reward:\n      metrics |= {'reward': reward}\n    with self._maybe_race_condition(\n        'Measurements can only be added to'):\n      self._trial_client.add_measurement(\n          vz.Measurement(metrics, elapsed_secs=elapse_secs, steps=step))\n\n  def set_metadata(self, key: str, value: Any, per_trial: bool = True) -> None:\n    \"\"\"Sets metadata for current trial or current sampling.\"\"\"\n    md = vz.Metadata()\n    md.ns(constants.METADATA_NAMESPACE)[key] = pg.to_json_str(value)\n    if per_trial:\n      self._trial_client.update_metadata(md)\n    else:\n      self._trial_client.study.update_metadata(md)\n\n  def get_metadata(self, key: str, per_trial: bool = True) -> Optional[Any]:\n    \"\"\"Gets metadata for current trial or current sampling.\"\"\"\n    if per_trial:\n      value = self._trial.metadata.ns(constants.METADATA_NAMESPACE).get(\n          key, None)\n    else:\n      value = self._trial_client.study.materialize_problem_statement(\n      ).metadata.ns(constants.METADATA_NAMESPACE).get(key, None)\n    return pg.from_json_str(value) if value is not None else None\n\n  def add_link(self, name: str, url: str) -> None:\n    \"\"\"Adds related link.\"\"\"\n    md = vz.Metadata()\n    md.ns(constants.METADATA_NAMESPACE).ns(\n        constants.RELATED_LINKS_NAMESPACE)[name] = url\n    self._trial_client.update_metadata(md)\n\n  def done(self,\n           metadata: Optional[dict[str, Any]] = None,\n           related_links: Optional[dict[str, str]] = None) -> None:\n    \"\"\"Marks current tuning trial as done, and export final object.\"\"\"\n    metadata = metadata or {}\n    related_links = related_links or {}\n    for key, value in metadata.items():\n      self.set_metadata(key, value)\n    for key, value in related_links.items():", "metadata": {"task_id": "google_vizier/155", "ground_truth": "      self.add_link(key, value)", "fpath_tuple": ["google_vizier", "vizier", "_src", "pyglove", "core.py"], "context_start_lineno": 57, "line_no": 219, "query_window": {"context": "      value = self._trial_client.study.materialize_problem_statement(\n      ).metadata.ns(constants.METADATA_NAMESPACE).get(key, None)\n    return pg.from_json_str(value) if value is not None else None\n\n  def add_link(self, name: str, url: str) -> None:\n    \"\"\"Adds related link.\"\"\"\n    md = vz.Metadata()\n    md.ns(constants.METADATA_NAMESPACE).ns(\n        constants.RELATED_LINKS_NAMESPACE)[name] = url\n    self._trial_client.update_metadata(md)\n\n  def done(self,\n           metadata: Optional[dict[str, Any]] = None,\n           related_links: Optional[dict[str, str]] = None) -> None:\n    \"\"\"Marks current tuning trial as done, and export final object.\"\"\"\n    metadata = metadata or {}\n    related_links = related_links or {}\n    for key, value in metadata.items():\n      self.set_metadata(key, value)\n    for key, value in related_links.items():", "metadata": {"fpath_tuple": ["google_vizier", "vizier", "_src", "pyglove", "core.py"], "line_no": 219, "task_id": "google_vizier/155", "start_line_no": 199, "end_line_no": 219, "window_size": 20, "context_start_lineno": 57, "repo": "google_vizier"}}, "top_k_context": [{"context": "def get_pyglove_metadata(trial: vz.Trial) -> dict[str, Any]:\n  \"\"\"Extracts only the pyglove-related metadata into a simple dict.\"\"\"\n  metadata = dict()\n  pg_metadata = trial.metadata.ns(constants.METADATA_NAMESPACE)\n  for key, value in pg_metadata.items():\n    if key not in constants.TRIAL_METADATA_KEYS and value is not None:\n      metadata[key] = pg.from_json_str(value)\n  return metadata\n\n\ndef get_pyglove_study_metadata(problem: vz.ProblemStatement) -> pg.Dict:\n  \"\"\"Extracts only the pyglove-related metadata into a simple dict.\"\"\"\n  metadata = pg.Dict()\n  pg_metadata = problem.metadata.ns(constants.METADATA_NAMESPACE)\n  for key, value in pg_metadata.items():\n    if key not in constants.STUDY_METADATA_KEYS and value is not None:\n      metadata[key] = pg.from_json_str(value)\n  return metadata\n\n", "metadata": [{"fpath_tuple": ["google_vizier", "vizier", "_src", "pyglove", "converters.py"], "line_no": 230, "start_line_no": 220, "end_line_no": 240, "window_size": 20, "repo": "google_vizier", "slice_size": 10}], "sim_score": 0.41304347826086957}, {"context": "\n\ndef get_pyglove_metadata(trial: vz.Trial) -> dict[str, Any]:\n  \"\"\"Extracts only the pyglove-related metadata into a simple dict.\"\"\"\n  metadata = dict()\n  pg_metadata = trial.metadata.ns(constants.METADATA_NAMESPACE)\n  for key, value in pg_metadata.items():\n    if key not in constants.TRIAL_METADATA_KEYS and value is not None:\n      metadata[key] = pg.from_json_str(value)\n  return metadata\n\n\ndef get_pyglove_study_metadata(problem: vz.ProblemStatement) -> pg.Dict:\n  \"\"\"Extracts only the pyglove-related metadata into a simple dict.\"\"\"\n  metadata = pg.Dict()\n  pg_metadata = problem.metadata.ns(constants.METADATA_NAMESPACE)\n  for key, value in pg_metadata.items():\n    if key not in constants.STUDY_METADATA_KEYS and value is not None:\n      metadata[key] = pg.from_json_str(value)\n  return metadata", "metadata": [{"fpath_tuple": ["google_vizier", "vizier", "_src", "pyglove", "converters.py"], "line_no": 228, "start_line_no": 218, "end_line_no": 238, "window_size": 20, "repo": "google_vizier", "slice_size": 10}], "sim_score": 0.41304347826086957}, {"context": "  metadata = dict()\n  pg_metadata = trial.metadata.ns(constants.METADATA_NAMESPACE)\n  for key, value in pg_metadata.items():\n    if key not in constants.TRIAL_METADATA_KEYS and value is not None:\n      metadata[key] = pg.from_json_str(value)\n  return metadata\n\n\ndef get_pyglove_study_metadata(problem: vz.ProblemStatement) -> pg.Dict:\n  \"\"\"Extracts only the pyglove-related metadata into a simple dict.\"\"\"\n  metadata = pg.Dict()\n  pg_metadata = problem.metadata.ns(constants.METADATA_NAMESPACE)\n  for key, value in pg_metadata.items():\n    if key not in constants.STUDY_METADATA_KEYS and value is not None:\n      metadata[key] = pg.from_json_str(value)\n  return metadata\n\n\n@attr.frozen\nclass VizierConverter:", "metadata": [{"fpath_tuple": ["google_vizier", "vizier", "_src", "pyglove", "converters.py"], "line_no": 232, "start_line_no": 222, "end_line_no": 242, "window_size": 20, "repo": "google_vizier", "slice_size": 10}], "sim_score": 0.3793103448275862}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# federatedscope/core/workers/server.py\n# --------------------------------------------------\n#         \"\"\"\n#         To terminate the FL course\n#         \"\"\"\n#         self.is_finish = True\n#         if self.model_num > 1:\n#             model_para = [model.state_dict() for model in self.models]\n#         else:\n#             model_para = self.model.state_dict()\n# \n#         self._monitor.finish_fl()\n# \n#         self.comm_manager.send(\n#             Message(msg_type=msg_type,\n#                     sender=self.ID,\n#                     receiver=list(self.comm_manager.neighbors.keys()),\n#                     state=self.state,\n#                     timestamp=self.cur_timestamp,\n#                     content=model_para))\n# \n#     def eval(self):\n# --------------------------------------------------\n# the below code fragment can be found in:\n# federatedscope/core/communication.py\n# --------------------------------------------------\n#             return stub, channel\n# \n#         stub, channel = _create_stub(receiver_address)\n#         request = message.transform(to_list=True)\n#         try:\n#             stub.sendMessage(request)\n#         except grpc._channel._InactiveRpcError:\n#             pass\n#         channel.close()\n# \n#     def send(self, message):\n#         receiver = message.receiver\n#         if receiver is not None:\n#             if not isinstance(receiver, list):\n#                 receiver = [receiver]\n#             for each_receiver in receiver:\n#                 if each_receiver in self.neighbors:\n#                     receiver_address = self.neighbors[each_receiver]\n#                     self._send(receiver_address, message)\n#         else:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# federatedscope/core/gRPC_server.py\n# --------------------------------------------------\n# from collections import deque\n# \n# from federatedscope.core.proto import gRPC_comm_manager_pb2, \\\n#     gRPC_comm_manager_pb2_grpc\n# \n# \n# class gRPCComServeFunc(gRPC_comm_manager_pb2_grpc.gRPCComServeFuncServicer):\n#     def __init__(self):\n#         self.msg_queue = deque()\n# \n#     def sendMessage(self, request, context):\n#         self.msg_queue.append(request)\n# \n#         return gRPC_comm_manager_pb2.MessageResponse(msg='ACK')\n# \n#     def receive(self):\n#         while len(self.msg_queue) == 0:\n#             continue\n#         msg = self.msg_queue.popleft()\n#         return msg\n# --------------------------------------------------\n# the below code fragment can be found in:\n# federatedscope/core/gRPC_server.py\n# --------------------------------------------------\n# from collections import deque\n# \n# from federatedscope.core.proto import gRPC_comm_manager_pb2, \\\n#     gRPC_comm_manager_pb2_grpc\n# \n# \n# class gRPCComServeFunc(gRPC_comm_manager_pb2_grpc.gRPCComServeFuncServicer):\n#     def __init__(self):\n#         self.msg_queue = deque()\n# \n#     def sendMessage(self, request, context):\n#         self.msg_queue.append(request)\n# \n#         return gRPC_comm_manager_pb2.MessageResponse(msg='ACK')\n# \n#     def receive(self):\n#         while len(self.msg_queue) == 0:\n#             continue\n#         msg = self.msg_queue.popleft()\n#         return msg\n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\n in\n    FederatedScope.\n    A message object includes:\n        msg_type: The type of message, which is used to trigger the\n        corresponding handlers of server/client\n        sender: The sender's ID\n        receiver: The receiver's ID\n        state: The training round of the message, which is determined by\n        the sender and used to filter out the outdated messages.\n        strategy: redundant attribute\n    \"\"\"\n    def __init__(self,\n                 msg_type=None,\n                 sender=0,\n                 receiver=0,\n                 state=0,\n                 content='None',\n                 timestamp=0,\n                 strategy=None):\n        self._msg_type = msg_type\n        self._sender = sender\n        self._receiver = receiver\n        self._state = state\n        self._content = content\n        self._timestamp = timestamp\n        self._strategy = strategy\n\n    @property\n    def msg_type(self):\n        return self._msg_type\n\n    @msg_type.setter\n    def msg_type(self, value):\n        self._msg_type = value\n\n    @property\n    def sender(self):\n        return self._sender\n\n    @sender.setter\n    def sender(self, value):\n        self._sender = value\n\n    @property\n    def receiver(self):\n        return self._receiver\n\n    @receiver.setter\n    def receiver(self, value):\n        self._receiver = value\n\n    @property\n    def state(self):\n        return self._state\n\n    @state.setter\n    def state(self, value):\n        self._state = value\n\n    @property\n    def content(self):\n        return self._content\n\n    @content.setter\n    def content(self, value):\n        self._content = value\n\n    @property\n    def timestamp(self):\n        return self._timestamp\n\n    @timestamp.setter\n    def timestamp(self, value):\n        assert isinstance(value, int) or isinstance(value, float), \\\n            \"We only support an int or a float value for timestamp\"\n        self._timestamp = value\n\n    @property\n    def strategy(self):\n        return self._strategy\n\n    @strategy.setter\n    def strategy(self, value):\n        self._strategy = value\n\n    def __lt__(self, other):\n        if self.timestamp != other.timestamp:\n            return self.timestamp < other.timestamp\n        else:\n            return self.state < other.state\n\n    def transform_to_list(self, x):\n        if isinstance(x, list) or isinstance(x, tuple):\n            return [self.transform_to_list(each_x) for each_x in x]\n        elif isinstance(x, dict):\n            for key in x.keys():\n                x[key] = self.transform_to_list(x[key])\n            return x\n        else:\n            if hasattr(x, 'tolist'):\n                return x.tolist()\n            else:\n                return x\n\n    def msg_to_json(self, to_list=False):\n        if to_list:\n            self.content = self.transform_to_list(self.content)\n\n        json_msg = {\n            'msg_type': self.msg_type,\n            'sender': self.sender,\n            'receiver': self.receiver,\n            'state': self.state,\n            'content': self.content,\n            'timestamp': self.timestamp,\n            'strategy': self.strategy,\n        }\n        return json.dumps(json_msg)\n\n    def json_to_msg(self, json_string):\n        json_msg = json.loads(json_string)\n        self.msg_type = json_msg['msg_type']\n        self.sender = json_msg['sender']\n        self.receiver = json_msg['receiver']\n        self.state = json_msg['state']\n        self.content = json_msg['content']\n        self.timestamp = json_msg['timestamp']\n        self.strategy = json_msg['strategy']\n\n    def create_by_type(self, value, nested=False):\n        if isinstance(value, dict):\n            if isinstance(list(value.keys())[0], str):\n                m_dict = gRPC_comm_manager_pb2.mDict_keyIsString()\n                key_type = 'string'\n            else:\n                m_dict = gRPC_comm_manager_pb2.mDict_keyIsInt()\n                key_type = 'int'\n\n            for key in value.keys():\n                m_dict.dict_value[key].MergeFrom(\n                    self.create_by_type(value[key], nested=True))\n            if nested:\n                msg_value = gRPC_comm_manager_pb2.MsgValue()\n                if key_type == 'string':\n                    msg_value.dict_msg_stringkey.MergeFrom(m_dict)\n                else:\n                    msg_value.dict_msg_intkey.MergeFrom(m_dict)\n                return msg_value\n            else:\n                return m_dict\n        elif isinstance(value, list) or isinstance(value, tuple):\n            m_list = gRPC_comm_manager_pb2.mList()\n            for each in value:\n                m_list.list_value.append(self.create_by_type(each,\n                                                             nested=True))\n            if nested:\n                msg_value = gRPC_comm_manager_pb2.MsgValue()\n                msg_value.list_msg.MergeFrom(m_list)\n                return msg_value\n            else:\n                return m_list\n        else:\n            m_single = gRPC_comm_manager_pb2.mSingle()\n            if type(value) in [int, np.int32]:\n                m_single.int_value = value\n            elif type(value) in [str]:\n                m_single.str_value = value\n            elif type(value) in [float, np.float32]:\n                m_single.float_value = value\n            else:\n                raise ValueError(\n                    'The data type {} has not been supported.'.format(\n                        type(value)))\n\n            if nested:\n                msg_value = gRPC_comm_manager_pb2.MsgValue()\n                msg_value.single_msg.MergeFrom(m_single)\n                return msg_value\n            else:\n                return m_single\n\n    def build_msg_value(self, value):\n        msg_value = gRPC_comm_manager_pb2.MsgValue()\n\n        if isinstance(value, list) or isinstance(value, tuple):\n            msg_value.list_msg.MergeFrom(self.create_by_type(value))\n        elif isinstance(value, dict):\n            if isinstance(list(value.keys())[0], str):\n                msg_value.dict_msg_stringkey.MergeFrom(\n                    self.create_by_type(value))\n            else:\n                msg_value.dict_msg_intkey.MergeFrom(self.create_by_type(value))\n        else:\n            msg_value.single_msg.MergeFrom(self.create_by_type(value))\n\n        return msg_value\n\n    def transform(self, to_list=False):\n        if to_list:\n            self.content = self.transform_to_list(self.content)\n\n        splited_msg = gRPC_comm_manager_pb2.MessageRequest()  # map/dict\n        splited_msg.msg['sender'].MergeFrom(self.build_msg_value(self.sender))\n        splited_msg.msg['receiver'].MergeFrom(\n            self.build_msg_value(self.receiver))\n        splited_msg.msg['state'].MergeFrom(self.build_msg_value(self.state))\n        splited_msg.msg['msg_type'].MergeFrom(\n            self.build_msg_value(self.msg_type))\n        splited_msg.msg['content'].MergeFrom(self.build_msg_value(\n            self.content))\n        splited_msg.msg['timestamp'].MergeFrom(", "metadata": {"task_id": "alibaba_FederatedScope/116", "ground_truth": "            self.build_msg_value(self.timestamp))", "fpath_tuple": ["alibaba_FederatedScope", "federatedscope", "core", "message.py"], "context_start_lineno": 7, "line_no": 218, "query_window": {"context": "                msg_value.dict_msg_intkey.MergeFrom(self.create_by_type(value))\n        else:\n            msg_value.single_msg.MergeFrom(self.create_by_type(value))\n\n        return msg_value\n\n    def transform(self, to_list=False):\n        if to_list:\n            self.content = self.transform_to_list(self.content)\n\n        splited_msg = gRPC_comm_manager_pb2.MessageRequest()  # map/dict\n        splited_msg.msg['sender'].MergeFrom(self.build_msg_value(self.sender))\n        splited_msg.msg['receiver'].MergeFrom(\n            self.build_msg_value(self.receiver))\n        splited_msg.msg['state'].MergeFrom(self.build_msg_value(self.state))\n        splited_msg.msg['msg_type'].MergeFrom(\n            self.build_msg_value(self.msg_type))\n        splited_msg.msg['content'].MergeFrom(self.build_msg_value(\n            self.content))\n        splited_msg.msg['timestamp'].MergeFrom(", "metadata": {"fpath_tuple": ["alibaba_FederatedScope", "federatedscope", "core", "message.py"], "line_no": 218, "task_id": "alibaba_FederatedScope/116", "start_line_no": 198, "end_line_no": 218, "window_size": 20, "context_start_lineno": 7, "repo": "alibaba_FederatedScope"}}, "top_k_context": [{"context": "\n    def sendMessage(self, request, context):\n        self.msg_queue.append(request)\n\n        return gRPC_comm_manager_pb2.MessageResponse(msg='ACK')\n\n    def receive(self):\n        while len(self.msg_queue) == 0:\n            continue\n        msg = self.msg_queue.popleft()\n        return msg", "metadata": [{"fpath_tuple": ["alibaba_FederatedScope", "federatedscope", "core", "gRPC_server.py"], "line_no": 20, "start_line_no": 10, "end_line_no": 21, "window_size": 20, "repo": "alibaba_FederatedScope", "slice_size": 10}], "sim_score": 0.3181818181818182}, {"context": "    def __init__(self):\n        self.msg_queue = deque()\n\n    def sendMessage(self, request, context):\n        self.msg_queue.append(request)\n\n        return gRPC_comm_manager_pb2.MessageResponse(msg='ACK')\n\n    def receive(self):\n        while len(self.msg_queue) == 0:\n            continue\n        msg = self.msg_queue.popleft()\n        return msg", "metadata": [{"fpath_tuple": ["alibaba_FederatedScope", "federatedscope", "core", "gRPC_server.py"], "line_no": 18, "start_line_no": 8, "end_line_no": 21, "window_size": 20, "repo": "alibaba_FederatedScope", "slice_size": 10}], "sim_score": 0.3010752688172043}, {"context": "                                                      0), ))\n            stub = gRPC_comm_manager_pb2_grpc.gRPCComServeFuncStub(channel)\n            return stub, channel\n\n        stub, channel = _create_stub(receiver_address)\n        request = message.transform(to_list=True)\n        try:\n            stub.sendMessage(request)\n        except grpc._channel._InactiveRpcError:\n            pass\n        channel.close()\n\n    def send(self, message):\n        receiver = message.receiver\n        if receiver is not None:\n            if not isinstance(receiver, list):\n                receiver = [receiver]\n            for each_receiver in receiver:\n                if each_receiver in self.neighbors:\n                    receiver_address = self.neighbors[each_receiver]", "metadata": [{"fpath_tuple": ["alibaba_FederatedScope", "federatedscope", "core", "communication.py"], "line_no": 126, "start_line_no": 116, "end_line_no": 136, "window_size": 20, "repo": "alibaba_FederatedScope", "slice_size": 10}], "sim_score": 0.28225806451612906}, {"context": "\n    def terminate(self, msg_type='finish'):\n        \"\"\"\n        To terminate the FL course\n        \"\"\"\n        self.is_finish = True\n        if self.model_num > 1:\n            model_para = [model.state_dict() for model in self.models]\n        else:\n            model_para = self.model.state_dict()\n\n        self._monitor.finish_fl()\n\n        self.comm_manager.send(\n            Message(msg_type=msg_type,\n                    sender=self.ID,\n                    receiver=list(self.comm_manager.neighbors.keys()),\n                    state=self.state,\n                    timestamp=self.cur_timestamp,\n                    content=model_para))", "metadata": [{"fpath_tuple": ["alibaba_FederatedScope", "federatedscope", "core", "workers", "server.py"], "line_no": 840, "start_line_no": 830, "end_line_no": 850, "window_size": 20, "repo": "alibaba_FederatedScope", "slice_size": 10}], "sim_score": 0.2818181818181818}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# examples/textual_inversion/textual_inversion_flax.py\n# examples/text_to_image/train_text_to_image_flax.py\n# --------------------------------------------------\n#         train_metric = jax_utils.unreplicate(train_metric)\n# \n#         train_step_progress_bar.close()\n#         epochs.write(f\"Epoch... ({epoch + 1}/{args.num_train_epochs} | Loss: {train_metric['loss']})\")\n# \n#     # Create the pipeline using using the trained modules and save it.\n#     if jax.process_index() == 0:\n#         scheduler = FlaxPNDMScheduler(\n#             beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", skip_prk_steps=True\n#         )\n#         safety_checker = FlaxStableDiffusionSafetyChecker.from_pretrained(\n#             \"CompVis/stable-diffusion-safety-checker\", from_pt=True\n#         )\n#         pipeline = FlaxStableDiffusionPipeline(\n#             text_encoder=text_encoder,\n#             vae=vae,\n#             unet=unet,\n#             tokenizer=tokenizer,\n#             scheduler=scheduler,\n#             safety_checker=safety_checker,\n# --------------------------------------------------\n# the below code fragment can be found in:\n# examples/textual_inversion/textual_inversion_flax.py\n# examples/text_to_image/train_text_to_image_flax.py\n# --------------------------------------------------\n#         train_step_progress_bar.close()\n#         epochs.write(f\"Epoch... ({epoch + 1}/{args.num_train_epochs} | Loss: {train_metric['loss']})\")\n# \n#     # Create the pipeline using using the trained modules and save it.\n#     if jax.process_index() == 0:\n#         scheduler = FlaxPNDMScheduler(\n#             beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", skip_prk_steps=True\n#         )\n#         safety_checker = FlaxStableDiffusionSafetyChecker.from_pretrained(\n#             \"CompVis/stable-diffusion-safety-checker\", from_pt=True\n#         )\n#         pipeline = FlaxStableDiffusionPipeline(\n#             text_encoder=text_encoder,\n#             vae=vae,\n#             unet=unet,\n#             tokenizer=tokenizer,\n#             scheduler=scheduler,\n#             safety_checker=safety_checker,\n#             feature_extractor=CLIPFeatureExtractor.from_pretrained(\"openai/clip-vit-base-patch32\"),\n#         )\n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\n.learning_rate = args.learning_rate * total_train_batch_size\n\n    constant_scheduler = optax.constant_schedule(args.learning_rate)\n\n    adamw = optax.adamw(\n        learning_rate=constant_scheduler,\n        b1=args.adam_beta1,\n        b2=args.adam_beta2,\n        eps=args.adam_epsilon,\n        weight_decay=args.adam_weight_decay,\n    )\n\n    optimizer = optax.chain(\n        optax.clip_by_global_norm(args.max_grad_norm),\n        adamw,\n    )\n\n    state = train_state.TrainState.create(apply_fn=unet.__call__, params=unet_params, tx=optimizer)\n\n    noise_scheduler = FlaxDDPMScheduler(\n        beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000\n    )\n    noise_scheduler_state = noise_scheduler.create_state()\n\n    # Initialize our training\n    rng = jax.random.PRNGKey(args.seed)\n    train_rngs = jax.random.split(rng, jax.local_device_count())\n\n    def train_step(state, text_encoder_params, vae_params, batch, train_rng):\n        dropout_rng, sample_rng, new_train_rng = jax.random.split(train_rng, 3)\n\n        def compute_loss(params):\n            # Convert images to latent space\n            vae_outputs = vae.apply(\n                {\"params\": vae_params}, batch[\"pixel_values\"], deterministic=True, method=vae.encode\n            )\n            latents = vae_outputs.latent_dist.sample(sample_rng)\n            # (NHWC) -> (NCHW)\n            latents = jnp.transpose(latents, (0, 3, 1, 2))\n            latents = latents * vae.config.scaling_factor\n\n            # Sample noise that we'll add to the latents\n            noise_rng, timestep_rng = jax.random.split(sample_rng)\n            noise = jax.random.normal(noise_rng, latents.shape)\n            # Sample a random timestep for each image\n            bsz = latents.shape[0]\n            timesteps = jax.random.randint(\n                timestep_rng,\n                (bsz,),\n                0,\n                noise_scheduler.config.num_train_timesteps,\n            )\n\n            # Add noise to the latents according to the noise magnitude at each timestep\n            # (this is the forward diffusion process)\n            noisy_latents = noise_scheduler.add_noise(noise_scheduler_state, latents, noise, timesteps)\n\n            # Get the text embedding for conditioning\n            encoder_hidden_states = text_encoder(\n                batch[\"input_ids\"],\n                params=text_encoder_params,\n                train=False,\n            )[0]\n\n            # Predict the noise residual and compute loss\n            model_pred = unet.apply(\n                {\"params\": params}, noisy_latents, timesteps, encoder_hidden_states, train=True\n            ).sample\n\n            # Get the target for loss depending on the prediction type\n            if noise_scheduler.config.prediction_type == \"epsilon\":\n                target = noise\n            elif noise_scheduler.config.prediction_type == \"v_prediction\":\n                target = noise_scheduler.get_velocity(noise_scheduler_state, latents, noise, timesteps)\n            else:\n                raise ValueError(f\"Unknown prediction type {noise_scheduler.config.prediction_type}\")\n\n            loss = (target - model_pred) ** 2\n            loss = loss.mean()\n\n            return loss\n\n        grad_fn = jax.value_and_grad(compute_loss)\n        loss, grad = grad_fn(state.params)\n        grad = jax.lax.pmean(grad, \"batch\")\n\n        new_state = state.apply_gradients(grads=grad)\n\n        metrics = {\"loss\": loss}\n        metrics = jax.lax.pmean(metrics, axis_name=\"batch\")\n\n        return new_state, metrics, new_train_rng\n\n    # Create parallel version of the train step\n    p_train_step = jax.pmap(train_step, \"batch\", donate_argnums=(0,))\n\n    # Replicate the train state on each device\n    state = jax_utils.replicate(state)\n    text_encoder_params = jax_utils.replicate(text_encoder.params)\n    vae_params = jax_utils.replicate(vae_params)\n\n    # Train!\n    num_update_steps_per_epoch = math.ceil(len(train_dataloader))\n\n    # Scheduler and math around the number of training steps.\n    if args.max_train_steps is None:\n        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n\n    args.num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)\n\n    logger.info(\"***** Running training *****\")\n    logger.info(f\"  Num examples = {len(train_dataset)}\")\n    logger.info(f\"  Num Epochs = {args.num_train_epochs}\")\n    logger.info(f\"  Instantaneous batch size per device = {args.train_batch_size}\")\n    logger.info(f\"  Total train batch size (w. parallel & distributed) = {total_train_batch_size}\")\n    logger.info(f\"  Total optimization steps = {args.max_train_steps}\")\n\n    global_step = 0\n\n    epochs = tqdm(range(args.num_train_epochs), desc=\"Epoch ... \", position=0)\n    for epoch in epochs:\n        # ======================== Training ================================\n\n        train_metrics = []\n\n        steps_per_epoch = len(train_dataset) // total_train_batch_size\n        train_step_progress_bar = tqdm(total=steps_per_epoch, desc=\"Training...\", position=1, leave=False)\n        # train\n        for batch in train_dataloader:\n            batch = shard(batch)\n            state, train_metric, train_rngs = p_train_step(state, text_encoder_params, vae_params, batch, train_rngs)\n            train_metrics.append(train_metric)\n\n            train_step_progress_bar.update(1)\n\n            global_step += 1\n            if global_step >= args.max_train_steps:\n                break\n\n        train_metric = jax_utils.unreplicate(train_metric)\n\n        train_step_progress_bar.close()\n        epochs.write(f\"Epoch... ({epoch + 1}/{args.num_train_epochs} | Loss: {train_metric['loss']})\")\n\n    # Create the pipeline using using the trained modules and save it.\n    if jax.process_index() == 0:\n        scheduler = FlaxPNDMScheduler(\n            beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", skip_prk_steps=True\n        )\n        safety_checker = FlaxStableDiffusionSafetyChecker.from_pretrained(\n            \"CompVis/stable-diffusion-safety-checker\", from_pt=True\n        )\n        pipeline = FlaxStableDiffusionPipeline(\n            text_encoder=text_encoder,\n            vae=vae,\n            unet=unet,\n            tokenizer=tokenizer,\n            scheduler=scheduler,\n            safety_checker=safety_checker,", "metadata": {"task_id": "huggingface_diffusers/67", "ground_truth": "            feature_extractor=CLIPFeatureExtractor.from_pretrained(\"openai/clip-vit-base-patch32\"),", "fpath_tuple": ["huggingface_diffusers", "examples", "text_to_image", "train_text_to_image_flax.py"], "context_start_lineno": 401, "line_no": 560, "query_window": {"context": "        train_metric = jax_utils.unreplicate(train_metric)\n\n        train_step_progress_bar.close()\n        epochs.write(f\"Epoch... ({epoch + 1}/{args.num_train_epochs} | Loss: {train_metric['loss']})\")\n\n    # Create the pipeline using using the trained modules and save it.\n    if jax.process_index() == 0:\n        scheduler = FlaxPNDMScheduler(\n            beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", skip_prk_steps=True\n        )\n        safety_checker = FlaxStableDiffusionSafetyChecker.from_pretrained(\n            \"CompVis/stable-diffusion-safety-checker\", from_pt=True\n        )\n        pipeline = FlaxStableDiffusionPipeline(\n            text_encoder=text_encoder,\n            vae=vae,\n            unet=unet,\n            tokenizer=tokenizer,\n            scheduler=scheduler,\n            safety_checker=safety_checker,", "metadata": {"fpath_tuple": ["huggingface_diffusers", "examples", "text_to_image", "train_text_to_image_flax.py"], "line_no": 560, "task_id": "huggingface_diffusers/67", "start_line_no": 540, "end_line_no": 560, "window_size": 20, "context_start_lineno": 401, "repo": "huggingface_diffusers"}}, "top_k_context": [{"context": "        train_metric = jax_utils.unreplicate(train_metric)\n\n        train_step_progress_bar.close()\n        epochs.write(f\"Epoch... ({epoch + 1}/{args.num_train_epochs} | Loss: {train_metric['loss']})\")\n\n    # Create the pipeline using using the trained modules and save it.\n    if jax.process_index() == 0:\n        scheduler = FlaxPNDMScheduler(\n            beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", skip_prk_steps=True\n        )\n        safety_checker = FlaxStableDiffusionSafetyChecker.from_pretrained(\n            \"CompVis/stable-diffusion-safety-checker\", from_pt=True\n        )\n        pipeline = FlaxStableDiffusionPipeline(\n            text_encoder=text_encoder,\n            vae=vae,\n            unet=unet,\n            tokenizer=tokenizer,\n            scheduler=scheduler,\n            safety_checker=safety_checker,", "metadata": [{"fpath_tuple": ["huggingface_diffusers", "examples", "textual_inversion", "textual_inversion_flax.py"], "line_no": 632, "start_line_no": 622, "end_line_no": 642, "window_size": 20, "repo": "huggingface_diffusers", "slice_size": 10}, {"fpath_tuple": ["huggingface_diffusers", "examples", "text_to_image", "train_text_to_image_flax.py"], "line_no": 550, "start_line_no": 540, "end_line_no": 560, "window_size": 20, "repo": "huggingface_diffusers", "slice_size": 10}], "sim_score": 1.0}, {"context": "                break\n\n        train_metric = jax_utils.unreplicate(train_metric)\n\n        train_step_progress_bar.close()\n        epochs.write(f\"Epoch... ({epoch + 1}/{args.num_train_epochs} | Loss: {train_metric['loss']})\")\n\n    # Create the pipeline using using the trained modules and save it.\n    if jax.process_index() == 0:\n        scheduler = FlaxPNDMScheduler(\n            beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", skip_prk_steps=True\n        )\n        safety_checker = FlaxStableDiffusionSafetyChecker.from_pretrained(\n            \"CompVis/stable-diffusion-safety-checker\", from_pt=True\n        )\n        pipeline = FlaxStableDiffusionPipeline(\n            text_encoder=text_encoder,\n            vae=vae,\n            unet=unet,\n            tokenizer=tokenizer,", "metadata": [{"fpath_tuple": ["huggingface_diffusers", "examples", "textual_inversion", "textual_inversion_flax.py"], "line_no": 630, "start_line_no": 620, "end_line_no": 640, "window_size": 20, "repo": "huggingface_diffusers", "slice_size": 10}, {"fpath_tuple": ["huggingface_diffusers", "examples", "text_to_image", "train_text_to_image_flax.py"], "line_no": 548, "start_line_no": 538, "end_line_no": 558, "window_size": 20, "repo": "huggingface_diffusers", "slice_size": 10}], "sim_score": 0.9774436090225563}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# metrics/competition_math/competition_math.py\n# --------------------------------------------------\n# import evaluate\n# \n# \n# _CITATION = \"\"\"\\\n# @article{hendrycksmath2021,\n#   title={Measuring Mathematical Problem Solving With the MATH Dataset},\n#   author={Dan Hendrycks\n#     and Collin Burns\n#     and Saurav Kadavath\n#     and Akul Arora\n#     and Steven Basart\n#     and Eric Tang\n#     and Dawn Song\n#     and Jacob Steinhardt},\n#   journal={arXiv preprint arXiv:2103.03874},\n#   year={2021}\n# }\n# \"\"\"\n# \n# \n# --------------------------------------------------\n# the below code fragment can be found in:\n# metrics/competition_math/competition_math.py\n# --------------------------------------------------\n#   author={Dan Hendrycks\n#     and Collin Burns\n#     and Saurav Kadavath\n#     and Akul Arora\n#     and Steven Basart\n#     and Eric Tang\n#     and Dawn Song\n#     and Jacob Steinhardt},\n#   journal={arXiv preprint arXiv:2103.03874},\n#   year={2021}\n# }\n# \"\"\"\n# \n# \n# _DESCRIPTION = \"\"\"\\\n# This metric is used to assess performance on the Mathematics Aptitude Test of Heuristics (MATH) dataset.\n# It first canonicalizes the inputs (e.g., converting \"1/2\" to \"\\\\frac{1}{2}\") and then computes accuracy.\n# \"\"\"\n# \n# \n# --------------------------------------------------\n# the below code fragment can be found in:\n# metrics/competition_math/competition_math.py\n# --------------------------------------------------\n# \n# _CITATION = \"\"\"\\\n# @article{hendrycksmath2021,\n#   title={Measuring Mathematical Problem Solving With the MATH Dataset},\n#   author={Dan Hendrycks\n#     and Collin Burns\n#     and Saurav Kadavath\n#     and Akul Arora\n#     and Steven Basart\n#     and Eric Tang\n#     and Dawn Song\n#     and Jacob Steinhardt},\n#   journal={arXiv preprint arXiv:2103.03874},\n#   year={2021}\n# }\n# \"\"\"\n# \n# \n# _DESCRIPTION = \"\"\"\\\n# This metric is used to assess performance on the Mathematics Aptitude Test of Heuristics (MATH) dataset.\n# --------------------------------------------------\n# the below code fragment can be found in:\n# metrics/competition_math/competition_math.py\n# --------------------------------------------------\n# @article{hendrycksmath2021,\n#   title={Measuring Mathematical Problem Solving With the MATH Dataset},\n#   author={Dan Hendrycks\n#     and Collin Burns\n#     and Saurav Kadavath\n#     and Akul Arora\n#     and Steven Basart\n#     and Eric Tang\n#     and Dawn Song\n#     and Jacob Steinhardt},\n#   journal={arXiv preprint arXiv:2103.03874},\n#   year={2021}\n# }\n# \"\"\"\n# \n# \n# _DESCRIPTION = \"\"\"\\\n# This metric is used to assess performance on the Mathematics Aptitude Test of Heuristics (MATH) dataset.\n# It first canonicalizes the inputs (e.g., converting \"1/2\" to \"\\\\frac{1}{2}\") and then computes accuracy.\n# \"\"\"\n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\n# Copyright 2020 The HuggingFace Evaluate Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\" CUAD metric. \"\"\"\n\nimport datasets\n\nimport evaluate\n\nfrom .compute_score import compute_score\n\n\n_CITATION = \"\"\"\\\n@article{hendrycks2021cuad,\n      title={CUAD: An Expert-Annotated NLP Dataset for Legal Contract Review},\n      author={Dan Hendrycks and Collin Burns and Anya Chen and Spencer Ball},\n      journal={arXiv preprint arXiv:2103.06268},\n      year={2021}\n}\n\"\"\"\n\n_DESCRIPTION = \"\"\"\nThis metric wrap the official scoring script for version 1 of the Contract\nUnderstanding Atticus Dataset (CUAD).\nContract Understanding Atticus Dataset (CUAD) v1 is a corpus of more than 13,000 labels in 510\ncommercial legal contracts that have been manually labeled to identify 41 categories of important\nclauses that lawyers look for when reviewing contracts in connection with corporate transactions.\n\"\"\"\n\n_KWARGS_DESCRIPTION = \"\"\"", "metadata": {"task_id": "huggingface_evaluate/186", "ground_truth": "Computes CUAD scores (EM, F1, AUPR, Precision@80%Recall, and Precision@90%Recall).", "fpath_tuple": ["huggingface_evaluate", "metrics", "cuad", "cuad.py"], "context_start_lineno": 0, "line_no": 40, "query_window": {"context": "\n\n_CITATION = \"\"\"\\\n@article{hendrycks2021cuad,\n      title={CUAD: An Expert-Annotated NLP Dataset for Legal Contract Review},\n      author={Dan Hendrycks and Collin Burns and Anya Chen and Spencer Ball},\n      journal={arXiv preprint arXiv:2103.06268},\n      year={2021}\n}\n\"\"\"\n\n_DESCRIPTION = \"\"\"\nThis metric wrap the official scoring script for version 1 of the Contract\nUnderstanding Atticus Dataset (CUAD).\nContract Understanding Atticus Dataset (CUAD) v1 is a corpus of more than 13,000 labels in 510\ncommercial legal contracts that have been manually labeled to identify 41 categories of important\nclauses that lawyers look for when reviewing contracts in connection with corporate transactions.\n\"\"\"\n\n_KWARGS_DESCRIPTION = \"\"\"", "metadata": {"fpath_tuple": ["huggingface_evaluate", "metrics", "cuad", "cuad.py"], "line_no": 40, "task_id": "huggingface_evaluate/186", "start_line_no": 20, "end_line_no": 40, "window_size": 20, "context_start_lineno": 0, "repo": "huggingface_evaluate"}}, "top_k_context": [{"context": "\n_CITATION = \"\"\"\\\n@article{hendrycksmath2021,\n  title={Measuring Mathematical Problem Solving With the MATH Dataset},\n  author={Dan Hendrycks\n    and Collin Burns\n    and Saurav Kadavath\n    and Akul Arora\n    and Steven Basart\n    and Eric Tang\n    and Dawn Song\n    and Jacob Steinhardt},\n  journal={arXiv preprint arXiv:2103.03874},\n  year={2021}\n}\n\"\"\"\n\n\n_DESCRIPTION = \"\"\"\\\nThis metric is used to assess performance on the Mathematics Aptitude Test of Heuristics (MATH) dataset.", "metadata": [{"fpath_tuple": ["huggingface_evaluate", "metrics", "competition_math", "competition_math.py"], "line_no": 30, "start_line_no": 20, "end_line_no": 40, "window_size": 20, "repo": "huggingface_evaluate", "slice_size": 10}], "sim_score": 0.30857142857142855}, {"context": "import evaluate\n\n\n_CITATION = \"\"\"\\\n@article{hendrycksmath2021,\n  title={Measuring Mathematical Problem Solving With the MATH Dataset},\n  author={Dan Hendrycks\n    and Collin Burns\n    and Saurav Kadavath\n    and Akul Arora\n    and Steven Basart\n    and Eric Tang\n    and Dawn Song\n    and Jacob Steinhardt},\n  journal={arXiv preprint arXiv:2103.03874},\n  year={2021}\n}\n\"\"\"\n\n", "metadata": [{"fpath_tuple": ["huggingface_evaluate", "metrics", "competition_math", "competition_math.py"], "line_no": 28, "start_line_no": 18, "end_line_no": 38, "window_size": 20, "repo": "huggingface_evaluate", "slice_size": 10}], "sim_score": 0.27607361963190186}, {"context": "@article{hendrycksmath2021,\n  title={Measuring Mathematical Problem Solving With the MATH Dataset},\n  author={Dan Hendrycks\n    and Collin Burns\n    and Saurav Kadavath\n    and Akul Arora\n    and Steven Basart\n    and Eric Tang\n    and Dawn Song\n    and Jacob Steinhardt},\n  journal={arXiv preprint arXiv:2103.03874},\n  year={2021}\n}\n\"\"\"\n\n\n_DESCRIPTION = \"\"\"\\\nThis metric is used to assess performance on the Mathematics Aptitude Test of Heuristics (MATH) dataset.\nIt first canonicalizes the inputs (e.g., converting \"1/2\" to \"\\\\frac{1}{2}\") and then computes accuracy.\n\"\"\"", "metadata": [{"fpath_tuple": ["huggingface_evaluate", "metrics", "competition_math", "competition_math.py"], "line_no": 32, "start_line_no": 22, "end_line_no": 42, "window_size": 20, "repo": "huggingface_evaluate", "slice_size": 10}], "sim_score": 0.26666666666666666}, {"context": "import math_equivalence  # From: git+https://github.com/hendrycks/math.git\n\nimport evaluate\n\n\n_CITATION = \"\"\"\\\n@article{hendrycksmath2021,\n  title={Measuring Mathematical Problem Solving With the MATH Dataset},\n  author={Dan Hendrycks\n    and Collin Burns\n    and Saurav Kadavath\n    and Akul Arora\n    and Steven Basart\n    and Eric Tang\n    and Dawn Song\n    and Jacob Steinhardt},\n  journal={arXiv preprint arXiv:2103.03874},\n  year={2021}\n}\n\"\"\"", "metadata": [{"fpath_tuple": ["huggingface_evaluate", "metrics", "competition_math", "competition_math.py"], "line_no": 26, "start_line_no": 16, "end_line_no": 36, "window_size": 20, "repo": "huggingface_evaluate", "slice_size": 10}], "sim_score": 0.2542372881355932}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# torchrl/objectives/sac.py\n# --------------------------------------------------\n#         self.register_buffer(\"gamma\", torch.tensor(gamma))\n#         self.priority_key = priotity_key\n#         self.loss_function = loss_function\n#         try:\n#             device = next(self.parameters()).device\n#         except AttributeError:\n#             device = torch.device(\"cpu\")\n#         self.register_buffer(\"alpha_init\", torch.tensor(alpha_init, device=device))\n#         self.register_buffer(\n#             \"min_log_alpha\", torch.tensor(min_alpha, device=device).log()\n#         )\n#         self.register_buffer(\n#             \"max_log_alpha\", torch.tensor(max_alpha, device=device).log()\n#         )\n#         self.fixed_alpha = fixed_alpha\n#         if fixed_alpha:\n#             self.register_buffer(\n#                 \"log_alpha\", torch.tensor(math.log(alpha_init), device=device)\n#             )\n#         else:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# torchrl/objectives/sac.py\n# --------------------------------------------------\n#             create_target_params=self.delay_qvalue,\n#             compare_against=list(actor_network.parameters()) + value_params,\n#         )\n# \n#         self.register_buffer(\"gamma\", torch.tensor(gamma))\n#         self.priority_key = priotity_key\n#         self.loss_function = loss_function\n#         try:\n#             device = next(self.parameters()).device\n#         except AttributeError:\n#             device = torch.device(\"cpu\")\n#         self.register_buffer(\"alpha_init\", torch.tensor(alpha_init, device=device))\n#         self.register_buffer(\n#             \"min_log_alpha\", torch.tensor(min_alpha, device=device).log()\n#         )\n#         self.register_buffer(\n#             \"max_log_alpha\", torch.tensor(max_alpha, device=device).log()\n#         )\n#         self.fixed_alpha = fixed_alpha\n#         if fixed_alpha:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# torchrl/objectives/sac.py\n# --------------------------------------------------\n#         )\n# \n#         self.register_buffer(\"gamma\", torch.tensor(gamma))\n#         self.priority_key = priotity_key\n#         self.loss_function = loss_function\n#         try:\n#             device = next(self.parameters()).device\n#         except AttributeError:\n#             device = torch.device(\"cpu\")\n#         self.register_buffer(\"alpha_init\", torch.tensor(alpha_init, device=device))\n#         self.register_buffer(\n#             \"min_log_alpha\", torch.tensor(min_alpha, device=device).log()\n#         )\n#         self.register_buffer(\n#             \"max_log_alpha\", torch.tensor(max_alpha, device=device).log()\n#         )\n#         self.fixed_alpha = fixed_alpha\n#         if fixed_alpha:\n#             self.register_buffer(\n#                 \"log_alpha\", torch.tensor(math.log(alpha_init), device=device)\n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\n# Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom functools import wraps\nfrom typing import List, Optional, Tuple, Union\n\nimport torch\nfrom tensordict.nn import dispatch_kwargs\nfrom tensordict.tensordict import TensorDictBase\nfrom torch import nn, Tensor\n\nfrom torchrl.envs.utils import step_mdp\nfrom torchrl.modules import SafeModule\n\nfrom torchrl.objectives.utils import hold_out_net\nfrom torchrl.objectives.value.functional import (\n    td_advantage_estimate,\n    td_lambda_advantage_estimate,\n    vec_generalized_advantage_estimate,\n    vec_td_lambda_advantage_estimate,\n)\n\n\ndef _self_set_grad_enabled(fun):\n    @wraps(fun)\n    def new_fun(self, *args, **kwargs):\n        with torch.set_grad_enabled(self.differentiable):\n            return fun(self, *args, **kwargs)\n\n    return new_fun\n\n\nclass TDEstimate(nn.Module):\n    \"\"\"Temporal Difference estimate of advantage function.\n\n    Args:\n        gamma (scalar): exponential mean discount.\n        value_network (SafeModule): value operator used to retrieve the value estimates.\n        average_rewards (bool, optional): if True, rewards will be standardized\n            before the TD is computed.\n        differentiable (bool, optional): if True, gradients are propagated throught\n            the computation of the value function. Default is :obj:`False`.\n        advantage_key (str or tuple of str, optional): the key of the advantage entry.\n            Defaults to \"advantage\".\n        value_target_key (str or tuple of str, optional): the key of the advantage entry.\n            Defaults to \"value_target\".\n        value_key (str or tuple of str, optional): the value key to read from the input tensordict.\n            Defaults to \"state_value\".\n\n    \"\"\"\n\n    def __init__(\n        self,\n        gamma: Union[float, torch.Tensor],\n        value_network: SafeModule,\n        average_rewards: bool = False,\n        differentiable: bool = False,\n        advantage_key: Union[str, Tuple] = \"advantage\",\n        value_target_key: Union[str, Tuple] = \"value_target\",\n        value_key: Union[str, Tuple] = \"state_value\",\n    ):\n        super().__init__()\n        try:\n            device = next(value_network.parameters()).device\n        except StopIteration:\n            device = torch.device(\"cpu\")\n        self.register_buffer(\"gamma\", torch.tensor(gamma, device=device))\n        self.value_network = value_network\n\n        self.average_rewards = average_rewards\n        self.differentiable = differentiable\n        self.value_key = value_key\n        if value_key not in value_network.out_keys:\n            raise KeyError(", "metadata": {"task_id": "pytorch_rl/178", "ground_truth": "                f\"value key '{value_key}' not found in value network out_keys.\"", "fpath_tuple": ["pytorch_rl", "torchrl", "objectives", "value", "advantages.py"], "context_start_lineno": 0, "line_no": 76, "query_window": {"context": "        value_network: SafeModule,\n        average_rewards: bool = False,\n        differentiable: bool = False,\n        advantage_key: Union[str, Tuple] = \"advantage\",\n        value_target_key: Union[str, Tuple] = \"value_target\",\n        value_key: Union[str, Tuple] = \"state_value\",\n    ):\n        super().__init__()\n        try:\n            device = next(value_network.parameters()).device\n        except StopIteration:\n            device = torch.device(\"cpu\")\n        self.register_buffer(\"gamma\", torch.tensor(gamma, device=device))\n        self.value_network = value_network\n\n        self.average_rewards = average_rewards\n        self.differentiable = differentiable\n        self.value_key = value_key\n        if value_key not in value_network.out_keys:\n            raise KeyError(", "metadata": {"fpath_tuple": ["pytorch_rl", "torchrl", "objectives", "value", "advantages.py"], "line_no": 76, "task_id": "pytorch_rl/178", "start_line_no": 56, "end_line_no": 76, "window_size": 20, "context_start_lineno": 0, "repo": "pytorch_rl"}}, "top_k_context": [{"context": "            create_target_params=self.delay_qvalue,\n            compare_against=list(actor_network.parameters()) + value_params,\n        )\n\n        self.register_buffer(\"gamma\", torch.tensor(gamma))\n        self.priority_key = priotity_key\n        self.loss_function = loss_function\n        try:\n            device = next(self.parameters()).device\n        except AttributeError:\n            device = torch.device(\"cpu\")\n        self.register_buffer(\"alpha_init\", torch.tensor(alpha_init, device=device))\n        self.register_buffer(\n            \"min_log_alpha\", torch.tensor(min_alpha, device=device).log()\n        )\n        self.register_buffer(\n            \"max_log_alpha\", torch.tensor(max_alpha, device=device).log()\n        )\n        self.fixed_alpha = fixed_alpha\n        if fixed_alpha:", "metadata": [{"fpath_tuple": ["pytorch_rl", "torchrl", "objectives", "sac.py"], "line_no": 144, "start_line_no": 134, "end_line_no": 154, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}], "sim_score": 0.40384615384615385}, {"context": "            \"qvalue_network\",\n            num_qvalue_nets,\n            create_target_params=self.delay_qvalue,\n            compare_against=list(actor_network.parameters()) + value_params,\n        )\n\n        self.register_buffer(\"gamma\", torch.tensor(gamma))\n        self.priority_key = priotity_key\n        self.loss_function = loss_function\n        try:\n            device = next(self.parameters()).device\n        except AttributeError:\n            device = torch.device(\"cpu\")\n        self.register_buffer(\"alpha_init\", torch.tensor(alpha_init, device=device))\n        self.register_buffer(\n            \"min_log_alpha\", torch.tensor(min_alpha, device=device).log()\n        )\n        self.register_buffer(\n            \"max_log_alpha\", torch.tensor(max_alpha, device=device).log()\n        )", "metadata": [{"fpath_tuple": ["pytorch_rl", "torchrl", "objectives", "sac.py"], "line_no": 142, "start_line_no": 132, "end_line_no": 152, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}], "sim_score": 0.3942307692307692}, {"context": "        )\n\n        self.register_buffer(\"gamma\", torch.tensor(gamma))\n        self.priority_key = priotity_key\n        self.loss_function = loss_function\n        try:\n            device = next(self.parameters()).device\n        except AttributeError:\n            device = torch.device(\"cpu\")\n        self.register_buffer(\"alpha_init\", torch.tensor(alpha_init, device=device))\n        self.register_buffer(\n            \"min_log_alpha\", torch.tensor(min_alpha, device=device).log()\n        )\n        self.register_buffer(\n            \"max_log_alpha\", torch.tensor(max_alpha, device=device).log()\n        )\n        self.fixed_alpha = fixed_alpha\n        if fixed_alpha:\n            self.register_buffer(\n                \"log_alpha\", torch.tensor(math.log(alpha_init), device=device)", "metadata": [{"fpath_tuple": ["pytorch_rl", "torchrl", "objectives", "sac.py"], "line_no": 146, "start_line_no": 136, "end_line_no": 156, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}], "sim_score": 0.3877551020408163}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# tests/test_CRA_gan_attack.py\n# --------------------------------------------------\n#         cfg.attack.target_label_ind = 3\n# \n#         return backup_cfg\n# \n#     def test_CRA_GAN_femnist_standalone(self):\n#         init_cfg = global_cfg.clone()\n#         backup_cfg = self.set_config_femnist(init_cfg)\n#         setup_seed(init_cfg.seed)\n#         update_logger(init_cfg, True)\n# \n#         data, modified_cfg = get_data(init_cfg.clone())\n#         init_cfg.merge_from_other_cfg(modified_cfg)\n#         self.assertIsNotNone(data)\n# \n#         Fed_runner = get_runner(data=data,\n#                                 server_class=get_server_cls(init_cfg),\n#                                 client_class=get_client_cls(init_cfg),\n#                                 config=init_cfg.clone())\n#         self.assertIsNotNone(Fed_runner)\n#         test_best_results = Fed_runner.run()\n# --------------------------------------------------\n# the below code fragment can be found in:\n# tests/test_ditto.py\n# --------------------------------------------------\n#         return backup_cfg\n# \n#     def test_femnist_standalone(self):\n#         init_cfg = global_cfg.clone()\n# \n#         backup_cfg = self.set_config_femnist(init_cfg)\n#         setup_seed(init_cfg.seed)\n#         update_logger(init_cfg, True)\n# \n#         data, modified_cfg = get_data(init_cfg.clone())\n#         init_cfg.merge_from_other_cfg(modified_cfg)\n#         self.assertIsNotNone(data)\n# \n#         Fed_runner = get_runner(data=data,\n#                                 server_class=get_server_cls(init_cfg),\n#                                 client_class=get_client_cls(init_cfg),\n#                                 config=init_cfg.clone())\n#         self.assertIsNotNone(Fed_runner)\n#         test_best_results = Fed_runner.run()\n#         print(test_best_results)\n# --------------------------------------------------\n# the below code fragment can be found in:\n# tests/test_CRA_gan_attack.py\n# --------------------------------------------------\n#         cfg.attack.attack_method = 'gan_attack'\n#         cfg.attack.attack_id = 5\n#         cfg.attack.target_label_ind = 3\n# \n#         return backup_cfg\n# \n#     def test_CRA_GAN_femnist_standalone(self):\n#         init_cfg = global_cfg.clone()\n#         backup_cfg = self.set_config_femnist(init_cfg)\n#         setup_seed(init_cfg.seed)\n#         update_logger(init_cfg, True)\n# \n#         data, modified_cfg = get_data(init_cfg.clone())\n#         init_cfg.merge_from_other_cfg(modified_cfg)\n#         self.assertIsNotNone(data)\n# \n#         Fed_runner = get_runner(data=data,\n#                                 server_class=get_server_cls(init_cfg),\n#                                 client_class=get_client_cls(init_cfg),\n#                                 config=init_cfg.clone())\n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\n# Copyright (c) Alibaba, Inc. and its affiliates.\nimport unittest\n\nfrom federatedscope.core.auxiliaries.data_builder import get_data\nfrom federatedscope.core.auxiliaries.utils import setup_seed\nfrom federatedscope.core.auxiliaries.logging import update_logger\nfrom federatedscope.core.configs.config import global_cfg\nfrom federatedscope.core.auxiliaries.runner_builder import get_runner\nfrom federatedscope.core.auxiliaries.worker_builder import get_server_cls, get_client_cls\n\n\nclass GradAscentTest(unittest.TestCase):\n    def setUp(self):\n        print(('Testing %s.%s' % (type(self).__name__, self._testMethodName)))\n\n    def set_config_femnist(self, cfg):\n        backup_cfg = cfg.clone()\n\n        import torch\n        cfg.use_gpu = torch.cuda.is_available()\n        cfg.device = 0\n        cfg.eval.freq = 10\n        cfg.eval.metrics = ['acc', 'loss_regular']\n\n        cfg.federate.mode = 'standalone'\n        cfg.train.local_update_steps = 5\n        cfg.federate.total_round_num = 20\n        cfg.federate.sample_client_num = 5\n        cfg.federate.client_num = 10\n\n        cfg.data.root = 'test_data/'\n        cfg.data.type = 'femnist'\n        cfg.data.splits = [0.6, 0.2, 0.2]\n        cfg.data.batch_size = 10\n        cfg.data.subsample = 0.01\n        cfg.data.transform = [['ToTensor'],\n                              [\n                                  'Normalize', {\n                                      'mean': [0.1307],\n                                      'std': [0.3081]\n                                  }\n                              ]]\n\n        cfg.model.type = 'convnet2'\n        cfg.model.hidden = 2048\n        cfg.model.out_channels = 62\n\n        cfg.train.optimizer.lr = 0.001\n        cfg.train.optimizer.weight_decay = 0.0\n\n        cfg.criterion.type = 'CrossEntropyLoss'\n        cfg.trainer.type = 'cvtrainer'\n        cfg.seed = 123\n\n        cfg.attack.attack_method = 'GradAscent'\n        cfg.attack.attacker_id = 5\n        cfg.attack.inject_round = 0\n\n        return backup_cfg\n\n    def test_GradAscent_femnist_standalone(self):\n        init_cfg = global_cfg.clone()\n        backup_cfg = self.set_config_femnist(init_cfg)\n        setup_seed(init_cfg.seed)\n        update_logger(init_cfg, True)\n\n        data, modified_cfg = get_data(init_cfg.clone())\n        init_cfg.merge_from_other_cfg(modified_cfg)\n        self.assertIsNotNone(data)\n\n        Fed_runner = get_runner(data=data,\n                                server_class=get_server_cls(init_cfg),", "metadata": {"task_id": "alibaba_FederatedScope/148", "ground_truth": "                                client_class=get_client_cls(init_cfg),", "fpath_tuple": ["alibaba_FederatedScope", "tests", "test_MIA_gradient_ascent.py"], "context_start_lineno": 0, "line_no": 72, "query_window": {"context": "        cfg.seed = 123\n\n        cfg.attack.attack_method = 'GradAscent'\n        cfg.attack.attacker_id = 5\n        cfg.attack.inject_round = 0\n\n        return backup_cfg\n\n    def test_GradAscent_femnist_standalone(self):\n        init_cfg = global_cfg.clone()\n        backup_cfg = self.set_config_femnist(init_cfg)\n        setup_seed(init_cfg.seed)\n        update_logger(init_cfg, True)\n\n        data, modified_cfg = get_data(init_cfg.clone())\n        init_cfg.merge_from_other_cfg(modified_cfg)\n        self.assertIsNotNone(data)\n\n        Fed_runner = get_runner(data=data,\n                                server_class=get_server_cls(init_cfg),", "metadata": {"fpath_tuple": ["alibaba_FederatedScope", "tests", "test_MIA_gradient_ascent.py"], "line_no": 72, "task_id": "alibaba_FederatedScope/148", "start_line_no": 52, "end_line_no": 72, "window_size": 20, "context_start_lineno": 0, "repo": "alibaba_FederatedScope"}}, "top_k_context": [{"context": "        cfg.seed = 123\n\n        cfg.attack.attack_method = 'gan_attack'\n        cfg.attack.attack_id = 5\n        cfg.attack.target_label_ind = 3\n\n        return backup_cfg\n\n    def test_CRA_GAN_femnist_standalone(self):\n        init_cfg = global_cfg.clone()\n        backup_cfg = self.set_config_femnist(init_cfg)\n        setup_seed(init_cfg.seed)\n        update_logger(init_cfg, True)\n\n        data, modified_cfg = get_data(init_cfg.clone())\n        init_cfg.merge_from_other_cfg(modified_cfg)\n        self.assertIsNotNone(data)\n\n        Fed_runner = get_runner(data=data,\n                                server_class=get_server_cls(init_cfg),", "metadata": [{"fpath_tuple": ["alibaba_FederatedScope", "tests", "test_CRA_gan_attack.py"], "line_no": 62, "start_line_no": 52, "end_line_no": 72, "window_size": 20, "repo": "alibaba_FederatedScope", "slice_size": 10}], "sim_score": 0.8068181818181818}, {"context": "        cfg.seed = 123\n\n        return backup_cfg\n\n    def test_femnist_standalone(self):\n        init_cfg = global_cfg.clone()\n\n        backup_cfg = self.set_config_femnist(init_cfg)\n        setup_seed(init_cfg.seed)\n        update_logger(init_cfg, True)\n\n        data, modified_cfg = get_data(init_cfg.clone())\n        init_cfg.merge_from_other_cfg(modified_cfg)\n        self.assertIsNotNone(data)\n\n        Fed_runner = get_runner(data=data,\n                                server_class=get_server_cls(init_cfg),\n                                client_class=get_client_cls(init_cfg),\n                                config=init_cfg.clone())\n        self.assertIsNotNone(Fed_runner)", "metadata": [{"fpath_tuple": ["alibaba_FederatedScope", "tests", "test_ditto.py"], "line_no": 70, "start_line_no": 60, "end_line_no": 80, "window_size": 20, "repo": "alibaba_FederatedScope", "slice_size": 10}], "sim_score": 0.7738095238095238}, {"context": "        cfg.attack.attack_method = 'gan_attack'\n        cfg.attack.attack_id = 5\n        cfg.attack.target_label_ind = 3\n\n        return backup_cfg\n\n    def test_CRA_GAN_femnist_standalone(self):\n        init_cfg = global_cfg.clone()\n        backup_cfg = self.set_config_femnist(init_cfg)\n        setup_seed(init_cfg.seed)\n        update_logger(init_cfg, True)\n\n        data, modified_cfg = get_data(init_cfg.clone())\n        init_cfg.merge_from_other_cfg(modified_cfg)\n        self.assertIsNotNone(data)\n\n        Fed_runner = get_runner(data=data,\n                                server_class=get_server_cls(init_cfg),\n                                client_class=get_client_cls(init_cfg),\n                                config=init_cfg.clone())", "metadata": [{"fpath_tuple": ["alibaba_FederatedScope", "tests", "test_CRA_gan_attack.py"], "line_no": 64, "start_line_no": 54, "end_line_no": 74, "window_size": 20, "repo": "alibaba_FederatedScope", "slice_size": 10}], "sim_score": 0.7692307692307693}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# metrics/sari/sari.py\n# --------------------------------------------------\n#         delscore_precision = deltmpscore1 / len(delgramcounter_rep)\n# \n#     # ADDITION\n#     addgramcounter = set(cgramcounter) - set(sgramcounter)\n#     addgramcountergood = set(addgramcounter) & set(rgramcounter)\n#     addgramcounterall = set(rgramcounter) - set(sgramcounter)\n# \n#     addtmpscore = 0\n#     for addgram in addgramcountergood:\n#         addtmpscore += 1\n# \n#     # Define 0/0=1 instead of 0 to give higher scores for predictions that match\n#     # a target exactly.\n#     addscore_precision = 1\n#     addscore_recall = 1\n#     if len(addgramcounter) > 0:\n#         addscore_precision = addtmpscore / len(addgramcounter)\n#     if len(addgramcounterall) > 0:\n#         addscore_recall = addtmpscore / len(addgramcounterall)\n#     addscore = 0\n# --------------------------------------------------\n# the below code fragment can be found in:\n# metrics/sari/sari.py\n# --------------------------------------------------\n#     # ADDITION\n#     addgramcounter = set(cgramcounter) - set(sgramcounter)\n#     addgramcountergood = set(addgramcounter) & set(rgramcounter)\n#     addgramcounterall = set(rgramcounter) - set(sgramcounter)\n# \n#     addtmpscore = 0\n#     for addgram in addgramcountergood:\n#         addtmpscore += 1\n# \n#     # Define 0/0=1 instead of 0 to give higher scores for predictions that match\n#     # a target exactly.\n#     addscore_precision = 1\n#     addscore_recall = 1\n#     if len(addgramcounter) > 0:\n#         addscore_precision = addtmpscore / len(addgramcounter)\n#     if len(addgramcounterall) > 0:\n#         addscore_recall = addtmpscore / len(addgramcounterall)\n#     addscore = 0\n#     if addscore_precision > 0 or addscore_recall > 0:\n#         addscore = 2 * addscore_precision * addscore_recall / (addscore_precision + addscore_recall)\n# --------------------------------------------------\n# the below code fragment can be found in:\n# metrics/sari/sari.py\n# --------------------------------------------------\n#     addgramcountergood = set(addgramcounter) & set(rgramcounter)\n#     addgramcounterall = set(rgramcounter) - set(sgramcounter)\n# \n#     addtmpscore = 0\n#     for addgram in addgramcountergood:\n#         addtmpscore += 1\n# \n#     # Define 0/0=1 instead of 0 to give higher scores for predictions that match\n#     # a target exactly.\n#     addscore_precision = 1\n#     addscore_recall = 1\n#     if len(addgramcounter) > 0:\n#         addscore_precision = addtmpscore / len(addgramcounter)\n#     if len(addgramcounterall) > 0:\n#         addscore_recall = addtmpscore / len(addgramcounterall)\n#     addscore = 0\n#     if addscore_precision > 0 or addscore_recall > 0:\n#         addscore = 2 * addscore_precision * addscore_recall / (addscore_precision + addscore_recall)\n# \n#     return (keepscore, delscore_precision, addscore)\n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\n\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\" WIKI_SPLIT metric.\"\"\"\n\nimport re\nimport string\nfrom collections import Counter\n\nimport datasets\nimport sacrebleu\nimport sacremoses\nfrom packaging import version\n\nimport evaluate\n\n\n_CITATION = \"\"\"\n@inproceedings{xu-etal-2016-optimizing,\n    title = {Optimizing Statistical Machine Translation for Text Simplification},\n    authors={Xu, Wei and Napoles, Courtney and Pavlick, Ellie and Chen, Quanze and Callison-Burch, Chris},\n    journal = {Transactions of the Association for Computational Linguistics},\n    volume = {4},\n    year={2016},\n    url = {https://www.aclweb.org/anthology/Q16-1029},\n    pages = {401--415\n},\n@inproceedings{post-2018-call,\n    title = \"A Call for Clarity in Reporting {BLEU} Scores\",\n    author = \"Post, Matt\",\n    booktitle = \"Proceedings of the Third Conference on Machine Translation: Research Papers\",\n    month = oct,\n    year = \"2018\",\n    address = \"Belgium, Brussels\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://www.aclweb.org/anthology/W18-6319\",\n    pages = \"186--191\",\n}\n\"\"\"\n\n_DESCRIPTION = \"\"\"\\\nWIKI_SPLIT is the combination of three metrics SARI, EXACT and SACREBLEU\nIt can be used to evaluate the quality of machine-generated texts.\n\"\"\"\n\n\n_KWARGS_DESCRIPTION = \"\"\"\nCalculates sari score (between 0 and 100) given a list of source and predicted\nsentences, and a list of lists of reference sentences. It also computes the BLEU score as well as the exact match score.\nArgs:\n    sources: list of source sentences where each sentence should be a string.\n    predictions: list of predicted sentences where each sentence should be a string.\n    references: list of lists of reference sentences where each sentence should be a string.\nReturns:\n    sari: sari score\n    sacrebleu: sacrebleu score\n    exact: exact score\n\nExamples:\n    >>> sources=[\"About 95 species are currently accepted .\"]\n    >>> predictions=[\"About 95 you now get in .\"]\n    >>> references=[[\"About 95 species are currently known .\"]]\n    >>> wiki_split = evaluate.load(\"wiki_split\")\n    >>> results = wiki_split.compute(sources=sources, predictions=predictions, references=references)\n    >>> print(results)\n    {'sari': 21.805555555555557, 'sacrebleu': 14.535768424205482, 'exact': 0.0}\n\"\"\"\n\n\ndef normalize_answer(s):\n    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n\n    def remove_articles(text):\n        regex = re.compile(r\"\\b(a|an|the)\\b\", re.UNICODE)\n        return re.sub(regex, \" \", text)\n\n    def white_space_fix(text):\n        return \" \".join(text.split())\n\n    def remove_punc(text):\n        exclude = set(string.punctuation)\n        return \"\".join(ch for ch in text if ch not in exclude)\n\n    def lower(text):\n        return text.lower()\n\n    return white_space_fix(remove_articles(remove_punc(lower(s))))\n\n\ndef compute_exact(a_gold, a_pred):\n    return int(normalize_answer(a_gold) == normalize_answer(a_pred))\n\n\ndef compute_em(predictions, references):\n    scores = [any([compute_exact(ref, pred) for ref in refs]) for pred, refs in zip(predictions, references)]\n    return (sum(scores) / len(scores)) * 100\n\n\ndef SARIngram(sgrams, cgrams, rgramslist, numref):\n    rgramsall = [rgram for rgrams in rgramslist for rgram in rgrams]\n    rgramcounter = Counter(rgramsall)\n\n    sgramcounter = Counter(sgrams)\n    sgramcounter_rep = Counter()\n    for sgram, scount in sgramcounter.items():\n        sgramcounter_rep[sgram] = scount * numref\n\n    cgramcounter = Counter(cgrams)\n    cgramcounter_rep = Counter()\n    for cgram, ccount in cgramcounter.items():\n        cgramcounter_rep[cgram] = ccount * numref\n\n    # KEEP\n    keepgramcounter_rep = sgramcounter_rep & cgramcounter_rep\n    keepgramcountergood_rep = keepgramcounter_rep & rgramcounter\n    keepgramcounterall_rep = sgramcounter_rep & rgramcounter\n\n    keeptmpscore1 = 0\n    keeptmpscore2 = 0\n    for keepgram in keepgramcountergood_rep:\n        keeptmpscore1 += keepgramcountergood_rep[keepgram] / keepgramcounter_rep[keepgram]\n        # Fix an alleged bug [2] in the keep score computation.\n        # keeptmpscore2 += keepgramcountergood_rep[keepgram] / keepgramcounterall_rep[keepgram]\n        keeptmpscore2 += keepgramcountergood_rep[keepgram]\n    # Define 0/0=1 instead of 0 to give higher scores for predictions that match\n    #      a target exactly.\n    keepscore_precision = 1\n    keepscore_recall = 1\n    if len(keepgramcounter_rep) > 0:\n        keepscore_precision = keeptmpscore1 / len(keepgramcounter_rep)\n    if len(keepgramcounterall_rep) > 0:\n        # Fix an alleged bug [2] in the keep score computation.\n        # keepscore_recall = keeptmpscore2 / len(keepgramcounterall_rep)\n        keepscore_recall = keeptmpscore2 / sum(keepgramcounterall_rep.values())\n    keepscore = 0\n    if keepscore_precision > 0 or keepscore_recall > 0:\n        keepscore = 2 * keepscore_precision * keepscore_recall / (keepscore_precision + keepscore_recall)\n\n    # DELETION\n    delgramcounter_rep = sgramcounter_rep - cgramcounter_rep\n    delgramcountergood_rep = delgramcounter_rep - rgramcounter\n    delgramcounterall_rep = sgramcounter_rep - rgramcounter\n    deltmpscore1 = 0\n    deltmpscore2 = 0\n    for delgram in delgramcountergood_rep:\n        deltmpscore1 += delgramcountergood_rep[delgram] / delgramcounter_rep[delgram]\n        deltmpscore2 += delgramcountergood_rep[delgram] / delgramcounterall_rep[delgram]\n    # Define 0/0=1 instead of 0 to give higher scores for predictions that match\n    # a target exactly.\n    delscore_precision = 1\n    if len(delgramcounter_rep) > 0:\n        delscore_precision = deltmpscore1 / len(delgramcounter_rep)\n\n    # ADDITION\n    addgramcounter = set(cgramcounter) - set(sgramcounter)\n    addgramcountergood = set(addgramcounter) & set(rgramcounter)\n    addgramcounterall = set(rgramcounter) - set(sgramcounter)\n\n    addtmpscore = 0\n    for addgram in addgramcountergood:\n        addtmpscore += 1\n\n    # Define 0/0=1 instead of 0 to give higher scores for predictions that match\n    # a target exactly.\n    addscore_precision = 1\n    addscore_recall = 1\n    if len(addgramcounter) > 0:\n        addscore_precision = addtmpscore / len(addgramcounter)\n    if len(addgramcounterall) > 0:\n        addscore_recall = addtmpscore / len(addgramcounterall)\n    addscore = 0\n    if addscore_precision > 0 or addscore_recall > 0:", "metadata": {"task_id": "huggingface_evaluate/1", "ground_truth": "        addscore = 2 * addscore_precision * addscore_recall / (addscore_precision + addscore_recall)", "fpath_tuple": ["huggingface_evaluate", "metrics", "wiki_split", "wiki_split.py"], "context_start_lineno": 9, "line_no": 182, "query_window": {"context": "\n    # ADDITION\n    addgramcounter = set(cgramcounter) - set(sgramcounter)\n    addgramcountergood = set(addgramcounter) & set(rgramcounter)\n    addgramcounterall = set(rgramcounter) - set(sgramcounter)\n\n    addtmpscore = 0\n    for addgram in addgramcountergood:\n        addtmpscore += 1\n\n    # Define 0/0=1 instead of 0 to give higher scores for predictions that match\n    # a target exactly.\n    addscore_precision = 1\n    addscore_recall = 1\n    if len(addgramcounter) > 0:\n        addscore_precision = addtmpscore / len(addgramcounter)\n    if len(addgramcounterall) > 0:\n        addscore_recall = addtmpscore / len(addgramcounterall)\n    addscore = 0\n    if addscore_precision > 0 or addscore_recall > 0:", "metadata": {"fpath_tuple": ["huggingface_evaluate", "metrics", "wiki_split", "wiki_split.py"], "line_no": 182, "task_id": "huggingface_evaluate/1", "start_line_no": 162, "end_line_no": 182, "window_size": 20, "context_start_lineno": 9, "repo": "huggingface_evaluate"}}, "top_k_context": [{"context": "    # ADDITION\n    addgramcounter = set(cgramcounter) - set(sgramcounter)\n    addgramcountergood = set(addgramcounter) & set(rgramcounter)\n    addgramcounterall = set(rgramcounter) - set(sgramcounter)\n\n    addtmpscore = 0\n    for addgram in addgramcountergood:\n        addtmpscore += 1\n\n    # Define 0/0=1 instead of 0 to give higher scores for predictions that match\n    # a target exactly.\n    addscore_precision = 1\n    addscore_recall = 1\n    if len(addgramcounter) > 0:\n        addscore_precision = addtmpscore / len(addgramcounter)\n    if len(addgramcounterall) > 0:\n        addscore_recall = addtmpscore / len(addgramcounterall)\n    addscore = 0\n    if addscore_precision > 0 or addscore_recall > 0:\n        addscore = 2 * addscore_precision * addscore_recall / (addscore_precision + addscore_recall)", "metadata": [{"fpath_tuple": ["huggingface_evaluate", "metrics", "sari", "sari.py"], "line_no": 146, "start_line_no": 136, "end_line_no": 156, "window_size": 20, "repo": "huggingface_evaluate", "slice_size": 10}], "sim_score": 0.9375}, {"context": "        delscore_precision = deltmpscore1 / len(delgramcounter_rep)\n\n    # ADDITION\n    addgramcounter = set(cgramcounter) - set(sgramcounter)\n    addgramcountergood = set(addgramcounter) & set(rgramcounter)\n    addgramcounterall = set(rgramcounter) - set(sgramcounter)\n\n    addtmpscore = 0\n    for addgram in addgramcountergood:\n        addtmpscore += 1\n\n    # Define 0/0=1 instead of 0 to give higher scores for predictions that match\n    # a target exactly.\n    addscore_precision = 1\n    addscore_recall = 1\n    if len(addgramcounter) > 0:\n        addscore_precision = addtmpscore / len(addgramcounter)\n    if len(addgramcounterall) > 0:\n        addscore_recall = addtmpscore / len(addgramcounterall)\n    addscore = 0", "metadata": [{"fpath_tuple": ["huggingface_evaluate", "metrics", "sari", "sari.py"], "line_no": 144, "start_line_no": 134, "end_line_no": 154, "window_size": 20, "repo": "huggingface_evaluate", "slice_size": 10}], "sim_score": 0.9365079365079365}, {"context": "    delscore_precision = 1\n    if len(delgramcounter_rep) > 0:\n        delscore_precision = deltmpscore1 / len(delgramcounter_rep)\n\n    # ADDITION\n    addgramcounter = set(cgramcounter) - set(sgramcounter)\n    addgramcountergood = set(addgramcounter) & set(rgramcounter)\n    addgramcounterall = set(rgramcounter) - set(sgramcounter)\n\n    addtmpscore = 0\n    for addgram in addgramcountergood:\n        addtmpscore += 1\n\n    # Define 0/0=1 instead of 0 to give higher scores for predictions that match\n    # a target exactly.\n    addscore_precision = 1\n    addscore_recall = 1\n    if len(addgramcounter) > 0:\n        addscore_precision = addtmpscore / len(addgramcounter)\n    if len(addgramcounterall) > 0:", "metadata": [{"fpath_tuple": ["huggingface_evaluate", "metrics", "sari", "sari.py"], "line_no": 142, "start_line_no": 132, "end_line_no": 152, "window_size": 20, "repo": "huggingface_evaluate", "slice_size": 10}], "sim_score": 0.9365079365079365}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# torchrl/modules/models/models.py\n# --------------------------------------------------\n#         self.activation_kwargs = (\n#             activation_kwargs if activation_kwargs is not None else {}\n#         )\n#         self.norm_class = norm_class\n#         self.norm_kwargs = norm_kwargs if norm_kwargs is not None else {}\n#         self.bias_last_layer = bias_last_layer\n#         self.aggregator_class = aggregator_class\n#         self.aggregator_kwargs = (\n#             aggregator_kwargs if aggregator_kwargs is not None else {\"ndims_in\": 3}\n#         )\n#         self.squeeze_output = squeeze_output\n#         # self.single_bias_last_layer = single_bias_last_layer\n# \n#         depth = _find_depth(depth, num_cells, kernel_sizes, strides, paddings)\n#         self.depth = depth\n#         if depth == 0:\n#             raise ValueError(\"Null depth is not permitted with ConvNet.\")\n# \n#         for _field, _value in zip(\n#             [\"num_cells\", \"kernel_sizes\", \"strides\", \"paddings\"],\n# --------------------------------------------------\n# the below code fragment can be found in:\n# torchrl/modules/models/models.py\n# --------------------------------------------------\n#         self.in_features = in_features\n#         self.activation_class = activation_class\n#         self.activation_kwargs = (\n#             activation_kwargs if activation_kwargs is not None else {}\n#         )\n#         self.norm_class = norm_class\n#         self.norm_kwargs = norm_kwargs if norm_kwargs is not None else {}\n#         self.bias_last_layer = bias_last_layer\n#         self.aggregator_class = aggregator_class\n#         self.aggregator_kwargs = (\n#             aggregator_kwargs if aggregator_kwargs is not None else {\"ndims_in\": 3}\n#         )\n#         self.squeeze_output = squeeze_output\n#         # self.single_bias_last_layer = single_bias_last_layer\n# \n#         depth = _find_depth(depth, num_cells, kernel_sizes, strides, paddings)\n#         self.depth = depth\n#         if depth == 0:\n#             raise ValueError(\"Null depth is not permitted with ConvNet.\")\n# \n# --------------------------------------------------\n# the below code fragment can be found in:\n# torchrl/modules/models/models.py\n# --------------------------------------------------\n#             num_cells = [32, 32, 32]\n# \n#         self.in_features = in_features\n#         self.activation_class = activation_class\n#         self.activation_kwargs = (\n#             activation_kwargs if activation_kwargs is not None else {}\n#         )\n#         self.norm_class = norm_class\n#         self.norm_kwargs = norm_kwargs if norm_kwargs is not None else {}\n#         self.bias_last_layer = bias_last_layer\n#         self.aggregator_class = aggregator_class\n#         self.aggregator_kwargs = (\n#             aggregator_kwargs if aggregator_kwargs is not None else {\"ndims_in\": 3}\n#         )\n#         self.squeeze_output = squeeze_output\n#         # self.single_bias_last_layer = single_bias_last_layer\n# \n#         depth = _find_depth(depth, num_cells, kernel_sizes, strides, paddings)\n#         self.depth = depth\n#         if depth == 0:\n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\n# Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\nimport argparse\nfrom numbers import Number\n\nimport pytest\nimport torch\nfrom _utils_internal import get_available_devices\nfrom mocking_classes import MockBatchedUnLockedEnv\nfrom packaging import version\nfrom tensordict import TensorDict\nfrom torch import nn\nfrom torchrl.data.tensor_specs import (\n    BoundedTensorSpec,\n    DiscreteTensorSpec,\n    OneHotDiscreteTensorSpec,\n)\nfrom torchrl.modules import (\n    ActorValueOperator,\n    CEMPlanner,\n    LSTMNet,\n    ProbabilisticActor,\n    QValueActor,\n    SafeModule,\n    ValueOperator,\n)\nfrom torchrl.modules.models import ConvNet, MLP, NoisyLazyLinear, NoisyLinear\nfrom torchrl.modules.models.model_based import (\n    DreamerActor,\n    ObsDecoder,\n    ObsEncoder,\n    RSSMPosterior,\n    RSSMPrior,\n    RSSMRollout,\n)\nfrom torchrl.modules.models.utils import SquashDims\nfrom torchrl.modules.planners.mppi import MPPIPlanner\nfrom torchrl.objectives.value import TDLambdaEstimate\n\n\n@pytest.fixture\ndef double_prec_fixture():\n    dtype = torch.get_default_dtype()\n    torch.set_default_dtype(torch.double)\n    yield\n    torch.set_default_dtype(dtype)\n\n\n@pytest.mark.parametrize(\"in_features\", [3, 10, None])\n@pytest.mark.parametrize(\"out_features\", [3, (3, 10)])\n@pytest.mark.parametrize(\"depth, num_cells\", [(3, 32), (None, (32, 32, 32))])\n@pytest.mark.parametrize(\n    \"activation_class, activation_kwargs\",\n    [(nn.ReLU, {\"inplace\": True}), (nn.ReLU, {}), (nn.PReLU, {})],\n)\n@pytest.mark.parametrize(\n    \"norm_class, norm_kwargs\",\n    [(nn.LazyBatchNorm1d, {}), (nn.BatchNorm1d, {\"num_features\": 32})],\n)\n@pytest.mark.parametrize(\"bias_last_layer\", [True, False])\n@pytest.mark.parametrize(\"single_bias_last_layer\", [True, False])\n@pytest.mark.parametrize(\"layer_class\", [nn.Linear, NoisyLinear])\n@pytest.mark.parametrize(\"device\", get_available_devices())\ndef test_mlp(\n    in_features,\n    out_features,\n    depth,\n    num_cells,\n    activation_class,\n    activation_kwargs,\n    bias_last_layer,\n    norm_class,\n    norm_kwargs,\n    single_bias_last_layer,\n    layer_class,\n    device,\n    seed=0,\n):\n    torch.manual_seed(seed)\n    batch = 2\n    mlp = MLP(\n        in_features=in_features,\n        out_features=out_features,\n        depth=depth,\n        num_cells=num_cells,\n        activation_class=activation_class,\n        activation_kwargs=activation_kwargs,\n        norm_class=norm_class,\n        norm_kwargs=norm_kwargs,\n        bias_last_layer=bias_last_layer,\n        single_bias_last_layer=False,\n        layer_class=layer_class,\n        device=device,\n    )\n    if in_features is None:\n        in_features = 5\n    x = torch.randn(batch, in_features, device=device)\n    y = mlp(x)\n    out_features = [out_features] if isinstance(out_features, Number) else out_features\n    assert y.shape == torch.Size([batch, *out_features])\n\n\n@pytest.mark.parametrize(\"in_features\", [3, 10, None])\n@pytest.mark.parametrize(\n    \"input_size, depth, num_cells, kernel_sizes, strides, paddings, expected_features\",\n    [(100, None, None, 3, 1, 0, 32 * 94 * 94), (100, 3, 32, 3, 1, 1, 32 * 100 * 100)],\n)\n@pytest.mark.parametrize(\n    \"activation_class, activation_kwargs\",\n    [(nn.ReLU, {\"inplace\": True}), (nn.ReLU, {}), (nn.PReLU, {})],\n)\n@pytest.mark.parametrize(\n    \"norm_class, norm_kwargs\",\n    [(None, None), (nn.LazyBatchNorm2d, {}), (nn.BatchNorm2d, {\"num_features\": 32})],\n)\n@pytest.mark.parametrize(\"bias_last_layer\", [True, False])\n@pytest.mark.parametrize(\n    \"aggregator_class, aggregator_kwargs\",\n    [(SquashDims, {})],\n)\n@pytest.mark.parametrize(\"squeeze_output\", [False])\n@pytest.mark.parametrize(\"device\", get_available_devices())\n@pytest.mark.parametrize(\"batch\", [(2,), (2, 2)])\ndef test_convnet(\n    batch,\n    in_features,\n    depth,\n    num_cells,\n    kernel_sizes,\n    strides,\n    paddings,\n    activation_class,\n    activation_kwargs,\n    norm_class,\n    norm_kwargs,\n    bias_last_layer,", "metadata": {"task_id": "pytorch_rl/104", "ground_truth": "    aggregator_class,", "fpath_tuple": ["pytorch_rl", "test", "test_modules.py"], "context_start_lineno": 0, "line_no": 138, "query_window": {"context": "@pytest.mark.parametrize(\n    \"aggregator_class, aggregator_kwargs\",\n    [(SquashDims, {})],\n)\n@pytest.mark.parametrize(\"squeeze_output\", [False])\n@pytest.mark.parametrize(\"device\", get_available_devices())\n@pytest.mark.parametrize(\"batch\", [(2,), (2, 2)])\ndef test_convnet(\n    batch,\n    in_features,\n    depth,\n    num_cells,\n    kernel_sizes,\n    strides,\n    paddings,\n    activation_class,\n    activation_kwargs,\n    norm_class,\n    norm_kwargs,\n    bias_last_layer,", "metadata": {"fpath_tuple": ["pytorch_rl", "test", "test_modules.py"], "line_no": 138, "task_id": "pytorch_rl/104", "start_line_no": 118, "end_line_no": 138, "window_size": 20, "context_start_lineno": 0, "repo": "pytorch_rl"}}, "top_k_context": [{"context": "    ):\n        if num_cells is None:\n            num_cells = [32, 32, 32]\n\n        self.in_features = in_features\n        self.activation_class = activation_class\n        self.activation_kwargs = (\n            activation_kwargs if activation_kwargs is not None else {}\n        )\n        self.norm_class = norm_class\n        self.norm_kwargs = norm_kwargs if norm_kwargs is not None else {}\n        self.bias_last_layer = bias_last_layer\n        self.aggregator_class = aggregator_class\n        self.aggregator_kwargs = (\n            aggregator_kwargs if aggregator_kwargs is not None else {\"ndims_in\": 3}\n        )\n        self.squeeze_output = squeeze_output\n        # self.single_bias_last_layer = single_bias_last_layer\n\n        depth = _find_depth(depth, num_cells, kernel_sizes, strides, paddings)", "metadata": [{"fpath_tuple": ["pytorch_rl", "torchrl", "modules", "models", "models.py"], "line_no": 368, "start_line_no": 358, "end_line_no": 378, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}], "sim_score": 0.37254901960784315}, {"context": "            num_cells = [32, 32, 32]\n\n        self.in_features = in_features\n        self.activation_class = activation_class\n        self.activation_kwargs = (\n            activation_kwargs if activation_kwargs is not None else {}\n        )\n        self.norm_class = norm_class\n        self.norm_kwargs = norm_kwargs if norm_kwargs is not None else {}\n        self.bias_last_layer = bias_last_layer\n        self.aggregator_class = aggregator_class\n        self.aggregator_kwargs = (\n            aggregator_kwargs if aggregator_kwargs is not None else {\"ndims_in\": 3}\n        )\n        self.squeeze_output = squeeze_output\n        # self.single_bias_last_layer = single_bias_last_layer\n\n        depth = _find_depth(depth, num_cells, kernel_sizes, strides, paddings)\n        self.depth = depth\n        if depth == 0:", "metadata": [{"fpath_tuple": ["pytorch_rl", "torchrl", "modules", "models", "models.py"], "line_no": 370, "start_line_no": 360, "end_line_no": 380, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}], "sim_score": 0.3592233009708738}, {"context": "        self.in_features = in_features\n        self.activation_class = activation_class\n        self.activation_kwargs = (\n            activation_kwargs if activation_kwargs is not None else {}\n        )\n        self.norm_class = norm_class\n        self.norm_kwargs = norm_kwargs if norm_kwargs is not None else {}\n        self.bias_last_layer = bias_last_layer\n        self.aggregator_class = aggregator_class\n        self.aggregator_kwargs = (\n            aggregator_kwargs if aggregator_kwargs is not None else {\"ndims_in\": 3}\n        )\n        self.squeeze_output = squeeze_output\n        # self.single_bias_last_layer = single_bias_last_layer\n\n        depth = _find_depth(depth, num_cells, kernel_sizes, strides, paddings)\n        self.depth = depth\n        if depth == 0:\n            raise ValueError(\"Null depth is not permitted with ConvNet.\")\n", "metadata": [{"fpath_tuple": ["pytorch_rl", "torchrl", "modules", "models", "models.py"], "line_no": 372, "start_line_no": 362, "end_line_no": 382, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}], "sim_score": 0.3394495412844037}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# torchrl/modules/tensordict_module/actors.py\n# --------------------------------------------------\n#         return out\n# \n#     @staticmethod\n#     def _categorical(value: torch.Tensor) -> torch.Tensor:\n#         return torch.argmax(value, dim=-1).to(torch.long)\n# \n#     def _mult_one_hot(self, value: torch.Tensor, support: torch.Tensor) -> torch.Tensor:\n#         values = value.split(self.var_nums, dim=-1)\n#         return torch.cat(\n#             [\n#                 QValueHook._one_hot(\n#                     _value,\n#                 )\n#                 for _value in values\n#             ],\n#             -1,\n#         )\n# \n#     @staticmethod\n#     def _binary(value: torch.Tensor, support: torch.Tensor) -> torch.Tensor:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# test/test_transforms.py\n# --------------------------------------------------\n#                 assert torch.equal(\n#                     result[key].space.maximum[i], observation_spec[key].space.maximum[0]\n#                 )\n#                 assert torch.equal(\n#                     result[key].space.minimum[i], observation_spec[key].space.minimum[0]\n#                 )\n# \n#     @pytest.mark.parametrize(\"device\", get_available_devices())\n#     @pytest.mark.parametrize(\"batch_size\", [(), (1,), (1, 2)])\n#     @pytest.mark.parametrize(\"d\", range(1, 4))\n#     @pytest.mark.parametrize(\"dim\", [-3, -2, 1])\n#     @pytest.mark.parametrize(\"N\", [2, 4])\n#     def test_catframes_buffer_check_latest_frame(self, device, d, batch_size, dim, N):\n#         key1 = \"first key\"\n#         key2 = \"second key\"\n#         keys = [key1, key2]\n#         extra_d = (3,) * (-dim - 1)\n#         key1_tensor = torch.ones(*batch_size, d, *extra_d, device=device) * 2\n#         key2_tensor = torch.ones(*batch_size, d, *extra_d, device=device)\n#         key_tensors = [key1_tensor, key2_tensor]\n# --------------------------------------------------\n# the below code fragment can be found in:\n# test/test_distributions.py\n# --------------------------------------------------\n#             yield t\n# \n# \n# @pytest.mark.parametrize(\n#     \"min\", [-torch.ones(3), -1, 3 * torch.tensor([-1.0, -2.0, -0.5]), -0.1]\n# )\n# @pytest.mark.parametrize(\n#     \"max\", [torch.ones(3), 1, 3 * torch.tensor([1.0, 2.0, 0.5]), 0.1]\n# )\n# @pytest.mark.parametrize(\n#     \"vecs\",\n#     [\n#         (torch.tensor([0.1, 10.0, 5.0]), torch.tensor([0.1, 10.0, 5.0])),\n#         (torch.zeros(7, 3), torch.ones(7, 3)),\n#     ],\n# )\n# @pytest.mark.parametrize(\n#     \"upscale\", [torch.ones(3), 1, 3 * torch.tensor([1.0, 2.0, 0.5]), 3]\n# )\n# @pytest.mark.parametrize(\"shape\", [torch.Size([]), torch.Size([3, 4])])\n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\n# Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport pytest\nimport torch\nfrom torchrl.modules.tensordict_module.actors import (\n    DistributionalQValueHook,\n    QValueHook,\n)\n\n\nclass TestQValue:\n    def test_qvalue_hook_wrong_action_space(self):\n        with pytest.raises(ValueError) as exc:\n            QValueHook(action_space=\"wrong_value\")\n        assert \"action_space must be one of\" in str(exc.value)\n\n    def test_distributional_qvalue_hook_wrong_action_space(self):\n        with pytest.raises(ValueError) as exc:\n            DistributionalQValueHook(action_space=\"wrong_value\", support=None)\n        assert \"action_space must be one of\" in str(exc.value)\n\n    @pytest.mark.parametrize(\n        \"action_space, expected_action\",\n        (\n            (\"one_hot\", [0, 0, 1, 0, 0]),\n            (\"categorical\", 2),\n        ),\n    )\n    def test_qvalue_hook_0_dim_batch(self, action_space, expected_action):\n        hook = QValueHook(action_space=action_space)\n\n        in_values = torch.tensor([1.0, -1.0, 100.0, -2.0, -3.0])\n        action, values, chosen_action_value = hook(\n            net=None, observation=None, values=in_values\n        )\n\n        assert (torch.tensor(expected_action, dtype=torch.long) == action).all()\n        assert (values == in_values).all()\n        assert (torch.tensor([100.0]) == chosen_action_value).all()\n\n    @pytest.mark.parametrize(\n        \"action_space, expected_action\",\n        (\n            (\"one_hot\", [[0, 0, 1, 0, 0], [1, 0, 0, 0, 0]]),\n            (\"categorical\", [2, 0]),\n        ),\n    )\n    def test_qvalue_hook_1_dim_batch(self, action_space, expected_action):\n        hook = QValueHook(action_space=action_space)\n\n        in_values = torch.tensor(\n            [\n                [1.0, -1.0, 100.0, -2.0, -3.0],\n                [5.0, 4.0, 3.0, 2.0, -5.0],\n            ]\n        )\n        action, values, chosen_action_value = hook(\n            net=None, observation=None, values=in_values\n        )\n\n        assert (torch.tensor(expected_action, dtype=torch.long) == action).all()\n        assert (values == in_values).all()\n        assert (torch.tensor([[100.0], [5.0]]) == chosen_action_value).all()\n\n    @pytest.mark.parametrize(\n        \"action_space, expected_action\",\n        (\n            (\"one_hot\", [0, 0, 1, 0, 0]),\n            (\"categorical\", 2),\n        ),\n    )\n    def test_distributional_qvalue_hook_0_dim_batch(\n        self, action_space, expected_action\n    ):\n        support = torch.tensor([-2.0, 0.0, 2.0])\n        hook = DistributionalQValueHook(action_space=action_space, support=support)\n\n        in_values = torch.nn.LogSoftmax(dim=-1)(\n            torch.tensor(\n                [\n                    [1.0, -1.0, 11.0, -2.0, 30.0],\n                    [1.0, -1.0, 1.0, -2.0, -3.0],\n                    [1.0, -1.0, 10.0, -2.0, -3.0],\n                ]\n            )\n        )\n        action, values = hook(net=None, observation=None, values=in_values)\n        expected_action = torch.tensor(expected_action, dtype=torch.long)\n\n        assert action.shape == expected_action.shape\n        assert (action == expected_action).all()\n        assert values.shape == in_values.shape\n        assert (values == in_values).all()\n\n    @pytest.mark.parametrize(\n        \"action_space, expected_action\",\n        (\n            (\"one_hot\", [[0, 0, 1, 0, 0], [1, 0, 0, 0, 0]]),\n            (\"categorical\", [2, 0]),\n        ),\n    )\n    def test_qvalue_hook_categorical_1_dim_batch(self, action_space, expected_action):\n        support = torch.tensor([-2.0, 0.0, 2.0])\n        hook = DistributionalQValueHook(action_space=action_space, support=support)\n\n        in_values = torch.nn.LogSoftmax(dim=-1)(\n            torch.tensor(\n                [\n                    [\n                        [1.0, -1.0, 11.0, -2.0, 30.0],\n                        [1.0, -1.0, 1.0, -2.0, -3.0],\n                        [1.0, -1.0, 10.0, -2.0, -3.0],\n                    ],\n                    [", "metadata": {"task_id": "pytorch_rl/83", "ground_truth": "                        [11.0, -1.0, 7.0, -1.0, 20.0],", "fpath_tuple": ["pytorch_rl", "test", "test_actors.py"], "context_start_lineno": 0, "line_no": 117, "query_window": {"context": "    @pytest.mark.parametrize(\n        \"action_space, expected_action\",\n        (\n            (\"one_hot\", [[0, 0, 1, 0, 0], [1, 0, 0, 0, 0]]),\n            (\"categorical\", [2, 0]),\n        ),\n    )\n    def test_qvalue_hook_categorical_1_dim_batch(self, action_space, expected_action):\n        support = torch.tensor([-2.0, 0.0, 2.0])\n        hook = DistributionalQValueHook(action_space=action_space, support=support)\n\n        in_values = torch.nn.LogSoftmax(dim=-1)(\n            torch.tensor(\n                [\n                    [\n                        [1.0, -1.0, 11.0, -2.0, 30.0],\n                        [1.0, -1.0, 1.0, -2.0, -3.0],\n                        [1.0, -1.0, 10.0, -2.0, -3.0],\n                    ],\n                    [", "metadata": {"fpath_tuple": ["pytorch_rl", "test", "test_actors.py"], "line_no": 117, "task_id": "pytorch_rl/83", "start_line_no": 97, "end_line_no": 117, "window_size": 20, "context_start_lineno": 0, "repo": "pytorch_rl"}}, "top_k_context": [{"context": "            yield t.to(device)\n        else:\n            yield t\n\n\n@pytest.mark.parametrize(\n    \"min\", [-torch.ones(3), -1, 3 * torch.tensor([-1.0, -2.0, -0.5]), -0.1]\n)\n@pytest.mark.parametrize(\n    \"max\", [torch.ones(3), 1, 3 * torch.tensor([1.0, 2.0, 0.5]), 0.1]\n)\n@pytest.mark.parametrize(\n    \"vecs\",\n    [\n        (torch.tensor([0.1, 10.0, 5.0]), torch.tensor([0.1, 10.0, 5.0])),\n        (torch.zeros(7, 3), torch.ones(7, 3)),\n    ],\n)\n@pytest.mark.parametrize(\n    \"upscale\", [torch.ones(3), 1, 3 * torch.tensor([1.0, 2.0, 0.5]), 3]", "metadata": [{"fpath_tuple": ["pytorch_rl", "test", "test_distributions.py"], "line_no": 72, "start_line_no": 62, "end_line_no": 82, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}], "sim_score": 0.34513274336283184}, {"context": "        for key in keys:\n            for i in range(N):\n                assert torch.equal(\n                    result[key].space.maximum[i], observation_spec[key].space.maximum[0]\n                )\n                assert torch.equal(\n                    result[key].space.minimum[i], observation_spec[key].space.minimum[0]\n                )\n\n    @pytest.mark.parametrize(\"device\", get_available_devices())\n    @pytest.mark.parametrize(\"batch_size\", [(), (1,), (1, 2)])\n    @pytest.mark.parametrize(\"d\", range(1, 4))\n    @pytest.mark.parametrize(\"dim\", [-3, -2, 1])\n    @pytest.mark.parametrize(\"N\", [2, 4])\n    def test_catframes_buffer_check_latest_frame(self, device, d, batch_size, dim, N):\n        key1 = \"first key\"\n        key2 = \"second key\"\n        keys = [key1, key2]\n        extra_d = (3,) * (-dim - 1)\n        key1_tensor = torch.ones(*batch_size, d, *extra_d, device=device) * 2", "metadata": [{"fpath_tuple": ["pytorch_rl", "test", "test_transforms.py"], "line_no": 1304, "start_line_no": 1294, "end_line_no": 1314, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}], "sim_score": 0.34285714285714286}, {"context": "    def _one_hot(value: torch.Tensor) -> torch.Tensor:\n        out = (value == value.max(dim=-1, keepdim=True)[0]).to(torch.long)\n        return out\n\n    @staticmethod\n    def _categorical(value: torch.Tensor) -> torch.Tensor:\n        return torch.argmax(value, dim=-1).to(torch.long)\n\n    def _mult_one_hot(self, value: torch.Tensor, support: torch.Tensor) -> torch.Tensor:\n        values = value.split(self.var_nums, dim=-1)\n        return torch.cat(\n            [\n                QValueHook._one_hot(\n                    _value,\n                )\n                for _value in values\n            ],\n            -1,\n        )\n", "metadata": [{"fpath_tuple": ["pytorch_rl", "torchrl", "modules", "tensordict_module", "actors.py"], "line_no": 284, "start_line_no": 274, "end_line_no": 294, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}], "sim_score": 0.33613445378151263}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# measurements/perplexity/perplexity.py\n# --------------------------------------------------\n#         ['perplexities', 'mean_perplexity']\n#         >>> print(round(results[\"mean_perplexity\"], 2)) # doctest: +SKIP\n#         576.76\n#         >>> print(round(results[\"perplexities\"][0], 2)) # doctest: +SKIP\n#         889.28\n# \"\"\"\n# \n# \n# @evaluate.utils.file_utils.add_start_docstrings(_DESCRIPTION, _KWARGS_DESCRIPTION)\n# class Perplexity(evaluate.Measurement):\n#     def _info(self):\n#         return evaluate.MeasurementInfo(\n#             module_type=\"measurement\",\n#             description=_DESCRIPTION,\n#             citation=_CITATION,\n#             inputs_description=_KWARGS_DESCRIPTION,\n#             features=datasets.Features(\n#                 {\n#                     \"data\": datasets.Value(\"string\"),\n#                 }\n# --------------------------------------------------\n# the below code fragment can be found in:\n# measurements/perplexity/perplexity.py\n# --------------------------------------------------\n#         >>> results = perplexity.compute(model_id='gpt2',\n#         ...                              add_start_token=False,\n#         ...                              data=data) # doctest:+ELLIPSIS\n#         >>> print(list(results.keys()))\n#         ['perplexities', 'mean_perplexity']\n#         >>> print(round(results[\"mean_perplexity\"], 0))\n#         647.0\n#         >>> print(round(results[\"perplexities\"][0], 0))\n#         32.0\n# \n#     Example 2:\n#         >>> from datasets import load_dataset\n#         >>> perplexity = evaluate.load(\"perplexity\", module_type=\"measurement\")\n#         >>> data = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")[\"text\"][:10] # doctest: +SKIP\n#         >>> data = [s for s in data if s!='']\n#         >>> results = perplexity.compute(model_id='gpt2',\n#         ...                              data=data)\n#         >>> print(list(results.keys()))\n#         ['perplexities', 'mean_perplexity']\n#         >>> print(round(results[\"mean_perplexity\"], 2)) # doctest: +SKIP\n# --------------------------------------------------\n# the below code fragment can be found in:\n# measurements/perplexity/perplexity.py\n# --------------------------------------------------\n#         >>> perplexity = evaluate.load(\"perplexity\", module_type=\"measurement\")\n#         >>> data = [\"lorem ipsum\", \"Happy Birthday!\", \"Bienvenue\"]\n#         >>> results = perplexity.compute(model_id='gpt2',\n#         ...                              add_start_token=False,\n#         ...                              data=data) # doctest:+ELLIPSIS\n#         >>> print(list(results.keys()))\n#         ['perplexities', 'mean_perplexity']\n#         >>> print(round(results[\"mean_perplexity\"], 0))\n#         647.0\n#         >>> print(round(results[\"perplexities\"][0], 0))\n#         32.0\n# \n#     Example 2:\n#         >>> from datasets import load_dataset\n#         >>> perplexity = evaluate.load(\"perplexity\", module_type=\"measurement\")\n#         >>> data = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")[\"text\"][:10] # doctest: +SKIP\n#         >>> data = [s for s in data if s!='']\n#         >>> results = perplexity.compute(model_id='gpt2',\n#         ...                              data=data)\n#         >>> print(list(results.keys()))\n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\n# Copyright 2022 The HuggingFace Datasets Authors and the current dataset script contributor.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Perplexity Metric.\"\"\"\n\nimport datasets\nimport numpy as np\nimport torch\nfrom torch.nn import CrossEntropyLoss\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nimport evaluate\nfrom evaluate import logging\n\n\n_CITATION = \"\"\"\\\n\n\"\"\"\n\n_DESCRIPTION = \"\"\"\nPerplexity (PPL) is one of the most common metrics for evaluating language models.\nIt is defined as the exponentiated average negative log-likelihood of a sequence, calculated with exponent base `e`.\n\nFor more information, see https://huggingface.co/docs/transformers/perplexity\n\"\"\"\n\n_KWARGS_DESCRIPTION = \"\"\"\nArgs:\n    model_id (str): model used for calculating Perplexity\n            NOTE: Perplexity can only be calculated for causal language models.\n                    This includes models such as gpt2, causal variations of bert,\n                    causal versions of t5, and more (the full list can be found\n                    in the AutoModelForCausalLM documentation here:\n                    https://huggingface.co/docs/transformers/master/en/model_doc/auto#transformers.AutoModelForCausalLM )\n\n    predictions (list of str): input text, each separate text snippet\n        is one list entry.\n    batch_size (int): the batch size to run texts through the model. Defaults to 16.\n    add_start_token (bool): whether to add the start token to the texts,\n        so the perplexity can include the probability of the first word. Defaults to True.\n    device (str): device to run on, defaults to 'cuda' when available\nReturns:\n    perplexity: dictionary containing the perplexity scores for the texts\n        in the input list, as well as the mean perplexity. If one of the input texts is\n        longer than the max input length of the model, then it is truncated to the\n        max length for the perplexity computation.\nExamples:\n    Example 1:\n        >>> perplexity = evaluate.load(\"perplexity\", module_type=\"metric\")\n        >>> input_texts = [\"lorem ipsum\", \"Happy Birthday!\", \"Bienvenue\"]\n        >>> results = perplexity.compute(model_id='gpt2',\n        ...                              add_start_token=False,\n        ...                              predictions=input_texts) # doctest:+ELLIPSIS\n        >>> print(list(results.keys()))\n        ['perplexities', 'mean_perplexity']\n        >>> print(round(results[\"mean_perplexity\"], 0))\n        647.0\n        >>> print(round(results[\"perplexities\"][0], 0))\n        32.0\n\n    Example 2:\n        >>> from datasets import load_dataset\n        >>> perplexity = evaluate.load(\"perplexity\", module_type=\"metric\")\n        >>> input_texts = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")[\"text\"][:10] # doctest: +SKIP\n        >>> input_texts = [s for s in input_texts if s!='']\n        >>> results = perplexity.compute(model_id='gpt2',", "metadata": {"task_id": "huggingface_evaluate/150", "ground_truth": "        ...                              predictions=input_texts)", "fpath_tuple": ["huggingface_evaluate", "metrics", "perplexity", "perplexity.py"], "context_start_lineno": 0, "line_no": 76, "query_window": {"context": "Examples:\n    Example 1:\n        >>> perplexity = evaluate.load(\"perplexity\", module_type=\"metric\")\n        >>> input_texts = [\"lorem ipsum\", \"Happy Birthday!\", \"Bienvenue\"]\n        >>> results = perplexity.compute(model_id='gpt2',\n        ...                              add_start_token=False,\n        ...                              predictions=input_texts) # doctest:+ELLIPSIS\n        >>> print(list(results.keys()))\n        ['perplexities', 'mean_perplexity']\n        >>> print(round(results[\"mean_perplexity\"], 0))\n        647.0\n        >>> print(round(results[\"perplexities\"][0], 0))\n        32.0\n\n    Example 2:\n        >>> from datasets import load_dataset\n        >>> perplexity = evaluate.load(\"perplexity\", module_type=\"metric\")\n        >>> input_texts = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")[\"text\"][:10] # doctest: +SKIP\n        >>> input_texts = [s for s in input_texts if s!='']\n        >>> results = perplexity.compute(model_id='gpt2',", "metadata": {"fpath_tuple": ["huggingface_evaluate", "metrics", "perplexity", "perplexity.py"], "line_no": 76, "task_id": "huggingface_evaluate/150", "start_line_no": 56, "end_line_no": 76, "window_size": 20, "context_start_lineno": 0, "repo": "huggingface_evaluate"}}, "top_k_context": [{"context": "Examples:\n    Example 1:\n        >>> perplexity = evaluate.load(\"perplexity\", module_type=\"measurement\")\n        >>> data = [\"lorem ipsum\", \"Happy Birthday!\", \"Bienvenue\"]\n        >>> results = perplexity.compute(model_id='gpt2',\n        ...                              add_start_token=False,\n        ...                              data=data) # doctest:+ELLIPSIS\n        >>> print(list(results.keys()))\n        ['perplexities', 'mean_perplexity']\n        >>> print(round(results[\"mean_perplexity\"], 0))\n        647.0\n        >>> print(round(results[\"perplexities\"][0], 0))\n        32.0\n\n    Example 2:\n        >>> from datasets import load_dataset\n        >>> perplexity = evaluate.load(\"perplexity\", module_type=\"measurement\")\n        >>> data = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")[\"text\"][:10] # doctest: +SKIP\n        >>> data = [s for s in data if s!='']\n        >>> results = perplexity.compute(model_id='gpt2',", "metadata": [{"fpath_tuple": ["huggingface_evaluate", "measurements", "perplexity", "perplexity.py"], "line_no": 66, "start_line_no": 56, "end_line_no": 76, "window_size": 20, "repo": "huggingface_evaluate", "slice_size": 10}], "sim_score": 0.9224806201550387}, {"context": "        >>> perplexity = evaluate.load(\"perplexity\", module_type=\"measurement\")\n        >>> data = [\"lorem ipsum\", \"Happy Birthday!\", \"Bienvenue\"]\n        >>> results = perplexity.compute(model_id='gpt2',\n        ...                              add_start_token=False,\n        ...                              data=data) # doctest:+ELLIPSIS\n        >>> print(list(results.keys()))\n        ['perplexities', 'mean_perplexity']\n        >>> print(round(results[\"mean_perplexity\"], 0))\n        647.0\n        >>> print(round(results[\"perplexities\"][0], 0))\n        32.0\n\n    Example 2:\n        >>> from datasets import load_dataset\n        >>> perplexity = evaluate.load(\"perplexity\", module_type=\"measurement\")\n        >>> data = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")[\"text\"][:10] # doctest: +SKIP\n        >>> data = [s for s in data if s!='']\n        >>> results = perplexity.compute(model_id='gpt2',\n        ...                              data=data)\n        >>> print(list(results.keys()))", "metadata": [{"fpath_tuple": ["huggingface_evaluate", "measurements", "perplexity", "perplexity.py"], "line_no": 68, "start_line_no": 58, "end_line_no": 78, "window_size": 20, "repo": "huggingface_evaluate", "slice_size": 10}], "sim_score": 0.9069767441860465}, {"context": "        ...                              data=data)\n        >>> print(list(results.keys()))\n        ['perplexities', 'mean_perplexity']\n        >>> print(round(results[\"mean_perplexity\"], 2)) # doctest: +SKIP\n        576.76\n        >>> print(round(results[\"perplexities\"][0], 2)) # doctest: +SKIP\n        889.28\n\"\"\"\n\n\n@evaluate.utils.file_utils.add_start_docstrings(_DESCRIPTION, _KWARGS_DESCRIPTION)\nclass Perplexity(evaluate.Measurement):\n    def _info(self):\n        return evaluate.MeasurementInfo(\n            module_type=\"measurement\",\n            description=_DESCRIPTION,\n            citation=_CITATION,\n            inputs_description=_KWARGS_DESCRIPTION,\n            features=datasets.Features(\n                {", "metadata": [{"fpath_tuple": ["huggingface_evaluate", "measurements", "perplexity", "perplexity.py"], "line_no": 86, "start_line_no": 76, "end_line_no": 96, "window_size": 20, "repo": "huggingface_evaluate", "slice_size": 10}], "sim_score": 0.30409356725146197}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# federatedscope/vertical_fl/dataset/abalone.py\n# --------------------------------------------------\n# \n#     def download(self):\n#         if self._check_existence():\n#             logger.info(\"Files already exist\")\n#             return\n#         download_and_extract_archive(self.url,\n#                                      os.path.join(self.root, self.base_folder),\n#                                      filename=self.url.split('/')[-1])\n# \n#     def _partition_data(self):\n# \n#         x = self.data_dict['train'][:, :-1]\n#         y = self.data_dict['train'][:, -1]\n# \n#         test_data = {\n#             'x': self.data_dict['test'][:, :-1],\n#             'y': self.data_dict['test'][:, -1]\n#         }\n# \n#         test_x = test_data['x']\n# --------------------------------------------------\n# the below code fragment can be found in:\n# federatedscope/vertical_fl/dataset/abalone.py\n# --------------------------------------------------\n#         if self._check_existence():\n#             logger.info(\"Files already exist\")\n#             return\n#         download_and_extract_archive(self.url,\n#                                      os.path.join(self.root, self.base_folder),\n#                                      filename=self.url.split('/')[-1])\n# \n#     def _partition_data(self):\n# \n#         x = self.data_dict['train'][:, :-1]\n#         y = self.data_dict['train'][:, -1]\n# \n#         test_data = {\n#             'x': self.data_dict['test'][:, :-1],\n#             'y': self.data_dict['test'][:, -1]\n#         }\n# \n#         test_x = test_data['x']\n#         test_y = test_data['y']\n# \n# --------------------------------------------------\n# the below code fragment can be found in:\n# federatedscope/vertical_fl/dataset/abalone.py\n# --------------------------------------------------\n#             return\n#         download_and_extract_archive(self.url,\n#                                      os.path.join(self.root, self.base_folder),\n#                                      filename=self.url.split('/')[-1])\n# \n#     def _partition_data(self):\n# \n#         x = self.data_dict['train'][:, :-1]\n#         y = self.data_dict['train'][:, -1]\n# \n#         test_data = {\n#             'x': self.data_dict['test'][:, :-1],\n#             'y': self.data_dict['test'][:, -1]\n#         }\n# \n#         test_x = test_data['x']\n#         test_y = test_data['y']\n# \n#         self.data = dict()\n#         for i in range(self.num_of_clients + 1):\n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\nimport logging\nimport os\nimport os.path as osp\n\nimport numpy as np\nimport pandas as pd\nfrom torchvision.datasets.utils import download_and_extract_archive\n\nlogger = logging.getLogger(__name__)\n\n\nclass Credit(object):\n    \"\"\"\n    Give Me Some Credit Data Set\n    (https://www.kaggle.com/competitions/GiveMeSomeCredit)\n    Data Set: cs-training.csv, 150000 instances and 12 attributes\n    The first attribute is the user ID which we do not need, the second\n    attribute is the label, determining whether a loan should be granted.\n\n    Arguments:\n        root (str): root path\n        num_of_clients(int): number of clients\n        feature_partition(list): the number of features\n                                    partitioned to each client\n        tr_frac (float): train set proportion for each task; default=0.8\n        args (dict): set Ture or False to decide whether\n                     to normalize or standardize the data or not,\n                     e.g., {'normalization': False, 'standardization': False}\n        algo(str): the running model, 'lr' or 'xgb'\n        download (bool): indicator to download dataset\n        seed: a random seed\n    \"\"\"\n    base_folder = 'givemesomecredit'\n    url = 'https://federatedscope.oss-cn-beijing.aliyuncs.com/cs-training.zip'\n    raw_file = 'cs-training.csv'\n\n    def __init__(self,\n                 root,\n                 num_of_clients,\n                 feature_partition,\n                 args,\n                 algo=None,\n                 tr_frac=0.8,\n                 download=True,\n                 seed=123):\n        super(Credit, self).__init__()\n        self.root = root\n        self.num_of_clients = num_of_clients\n        self.feature_partition = feature_partition\n        self.tr_frac = tr_frac\n        self.seed = seed\n        self.args = args\n        self.algo = algo\n        self.data_dict = {}\n        self.data = {}\n\n        if download:\n            self.download()\n        if not self._check_existence():\n            raise RuntimeError(\"Dataset not found or corrupted.\" +\n                               \"You can use download=True to download it\")\n\n        self._get_data()\n        self._partition_data()\n\n    def _get_data(self):\n        fpath = os.path.join(self.root, self.base_folder)\n        file = osp.join(fpath, self.raw_file)\n        data = self._read_raw(file)\n        data = data[:, 1:]\n\n        # the following codes are used to choose balanced data\n        # they may be removed later\n        # '''\n        sample_size = 150000\n\n        def balance_sample(sample_size, y):\n            y_ones_idx = (y == 1).nonzero()[0]\n            y_ones_idx = np.random.choice(y_ones_idx,\n                                          size=int(sample_size / 2))\n            y_zeros_idx = (y == 0).nonzero()[0]\n            y_zeros_idx = np.random.choice(y_zeros_idx,\n                                           size=int(sample_size / 2))\n\n            y_index = np.concatenate([y_zeros_idx, y_ones_idx], axis=0)\n            np.random.shuffle(y_index)\n            return y_index\n\n        sample_idx = balance_sample(sample_size, data[:, 0])\n        data = data[sample_idx]\n        # '''\n\n        train_num = int(self.tr_frac * len(data))\n\n        self.data_dict['train'] = data[:train_num]\n        self.data_dict['test'] = data[train_num:]\n\n    def _read_raw(self, file_path):\n        data = pd.read_csv(file_path)\n        data = data.values\n        return data\n\n    def _check_existence(self):\n        fpath = os.path.join(self.root, self.base_folder, self.raw_file)\n        return osp.exists(fpath)\n\n    def download(self):\n        if self._check_existence():\n            logger.info(\"Files already exist\")\n            return\n        download_and_extract_archive(self.url,\n                                     os.path.join(self.root, self.base_folder),\n                                     filename=self.url.split('/')[-1])\n\n    def _partition_data(self):\n\n        x = self.data_dict['train'][:, 1:]\n        y = self.data_dict['train'][:, 0]\n\n        test_data = {\n            'x': self.data_dict['test'][:, 1:],\n            'y': self.data_dict['test'][:, 0]\n        }\n        test_x = test_data['x']\n        test_y = test_data['y']\n\n        self.data = dict()", "metadata": {"task_id": "alibaba_FederatedScope/199", "ground_truth": "        for i in range(self.num_of_clients + 1):", "fpath_tuple": ["alibaba_FederatedScope", "federatedscope", "vertical_fl", "dataset", "credit.py"], "context_start_lineno": 0, "line_no": 127, "query_window": {"context": "        if self._check_existence():\n            logger.info(\"Files already exist\")\n            return\n        download_and_extract_archive(self.url,\n                                     os.path.join(self.root, self.base_folder),\n                                     filename=self.url.split('/')[-1])\n\n    def _partition_data(self):\n\n        x = self.data_dict['train'][:, 1:]\n        y = self.data_dict['train'][:, 0]\n\n        test_data = {\n            'x': self.data_dict['test'][:, 1:],\n            'y': self.data_dict['test'][:, 0]\n        }\n        test_x = test_data['x']\n        test_y = test_data['y']\n\n        self.data = dict()", "metadata": {"fpath_tuple": ["alibaba_FederatedScope", "federatedscope", "vertical_fl", "dataset", "credit.py"], "line_no": 127, "task_id": "alibaba_FederatedScope/199", "start_line_no": 107, "end_line_no": 127, "window_size": 20, "context_start_lineno": 0, "repo": "alibaba_FederatedScope"}}, "top_k_context": [{"context": "        if self._check_existence():\n            logger.info(\"Files already exist\")\n            return\n        download_and_extract_archive(self.url,\n                                     os.path.join(self.root, self.base_folder),\n                                     filename=self.url.split('/')[-1])\n\n    def _partition_data(self):\n\n        x = self.data_dict['train'][:, :-1]\n        y = self.data_dict['train'][:, -1]\n\n        test_data = {\n            'x': self.data_dict['test'][:, :-1],\n            'y': self.data_dict['test'][:, -1]\n        }\n\n        test_x = test_data['x']\n        test_y = test_data['y']\n", "metadata": [{"fpath_tuple": ["alibaba_FederatedScope", "federatedscope", "vertical_fl", "dataset", "abalone.py"], "line_no": 114, "start_line_no": 104, "end_line_no": 124, "window_size": 20, "repo": "alibaba_FederatedScope", "slice_size": 10}], "sim_score": 0.9036144578313253}, {"context": "\n    def download(self):\n        if self._check_existence():\n            logger.info(\"Files already exist\")\n            return\n        download_and_extract_archive(self.url,\n                                     os.path.join(self.root, self.base_folder),\n                                     filename=self.url.split('/')[-1])\n\n    def _partition_data(self):\n\n        x = self.data_dict['train'][:, :-1]\n        y = self.data_dict['train'][:, -1]\n\n        test_data = {\n            'x': self.data_dict['test'][:, :-1],\n            'y': self.data_dict['test'][:, -1]\n        }\n\n        test_x = test_data['x']", "metadata": [{"fpath_tuple": ["alibaba_FederatedScope", "federatedscope", "vertical_fl", "dataset", "abalone.py"], "line_no": 112, "start_line_no": 102, "end_line_no": 122, "window_size": 20, "repo": "alibaba_FederatedScope", "slice_size": 10}], "sim_score": 0.9036144578313253}, {"context": "        fpath = os.path.join(self.root, self.base_folder, self.raw_file)\n        return osp.exists(fpath)\n\n    def download(self):\n        if self._check_existence():\n            logger.info(\"Files already exist\")\n            return\n        download_and_extract_archive(self.url,\n                                     os.path.join(self.root, self.base_folder),\n                                     filename=self.url.split('/')[-1])\n\n    def _partition_data(self):\n\n        x = self.data_dict['train'][:, :-1]\n        y = self.data_dict['train'][:, -1]\n\n        test_data = {\n            'x': self.data_dict['test'][:, :-1],\n            'y': self.data_dict['test'][:, -1]\n        }", "metadata": [{"fpath_tuple": ["alibaba_FederatedScope", "federatedscope", "vertical_fl", "dataset", "abalone.py"], "line_no": 110, "start_line_no": 100, "end_line_no": 120, "window_size": 20, "repo": "alibaba_FederatedScope", "slice_size": 10}], "sim_score": 0.8043478260869565}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# examples/unconditional_image_generation/train_unconditional.py\n# --------------------------------------------------\n# import argparse\n# import copy\n# import inspect\n# import logging\n# import math\n# import os\n# from pathlib import Path\n# from typing import Optional\n# \n# import torch\n# import torch.nn.functional as F\n# \n# import datasets\n# import diffusers\n# --------------------------------------------------\n# the below code fragment can be found in:\n# examples/research_projects/intel_opts/textual_inversion/textual_inversion_bf16.py\n# --------------------------------------------------\n# import argparse\n# import itertools\n# import math\n# import os\n# import random\n# from pathlib import Path\n# from typing import Optional\n# \n# import numpy as np\n# import torch\n# import torch.nn.functional as F\n# import torch.utils.checkpoint\n# from torch.utils.data import Dataset\n# \n# --------------------------------------------------\n# the below code fragment can be found in:\n# examples/textual_inversion/textual_inversion_flax.py\n# examples/text_to_image/train_text_to_image_flax.py\n# --------------------------------------------------\n# import argparse\n# import logging\n# import math\n# import os\n# import random\n# from pathlib import Path\n# from typing import Optional\n# \n# import numpy as np\n# import torch\n# import torch.utils.checkpoint\n# from torch.utils.data import Dataset\n# --------------------------------------------------\n# the below code fragment can be found in:\n# examples/unconditional_image_generation/train_unconditional.py\n# --------------------------------------------------\n# import argparse\n# import copy\n# import inspect\n# import logging\n# import math\n# import os\n# from pathlib import Path\n# from typing import Optional\n# \n# import torch\n# import torch.nn.functional as F\n# \n# --------------------------------------------------\n# the below code fragment can be found in:\n# examples/dreambooth/train_dreambooth_flax.py\n# --------------------------------------------------\n# import argparse\n# import hashlib\n# import logging\n# import math\n# import os\n# from pathlib import Path\n# from typing import Optional\n# \n# import numpy as np\n# import torch\n# import torch.utils.checkpoint\n# from torch.utils.data import Dataset\n# --------------------------------------------------\n# the below code fragment can be found in:\n# examples/research_projects/dreambooth_inpaint/train_dreambooth_inpaint.py\n# --------------------------------------------------\n# import argparse\n# import hashlib\n# import itertools\n# import math\n# import os\n# import random\n# from pathlib import Path\n# from typing import Optional\n# \n# import numpy as np\n# import torch\n# import torch.nn.functional as F\n# import torch.utils.checkpoint\n# from torch.utils.data import Dataset\n# --------------------------------------------------\n# the below code fragment can be found in:\n# examples/research_projects/intel_opts/textual_inversion/textual_inversion_bf16.py\n# --------------------------------------------------\n# import argparse\n# import itertools\n# import math\n# import os\n# import random\n# from pathlib import Path\n# from typing import Optional\n# \n# import numpy as np\n# import torch\n# import torch.nn.functional as F\n# import torch.utils.checkpoint\n# --------------------------------------------------\n# the below code fragment can be found in:\n# examples/research_projects/dreambooth_inpaint/train_dreambooth_inpaint.py\n# --------------------------------------------------\n# import argparse\n# import hashlib\n# import itertools\n# import math\n# import os\n# import random\n# from pathlib import Path\n# from typing import Optional\n# \n# import numpy as np\n# import torch\n# import torch.nn.functional as F\n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\nimport argparse\nimport hashlib\nimport itertools\nimport logging\nimport math", "metadata": {"task_id": "huggingface_diffusers/3", "ground_truth": "import os", "fpath_tuple": ["huggingface_diffusers", "examples", "research_projects", "multi_subject_dreambooth", "train_multi_subject_dreambooth.py"], "context_start_lineno": 0, "line_no": 5, "query_window": {"context": "import argparse\nimport hashlib\nimport itertools\nimport logging\nimport math", "metadata": {"fpath_tuple": ["huggingface_diffusers", "examples", "research_projects", "multi_subject_dreambooth", "train_multi_subject_dreambooth.py"], "line_no": 5, "task_id": "huggingface_diffusers/3", "start_line_no": 0, "end_line_no": 5, "window_size": 20, "context_start_lineno": 0, "repo": "huggingface_diffusers"}}, "top_k_context": [{"context": "import argparse\nimport hashlib\nimport itertools\nimport math\nimport os\nimport random\nfrom pathlib import Path\nfrom typing import Optional\n\nimport numpy as np", "metadata": [{"fpath_tuple": ["huggingface_diffusers", "examples", "research_projects", "dreambooth_inpaint", "train_dreambooth_inpaint.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 10, "window_size": 20, "repo": "huggingface_diffusers", "slice_size": 10}], "sim_score": 0.43478260869565216}, {"context": "import argparse\nimport itertools\nimport math\nimport os\nimport random\nfrom pathlib import Path\nfrom typing import Optional\n\nimport numpy as np\nimport torch", "metadata": [{"fpath_tuple": ["huggingface_diffusers", "examples", "research_projects", "intel_opts", "textual_inversion", "textual_inversion_bf16.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 10, "window_size": 20, "repo": "huggingface_diffusers", "slice_size": 10}], "sim_score": 0.375}, {"context": "import argparse\nimport hashlib\nimport itertools\nimport math\nimport os\nimport random\nfrom pathlib import Path\nfrom typing import Optional\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F", "metadata": [{"fpath_tuple": ["huggingface_diffusers", "examples", "research_projects", "dreambooth_inpaint", "train_dreambooth_inpaint.py"], "line_no": 2, "start_line_no": 0, "end_line_no": 12, "window_size": 20, "repo": "huggingface_diffusers", "slice_size": 10}], "sim_score": 0.35714285714285715}, {"context": "import argparse\nimport hashlib\nimport logging\nimport math\nimport os\nfrom pathlib import Path\nfrom typing import Optional\n\nimport numpy as np\nimport torch", "metadata": [{"fpath_tuple": ["huggingface_diffusers", "examples", "dreambooth", "train_dreambooth_flax.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 10, "window_size": 20, "repo": "huggingface_diffusers", "slice_size": 10}], "sim_score": 0.34782608695652173}, {"context": "import argparse\nimport copy\nimport inspect\nimport logging\nimport math\nimport os\nfrom pathlib import Path\nfrom typing import Optional\n\nimport torch", "metadata": [{"fpath_tuple": ["huggingface_diffusers", "examples", "unconditional_image_generation", "train_unconditional.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 10, "window_size": 20, "repo": "huggingface_diffusers", "slice_size": 10}], "sim_score": 0.3333333333333333}, {"context": "import argparse\nimport logging\nimport math\nimport os\nimport random\nfrom pathlib import Path\nfrom typing import Optional\n\nimport numpy as np\nimport torch", "metadata": [{"fpath_tuple": ["huggingface_diffusers", "examples", "textual_inversion", "textual_inversion_flax.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 10, "window_size": 20, "repo": "huggingface_diffusers", "slice_size": 10}, {"fpath_tuple": ["huggingface_diffusers", "examples", "text_to_image", "train_text_to_image_flax.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 10, "window_size": 20, "repo": "huggingface_diffusers", "slice_size": 10}], "sim_score": 0.2916666666666667}, {"context": "import argparse\nimport itertools\nimport math\nimport os\nimport random\nfrom pathlib import Path\nfrom typing import Optional\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nimport torch.utils.checkpoint", "metadata": [{"fpath_tuple": ["huggingface_diffusers", "examples", "research_projects", "intel_opts", "textual_inversion", "textual_inversion_bf16.py"], "line_no": 2, "start_line_no": 0, "end_line_no": 12, "window_size": 20, "repo": "huggingface_diffusers", "slice_size": 10}], "sim_score": 0.2903225806451613}, {"context": "import argparse\nimport copy\nimport inspect\nimport logging\nimport math\nimport os\nfrom pathlib import Path\nfrom typing import Optional\n\nimport torch\nimport torch.nn.functional as F\n", "metadata": [{"fpath_tuple": ["huggingface_diffusers", "examples", "unconditional_image_generation", "train_unconditional.py"], "line_no": 2, "start_line_no": 0, "end_line_no": 12, "window_size": 20, "repo": "huggingface_diffusers", "slice_size": 10}], "sim_score": 0.2692307692307692}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# ding/envs/env_wrappers/env_wrappers.py\n# --------------------------------------------------\n#             obs_space = (obs_space, )\n#         self.observation_space = gym.spaces.tuple.Tuple(\n#             [\n#                 gym.spaces.Box(\n#                     low=np.min(obs_space[0].low),\n#                     high=np.max(obs_space[0].high),\n#                     shape=(self.size, self.size),\n#                     dtype=obs_space[0].dtype\n#                 ) for _ in range(len(obs_space))\n#             ]\n#         )\n#         if len(self.observation_space) == 1:\n#             self.observation_space = self.observation_space[0]\n# \n#     def observation(self, frame):\n#         \"\"\"\n#         Overview:\n#             Returns the current observation from a frame\n#         Arguments:\n#             - frame (:obj:`Any`): the frame to get observation from\n# --------------------------------------------------\n# the below code fragment can be found in:\n# ding/envs/env_wrappers/env_wrappers.py\n# --------------------------------------------------\n#             obs_space = (obs_space, )\n#         shape = (n_frames, ) + obs_space[0].shape\n#         self.observation_space = gym.spaces.tuple.Tuple(\n#             [\n#                 gym.spaces.Box(\n#                     low=np.min(obs_space[0].low), high=np.max(obs_space[0].high), shape=shape, dtype=obs_space[0].dtype\n#                 ) for _ in range(len(obs_space))\n#             ]\n#         )\n#         if len(self.observation_space) == 1:\n#             self.observation_space = self.observation_space[0]\n# \n#     def reset(self):\n#         \"\"\"\n#         Overview:\n#             Resets the state of the environment and append new observation to frames\n#         Returns:\n#             - ``self._get_ob()``: observation\n#         \"\"\"\n#         obs = self.env.reset()\n# --------------------------------------------------\n# the below code fragment can be found in:\n# ding/envs/env_wrappers/env_wrappers.py\n# --------------------------------------------------\n#         )\n#         if len(self.observation_space) == 1:\n#             self.observation_space = self.observation_space[0]\n# \n#     def reset(self):\n#         \"\"\"\n#         Overview:\n#             Resets the state of the environment and append new observation to frames\n#         Returns:\n#             - ``self._get_ob()``: observation\n#         \"\"\"\n#         obs = self.env.reset()\n#         for _ in range(self.n_frames):\n#             self.frames.append(obs)\n#         return self._get_ob()\n# \n#     def step(self, action):\n#         \"\"\"\n#         Overview:\n#             Step the environment with the given action. Repeat action, sum reward,  \\\n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\n joined.\n        self._parallel.run((c.join_game, join)\n                           for c, join in zip(self._controllers, join_reqs))\n\n        self._game_info = self._parallel.run(c.game_info for c in self._controllers)\n        for g, interface in zip(self._game_info, self._interface_options):\n            if g.options.render != interface.render:\n                logging.warning(\n                    \"Actual interface options don't match requested options:\\n\"\n                    \"Requested:\\n%s\\n\\nActual:\\n%s\", interface, g.options)\n\n        self._features = None\n\n\n    def _launch(self):\n        self.old_unit_tags = set()\n        print(\"*****LAUNCH FUNCTION CALLED*****\")\n        SC2Env.__init__(\n            self,\n            map_name=self.map_name,\n            battle_net_map=False,\n            players=self.players,\n            agent_interface_format=self.agent_interface_format,\n            discount=None,\n            discount_zero_after_timeout=False,\n            visualize=False,\n            step_mul=8,\n            realtime=False,\n            save_replay_episodes=self.save_replay_episodes,\n            replay_dir=None if self.save_replay_episodes is None else \".\",\n            replay_prefix=None,\n            game_steps_per_episode=self.game_steps_per_episode,\n            score_index=None,\n            score_multiplier=None,\n            random_seed=self._seed,\n            disable_fog=False,\n            ensure_available_actions=True,\n            version=None\n        )\n        self._parallel.run((c.step, 2) for c in self._controllers)\n        self._init_map()\n\n    def _episode_restart(self):\n        \"\"\"Restart the environment by killing all units on the map.\n        There is a trigger in the SC2Map file, which restarts the\n        episode when there are no units left.\n        \"\"\"\n        try:\n            # save current units' tag\n            self._update_obs()\n            self.old_unit_tags = set(unit.tag for unit in self._obs.observation.raw_data.units)\n            # kill current units\n            run_commands = [\n                (\n                    self._controllers[0].debug,\n                    d_pb.DebugCommand(\n                        kill_unit=d_pb.DebugKillUnit(\n                            tag=[unit.tag for unit in self._obs.observation.raw_data.units]\n                        )\n                    )\n                )\n            ]\n            # Kill all units on the map.\n            self._parallel.run(run_commands)\n            # Forward 2 step to make sure all units revive.\n            self._parallel.run((c.step, 2) for c in self._controllers)\n        except (protocol.ProtocolError, protocol.ConnectionError) as e:\n            print(\"Error happen in _restart. Error: \", e)\n            self._env_restart()\n\n    def _env_restart(self):\n        self.close()\n        self._launch()\n        self._force_restarts += 1\n\n    def reset(self):\n        if self._launch_env_flag:\n            # Launch StarCraft II\n            print(\"*************LAUNCH TOTAL GAME********************\")\n            self._launch()\n        elif self._abnormal_env_flag or (self._total_steps >= self._next_reset_steps) or (\n                self.save_replay_episodes is not None):\n            # Avoid hitting the real episode limit of SC2 env\n            print(\"We are full restarting the environment! save_replay_episodes: \", self.save_replay_episodes)\n            self._env_restart()\n            self._next_reset_steps += FORCE_RESTART_INTERVAL\n        else:\n            self._episode_restart()\n\n        init_flag = False\n        for i in range(5):\n            for j in range(10):\n                self._update_obs()\n                init_flag = self._init_units()\n                if init_flag:\n                    break\n                else:\n                    self._episode_restart()\n            if init_flag:\n                break\n            else:\n                self._env_restart()\n        if not init_flag:\n            raise RuntimeError(\"reset 5 times error\")\n\n        self._episode_steps = 0\n        self._final_eval_fake_reward = 0.\n\n        self._launch_env_flag = False\n        self._abnormal_env_flag = False\n\n        self._init_units_attr()\n        self._init_rewards()\n        self._init_states()\n        return self.get_obs()\n\n    def _init_map(self):\n        game_info = self._game_info[0]\n        map_info = game_info.start_raw\n        map_play_area_min = map_info.playable_area.p0\n        map_play_area_max = map_info.playable_area.p1\n        self.max_distance_x = map_play_area_max.x - map_play_area_min.x\n        self.max_distance_y = map_play_area_max.y - map_play_area_min.y\n        self.map_x = map_info.map_size.x\n        self.map_y = map_info.map_size.y\n        if map_info.pathing_grid.bits_per_pixel == 1:\n            vals = np.array(list(map_info.pathing_grid.data)).reshape(self.map_x, int(self.map_y / 8))\n            self.pathing_grid = np.transpose(\n                np.array([[(b >> i) & 1 for b in row for i in range(7, -1, -1)] for row in vals], dtype=np.bool)\n            )\n        else:\n            self.pathing_grid = np.invert(\n                np.flip(\n                    np.transpose(\n                        np.array(list(map_info.pathing_grid.data), dtype=np.bool).reshape(self.map_x, self.map_y)),\n                    axis=1\n                )\n            )\n        self.terrain_height = np.flip(\n            np.transpose(np.array(list(map_info.terrain_height.data)).reshape(self.map_x, self.map_y)), 1\n        ) / 255\n\n    def _init_units(self):\n        # Sometimes not all units have yet been created by SC2 ToDO: check if use list not dict is a bug\n        self.agents = [\n            unit for unit in self._obs.observation.raw_data.units\n            if (unit.owner == 1) and (unit.tag not in self.old_unit_tags)\n        ]\n        self.agents = sorted(\n            self.agents,\n            key=attrgetter(\"unit_type\", \"pos.x\", \"pos.y\"),\n            reverse=False,\n        )\n        self.enemies = [\n            unit for unit in self._obs.observation.raw_data.units\n            if (unit.owner == 2) and (unit.tag not in self.old_unit_tags)\n        ]\n\n        all_agents_created = (len(self.agents) == self.n_agents)\n        all_enemies_created = (len(self.enemies) == self.n_enemies)\n        all_agents_health = all(u.health > 0 for u in self.agents)\n        all_enemies_health = all(u.health > 0 for u in self.enemies)\n\n        if all_agents_created and all_enemies_created and all_agents_health and all_enemies_health:  # all good\n            return True\n        else:\n            if not all_agents_created:\n                print('not all agents created: {} vs {}'.format(len(self.agents), self.n_agents))\n            if not all_agents_created:\n                print('not all enemies created: {} vs {}'.format(len(self.enemies), self.n_enemies))\n            if not all_agents_health:\n                print('not all agents health')", "metadata": {"task_id": "opendilab_ACE/48", "ground_truth": "            if not all_enemies_health:", "fpath_tuple": ["opendilab_ACE", "dizoo", "smac", "envs", "smac_env_ace.py"], "context_start_lineno": 244, "line_no": 416, "query_window": {"context": "        )\n        self.enemies = [\n            unit for unit in self._obs.observation.raw_data.units\n            if (unit.owner == 2) and (unit.tag not in self.old_unit_tags)\n        ]\n\n        all_agents_created = (len(self.agents) == self.n_agents)\n        all_enemies_created = (len(self.enemies) == self.n_enemies)\n        all_agents_health = all(u.health > 0 for u in self.agents)\n        all_enemies_health = all(u.health > 0 for u in self.enemies)\n\n        if all_agents_created and all_enemies_created and all_agents_health and all_enemies_health:  # all good\n            return True\n        else:\n            if not all_agents_created:\n                print('not all agents created: {} vs {}'.format(len(self.agents), self.n_agents))\n            if not all_agents_created:\n                print('not all enemies created: {} vs {}'.format(len(self.enemies), self.n_enemies))\n            if not all_agents_health:\n                print('not all agents health')", "metadata": {"fpath_tuple": ["opendilab_ACE", "dizoo", "smac", "envs", "smac_env_ace.py"], "line_no": 416, "task_id": "opendilab_ACE/48", "start_line_no": 396, "end_line_no": 416, "window_size": 20, "context_start_lineno": 244, "repo": "opendilab_ACE"}}, "top_k_context": [{"context": "                ) for _ in range(len(obs_space))\n            ]\n        )\n        if len(self.observation_space) == 1:\n            self.observation_space = self.observation_space[0]\n\n    def reset(self):\n        \"\"\"\n        Overview:\n            Resets the state of the environment and append new observation to frames\n        Returns:\n            - ``self._get_ob()``: observation\n        \"\"\"\n        obs = self.env.reset()\n        for _ in range(self.n_frames):\n            self.frames.append(obs)\n        return self._get_ob()\n\n    def step(self, action):\n        \"\"\"", "metadata": [{"fpath_tuple": ["opendilab_ACE", "ding", "envs", "env_wrappers", "env_wrappers.py"], "line_no": 332, "start_line_no": 322, "end_line_no": 342, "window_size": 20, "repo": "opendilab_ACE", "slice_size": 10}], "sim_score": 0.26605504587155965}, {"context": "        obs_space = env.observation_space\n        if not isinstance(obs_space, gym.spaces.tuple.Tuple):\n            obs_space = (obs_space, )\n        shape = (n_frames, ) + obs_space[0].shape\n        self.observation_space = gym.spaces.tuple.Tuple(\n            [\n                gym.spaces.Box(\n                    low=np.min(obs_space[0].low), high=np.max(obs_space[0].high), shape=shape, dtype=obs_space[0].dtype\n                ) for _ in range(len(obs_space))\n            ]\n        )\n        if len(self.observation_space) == 1:\n            self.observation_space = self.observation_space[0]\n\n    def reset(self):\n        \"\"\"\n        Overview:\n            Resets the state of the environment and append new observation to frames\n        Returns:\n            - ``self._get_ob()``: observation", "metadata": [{"fpath_tuple": ["opendilab_ACE", "ding", "envs", "env_wrappers", "env_wrappers.py"], "line_no": 324, "start_line_no": 314, "end_line_no": 334, "window_size": 20, "repo": "opendilab_ACE", "slice_size": 10}], "sim_score": 0.24615384615384617}, {"context": "        obs_space = env.observation_space\n        if not isinstance(obs_space, gym.spaces.tuple.Tuple):\n            obs_space = (obs_space, )\n        self.observation_space = gym.spaces.tuple.Tuple(\n            [\n                gym.spaces.Box(\n                    low=np.min(obs_space[0].low),\n                    high=np.max(obs_space[0].high),\n                    shape=(self.size, self.size),\n                    dtype=obs_space[0].dtype\n                ) for _ in range(len(obs_space))\n            ]\n        )\n        if len(self.observation_space) == 1:\n            self.observation_space = self.observation_space[0]\n\n    def observation(self, frame):\n        \"\"\"\n        Overview:\n            Returns the current observation from a frame", "metadata": [{"fpath_tuple": ["opendilab_ACE", "ding", "envs", "env_wrappers", "env_wrappers.py"], "line_no": 166, "start_line_no": 156, "end_line_no": 176, "window_size": 20, "repo": "opendilab_ACE", "slice_size": 10}], "sim_score": 0.2457627118644068}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# test/test_modules.py\n# --------------------------------------------------\n#             SafeModule(\n#                 rssm_prior,\n#                 in_keys=[\"state\", \"belief\", \"action\"],\n#                 out_keys=[\n#                     (\"next\", \"prior_mean\"),\n#                     (\"next\", \"prior_std\"),\n#                     \"_\",\n#                     (\"next\", \"belief\"),\n#                 ],\n#             ),\n#             SafeModule(\n#                 rssm_posterior,\n#                 in_keys=[(\"next\", \"belief\"), (\"next\", \"encoded_latents\")],\n#                 out_keys=[\n#                     (\"next\", \"posterior_mean\"),\n#                     (\"next\", \"posterior_std\"),\n#                     (\"next\", \"state\"),\n#                 ],\n#             ),\n#         )\n# --------------------------------------------------\n# the below code fragment can be found in:\n# test/test_cost.py\n# test/test_modules.py\n# --------------------------------------------------\n#                 in_keys=[\"state\", \"belief\", \"action\"],\n#                 out_keys=[\n#                     (\"next\", \"prior_mean\"),\n#                     (\"next\", \"prior_std\"),\n#                     \"_\",\n#                     (\"next\", \"belief\"),\n#                 ],\n#             ),\n#             SafeModule(\n#                 rssm_posterior,\n#                 in_keys=[(\"next\", \"belief\"), (\"next\", \"encoded_latents\")],\n#                 out_keys=[\n#                     (\"next\", \"posterior_mean\"),\n#                     (\"next\", \"posterior_std\"),\n#                     (\"next\", \"state\"),\n#                 ],\n#             ),\n#         )\n#         reward_module = MLP(\n#             out_features=1, depth=2, num_cells=mlp_num_units, activation_class=nn.ELU\n# --------------------------------------------------\n# the below code fragment can be found in:\n# test/test_cost.py\n# --------------------------------------------------\n#             SafeModule(\n#                 rssm_prior,\n#                 in_keys=[\"state\", \"belief\", \"action\"],\n#                 out_keys=[\n#                     (\"next\", \"prior_mean\"),\n#                     (\"next\", \"prior_std\"),\n#                     \"_\",\n#                     (\"next\", \"belief\"),\n#                 ],\n#             ),\n#             SafeModule(\n#                 rssm_posterior,\n#                 in_keys=[(\"next\", \"belief\"), (\"next\", \"encoded_latents\")],\n#                 out_keys=[\n#                     (\"next\", \"posterior_mean\"),\n#                     (\"next\", \"posterior_std\"),\n#                     (\"next\", \"state\"),\n#                 ],\n#             ),\n#         )\n# --------------------------------------------------\n# the below code fragment can be found in:\n# test/test_cost.py\n# --------------------------------------------------\n#         world_modeler = SafeSequential(\n#             SafeModule(\n#                 obs_encoder,\n#                 in_keys=[(\"next\", \"pixels\")],\n#                 out_keys=[(\"next\", \"encoded_latents\")],\n#             ),\n#             rssm_rollout,\n#             SafeModule(\n#                 obs_decoder,\n#                 in_keys=[(\"next\", \"state\"), (\"next\", \"belief\")],\n#                 out_keys=[(\"next\", \"reco_pixels\")],\n#             ),\n#         )\n#         reward_module = SafeModule(\n#             reward_module,\n#             in_keys=[(\"next\", \"state\"), (\"next\", \"belief\")],\n#             out_keys=[\"reward\"],\n#         )\n#         world_model = WorldModelWrapper(world_modeler, reward_module)\n# \n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\n\n        nn.TensorDictModel: Dreamer Model based environnement.\n        nn.TensorDictModel: Dreamer Actor the world model space.\n        nn.TensorDictModel: Dreamer Value model.\n        nn.TensorDictModel: Dreamer Actor for the real world space.\n\n    \"\"\"\n    proof_env_is_none = proof_environment is None\n    if proof_env_is_none:\n        proof_environment = transformed_env_constructor(\n            cfg=cfg, use_env_creator=False, obs_norm_state_dict=obs_norm_state_dict\n        )()\n\n    # Modules\n    obs_encoder = ObsEncoder()\n    obs_decoder = ObsDecoder()\n\n    rssm_prior = RSSMPrior(\n        hidden_dim=cfg.rssm_hidden_dim,\n        rnn_hidden_dim=cfg.rssm_hidden_dim,\n        state_dim=cfg.state_dim,\n        action_spec=proof_environment.action_spec,\n    )\n    rssm_posterior = RSSMPosterior(\n        hidden_dim=cfg.rssm_hidden_dim, state_dim=cfg.state_dim\n    )\n    reward_module = MLP(\n        out_features=1, depth=2, num_cells=cfg.mlp_num_units, activation_class=nn.ELU\n    )\n\n    world_model = _dreamer_make_world_model(\n        obs_encoder, obs_decoder, rssm_prior, rssm_posterior, reward_module\n    ).to(device)\n    with torch.no_grad(), set_exploration_mode(\"random\"):\n        tensordict = proof_environment.rollout(4)\n        tensordict = tensordict.to_tensordict().to(device)\n        tensordict = tensordict.to(device)\n        world_model(tensordict)\n\n    model_based_env = _dreamer_make_mbenv(\n        reward_module,\n        rssm_prior,\n        obs_decoder,\n        proof_environment,\n        use_decoder_in_env,\n        cfg.state_dim,\n        cfg.rssm_hidden_dim,\n    )\n    model_based_env = model_based_env.to(device)\n\n    actor_simulator, actor_realworld = _dreamer_make_actors(\n        obs_encoder,\n        rssm_prior,\n        rssm_posterior,\n        cfg.mlp_num_units,\n        action_key,\n        proof_environment,\n    )\n    actor_simulator = actor_simulator.to(device)\n\n    value_model = _dreamer_make_value_model(cfg.mlp_num_units, value_key)\n    value_model = value_model.to(device)\n    with torch.no_grad(), set_exploration_mode(\"random\"):\n        tensordict = model_based_env.rollout(4)\n        tensordict = tensordict.to(device)\n        tensordict = actor_simulator(tensordict)\n        value_model(tensordict)\n\n    actor_realworld = actor_realworld.to(device)\n    if proof_env_is_none:\n        proof_environment.close()\n        torch.cuda.empty_cache()\n        del proof_environment\n\n    del tensordict\n    return world_model, model_based_env, actor_simulator, value_model, actor_realworld\n\n\ndef _dreamer_make_world_model(\n    obs_encoder, obs_decoder, rssm_prior, rssm_posterior, reward_module\n):\n    # World Model and reward model\n    rssm_rollout = RSSMRollout(\n        SafeModule(\n            rssm_prior,\n            in_keys=[\"state\", \"belief\", \"action\"],\n            out_keys=[\n                (\"next\", \"prior_mean\"),\n                (\"next\", \"prior_std\"),\n                \"_\",\n                (\"next\", \"belief\"),\n            ],\n        ),\n        SafeModule(\n            rssm_posterior,\n            in_keys=[(\"next\", \"belief\"), (\"next\", \"encoded_latents\")],\n            out_keys=[\n                (\"next\", \"posterior_mean\"),\n                (\"next\", \"posterior_std\"),\n                (\"next\", \"state\"),\n            ],\n        ),\n    )\n\n    transition_model = SafeSequential(\n        SafeModule(\n            obs_encoder,\n            in_keys=[(\"next\", \"pixels\")],\n            out_keys=[(\"next\", \"encoded_latents\")],\n        ),\n        rssm_rollout,\n        SafeModule(\n            obs_decoder,\n            in_keys=[(\"next\", \"state\"), (\"next\", \"belief\")],\n            out_keys=[(\"next\", \"reco_pixels\")],\n        ),\n    )\n    reward_model = SafeModule(\n        reward_module,\n        in_keys=[(\"next\", \"state\"), (\"next\", \"belief\")],\n        out_keys=[\"reward\"],\n    )\n    world_model = WorldModelWrapper(\n        transition_model,\n        reward_model,\n    )\n    return world_model\n\n\ndef _dreamer_make_actors(\n    obs_encoder,\n    rssm_prior,\n    rssm_posterior,\n    mlp_num_units,\n    action_key,\n    proof_environment,\n):\n    actor_module = DreamerActor(\n        out_features=proof_environment.action_spec.shape[0],\n        depth=3,\n        num_cells=mlp_num_units,\n        activation_class=nn.ELU,\n    )\n    actor_simulator = _dreamer_make_actor_sim(\n        action_key, proof_environment, actor_module\n    )\n    actor_realworld = _dreamer_make_actor_real(\n        obs_encoder,\n        rssm_prior,\n        rssm_posterior,\n        actor_module,\n        action_key,\n        proof_environment,\n    )\n    return actor_simulator, actor_realworld\n\n\ndef _dreamer_make_actor_sim(action_key, proof_environment, actor_module):\n    actor_simulator = SafeProbabilisticSequential(\n        SafeModule(\n            actor_module,\n            in_keys=[\"state\", \"belief\"],\n            out_keys=[\"loc\", \"scale\"],\n            spec=CompositeSpec(\n                **{\n                    \"loc\": UnboundedContinuousTensorSpec(\n                        proof_environment.action_spec.shape,\n                        device=proof_environment.action_spec.device,\n                    ),\n                    \"scale\": UnboundedContinuousTensorSpec(\n                        proof_environment.action_spec.shape,\n                        device=proof_environment.action_spec.device,\n                    ),\n                }\n            ),\n        ),\n        SafeProbabilisticModule(\n            in_keys=[\"loc\", \"scale\"],\n            out_keys=[action_key],\n            default_interaction_mode=\"random\",\n            distribution_class=TanhNormal,\n            spec=CompositeSpec(**{action_key: proof_environment.action_spec}),\n        ),\n    )\n    return actor_simulator\n\n\ndef _dreamer_make_actor_real(\n    obs_encoder, rssm_prior, rssm_posterior, actor_module, action_key, proof_environment\n):\n    # actor for real world: interacts with states ~ posterior\n    # Out actor differs from the original paper where first they compute prior and posterior and then act on it\n    # but we found that this approach worked better.\n    actor_realworld = SafeSequential(\n        SafeModule(\n            obs_encoder,\n            in_keys=[\"pixels\"],\n            out_keys=[\"encoded_latents\"],\n        ),\n        SafeModule(\n            rssm_posterior,\n            in_keys=[\"belief\", \"encoded_latents\"],\n            out_keys=[\n                \"_\",\n                \"_\",\n                \"state\",\n            ],\n        ),", "metadata": {"task_id": "pytorch_rl/159", "ground_truth": "        SafeProbabilisticSequential(", "fpath_tuple": ["pytorch_rl", "torchrl", "trainers", "helpers", "models.py"], "context_start_lineno": 1488, "line_no": 1696, "query_window": {"context": "    obs_encoder, rssm_prior, rssm_posterior, actor_module, action_key, proof_environment\n):\n    # actor for real world: interacts with states ~ posterior\n    # Out actor differs from the original paper where first they compute prior and posterior and then act on it\n    # but we found that this approach worked better.\n    actor_realworld = SafeSequential(\n        SafeModule(\n            obs_encoder,\n            in_keys=[\"pixels\"],\n            out_keys=[\"encoded_latents\"],\n        ),\n        SafeModule(\n            rssm_posterior,\n            in_keys=[\"belief\", \"encoded_latents\"],\n            out_keys=[\n                \"_\",\n                \"_\",\n                \"state\",\n            ],\n        ),", "metadata": {"fpath_tuple": ["pytorch_rl", "torchrl", "trainers", "helpers", "models.py"], "line_no": 1696, "task_id": "pytorch_rl/159", "start_line_no": 1676, "end_line_no": 1696, "window_size": 20, "context_start_lineno": 1488, "repo": "pytorch_rl"}}, "top_k_context": [{"context": "        )\n        # World Model and reward model\n        world_modeler = SafeSequential(\n            SafeModule(\n                obs_encoder,\n                in_keys=[(\"next\", \"pixels\")],\n                out_keys=[(\"next\", \"encoded_latents\")],\n            ),\n            rssm_rollout,\n            SafeModule(\n                obs_decoder,\n                in_keys=[(\"next\", \"state\"), (\"next\", \"belief\")],\n                out_keys=[(\"next\", \"reco_pixels\")],\n            ),\n        )\n        reward_module = SafeModule(\n            reward_module,\n            in_keys=[(\"next\", \"state\"), (\"next\", \"belief\")],\n            out_keys=[\"reward\"],\n        )", "metadata": [{"fpath_tuple": ["pytorch_rl", "test", "test_cost.py"], "line_no": 2504, "start_line_no": 2494, "end_line_no": 2514, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}], "sim_score": 0.37735849056603776}, {"context": "        # World Model and reward model\n        rssm_rollout = RSSMRollout(\n            SafeModule(\n                rssm_prior,\n                in_keys=[\"state\", \"belief\", \"action\"],\n                out_keys=[\n                    (\"next\", \"prior_mean\"),\n                    (\"next\", \"prior_std\"),\n                    \"_\",\n                    (\"next\", \"belief\"),\n                ],\n            ),\n            SafeModule(\n                rssm_posterior,\n                in_keys=[(\"next\", \"belief\"), (\"next\", \"encoded_latents\")],\n                out_keys=[\n                    (\"next\", \"posterior_mean\"),\n                    (\"next\", \"posterior_std\"),\n                    (\"next\", \"state\"),\n                ],", "metadata": [{"fpath_tuple": ["pytorch_rl", "test", "test_cost.py"], "line_no": 2480, "start_line_no": 2470, "end_line_no": 2490, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}], "sim_score": 0.36792452830188677}, {"context": "            SafeModule(\n                rssm_prior,\n                in_keys=[\"state\", \"belief\", \"action\"],\n                out_keys=[\n                    (\"next\", \"prior_mean\"),\n                    (\"next\", \"prior_std\"),\n                    \"_\",\n                    (\"next\", \"belief\"),\n                ],\n            ),\n            SafeModule(\n                rssm_posterior,\n                in_keys=[(\"next\", \"belief\"), (\"next\", \"encoded_latents\")],\n                out_keys=[\n                    (\"next\", \"posterior_mean\"),\n                    (\"next\", \"posterior_std\"),\n                    (\"next\", \"state\"),\n                ],\n            ),\n        )", "metadata": [{"fpath_tuple": ["pytorch_rl", "test", "test_cost.py"], "line_no": 2482, "start_line_no": 2472, "end_line_no": 2492, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}, {"fpath_tuple": ["pytorch_rl", "test", "test_modules.py"], "line_no": 630, "start_line_no": 620, "end_line_no": 640, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}], "sim_score": 0.3673469387755102}, {"context": "\n        rssm_rollout = RSSMRollout(\n            SafeModule(\n                rssm_prior,\n                in_keys=[\"state\", \"belief\", \"action\"],\n                out_keys=[\n                    (\"next\", \"prior_mean\"),\n                    (\"next\", \"prior_std\"),\n                    \"_\",\n                    (\"next\", \"belief\"),\n                ],\n            ),\n            SafeModule(\n                rssm_posterior,\n                in_keys=[(\"next\", \"belief\"), (\"next\", \"encoded_latents\")],\n                out_keys=[\n                    (\"next\", \"posterior_mean\"),\n                    (\"next\", \"posterior_std\"),\n                    (\"next\", \"state\"),\n                ],", "metadata": [{"fpath_tuple": ["pytorch_rl", "test", "test_modules.py"], "line_no": 628, "start_line_no": 618, "end_line_no": 638, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}], "sim_score": 0.3627450980392157}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# torchrl/envs/transforms/vip.py\n# --------------------------------------------------\n# \n#     @torch.no_grad()\n#     def _apply_transform(self, obs: torch.Tensor) -> None:\n#         shape = None\n#         if obs.ndimension() > 4:\n#             shape = obs.shape[:-3]\n#             obs = obs.flatten(0, -4)\n#         out = self.convnet(obs)\n#         if shape is not None:\n#             out = out.view(*shape, *out.shape[1:])\n#         return out\n# \n#     def transform_observation_spec(self, observation_spec: TensorSpec) -> TensorSpec:\n#         if not isinstance(observation_spec, CompositeSpec):\n#             raise ValueError(\"_VIPNet can only infer CompositeSpec\")\n# \n#         keys = [key for key in observation_spec._specs.keys() if key in self.in_keys]\n#         device = observation_spec[keys[0]].device\n#         dim = observation_spec[keys[0]].shape[:-3]\n# \n# --------------------------------------------------\n# the below code fragment can be found in:\n# torchrl/envs/transforms/vip.py\n# --------------------------------------------------\n#         if obs.ndimension() > 4:\n#             shape = obs.shape[:-3]\n#             obs = obs.flatten(0, -4)\n#         out = self.convnet(obs)\n#         if shape is not None:\n#             out = out.view(*shape, *out.shape[1:])\n#         return out\n# \n#     def transform_observation_spec(self, observation_spec: TensorSpec) -> TensorSpec:\n#         if not isinstance(observation_spec, CompositeSpec):\n#             raise ValueError(\"_VIPNet can only infer CompositeSpec\")\n# \n#         keys = [key for key in observation_spec._specs.keys() if key in self.in_keys]\n#         device = observation_spec[keys[0]].device\n#         dim = observation_spec[keys[0]].shape[:-3]\n# \n#         observation_spec = CompositeSpec(observation_spec)\n#         if self.del_keys:\n#             for in_key in keys:\n#                 del observation_spec[in_key]\n# --------------------------------------------------\n# the below code fragment can be found in:\n# torchrl/envs/transforms/vip.py\n# --------------------------------------------------\n#     def _apply_transform(self, obs: torch.Tensor) -> None:\n#         shape = None\n#         if obs.ndimension() > 4:\n#             shape = obs.shape[:-3]\n#             obs = obs.flatten(0, -4)\n#         out = self.convnet(obs)\n#         if shape is not None:\n#             out = out.view(*shape, *out.shape[1:])\n#         return out\n# \n#     def transform_observation_spec(self, observation_spec: TensorSpec) -> TensorSpec:\n#         if not isinstance(observation_spec, CompositeSpec):\n#             raise ValueError(\"_VIPNet can only infer CompositeSpec\")\n# \n#         keys = [key for key in observation_spec._specs.keys() if key in self.in_keys]\n#         device = observation_spec[keys[0]].device\n#         dim = observation_spec[keys[0]].shape[:-3]\n# \n#         observation_spec = CompositeSpec(observation_spec)\n#         if self.del_keys:\n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\n# Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom typing import List, Optional, Union\n\nimport torch\nfrom tensordict import TensorDict\nfrom torch.hub import load_state_dict_from_url\nfrom torch.nn import Identity\n\nfrom torchrl.data.tensor_specs import (\n    CompositeSpec,\n    TensorSpec,\n    UnboundedContinuousTensorSpec,\n)\nfrom torchrl.data.utils import DEVICE_TYPING\nfrom torchrl.envs.transforms.transforms import (\n    CatTensors,\n    Compose,\n    FlattenObservation,\n    ObservationNorm,\n    Resize,\n    ToTensorImage,\n    Transform,\n    UnsqueezeTransform,\n)\n\ntry:\n    from torchvision import models\n\n    _has_tv = True\nexcept ImportError:\n    _has_tv = False\n\ntry:\n    from torchvision.models import ResNet18_Weights, ResNet34_Weights, ResNet50_Weights\n    from torchvision.models._api import WeightsEnum\nexcept ImportError:\n\n    class WeightsEnum:  # noqa: D101\n        # placeholder\n        pass\n\n\nR3M_MODEL_MAP = {\n    \"resnet18\": \"r3m_18\",\n    \"resnet34\": \"r3m_34\",\n    \"resnet50\": \"r3m_50\",\n}\n\n\nclass _R3MNet(Transform):\n\n    inplace = False\n\n    def __init__(self, in_keys, out_keys, model_name, del_keys: bool = True):\n        if not _has_tv:\n            raise ImportError(\n                \"Tried to instantiate R3M without torchvision. Make sure you have \"\n                \"torchvision installed in your environment.\"\n            )\n        self.model_name = model_name\n        if model_name == \"resnet18\":\n            # self.model_name = \"r3m_18\"\n            self.outdim = 512\n            convnet = models.resnet18(None)\n        elif model_name == \"resnet34\":\n            # self.model_name = \"r3m_34\"\n            self.outdim = 512\n            convnet = models.resnet34(None)\n        elif model_name == \"resnet50\":\n            # self.model_name = \"r3m_50\"\n            self.outdim = 2048\n            convnet = models.resnet50(None)\n        else:\n            raise NotImplementedError(\n                f\"model {model_name} is currently not supported by R3M\"\n            )\n        convnet.fc = Identity()\n        super().__init__(in_keys=in_keys, out_keys=out_keys)\n        self.convnet = convnet\n        self.del_keys = del_keys\n\n    def _call(self, tensordict):\n        tensordict_view = tensordict.view(-1)\n        super()._call(tensordict_view)\n        if self.del_keys:\n            tensordict.exclude(*self.in_keys, inplace=True)\n        return tensordict\n\n    @torch.no_grad()\n    def _apply_transform(self, obs: torch.Tensor) -> None:\n        shape = None\n        if obs.ndimension() > 4:\n            shape = obs.shape[:-3]\n            obs = obs.flatten(0, -4)\n        out = self.convnet(obs)\n        if shape is not None:\n            out = out.view(*shape, *out.shape[1:])\n        return out\n\n    def transform_observation_spec(self, observation_spec: TensorSpec) -> TensorSpec:\n        if not isinstance(observation_spec, CompositeSpec):\n            raise ValueError(\"_R3MNet can only infer CompositeSpec\")\n\n        keys = [key for key in observation_spec._specs.keys() if key in self.in_keys]\n        device = observation_spec[keys[0]].device\n        dim = observation_spec[keys[0]].shape[:-3]\n\n        observation_spec = CompositeSpec(observation_spec)", "metadata": {"task_id": "pytorch_rl/110", "ground_truth": "        if self.del_keys:", "fpath_tuple": ["pytorch_rl", "torchrl", "envs", "transforms", "r3m.py"], "context_start_lineno": 0, "line_no": 112, "query_window": {"context": "    @torch.no_grad()\n    def _apply_transform(self, obs: torch.Tensor) -> None:\n        shape = None\n        if obs.ndimension() > 4:\n            shape = obs.shape[:-3]\n            obs = obs.flatten(0, -4)\n        out = self.convnet(obs)\n        if shape is not None:\n            out = out.view(*shape, *out.shape[1:])\n        return out\n\n    def transform_observation_spec(self, observation_spec: TensorSpec) -> TensorSpec:\n        if not isinstance(observation_spec, CompositeSpec):\n            raise ValueError(\"_R3MNet can only infer CompositeSpec\")\n\n        keys = [key for key in observation_spec._specs.keys() if key in self.in_keys]\n        device = observation_spec[keys[0]].device\n        dim = observation_spec[keys[0]].shape[:-3]\n\n        observation_spec = CompositeSpec(observation_spec)", "metadata": {"fpath_tuple": ["pytorch_rl", "torchrl", "envs", "transforms", "r3m.py"], "line_no": 112, "task_id": "pytorch_rl/110", "start_line_no": 92, "end_line_no": 112, "window_size": 20, "context_start_lineno": 0, "repo": "pytorch_rl"}}, "top_k_context": [{"context": "\n    @torch.no_grad()\n    def _apply_transform(self, obs: torch.Tensor) -> None:\n        shape = None\n        if obs.ndimension() > 4:\n            shape = obs.shape[:-3]\n            obs = obs.flatten(0, -4)\n        out = self.convnet(obs)\n        if shape is not None:\n            out = out.view(*shape, *out.shape[1:])\n        return out\n\n    def transform_observation_spec(self, observation_spec: TensorSpec) -> TensorSpec:\n        if not isinstance(observation_spec, CompositeSpec):\n            raise ValueError(\"_VIPNet can only infer CompositeSpec\")\n\n        keys = [key for key in observation_spec._specs.keys() if key in self.in_keys]\n        device = observation_spec[keys[0]].device\n        dim = observation_spec[keys[0]].shape[:-3]\n", "metadata": [{"fpath_tuple": ["pytorch_rl", "torchrl", "envs", "transforms", "vip.py"], "line_no": 90, "start_line_no": 80, "end_line_no": 100, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}], "sim_score": 0.9595959595959596}, {"context": "    def _apply_transform(self, obs: torch.Tensor) -> None:\n        shape = None\n        if obs.ndimension() > 4:\n            shape = obs.shape[:-3]\n            obs = obs.flatten(0, -4)\n        out = self.convnet(obs)\n        if shape is not None:\n            out = out.view(*shape, *out.shape[1:])\n        return out\n\n    def transform_observation_spec(self, observation_spec: TensorSpec) -> TensorSpec:\n        if not isinstance(observation_spec, CompositeSpec):\n            raise ValueError(\"_VIPNet can only infer CompositeSpec\")\n\n        keys = [key for key in observation_spec._specs.keys() if key in self.in_keys]\n        device = observation_spec[keys[0]].device\n        dim = observation_spec[keys[0]].shape[:-3]\n\n        observation_spec = CompositeSpec(observation_spec)\n        if self.del_keys:", "metadata": [{"fpath_tuple": ["pytorch_rl", "torchrl", "envs", "transforms", "vip.py"], "line_no": 92, "start_line_no": 82, "end_line_no": 102, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}], "sim_score": 0.9}, {"context": "            tensordict.exclude(*self.in_keys, inplace=True)\n        return tensordict\n\n    @torch.no_grad()\n    def _apply_transform(self, obs: torch.Tensor) -> None:\n        shape = None\n        if obs.ndimension() > 4:\n            shape = obs.shape[:-3]\n            obs = obs.flatten(0, -4)\n        out = self.convnet(obs)\n        if shape is not None:\n            out = out.view(*shape, *out.shape[1:])\n        return out\n\n    def transform_observation_spec(self, observation_spec: TensorSpec) -> TensorSpec:\n        if not isinstance(observation_spec, CompositeSpec):\n            raise ValueError(\"_VIPNet can only infer CompositeSpec\")\n\n        keys = [key for key in observation_spec._specs.keys() if key in self.in_keys]\n        device = observation_spec[keys[0]].device", "metadata": [{"fpath_tuple": ["pytorch_rl", "torchrl", "envs", "transforms", "vip.py"], "line_no": 88, "start_line_no": 78, "end_line_no": 98, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}], "sim_score": 0.8785046728971962}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# torchrl/modules/tensordict_module/probabilistic.py\n# --------------------------------------------------\n#             tensordict along with the sample. Those parameters can be used to re-compute\n#             the original distribution later on (e.g. to compute the divergence between\n#             the distribution used to sample the action and the updated distribution in\n#             PPO). Default is `False`.\n#         n_empirical_estimate (int, optional): number of samples to compute the empirical\n#             mean when it is not available. Default is 1000\n# \n#     \"\"\"\n# \n#     def __init__(\n#         self,\n#         in_keys: Union[str, Sequence[str], dict],\n#         out_keys: Union[str, Sequence[str]],\n#         spec: Optional[TensorSpec] = None,\n#         safe: bool = False,\n#         default_interaction_mode: str = \"mode\",\n#         distribution_class: Type = Delta,\n#         distribution_kwargs: Optional[dict] = None,\n#         return_log_prob: bool = False,\n#         cache_dist: bool = False,\n# --------------------------------------------------\n# the below code fragment can be found in:\n# torchrl/objectives/value/advantages.py\n# --------------------------------------------------\n#         differentiable (bool, optional): if True, gradients are propagated throught\n#             the computation of the value function. Default is :obj:`False`.\n#         advantage_key (str or tuple of str, optional): the key of the advantage entry.\n#             Defaults to \"advantage\".\n#         value_target_key (str or tuple of str, optional): the key of the advantage entry.\n#             Defaults to \"value_target\".\n#         value_key (str or tuple of str, optional): the value key to read from the input tensordict.\n#             Defaults to \"state_value\".\n# \n#     \"\"\"\n# \n#     def __init__(\n#         self,\n#         gamma: Union[float, torch.Tensor],\n#         value_network: SafeModule,\n#         average_rewards: bool = False,\n#         differentiable: bool = False,\n#         advantage_key: Union[str, Tuple] = \"advantage\",\n#         value_target_key: Union[str, Tuple] = \"value_target\",\n#         value_key: Union[str, Tuple] = \"state_value\",\n# --------------------------------------------------\n# the below code fragment can be found in:\n# torchrl/modules/tensordict_module/probabilistic.py\n# --------------------------------------------------\n#         cache_dist (bool, optional): EXPERIMENTAL: if True, the parameters of the\n#             distribution (i.e. the output of the module) will be written to the\n#             tensordict along with the sample. Those parameters can be used to re-compute\n#             the original distribution later on (e.g. to compute the divergence between\n#             the distribution used to sample the action and the updated distribution in\n#             PPO). Default is `False`.\n#         n_empirical_estimate (int, optional): number of samples to compute the empirical\n#             mean when it is not available. Default is 1000\n# \n#     \"\"\"\n# \n#     def __init__(\n#         self,\n#         in_keys: Union[str, Sequence[str], dict],\n#         out_keys: Union[str, Sequence[str]],\n#         spec: Optional[TensorSpec] = None,\n#         safe: bool = False,\n#         default_interaction_mode: str = \"mode\",\n#         distribution_class: Type = Delta,\n#         distribution_kwargs: Optional[dict] = None,\n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\n# Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom typing import Optional, Union\n\nimport numpy as np\nimport torch\nfrom tensordict.nn import TensorDictModuleWrapper\nfrom tensordict.tensordict import TensorDictBase\nfrom tensordict.utils import expand_as_right\n\nfrom torchrl.data import CompositeSpec, TensorSpec\nfrom torchrl.envs.utils import exploration_mode\nfrom torchrl.modules.tensordict_module.common import (\n    _forward_hook_safe_action,\n    SafeModule,\n)\n\n\n__all__ = [\n    \"EGreedyWrapper\",\n    \"AdditiveGaussianWrapper\",\n    \"OrnsteinUhlenbeckProcessWrapper\",\n]\n\n\nclass EGreedyWrapper(TensorDictModuleWrapper):\n    \"\"\"Epsilon-Greedy PO wrapper.\n\n    Args:\n        policy (SafeModule): a deterministic policy.\n        eps_init (scalar, optional): initial epsilon value.\n            default: 1.0\n        eps_end (scalar, optional): final epsilon value.\n            default: 0.1\n        annealing_num_steps (int, optional): number of steps it will take for epsilon to reach the eps_end value\n        action_key (str, optional): if the policy module has more than one output key,\n            its output spec will be of type CompositeSpec. One needs to know where to\n            find the action spec.\n            Default is \"action\".\n        spec (TensorSpec, optional): if provided, the sampled action will be\n            projected onto the valid action space once explored. If not provided,\n            the exploration wrapper will attempt to recover it from the policy.\n\n    Examples:\n        >>> import torch\n        >>> from tensordict import TensorDict\n        >>> from torchrl.modules import EGreedyWrapper, Actor\n        >>> from torchrl.data import BoundedTensorSpec\n        >>> torch.manual_seed(0)\n        >>> spec = BoundedTensorSpec(-1, 1, torch.Size([4]))\n        >>> module = torch.nn.Linear(4, 4, bias=False)\n        >>> policy = Actor(spec=spec, module=module)\n        >>> explorative_policy = EGreedyWrapper(policy, eps_init=0.2)\n        >>> td = TensorDict({\"observation\": torch.zeros(10, 4)}, batch_size=[10])\n        >>> print(explorative_policy(td).get(\"action\"))\n        tensor([[ 0.0000,  0.0000,  0.0000,  0.0000],\n                [ 0.0000,  0.0000,  0.0000,  0.0000],\n                [-0.6986, -0.9366, -0.5837,  0.8596],\n                [ 0.0000,  0.0000,  0.0000,  0.0000],\n                [ 0.0000,  0.0000,  0.0000,  0.0000],\n                [ 0.0000,  0.0000,  0.0000,  0.0000],\n                [ 0.0000,  0.0000,  0.0000,  0.0000],\n                [ 0.0000,  0.0000,  0.0000,  0.0000],\n                [ 0.0000,  0.0000,  0.0000,  0.0000],\n                [ 0.0000,  0.0000,  0.0000,  0.0000]], grad_fn=<AddBackward0>)\n\n    \"\"\"\n\n    def __init__(\n        self,\n        policy: SafeModule,\n        eps_init: float = 1.0,\n        eps_end: float = 0.1,\n        annealing_num_steps: int = 1000,\n        action_key: str = \"action\",\n        spec: Optional[TensorSpec] = None,\n    ):\n        super().__init__(policy)\n        self.register_buffer(\"eps_init\", torch.tensor([eps_init]))\n        self.register_buffer(\"eps_end\", torch.tensor([eps_end]))\n        if self.eps_end > self.eps_init:\n            raise RuntimeError(\"eps should decrease over time or be constant\")\n        self.annealing_num_steps = annealing_num_steps\n        self.register_buffer(\"eps\", torch.tensor([eps_init]))\n        self.action_key = action_key\n        self.spec = (\n            spec\n            if spec is not None\n            else policy.spec\n            if hasattr(policy, \"spec\")\n            else None\n        )\n\n    def step(self, frames: int = 1) -> None:\n        \"\"\"A step of epsilon decay.\n\n        After self.annealing_num_steps, this function is a no-op.\n\n        Args:\n            frames (int): number of frames since last step.\n\n        \"\"\"\n        for _ in range(frames):\n            self.eps.data[0] = max(\n                self.eps_end.item(),\n                (\n                    self.eps - (self.eps_init - self.eps_end) / self.annealing_num_steps\n                ).item(),\n            )\n\n    def forward(self, tensordict: TensorDictBase) -> TensorDictBase:\n        tensordict = self.td_module.forward(tensordict)\n        if exploration_mode() == \"random\" or exploration_mode() is None:\n            out = tensordict.get(self.td_module.out_keys[0])\n            eps = self.eps.item()\n            cond = (torch.rand(tensordict.shape, device=tensordict.device) < eps).to(\n                out.dtype\n            )\n            cond = expand_as_right(cond, out)\n            spec = self.spec\n            if spec is not None:\n                if isinstance(spec, CompositeSpec):\n                    spec = spec[self.action_key]\n                out = (\n                    cond * spec.rand(tensordict.shape).to(out.device) + (1 - cond) * out\n                )\n            else:\n                raise RuntimeError(\n                    \"spec must be provided by the policy or directly to the exploration wrapper.\"\n                )\n            tensordict.set(self.td_module.out_keys[0], out)\n        return tensordict\n\n\nclass AdditiveGaussianWrapper(TensorDictModuleWrapper):\n    \"\"\"Additive Gaussian PO wrapper.\n\n    Args:\n        policy (SafeModule): a policy.\n        sigma_init (scalar, optional): initial epsilon value.\n            default: 1.0\n        sigma_end (scalar, optional): final epsilon value.\n            default: 0.1\n        annealing_num_steps (int, optional): number of steps it will take for\n            sigma to reach the :obj:`sigma_end` value.\n        mean (float, optional): mean of each output element\u2019s normal distribution.\n        std (float, optional): standard deviation of each output element\u2019s normal distribution.\n        action_key (str, optional): if the policy module has more than one output key,\n            its output spec will be of type CompositeSpec. One needs to know where to\n            find the action spec.\n            Default is \"action\".\n        spec (TensorSpec, optional): if provided, the sampled action will be\n            projected onto the valid action space once explored. If not provided,\n            the exploration wrapper will attempt to recover it from the policy.\n        safe (boolean, optional): if False, the TensorSpec can be None. If it\n            is set to False but the spec is passed, the projection will still\n            happen.\n            Default is True.\n\n    \"\"\"\n\n    def __init__(\n        self,", "metadata": {"task_id": "pytorch_rl/171", "ground_truth": "        policy: SafeModule,", "fpath_tuple": ["pytorch_rl", "torchrl", "modules", "tensordict_module", "exploration.py"], "context_start_lineno": 0, "line_no": 166, "query_window": {"context": "        annealing_num_steps (int, optional): number of steps it will take for\n            sigma to reach the :obj:`sigma_end` value.\n        mean (float, optional): mean of each output element\u2019s normal distribution.\n        std (float, optional): standard deviation of each output element\u2019s normal distribution.\n        action_key (str, optional): if the policy module has more than one output key,\n            its output spec will be of type CompositeSpec. One needs to know where to\n            find the action spec.\n            Default is \"action\".\n        spec (TensorSpec, optional): if provided, the sampled action will be\n            projected onto the valid action space once explored. If not provided,\n            the exploration wrapper will attempt to recover it from the policy.\n        safe (boolean, optional): if False, the TensorSpec can be None. If it\n            is set to False but the spec is passed, the projection will still\n            happen.\n            Default is True.\n\n    \"\"\"\n\n    def __init__(\n        self,", "metadata": {"fpath_tuple": ["pytorch_rl", "torchrl", "modules", "tensordict_module", "exploration.py"], "line_no": 166, "task_id": "pytorch_rl/171", "start_line_no": 146, "end_line_no": 166, "window_size": 20, "context_start_lineno": 0, "repo": "pytorch_rl"}}, "top_k_context": [{"context": "            distribution sample will be written in the tensordict with the key\n            `'sample_log_prob'`. Default is `False`.\n        cache_dist (bool, optional): EXPERIMENTAL: if True, the parameters of the\n            distribution (i.e. the output of the module) will be written to the\n            tensordict along with the sample. Those parameters can be used to re-compute\n            the original distribution later on (e.g. to compute the divergence between\n            the distribution used to sample the action and the updated distribution in\n            PPO). Default is `False`.\n        n_empirical_estimate (int, optional): number of samples to compute the empirical\n            mean when it is not available. Default is 1000\n\n    \"\"\"\n\n    def __init__(\n        self,\n        in_keys: Union[str, Sequence[str], dict],\n        out_keys: Union[str, Sequence[str]],\n        spec: Optional[TensorSpec] = None,\n        safe: bool = False,\n        default_interaction_mode: str = \"mode\",", "metadata": [{"fpath_tuple": ["pytorch_rl", "torchrl", "modules", "tensordict_module", "probabilistic.py"], "line_no": 92, "start_line_no": 82, "end_line_no": 102, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}], "sim_score": 0.26344086021505375}, {"context": "        average_rewards (bool, optional): if True, rewards will be standardized\n            before the TD is computed.\n        differentiable (bool, optional): if True, gradients are propagated throught\n            the computation of the value function. Default is :obj:`False`.\n        advantage_key (str or tuple of str, optional): the key of the advantage entry.\n            Defaults to \"advantage\".\n        value_target_key (str or tuple of str, optional): the key of the advantage entry.\n            Defaults to \"value_target\".\n        value_key (str or tuple of str, optional): the value key to read from the input tensordict.\n            Defaults to \"state_value\".\n\n    \"\"\"\n\n    def __init__(\n        self,\n        gamma: Union[float, torch.Tensor],\n        value_network: SafeModule,\n        average_rewards: bool = False,\n        differentiable: bool = False,\n        advantage_key: Union[str, Tuple] = \"advantage\",", "metadata": [{"fpath_tuple": ["pytorch_rl", "torchrl", "objectives", "value", "advantages.py"], "line_no": 50, "start_line_no": 40, "end_line_no": 60, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}], "sim_score": 0.2606060606060606}, {"context": "        cache_dist (bool, optional): EXPERIMENTAL: if True, the parameters of the\n            distribution (i.e. the output of the module) will be written to the\n            tensordict along with the sample. Those parameters can be used to re-compute\n            the original distribution later on (e.g. to compute the divergence between\n            the distribution used to sample the action and the updated distribution in\n            PPO). Default is `False`.\n        n_empirical_estimate (int, optional): number of samples to compute the empirical\n            mean when it is not available. Default is 1000\n\n    \"\"\"\n\n    def __init__(\n        self,\n        in_keys: Union[str, Sequence[str], dict],\n        out_keys: Union[str, Sequence[str]],\n        spec: Optional[TensorSpec] = None,\n        safe: bool = False,\n        default_interaction_mode: str = \"mode\",\n        distribution_class: Type = Delta,\n        distribution_kwargs: Optional[dict] = None,", "metadata": [{"fpath_tuple": ["pytorch_rl", "torchrl", "modules", "tensordict_module", "probabilistic.py"], "line_no": 94, "start_line_no": 84, "end_line_no": 104, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}], "sim_score": 0.25668449197860965}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# fortuna/prob_model/likelihood/classification.py\n# --------------------------------------------------\n#             \\text{Var}_{\\tilde{Y}|w,x}[\\tilde{Y}],\n# \n#         where:\n#          - :math:`x` is an observed input variable;\n#          - :math:`\\tilde{Y}` is a one-hot encoded random target variable;\n#          - :math:`w` denotes the observed model parameters.\n# \n#         Parameters\n#         ----------\n#         params : Params\n#             The random parameters of the probabilistic model.\n#         inputs_loader : InputsLoader\n#             A loader of input data points.\n#         mutable : Optional[Mutable]\n#             The mutable objects used to evaluate the models.\n#         calib_params : Optional[CalibParams]\n#             The calibration parameters of the probabilistic model.\n#         calib_mutable : Optional[CalibMutable]\n#             The calibration mutable objects used to evaluate the calibrators.\n#         distribute: bool\n# --------------------------------------------------\n# the below code fragment can be found in:\n# fortuna/prob_model/likelihood/classification.py\n# --------------------------------------------------\n#         where:\n#          - :math:`x` is an observed input variable;\n#          - :math:`\\tilde{Y}` is a one-hot encoded random target variable;\n#          - :math:`w` denotes the observed model parameters.\n# \n#         Parameters\n#         ----------\n#         params : Params\n#             The random parameters of the probabilistic model.\n#         inputs_loader : InputsLoader\n#             A loader of input data points.\n#         mutable : Optional[Mutable]\n#             The mutable objects used to evaluate the models.\n#         calib_params : Optional[CalibParams]\n#             The calibration parameters of the probabilistic model.\n#         calib_mutable : Optional[CalibMutable]\n#             The calibration mutable objects used to evaluate the calibrators.\n#         distribute: bool\n#             Whether to distribute computation over multiple devices, if available.\n# \n# --------------------------------------------------\n# the below code fragment can be found in:\n# fortuna/prob_model/likelihood/classification.py\n# --------------------------------------------------\n# \n#         where:\n#          - :math:`x` is an observed input variable;\n#          - :math:`\\tilde{Y}` is a one-hot encoded random target variable;\n#          - :math:`w` denotes the observed model parameters.\n# \n#         Parameters\n#         ----------\n#         params : Params\n#             The random parameters of the probabilistic model.\n#         inputs_loader : InputsLoader\n#             A loader of input data points.\n#         mutable : Optional[Mutable]\n#             The mutable objects used to evaluate the models.\n#         calib_params : Optional[CalibParams]\n#             The calibration parameters of the probabilistic model.\n#         calib_mutable : Optional[CalibMutable]\n#             The calibration mutable objects used to evaluate the calibrators.\n#         distribute: bool\n#             Whether to distribute computation over multiple devices, if available.\n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\n.\n        rng: Optional[PRNGKeyArray]\n            A random number generator. If not passed, this will be taken from the attributes of this class.\n        distribute: bool\n            Whether to distribute computation over multiple devices, if available.\n\n        Returns\n        -------\n        Union[jnp.ndarray, Tuple[jnp.ndarray, dict]]\n            The samples of the target variable for each input. If `return_aux` is given, the corresponding auxiliary\n            objects are also returned.\n        \"\"\"\n        if return_aux is None:\n            return_aux = []\n        supported_aux = [\"outputs\"]\n        unsupported_aux = [s for s in return_aux if s not in supported_aux]\n        if sum(unsupported_aux) > 0:\n            raise Exception(\n                \"\"\"The auxiliary objects {} are unknown. Please make sure that all elements of `return_aux` \n                            belong to the following list: {}\"\"\".format(\n                    unsupported_aux, supported_aux\n                )\n            )\n\n        outputs = self.get_calibrated_outputs(\n            params,\n            inputs_loader,\n            mutable,\n            calib_params,\n            calib_mutable,\n            distribute,\n            **kwargs\n        )\n\n        samples = self.prob_output_layer.sample(\n            n_target_samples, outputs, rng=rng, **kwargs\n        )\n        if len(return_aux) > 0:\n            return samples, dict(outputs=outputs)\n        return samples\n\n    def _batched_sample(\n        self,\n        n_target_samples: int,\n        params: Params,\n        inputs: Array,\n        mutable: Optional[Mutable] = None,\n        calib_params: Optional[CalibParams] = None,\n        calib_mutable: Optional[CalibMutable] = None,\n        return_aux: Optional[List[str]] = None,\n        rng: Optional[PRNGKeyArray] = None,\n        **kwargs\n    ) -> Union[jnp.ndarray, Tuple[jnp.ndarray, dict]]:\n        if return_aux is None:\n            return_aux = []\n        supported_aux = [\"outputs\"]\n        unsupported_aux = [s for s in return_aux if s not in supported_aux]\n        if sum(unsupported_aux) > 0:\n            raise Exception(\n                \"\"\"The auxiliary objects {} are unknown. Please make sure that all elements of `return_aux` \n                            belong to the following list: {}\"\"\".format(\n                    unsupported_aux, supported_aux\n                )\n            )\n\n        outputs = self._get_batched_calibrated_outputs(\n            params, inputs, mutable, calib_params, calib_mutable, **kwargs\n        )\n\n        samples = self.prob_output_layer.sample(\n            n_target_samples, outputs, rng=rng, **kwargs\n        )\n        if len(return_aux) > 0:\n            return samples, dict(outputs=outputs)\n        return samples\n\n    def get_calibrated_outputs(\n        self,\n        params: Params,\n        inputs_loader: InputsLoader,\n        mutable: Optional[Mutable] = None,\n        calib_params: Optional[CalibParams] = None,\n        calib_mutable: Optional[CalibMutable] = None,\n        distribute: bool = True,\n        **kwargs\n    ) -> jnp.ndarray:\n        \"\"\"\n        Compute the outputs and their calibrated version.\n\n        Parameters\n        ----------\n        params : Params\n            The random parameters of the probabilistic model.\n        inputs_loader : InputsLoader\n            A loader of input data points.\n        mutable : Optional[Mutable]\n            The mutable objects used to evaluate the models.\n        calib_params : Optional[CalibParams]\n            The calibration parameters of the probabilistic model.\n        calib_mutable : Optional[CalibMutable]\n            The calibration mutable objects used to evaluate the calibrators.\n        distribute: bool\n            Whether to distribute computation over multiple devices, if available.\n\n        Returns\n        -------\n        jnp.ndarray\n            The calibrated outputs.\n        \"\"\"\n        outputs = self.get_outputs(params, inputs_loader, mutable, distribute, **kwargs)\n\n        if self.output_calib_manager is not None:\n            outputs = self.output_calib_manager.apply(\n                params=calib_params[\"output_calibrator\"]\n                if calib_params is not None\n                else None,\n                mutable=calib_mutable[\"output_calibrator\"]\n                if calib_mutable is not None\n                else None,\n                outputs=outputs,\n                **kwargs\n            )\n        return outputs\n\n    def _get_batched_calibrated_outputs(\n        self,\n        params: Params,\n        inputs: Array,\n        mutable: Optional[Mutable] = None,\n        calib_params: Optional[CalibParams] = None,\n        calib_mutable: Optional[CalibMutable] = None,\n        **kwargs\n    ) -> jnp.ndarray:\n        outputs = self.model_manager.apply(params, inputs, mutable, **kwargs)\n\n        if self.output_calib_manager is not None:\n            outputs = self.output_calib_manager.apply(\n                params=calib_params[\"output_calibrator\"]\n                if calib_params is not None\n                else None,\n                mutable=calib_mutable[\"output_calibrator\"]\n                if calib_mutable is not None\n                else None,\n                outputs=outputs,\n                **kwargs\n            )\n        return outputs\n\n    def get_outputs(\n        self,\n        params: Params,\n        inputs_loader: InputsLoader,\n        mutable: Optional[Mutable] = None,\n        distribute: bool = True,\n        **kwargs\n    ) -> jnp.ndarray:\n        \"\"\"\n        Compute the outputs and their calibrated version.\n\n        Parameters\n        ----------\n        params : Params\n            The random parameters of the probabilistic model.\n        inputs_loader : InputsLoader\n            A loader of input data points.\n        mutable : Optional[Mutable]\n            The mutable objects used to evaluate the models.\n        distribute: bool\n            Whether to distribute computation over multiple devices, if available.\n\n        Returns\n        -------\n        jnp.ndarray\n            The calibrated outputs.\n        \"\"\"\n        if distribute and jax.local_device_count() <= 1:\n            distribute = False\n\n        if distribute:\n            inputs_loader = DeviceDimensionAugmentedInputsLoader(inputs_loader)\n\n        @jit\n        def fun(_inputs):\n            return self.model_manager.apply(params, _inputs, mutable, **kwargs)\n\n        outputs = []\n        for inputs in inputs_loader:\n            outputs.append(\n                self._unshard_array(pmap(fun)(inputs)) if distribute else fun(inputs)\n            )\n        return jnp.concatenate(outputs, 0)\n\n    def mean(\n        self,\n        params: Params,\n        inputs_loader: InputsLoader,\n        mutable: Optional[Mutable] = None,\n        calib_params: Optional[CalibParams] = None,\n        calib_mutable: Optional[CalibMutable] = None,\n        distribute: bool = True,\n        **kwargs\n    ) -> jnp.ndarray:\n        r\"\"\"\n        Estimate the likelihood mean of the target variable, that is\n\n        .. math::\n            \\mathbb{E}_{Y|w, x}[Y],\n\n        where:\n         - :math:`x` is an observed input variable;\n         - :math:`Y` is a random target variable;\n         - :math:`w` denotes the observed model parameters.\n\n        Parameters\n        ----------\n        params : Params\n            The random parameters of the probabilistic model.\n        inputs_loader : InputsLoader\n            A loader of input data points.\n        mutable : Optional[Mutable]\n            The mutable objects used to evaluate the models.\n        calib_params : Optional[CalibParams]\n            The calibration parameters of the probabilistic model.\n        calib_mutable : Optional[CalibMutable]\n            The calibration mutable objects used to evaluate the calibrators.\n        distribute: bool", "metadata": {"task_id": "awslabs_fortuna/80", "ground_truth": "            Whether to distribute computation over multiple devices, if available.", "fpath_tuple": ["awslabs_fortuna", "fortuna", "prob_model", "likelihood", "base.py"], "context_start_lineno": 273, "line_no": 499, "query_window": {"context": "            \\mathbb{E}_{Y|w, x}[Y],\n\n        where:\n         - :math:`x` is an observed input variable;\n         - :math:`Y` is a random target variable;\n         - :math:`w` denotes the observed model parameters.\n\n        Parameters\n        ----------\n        params : Params\n            The random parameters of the probabilistic model.\n        inputs_loader : InputsLoader\n            A loader of input data points.\n        mutable : Optional[Mutable]\n            The mutable objects used to evaluate the models.\n        calib_params : Optional[CalibParams]\n            The calibration parameters of the probabilistic model.\n        calib_mutable : Optional[CalibMutable]\n            The calibration mutable objects used to evaluate the calibrators.\n        distribute: bool", "metadata": {"fpath_tuple": ["awslabs_fortuna", "fortuna", "prob_model", "likelihood", "base.py"], "line_no": 499, "task_id": "awslabs_fortuna/80", "start_line_no": 479, "end_line_no": 499, "window_size": 20, "context_start_lineno": 273, "repo": "awslabs_fortuna"}}, "top_k_context": [{"context": "        .. math::\n            \\mathbb{E}_{\\tilde{Y}|w, x}[\\tilde{Y}],\n\n        where:\n         - :math:`x` is an observed input variable;\n         - :math:`\\tilde{Y}` is a one-hot encoded random target variable;\n         - :math:`w` denotes the observed model parameters.\n\n        Parameters\n        ----------\n        params : Params\n            The random parameters of the probabilistic model.\n        inputs_loader : InputsLoader\n            A loader of input data points.\n        mutable : Optional[Mutable]\n            The mutable objects used to evaluate the models.\n        calib_params : Optional[CalibParams]\n            The calibration parameters of the probabilistic model.\n        calib_mutable : Optional[CalibMutable]\n            The calibration mutable objects used to evaluate the calibrators.", "metadata": [{"fpath_tuple": ["awslabs_fortuna", "fortuna", "prob_model", "likelihood", "classification.py"], "line_no": 70, "start_line_no": 60, "end_line_no": 80, "window_size": 20, "repo": "awslabs_fortuna", "slice_size": 10}], "sim_score": 0.8695652173913043}, {"context": "            \\text{Var}_{\\tilde{Y}|w,x}[\\tilde{Y}],\n\n        where:\n         - :math:`x` is an observed input variable;\n         - :math:`\\tilde{Y}` is a one-hot encoded random target variable;\n         - :math:`w` denotes the observed model parameters.\n\n        Parameters\n        ----------\n        params : Params\n            The random parameters of the probabilistic model.\n        inputs_loader : InputsLoader\n            A loader of input data points.\n        mutable : Optional[Mutable]\n            The mutable objects used to evaluate the models.\n        calib_params : Optional[CalibParams]\n            The calibration parameters of the probabilistic model.\n        calib_mutable : Optional[CalibMutable]\n            The calibration mutable objects used to evaluate the calibrators.\n        distribute: bool", "metadata": [{"fpath_tuple": ["awslabs_fortuna", "fortuna", "prob_model", "likelihood", "classification.py"], "line_no": 150, "start_line_no": 140, "end_line_no": 160, "window_size": 20, "repo": "awslabs_fortuna", "slice_size": 10}], "sim_score": 0.8681318681318682}, {"context": "\n        .. math::\n            \\text{Var}_{\\tilde{Y}|w,x}[\\tilde{Y}],\n\n        where:\n         - :math:`x` is an observed input variable;\n         - :math:`\\tilde{Y}` is a one-hot encoded random target variable;\n         - :math:`w` denotes the observed model parameters.\n\n        Parameters\n        ----------\n        params : Params\n            The random parameters of the probabilistic model.\n        inputs_loader : InputsLoader\n            A loader of input data points.\n        mutable : Optional[Mutable]\n            The mutable objects used to evaluate the models.\n        calib_params : Optional[CalibParams]\n            The calibration parameters of the probabilistic model.\n        calib_mutable : Optional[CalibMutable]", "metadata": [{"fpath_tuple": ["awslabs_fortuna", "fortuna", "prob_model", "likelihood", "classification.py"], "line_no": 148, "start_line_no": 138, "end_line_no": 158, "window_size": 20, "repo": "awslabs_fortuna", "slice_size": 10}], "sim_score": 0.7978723404255319}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# ding/model/template/coma.py\n# --------------------------------------------------\n# \n# \n# class COMAActorNetwork(nn.Module):\n#     \"\"\"\n#     Overview:\n#         Decentralized actor network in COMA\n#     Interface:\n#         __init__, forward\n#     \"\"\"\n# \n#     def __init__(\n#         self,\n#         obs_shape: int,\n#         action_shape: int,\n#         hidden_size_list: SequenceType = [128, 128, 64],\n#     ):\n#         \"\"\"\n#         Overview:\n#             initialize COMA actor network\n#         Arguments:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# ding/model/template/qmix.py\n# --------------------------------------------------\n# class QMix(nn.Module):\n#     \"\"\"\n#     Overview:\n#         QMIX network\n#     Interface:\n#         __init__, forward, _setup_global_encoder\n#     \"\"\"\n# \n#     def __init__(\n#             self,\n#             agent_num: int,\n#             obs_shape: int,\n#             global_obs_shape: int,\n#             action_shape: int,\n#             hidden_size_list: list,\n#             mixer: bool = True,\n#             lstm_type: str = 'gru',\n#             dueling: bool = False\n#     ) -> None:\n#         \"\"\"\n# --------------------------------------------------\n# the below code fragment can be found in:\n# ding/model/template/wqmix.py\n# --------------------------------------------------\n# \n# @MODEL_REGISTRY.register('wqmix')\n# class WQMix(nn.Module):\n#     \"\"\"\n#     Overview:\n#         WQMIX network, which is same as Qmix network\n#     Interface:\n#         __init__, forward, _setup_global_encoder\n#     \"\"\"\n# \n#     def __init__(\n#             self,\n#             agent_num: int,\n#             obs_shape: int,\n#             global_obs_shape: int,\n#             action_shape: int,\n#             hidden_size_list: list,\n#             lstm_type: str = 'gru',\n#             dueling: bool = False\n#     ) -> None:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# ding/model/template/qmix.py\n# --------------------------------------------------\n# \n# @MODEL_REGISTRY.register('qmix')\n# class QMix(nn.Module):\n#     \"\"\"\n#     Overview:\n#         QMIX network\n#     Interface:\n#         __init__, forward, _setup_global_encoder\n#     \"\"\"\n# \n#     def __init__(\n#             self,\n#             agent_num: int,\n#             obs_shape: int,\n#             global_obs_shape: int,\n#             action_shape: int,\n#             hidden_size_list: list,\n#             mixer: bool = True,\n#             lstm_type: str = 'gru',\n#             dueling: bool = False\n# --------------------------------------------------\n# the below code fragment can be found in:\n# ding/model/template/qmix.py\n# --------------------------------------------------\n# \n# @MODEL_REGISTRY.register('collaq')\n# class CollaQ(nn.Module):\n#     \"\"\"\n#     Overview:\n#         CollaQ network\n#     Interface:\n#         __init__, forward, _setup_global_encoder\n#     \"\"\"\n# \n#     def __init__(\n#             self,\n#             agent_num: int,\n#             obs_shape: int,\n#             alone_obs_shape: int,\n#             global_obs_shape: int,\n#             action_shape: int,\n#             hidden_size_list: list,\n#             attention: bool = False,\n#             self_feature_range: Union[List[int], None] = None,\n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\nfrom typing import Union, List\nimport torch\nimport numpy as np\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom functools import reduce\nfrom ding.utils import list_split, squeeze, MODEL_REGISTRY\nfrom ding.torch_utils.network.nn_module import fc_block, MLP\nfrom ding.torch_utils.network.transformer import ScaledDotProductAttention\nfrom ding.torch_utils import to_tensor, tensor_to_list\nfrom .q_learning import DRQN\n\n\n@MODEL_REGISTRY.register('qtran')\nclass QTran(nn.Module):\n    \"\"\"\n    Overview:\n        QTRAN network\n    Interface:\n        __init__, forward\n    \"\"\"\n\n    def __init__(\n            self,\n            agent_num: int,\n            obs_shape: int,\n            global_obs_shape: int,\n            action_shape: int,\n            hidden_size_list: list,", "metadata": {"task_id": "opendilab_ACE/190", "ground_truth": "            embedding_size: int,", "fpath_tuple": ["opendilab_ACE", "ding", "model", "template", "qtran.py"], "context_start_lineno": 0, "line_no": 29, "query_window": {"context": "from ding.torch_utils import to_tensor, tensor_to_list\nfrom .q_learning import DRQN\n\n\n@MODEL_REGISTRY.register('qtran')\nclass QTran(nn.Module):\n    \"\"\"\n    Overview:\n        QTRAN network\n    Interface:\n        __init__, forward\n    \"\"\"\n\n    def __init__(\n            self,\n            agent_num: int,\n            obs_shape: int,\n            global_obs_shape: int,\n            action_shape: int,\n            hidden_size_list: list,", "metadata": {"fpath_tuple": ["opendilab_ACE", "ding", "model", "template", "qtran.py"], "line_no": 29, "task_id": "opendilab_ACE/190", "start_line_no": 9, "end_line_no": 29, "window_size": 20, "context_start_lineno": 0, "repo": "opendilab_ACE"}}, "top_k_context": [{"context": "        return obs\n\n\n@MODEL_REGISTRY.register('collaq')\nclass CollaQ(nn.Module):\n    \"\"\"\n    Overview:\n        CollaQ network\n    Interface:\n        __init__, forward, _setup_global_encoder\n    \"\"\"\n\n    def __init__(\n            self,\n            agent_num: int,\n            obs_shape: int,\n            alone_obs_shape: int,\n            global_obs_shape: int,\n            action_shape: int,\n            hidden_size_list: list,", "metadata": [{"fpath_tuple": ["opendilab_ACE", "ding", "model", "template", "qmix.py"], "line_no": 362, "start_line_no": 352, "end_line_no": 372, "window_size": 20, "repo": "opendilab_ACE", "slice_size": 10}], "sim_score": 0.5679012345679012}, {"context": "        return q_tot\n\n\n@MODEL_REGISTRY.register('qmix')\nclass QMix(nn.Module):\n    \"\"\"\n    Overview:\n        QMIX network\n    Interface:\n        __init__, forward, _setup_global_encoder\n    \"\"\"\n\n    def __init__(\n            self,\n            agent_num: int,\n            obs_shape: int,\n            global_obs_shape: int,\n            action_shape: int,\n            hidden_size_list: list,\n            mixer: bool = True,", "metadata": [{"fpath_tuple": ["opendilab_ACE", "ding", "model", "template", "qmix.py"], "line_no": 90, "start_line_no": 80, "end_line_no": 100, "window_size": 20, "repo": "opendilab_ACE", "slice_size": 10}], "sim_score": 0.5581395348837209}, {"context": "        return q_tot\n\n\n@MODEL_REGISTRY.register('wqmix')\nclass WQMix(nn.Module):\n    \"\"\"\n    Overview:\n        WQMIX network, which is same as Qmix network\n    Interface:\n        __init__, forward, _setup_global_encoder\n    \"\"\"\n\n    def __init__(\n            self,\n            agent_num: int,\n            obs_shape: int,\n            global_obs_shape: int,\n            action_shape: int,\n            hidden_size_list: list,\n            lstm_type: str = 'gru',", "metadata": [{"fpath_tuple": ["opendilab_ACE", "ding", "model", "template", "wqmix.py"], "line_no": 82, "start_line_no": 72, "end_line_no": 92, "window_size": 20, "repo": "opendilab_ACE", "slice_size": 10}], "sim_score": 0.5051546391752577}, {"context": "\n@MODEL_REGISTRY.register('qmix')\nclass QMix(nn.Module):\n    \"\"\"\n    Overview:\n        QMIX network\n    Interface:\n        __init__, forward, _setup_global_encoder\n    \"\"\"\n\n    def __init__(\n            self,\n            agent_num: int,\n            obs_shape: int,\n            global_obs_shape: int,\n            action_shape: int,\n            hidden_size_list: list,\n            mixer: bool = True,\n            lstm_type: str = 'gru',\n            dueling: bool = False", "metadata": [{"fpath_tuple": ["opendilab_ACE", "ding", "model", "template", "qmix.py"], "line_no": 92, "start_line_no": 82, "end_line_no": 102, "window_size": 20, "repo": "opendilab_ACE", "slice_size": 10}], "sim_score": 0.5}, {"context": "from ding.utils import squeeze, list_split, MODEL_REGISTRY, SequenceType\nfrom .q_learning import DRQN\n\n\nclass COMAActorNetwork(nn.Module):\n    \"\"\"\n    Overview:\n        Decentralized actor network in COMA\n    Interface:\n        __init__, forward\n    \"\"\"\n\n    def __init__(\n        self,\n        obs_shape: int,\n        action_shape: int,\n        hidden_size_list: SequenceType = [128, 128, 64],\n    ):\n        \"\"\"\n        Overview:", "metadata": [{"fpath_tuple": ["opendilab_ACE", "ding", "model", "template", "coma.py"], "line_no": 16, "start_line_no": 6, "end_line_no": 26, "window_size": 20, "repo": "opendilab_ACE", "slice_size": 10}], "sim_score": 0.4838709677419355}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# fortuna/calibration/calibrator.py\n# --------------------------------------------------\n#             val_losses_and_metrics_epoch_all_steps.append(\n#                 val_losses_and_metrics_current_batch\n#             )\n#         # compute validation losses and metrics for the current epoch\n#         val_losses_and_metrics_current_epoch = self.val_epoch_end(\n#             val_losses_and_metrics_epoch_all_steps, state\n#         )\n#         # logging\n#         if verbose:\n#             val_epoch_metrics_str = \" | \".join(\n#                 [\n#                     f\"{m}: {round(float(v), 5)}\"\n#                     for m, v in val_losses_and_metrics_current_epoch.items()\n#                 ]\n#             )\n#         return val_losses_and_metrics_current_epoch, val_epoch_metrics_str\n# \n#     def val_step(\n#         self,\n#         state: CalibState,\n# --------------------------------------------------\n# the below code fragment can be found in:\n# fortuna/calibration/calibrator.py\n# --------------------------------------------------\n#             )\n#         # compute validation losses and metrics for the current epoch\n#         val_losses_and_metrics_current_epoch = self.val_epoch_end(\n#             val_losses_and_metrics_epoch_all_steps, state\n#         )\n#         # logging\n#         if verbose:\n#             val_epoch_metrics_str = \" | \".join(\n#                 [\n#                     f\"{m}: {round(float(v), 5)}\"\n#                     for m, v in val_losses_and_metrics_current_epoch.items()\n#                 ]\n#             )\n#         return val_losses_and_metrics_current_epoch, val_epoch_metrics_str\n# \n#     def val_step(\n#         self,\n#         state: CalibState,\n#         batch: Batch,\n#         outputs: Array,\n# --------------------------------------------------\n# the below code fragment can be found in:\n# fortuna/calibration/calibrator.py\n# --------------------------------------------------\n#         val_losses_and_metrics_current_epoch = self.val_epoch_end(\n#             val_losses_and_metrics_epoch_all_steps, state\n#         )\n#         # logging\n#         if verbose:\n#             val_epoch_metrics_str = \" | \".join(\n#                 [\n#                     f\"{m}: {round(float(v), 5)}\"\n#                     for m, v in val_losses_and_metrics_current_epoch.items()\n#                 ]\n#             )\n#         return val_losses_and_metrics_current_epoch, val_epoch_metrics_str\n# \n#     def val_step(\n#         self,\n#         state: CalibState,\n#         batch: Batch,\n#         outputs: Array,\n#         fun: Callable,\n#         rng: PRNGKeyArray,\n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\n            Tuple[Callable[[jnp.ndarray, jnp.ndarray, Array], Array], ...]\n        ],\n        rng: PRNGKeyArray,\n        state: CalibState,\n        targets: Array,\n        outputs: Array,\n        verbose: bool,\n        progress_bar: TqdmDecorator,\n    ) -> Tuple[CalibState, Dict[str, float], str]:\n        training_losses_and_metrics_epoch_all_steps = []\n        training_batch_metrics_str = \"\"\n        # forward and backward pass\n        state, aux = self.training_step(state, targets, outputs, fun, rng)\n        # compute training losses and metrics for the current batch\n        training_losses_and_metrics_current_batch = self.training_step_end(\n            current_epoch=current_epoch,\n            state=state,\n            aux=aux,\n            targets=targets,\n            metrics=metrics,\n        )\n        # keep track of training losses and metrics [granularity=batch]\n        training_losses_and_metrics_epoch_all_steps.append(\n            training_losses_and_metrics_current_batch\n        )\n        # logging\n        if verbose:\n            training_batch_metrics_str = \" | \".join(\n                [\n                    f\"{m}: {round(float(v), 5)}\"\n                    for m, v in training_losses_and_metrics_current_batch.items()\n                ]\n            )\n            progress_bar.set_description(\n                f\"Epoch: {current_epoch + 1} | \" + training_batch_metrics_str,\n                refresh=True,\n            )\n\n        # compute training losses and metrics avg for the current epoch + other ops (if needed)\n        training_losses_and_metrics_current_epoch = self.training_epoch_end(\n            training_losses_and_metrics_epoch_all_steps\n        )\n\n        return (\n            state,\n            training_losses_and_metrics_current_epoch,\n            training_batch_metrics_str,\n        )\n\n    def training_step(\n        self,\n        state: CalibState,\n        targets: Array,\n        outputs: Array,\n        fun: Callable,\n        rng: PRNGKeyArray,\n    ) -> Tuple[CalibState, Dict[str, Any]]:\n        # ensure to use a different key at each step\n        model_key = random.fold_in(rng, state.step)\n\n        grad_fn = value_and_grad(\n            lambda params: self.training_loss_step(\n                fun, params, targets, outputs, state.mutable, model_key,\n            ),\n            has_aux=True,\n        )\n        (loss, aux), grad = grad_fn(state.params)\n        grad, loss = self.sync_gradients_and_loss(grad, loss)\n\n        state = state.apply_gradients(grads=grad, mutable=aux[\"mutable\"])\n        return (\n            state,\n            {\n                \"loss\": loss,\n                \"outputs\": aux[\"outputs\"],\n                \"logging_kwargs\": aux[\"logging_kwargs\"],\n            },\n        )\n\n    def training_loss_step(\n        self,\n        fun: Callable[[Any], Union[float, Tuple[float, dict]]],\n        params: CalibParams,\n        targets: Array,\n        outputs: Array,\n        mutable: CalibMutable,\n        rng: PRNGKeyArray,\n    ) -> Tuple[jnp.ndarray, Dict[str, Any]]:\n        return_aux = [\"outputs\"]\n        if mutable is not None:\n            return_aux += [\"mutable\"]\n        loss, aux = fun(\n            params=params,\n            targets=targets,\n            outputs=outputs,\n            mutable=mutable,\n            rng=rng,\n            return_aux=[\"outputs\", \"mutable\"],\n        )\n        loss = -loss\n        logging_kwargs = None\n        return (\n            loss,\n            {\n                \"outputs\": aux.get(\"outputs\"),\n                \"mutable\": aux.get(\"mutable\"),\n                \"logging_kwargs\": logging_kwargs,\n            },\n        )\n\n    def training_step_end(\n        self,\n        current_epoch: int,\n        state: CalibState,\n        aux: Dict[str, Any],\n        targets: Array,\n        metrics: Optional[\n            Tuple[Callable[[jnp.ndarray, jnp.ndarray, Array], Array], ...]\n        ],\n    ) -> Dict[str, jnp.ndarray]:\n        if (\n            self.save_checkpoint_dir\n            and self.save_every_n_steps\n            and current_epoch % self.save_every_n_steps == 0\n        ):\n            self.save_checkpoint(\n                state, self.save_checkpoint_dir, keep=self.keep_top_n_checkpoints\n            )\n        training_losses_and_metrics = {\"loss\": aux[\"loss\"]}\n\n        if aux[\"logging_kwargs\"] is not None:\n            for k, v in aux[\"logging_kwargs\"].items():\n                training_losses_and_metrics[k] = v\n\n        if not self.disable_training_metrics_computation and metrics is not None:\n            preds = self.predict_fn(aux[\"outputs\"])\n            uncertainties = self.uncertainty_fn(aux[\"outputs\"])\n            if self.multi_device:\n                training_batch_metrics = self.compute_metrics(\n                    preds.reshape((preds.shape[0] * preds.shape[1],) + preds.shape[2:]),\n                    uncertainties.reshape(\n                        (uncertainties.shape[0] * uncertainties.shape[1],)\n                        + uncertainties.shape[2:]\n                    ),\n                    targets.reshape(\n                        (targets.shape[0] * targets.shape[1],) + targets.shape[2:]\n                    ),\n                    metrics,\n                )\n            else:\n                training_batch_metrics = self.compute_metrics(\n                    preds, uncertainties, targets, metrics\n                )\n            for k, v in training_batch_metrics.items():\n                training_losses_and_metrics[k] = v\n        return training_losses_and_metrics\n\n    def _val_loop(\n        self,\n        fun: Callable,\n        metrics: Optional[\n            Tuple[Callable[[jnp.ndarray, jnp.ndarray, Array], Array], ...]\n        ],\n        rng: PRNGKeyArray,\n        state: CalibState,\n        targets: Array,\n        outputs: Array,\n        verbose: bool = True,\n    ) -> Tuple[Dict[str, float], str]:\n        val_losses_and_metrics_epoch_all_steps = []\n        val_epoch_metrics_str = \"\"\n        val_losses_and_metrics_current_batch = self.val_step(\n            state, targets, outputs, fun, rng, metrics,\n        )\n        val_losses_and_metrics_epoch_all_steps.append(\n            val_losses_and_metrics_current_batch\n        )\n        # compute validation losses and metrics for the current epoch\n        val_losses_and_metrics_current_epoch = self.val_epoch_end(\n            val_losses_and_metrics_epoch_all_steps, state\n        )\n        # logging\n        if verbose:\n            val_epoch_metrics_str = \" | \".join(\n                [\n                    f\"{m}: {round(float(v), 5)}\"\n                    for m, v in val_losses_and_metrics_current_epoch.items()\n                ]\n            )\n        return val_losses_and_metrics_current_epoch, val_epoch_metrics_str\n\n    def val_step(\n        self,\n        state: CalibState,\n        targets: Array,\n        outputs: Array,\n        fun: Callable,", "metadata": {"task_id": "awslabs_fortuna/9", "ground_truth": "        rng: PRNGKeyArray,", "fpath_tuple": ["awslabs_fortuna", "fortuna", "calib_model", "calib_model_calibrator.py"], "context_start_lineno": 152, "line_no": 349, "query_window": {"context": "        # compute validation losses and metrics for the current epoch\n        val_losses_and_metrics_current_epoch = self.val_epoch_end(\n            val_losses_and_metrics_epoch_all_steps, state\n        )\n        # logging\n        if verbose:\n            val_epoch_metrics_str = \" | \".join(\n                [\n                    f\"{m}: {round(float(v), 5)}\"\n                    for m, v in val_losses_and_metrics_current_epoch.items()\n                ]\n            )\n        return val_losses_and_metrics_current_epoch, val_epoch_metrics_str\n\n    def val_step(\n        self,\n        state: CalibState,\n        targets: Array,\n        outputs: Array,\n        fun: Callable,", "metadata": {"fpath_tuple": ["awslabs_fortuna", "fortuna", "calib_model", "calib_model_calibrator.py"], "line_no": 349, "task_id": "awslabs_fortuna/9", "start_line_no": 329, "end_line_no": 349, "window_size": 20, "context_start_lineno": 152, "repo": "awslabs_fortuna"}}, "top_k_context": [{"context": "            )\n        # compute validation losses and metrics for the current epoch\n        val_losses_and_metrics_current_epoch = self.val_epoch_end(\n            val_losses_and_metrics_epoch_all_steps, state\n        )\n        # logging\n        if verbose:\n            val_epoch_metrics_str = \" | \".join(\n                [\n                    f\"{m}: {round(float(v), 5)}\"\n                    for m, v in val_losses_and_metrics_current_epoch.items()\n                ]\n            )\n        return val_losses_and_metrics_current_epoch, val_epoch_metrics_str\n\n    def val_step(\n        self,\n        state: CalibState,\n        batch: Batch,\n        outputs: Array,", "metadata": [{"fpath_tuple": ["awslabs_fortuna", "fortuna", "calibration", "calibrator.py"], "line_no": 332, "start_line_no": 322, "end_line_no": 342, "window_size": 20, "repo": "awslabs_fortuna", "slice_size": 10}], "sim_score": 0.9156626506024096}, {"context": "            val_losses_and_metrics_epoch_all_steps.append(\n                val_losses_and_metrics_current_batch\n            )\n        # compute validation losses and metrics for the current epoch\n        val_losses_and_metrics_current_epoch = self.val_epoch_end(\n            val_losses_and_metrics_epoch_all_steps, state\n        )\n        # logging\n        if verbose:\n            val_epoch_metrics_str = \" | \".join(\n                [\n                    f\"{m}: {round(float(v), 5)}\"\n                    for m, v in val_losses_and_metrics_current_epoch.items()\n                ]\n            )\n        return val_losses_and_metrics_current_epoch, val_epoch_metrics_str\n\n    def val_step(\n        self,\n        state: CalibState,", "metadata": [{"fpath_tuple": ["awslabs_fortuna", "fortuna", "calibration", "calibrator.py"], "line_no": 330, "start_line_no": 320, "end_line_no": 340, "window_size": 20, "repo": "awslabs_fortuna", "slice_size": 10}], "sim_score": 0.9024390243902439}, {"context": "                state, batch, outputs, fun, rng, val_dataset_size, metrics,\n            )\n            val_losses_and_metrics_epoch_all_steps.append(\n                val_losses_and_metrics_current_batch\n            )\n        # compute validation losses and metrics for the current epoch\n        val_losses_and_metrics_current_epoch = self.val_epoch_end(\n            val_losses_and_metrics_epoch_all_steps, state\n        )\n        # logging\n        if verbose:\n            val_epoch_metrics_str = \" | \".join(\n                [\n                    f\"{m}: {round(float(v), 5)}\"\n                    for m, v in val_losses_and_metrics_current_epoch.items()\n                ]\n            )\n        return val_losses_and_metrics_current_epoch, val_epoch_metrics_str\n\n    def val_step(", "metadata": [{"fpath_tuple": ["awslabs_fortuna", "fortuna", "calibration", "calibrator.py"], "line_no": 328, "start_line_no": 318, "end_line_no": 338, "window_size": 20, "repo": "awslabs_fortuna", "slice_size": 10}], "sim_score": 0.8202247191011236}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# torchrl/modules/models/models.py\n# --------------------------------------------------\n#             >>> mlp_kwargs = {\n#             ...     \"depth\": 1,\n#             ...     \"activation_class\": nn.ELU,\n#             ...     \"num_cells\": 512,\n#             ...     \"bias_last_layer\": True,\n#             ... }\n# \n#         device (Optional[DEVICE_TYPING]): device to create the module on.\n#     \"\"\"\n# \n#     def __init__(\n#         self,\n#         out_features: int,\n#         out_features_value: int = 1,\n#         cnn_kwargs: Optional[dict] = None,\n#         mlp_kwargs: Optional[dict] = None,\n#         device: Optional[DEVICE_TYPING] = None,\n#     ):\n#         super().__init__()\n# \n# --------------------------------------------------\n# the below code fragment can be found in:\n# torchrl/modules/models/models.py\n# --------------------------------------------------\n#             ...     \"num_cells\": 512,\n#             ...     \"bias_last_layer\": True,\n#             ... }\n# \n#         device (Optional[DEVICE_TYPING]): device to create the module on.\n#     \"\"\"\n# \n#     def __init__(\n#         self,\n#         out_features: int,\n#         out_features_value: int = 1,\n#         mlp_kwargs_feature: Optional[dict] = None,\n#         mlp_kwargs_output: Optional[dict] = None,\n#         device: Optional[DEVICE_TYPING] = None,\n#     ):\n#         super().__init__()\n# \n#         mlp_kwargs_feature = (\n#             mlp_kwargs_feature if mlp_kwargs_feature is not None else {}\n#         )\n# --------------------------------------------------\n# the below code fragment can be found in:\n# torchrl/modules/models/models.py\n# --------------------------------------------------\n#             ...     \"depth\": 1,\n#             ...     \"activation_class\": nn.ELU,\n#             ...     \"num_cells\": 512,\n#             ...     \"bias_last_layer\": True,\n#             ... }\n# \n#         device (Optional[DEVICE_TYPING]): device to create the module on.\n#     \"\"\"\n# \n#     def __init__(\n#         self,\n#         out_features: int,\n#         out_features_value: int = 1,\n#         mlp_kwargs_feature: Optional[dict] = None,\n#         mlp_kwargs_output: Optional[dict] = None,\n#         device: Optional[DEVICE_TYPING] = None,\n#     ):\n#         super().__init__()\n# \n#         mlp_kwargs_feature = (\n# --------------------------------------------------\n# the below code fragment can be found in:\n# torchrl/modules/models/models.py\n# --------------------------------------------------\n#             ...     \"activation_class\": nn.ELU,\n#             ...     \"num_cells\": 512,\n#             ...     \"bias_last_layer\": True,\n#             ... }\n# \n#         device (Optional[DEVICE_TYPING]): device to create the module on.\n#     \"\"\"\n# \n#     def __init__(\n#         self,\n#         out_features: int,\n#         out_features_value: int = 1,\n#         cnn_kwargs: Optional[dict] = None,\n#         mlp_kwargs: Optional[dict] = None,\n#         device: Optional[DEVICE_TYPING] = None,\n#     ):\n#         super().__init__()\n# \n#         cnn_kwargs = cnn_kwargs if cnn_kwargs is not None else {}\n#         _cnn_kwargs = {\n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\n.convnet(observation)\n        action = self.mlp(hidden)\n        return action, hidden\n\n\nclass DdpgMlpActor(nn.Module):\n    \"\"\"DDPG Actor class.\n\n    Presented in \"CONTINUOUS CONTROL WITH DEEP REINFORCEMENT LEARNING\",\n    https://arxiv.org/pdf/1509.02971.pdf\n\n    The DDPG Actor takes as input an observation vector and returns an action from it.\n    It is trained to maximise the value returned by the DDPG Q Value network.\n\n    Args:\n        action_dim (int): length of the action vector\n        mlp_net_kwargs (dict, optional): kwargs for MLP.\n            Default: {\n            'in_features': None,\n            'out_features': action_dim,\n            'depth': 2,\n            'num_cells': [400, 300],\n            'activation_class': nn.ELU,\n            'bias_last_layer': True,\n        }\n        device (Optional[DEVICE_TYPING]): device to create the module on.\n    \"\"\"\n\n    def __init__(\n        self,\n        action_dim: int,\n        mlp_net_kwargs: Optional[dict] = None,\n        device: Optional[DEVICE_TYPING] = None,\n    ):\n        super().__init__()\n        mlp_net_default_kwargs = {\n            \"in_features\": None,\n            \"out_features\": action_dim,\n            \"depth\": 2,\n            \"num_cells\": [400, 300],\n            \"activation_class\": nn.ELU,\n            \"bias_last_layer\": True,\n        }\n        mlp_net_kwargs = mlp_net_kwargs if mlp_net_kwargs is not None else {}\n        mlp_net_default_kwargs.update(mlp_net_kwargs)\n        self.mlp = MLP(device=device, **mlp_net_default_kwargs)\n        ddpg_init_last_layer(self.mlp[-1], 6e-3, device=device)\n\n    def forward(self, observation: torch.Tensor) -> torch.Tensor:\n        action = self.mlp(observation)\n        return action\n\n\nclass DdpgCnnQNet(nn.Module):\n    \"\"\"DDPG Convolutional Q-value class.\n\n    Presented in \"CONTINUOUS CONTROL WITH DEEP REINFORCEMENT LEARNING\",\n    https://arxiv.org/pdf/1509.02971.pdf\n\n    The DDPG Q-value network takes as input an observation and an action, and returns a scalar from it.\n\n    Args:\n        conv_net_kwargs (dict, optional): kwargs for the convolutional network.\n            default: {\n            'in_features': None,\n            \"num_cells\": [32, 64, 128],\n            \"kernel_sizes\": [8, 4, 3],\n            \"strides\": [4, 2, 1],\n            \"paddings\": [0, 0, 1],\n            'activation_class': nn.ELU,\n            'norm_class': None,\n            'aggregator_class': nn.AdaptiveAvgPool2d,\n            'aggregator_kwargs': {},\n            'squeeze_output': True,\n        }\n        mlp_net_kwargs (dict, optional): kwargs for MLP.\n            Default: {\n            'in_features': None,\n            'out_features': 1,\n            'depth': 2,\n            'num_cells': 200,\n            'activation_class': nn.ELU,\n            'bias_last_layer': True,\n        }\n        use_avg_pooling (bool, optional): if True, a nn.AvgPooling layer is\n            used to aggregate the output. Default is :obj:`True`.\n        device (Optional[DEVICE_TYPING]): device to create the module on.\n    \"\"\"\n\n    def __init__(\n        self,\n        conv_net_kwargs: Optional[dict] = None,\n        mlp_net_kwargs: Optional[dict] = None,\n        use_avg_pooling: bool = True,\n        device: Optional[DEVICE_TYPING] = None,\n    ):\n        super().__init__()\n        conv_net_default_kwargs = {\n            \"in_features\": None,\n            \"num_cells\": [32, 64, 128],\n            \"kernel_sizes\": [8, 4, 3],\n            \"strides\": [4, 2, 1],\n            \"paddings\": [0, 0, 1],\n            \"activation_class\": nn.ELU,\n            \"norm_class\": None,\n            \"aggregator_class\": SquashDims\n            if not use_avg_pooling\n            else nn.AdaptiveAvgPool2d,\n            \"aggregator_kwargs\": {\"ndims_in\": 3}\n            if not use_avg_pooling\n            else {\"output_size\": (1, 1)},\n            \"squeeze_output\": use_avg_pooling,\n        }\n        conv_net_kwargs = conv_net_kwargs if conv_net_kwargs is not None else {}\n        conv_net_default_kwargs.update(conv_net_kwargs)\n        mlp_net_default_kwargs = {\n            \"in_features\": None,\n            \"out_features\": 1,\n            \"depth\": 2,\n            \"num_cells\": 200,\n            \"activation_class\": nn.ELU,\n            \"bias_last_layer\": True,\n        }\n        mlp_net_kwargs = mlp_net_kwargs if mlp_net_kwargs is not None else {}\n        mlp_net_default_kwargs.update(mlp_net_kwargs)\n        self.convnet = ConvNet(device=device, **conv_net_default_kwargs)\n        self.mlp = MLP(device=device, **mlp_net_default_kwargs)\n        ddpg_init_last_layer(self.mlp[-1], 6e-4, device=device)\n\n    def forward(self, observation: torch.Tensor, action: torch.Tensor) -> torch.Tensor:\n        hidden = torch.cat([self.convnet(observation), action], -1)\n        value = self.mlp(hidden)\n        return value\n\n\nclass DdpgMlpQNet(nn.Module):\n    \"\"\"DDPG Q-value MLP class.\n\n    Presented in \"CONTINUOUS CONTROL WITH DEEP REINFORCEMENT LEARNING\",\n    https://arxiv.org/pdf/1509.02971.pdf\n\n    The DDPG Q-value network takes as input an observation and an action, and returns a scalar from it.\n    Because actions are integrated later than observations, two networks are created.\n\n    Args:\n        mlp_net_kwargs_net1 (dict, optional): kwargs for MLP.\n            Default: {\n            'in_features': None,\n            'out_features': 400,\n            'depth': 0,\n            'num_cells': [],\n            'activation_class': nn.ELU,\n            'bias_last_layer': True,\n            'activate_last_layer': True,\n        }\n        mlp_net_kwargs_net2\n            Default: {\n            'in_features': None,\n            'out_features': 1,\n            'depth': 1,\n            'num_cells': [300, ],\n            'activation_class': nn.ELU,\n            'bias_last_layer': True,\n        }\n        device (Optional[DEVICE_TYPING]): device to create the module on.\n    \"\"\"\n\n    def __init__(\n        self,\n        mlp_net_kwargs_net1: Optional[dict] = None,\n        mlp_net_kwargs_net2: Optional[dict] = None,\n        device: Optional[DEVICE_TYPING] = None,\n    ):\n        super().__init__()\n        mlp1_net_default_kwargs = {\n            \"in_features\": None,\n            \"out_features\": 400,", "metadata": {"task_id": "pytorch_rl/180", "ground_truth": "            \"depth\": 0,", "fpath_tuple": ["pytorch_rl", "torchrl", "modules", "models", "models.py"], "context_start_lineno": 765, "line_no": 942, "query_window": {"context": "            'in_features': None,\n            'out_features': 1,\n            'depth': 1,\n            'num_cells': [300, ],\n            'activation_class': nn.ELU,\n            'bias_last_layer': True,\n        }\n        device (Optional[DEVICE_TYPING]): device to create the module on.\n    \"\"\"\n\n    def __init__(\n        self,\n        mlp_net_kwargs_net1: Optional[dict] = None,\n        mlp_net_kwargs_net2: Optional[dict] = None,\n        device: Optional[DEVICE_TYPING] = None,\n    ):\n        super().__init__()\n        mlp1_net_default_kwargs = {\n            \"in_features\": None,\n            \"out_features\": 400,", "metadata": {"fpath_tuple": ["pytorch_rl", "torchrl", "modules", "models", "models.py"], "line_no": 942, "task_id": "pytorch_rl/180", "start_line_no": 922, "end_line_no": 942, "window_size": 20, "context_start_lineno": 765, "repo": "pytorch_rl"}}, "top_k_context": [{"context": "            >>> mlp_kwargs = {\n            ...     \"depth\": 1,\n            ...     \"activation_class\": nn.ELU,\n            ...     \"num_cells\": 512,\n            ...     \"bias_last_layer\": True,\n            ... }\n\n        device (Optional[DEVICE_TYPING]): device to create the module on.\n    \"\"\"\n\n    def __init__(\n        self,\n        out_features: int,\n        out_features_value: int = 1,\n        cnn_kwargs: Optional[dict] = None,\n        mlp_kwargs: Optional[dict] = None,\n        device: Optional[DEVICE_TYPING] = None,\n    ):\n        super().__init__()\n", "metadata": [{"fpath_tuple": ["pytorch_rl", "torchrl", "modules", "models", "models.py"], "line_no": 588, "start_line_no": 578, "end_line_no": 598, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}], "sim_score": 0.7529411764705882}, {"context": "\n            >>> mlp_kwargs_output = {\n            ...     \"depth\": 1,\n            ...     \"activation_class\": nn.ELU,\n            ...     \"num_cells\": 512,\n            ...     \"bias_last_layer\": True,\n            ... }\n\n        device (Optional[DEVICE_TYPING]): device to create the module on.\n    \"\"\"\n\n    def __init__(\n        self,\n        out_features: int,\n        out_features_value: int = 1,\n        mlp_kwargs_feature: Optional[dict] = None,\n        mlp_kwargs_output: Optional[dict] = None,\n        device: Optional[DEVICE_TYPING] = None,\n    ):\n        super().__init__()", "metadata": [{"fpath_tuple": ["pytorch_rl", "torchrl", "modules", "models", "models.py"], "line_no": 506, "start_line_no": 496, "end_line_no": 516, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}], "sim_score": 0.7529411764705882}, {"context": "            ...     \"depth\": 1,\n            ...     \"activation_class\": nn.ELU,\n            ...     \"num_cells\": 512,\n            ...     \"bias_last_layer\": True,\n            ... }\n\n        device (Optional[DEVICE_TYPING]): device to create the module on.\n    \"\"\"\n\n    def __init__(\n        self,\n        out_features: int,\n        out_features_value: int = 1,\n        mlp_kwargs_feature: Optional[dict] = None,\n        mlp_kwargs_output: Optional[dict] = None,\n        device: Optional[DEVICE_TYPING] = None,\n    ):\n        super().__init__()\n\n        mlp_kwargs_feature = (", "metadata": [{"fpath_tuple": ["pytorch_rl", "torchrl", "modules", "models", "models.py"], "line_no": 508, "start_line_no": 498, "end_line_no": 518, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}], "sim_score": 0.75}, {"context": "            Default is\n\n            >>> mlp_kwargs = {\n            ...     \"depth\": 1,\n            ...     \"activation_class\": nn.ELU,\n            ...     \"num_cells\": 512,\n            ...     \"bias_last_layer\": True,\n            ... }\n\n        device (Optional[DEVICE_TYPING]): device to create the module on.\n    \"\"\"\n\n    def __init__(\n        self,\n        out_features: int,\n        out_features_value: int = 1,\n        cnn_kwargs: Optional[dict] = None,\n        mlp_kwargs: Optional[dict] = None,\n        device: Optional[DEVICE_TYPING] = None,\n    ):", "metadata": [{"fpath_tuple": ["pytorch_rl", "torchrl", "modules", "models", "models.py"], "line_no": 586, "start_line_no": 576, "end_line_no": 596, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}], "sim_score": 0.7011494252873564}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# examples/community/lpw_stable_diffusion_onnx.py\n# --------------------------------------------------\n#         r\"\"\"\n#         Function for inpaint.\n#         Args:\n#             image (`np.ndarray` or `PIL.Image.Image`):\n#                 `Image`, or tensor representing an image batch, that will be used as the starting point for the\n#                 process. This is the image whose masked region will be inpainted.\n#             mask_image (`np.ndarray` or `PIL.Image.Image`):\n#                 `Image`, or tensor representing an image batch, to mask `image`. White pixels in the mask will be\n#                 replaced by noise and therefore repainted, while black pixels will be preserved. If `mask_image` is a\n#                 PIL image, it will be converted to a single channel (luminance) before use. If it's a tensor, it should\n#                 contain one color channel (L) instead of 3, so the expected shape would be `(B, H, W, 1)`.\n#             prompt (`str` or `List[str]`):\n#                 The prompt or prompts to guide the image generation.\n#             negative_prompt (`str` or `List[str]`, *optional*):\n#                 The prompt or prompts not to guide the image generation. Ignored when not using guidance (i.e., ignored\n#                 if `guidance_scale` is less than `1`).\n#             strength (`float`, *optional*, defaults to 0.8):\n#                 Conceptually, indicates how much to inpaint the masked area. Must be between 0 and 1. When `strength`\n#                 is 1, the denoising process will be run on the masked area for the full number of iterations specified\n#                 in `num_inference_steps`. `image` will be used as a reference for the masked area, adding more\n# --------------------------------------------------\n# the below code fragment can be found in:\n# examples/community/lpw_stable_diffusion_onnx.py\n# --------------------------------------------------\n#         Args:\n#             image (`np.ndarray` or `PIL.Image.Image`):\n#                 `Image`, or tensor representing an image batch, that will be used as the starting point for the\n#                 process. This is the image whose masked region will be inpainted.\n#             mask_image (`np.ndarray` or `PIL.Image.Image`):\n#                 `Image`, or tensor representing an image batch, to mask `image`. White pixels in the mask will be\n#                 replaced by noise and therefore repainted, while black pixels will be preserved. If `mask_image` is a\n#                 PIL image, it will be converted to a single channel (luminance) before use. If it's a tensor, it should\n#                 contain one color channel (L) instead of 3, so the expected shape would be `(B, H, W, 1)`.\n#             prompt (`str` or `List[str]`):\n#                 The prompt or prompts to guide the image generation.\n#             negative_prompt (`str` or `List[str]`, *optional*):\n#                 The prompt or prompts not to guide the image generation. Ignored when not using guidance (i.e., ignored\n#                 if `guidance_scale` is less than `1`).\n#             strength (`float`, *optional*, defaults to 0.8):\n#                 Conceptually, indicates how much to inpaint the masked area. Must be between 0 and 1. When `strength`\n#                 is 1, the denoising process will be run on the masked area for the full number of iterations specified\n#                 in `num_inference_steps`. `image` will be used as a reference for the masked area, adding more\n#                 noise to that region the larger the `strength`. If `strength` is 0, no inpainting will occur.\n#             num_inference_steps (`int`, *optional*, defaults to 50):\n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\n ):\n        r\"\"\"\n        Function for image-to-image generation.\n        Args:\n            image (`torch.FloatTensor` or `PIL.Image.Image`):\n                `Image`, or tensor representing an image batch, that will be used as the starting point for the\n                process.\n            prompt (`str` or `List[str]`):\n                The prompt or prompts to guide the image generation.\n            negative_prompt (`str` or `List[str]`, *optional*):\n                The prompt or prompts not to guide the image generation. Ignored when not using guidance (i.e., ignored\n                if `guidance_scale` is less than `1`).\n            strength (`float`, *optional*, defaults to 0.8):\n                Conceptually, indicates how much to transform the reference `image`. Must be between 0 and 1.\n                `image` will be used as a starting point, adding more noise to it the larger the `strength`. The\n                number of denoising steps depends on the amount of noise initially added. When `strength` is 1, added\n                noise will be maximum and the denoising process will run for the full number of iterations specified in\n                `num_inference_steps`. A value of 1, therefore, essentially ignores `image`.\n            num_inference_steps (`int`, *optional*, defaults to 50):\n                The number of denoising steps. More denoising steps usually lead to a higher quality image at the\n                expense of slower inference. This parameter will be modulated by `strength`.\n            guidance_scale (`float`, *optional*, defaults to 7.5):\n                Guidance scale as defined in [Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598).\n                `guidance_scale` is defined as `w` of equation 2. of [Imagen\n                Paper](https://arxiv.org/pdf/2205.11487.pdf). Guidance scale is enabled by setting `guidance_scale >\n                1`. Higher guidance scale encourages to generate images that are closely linked to the text `prompt`,\n                usually at the expense of lower image quality.\n            num_images_per_prompt (`int`, *optional*, defaults to 1):\n                The number of images to generate per prompt.\n            eta (`float`, *optional*, defaults to 0.0):\n                Corresponds to parameter eta (\u03b7) in the DDIM paper: https://arxiv.org/abs/2010.02502. Only applies to\n                [`schedulers.DDIMScheduler`], will be ignored for others.\n            generator (`torch.Generator`, *optional*):\n                A [torch generator](https://pytorch.org/docs/stable/generated/torch.Generator.html) to make generation\n                deterministic.\n            max_embeddings_multiples (`int`, *optional*, defaults to `3`):\n                The max multiple length of prompt embeddings compared to the max output length of text encoder.\n            output_type (`str`, *optional*, defaults to `\"pil\"`):\n                The output format of the generate image. Choose between\n                [PIL](https://pillow.readthedocs.io/en/stable/): `PIL.Image.Image` or `np.array`.\n            return_dict (`bool`, *optional*, defaults to `True`):\n                Whether or not to return a [`~pipelines.stable_diffusion.StableDiffusionPipelineOutput`] instead of a\n                plain tuple.\n            callback (`Callable`, *optional*):\n                A function that will be called every `callback_steps` steps during inference. The function will be\n                called with the following arguments: `callback(step: int, timestep: int, latents: torch.FloatTensor)`.\n            is_cancelled_callback (`Callable`, *optional*):\n                A function that will be called every `callback_steps` steps during inference. If the function returns\n                `True`, the inference will be cancelled.\n            callback_steps (`int`, *optional*, defaults to 1):\n                The frequency at which the `callback` function will be called. If not specified, the callback will be\n                called at every step.\n        Returns:\n            [`~pipelines.stable_diffusion.StableDiffusionPipelineOutput`] or `tuple`:\n            [`~pipelines.stable_diffusion.StableDiffusionPipelineOutput`] if `return_dict` is True, otherwise a `tuple.\n            When returning a tuple, the first element is a list with the generated images, and the second element is a\n            list of `bool`s denoting whether the corresponding generated image likely represents \"not-safe-for-work\"\n            (nsfw) content, according to the `safety_checker`.\n        \"\"\"\n        return self.__call__(\n            prompt=prompt,\n            negative_prompt=negative_prompt,\n            image=image,\n            num_inference_steps=num_inference_steps,\n            guidance_scale=guidance_scale,\n            strength=strength,\n            num_images_per_prompt=num_images_per_prompt,\n            eta=eta,\n            generator=generator,\n            max_embeddings_multiples=max_embeddings_multiples,\n            output_type=output_type,\n            return_dict=return_dict,\n            callback=callback,\n            is_cancelled_callback=is_cancelled_callback,\n            callback_steps=callback_steps,\n            **kwargs,\n        )\n\n    def inpaint(\n        self,\n        image: Union[torch.FloatTensor, PIL.Image.Image],\n        mask_image: Union[torch.FloatTensor, PIL.Image.Image],\n        prompt: Union[str, List[str]],\n        negative_prompt: Optional[Union[str, List[str]]] = None,\n        strength: float = 0.8,\n        num_inference_steps: Optional[int] = 50,\n        guidance_scale: Optional[float] = 7.5,\n        num_images_per_prompt: Optional[int] = 1,\n        eta: Optional[float] = 0.0,\n        generator: Optional[torch.Generator] = None,\n        max_embeddings_multiples: Optional[int] = 3,\n        output_type: Optional[str] = \"pil\",\n        return_dict: bool = True,\n        callback: Optional[Callable[[int, int, torch.FloatTensor], None]] = None,\n        is_cancelled_callback: Optional[Callable[[], bool]] = None,\n        callback_steps: Optional[int] = 1,\n        **kwargs,\n    ):\n        r\"\"\"\n        Function for inpaint.\n        Args:\n            image (`torch.FloatTensor` or `PIL.Image.Image`):\n                `Image`, or tensor representing an image batch, that will be used as the starting point for the\n                process. This is the image whose masked region will be inpainted.\n            mask_image (`torch.FloatTensor` or `PIL.Image.Image`):\n                `Image`, or tensor representing an image batch, to mask `image`. White pixels in the mask will be\n                replaced by noise and therefore repainted, while black pixels will be preserved. If `mask_image` is a\n                PIL image, it will be converted to a single channel (luminance) before use. If it's a tensor, it should\n                contain one color channel (L) instead of 3, so the expected shape would be `(B, H, W, 1)`.\n            prompt (`str` or `List[str]`):\n                The prompt or prompts to guide the image generation.\n            negative_prompt (`str` or `List[str]`, *optional*):\n                The prompt or prompts not to guide the image generation. Ignored when not using guidance (i.e., ignored\n                if `guidance_scale` is less than `1`).\n            strength (`float`, *optional*, defaults to 0.8):\n                Conceptually, indicates how much to inpaint the masked area. Must be between 0 and 1. When `strength`\n                is 1, the denoising process will be run on the masked area for the full number of iterations specified\n                in `num_inference_steps`. `image` will be used as a reference for the masked area, adding more", "metadata": {"task_id": "huggingface_diffusers/18", "ground_truth": "                noise to that region the larger the `strength`. If `strength` is 0, no inpainting will occur.", "fpath_tuple": ["huggingface_diffusers", "examples", "community", "lpw_stable_diffusion.py"], "context_start_lineno": 983, "line_no": 1101, "query_window": {"context": "        r\"\"\"\n        Function for inpaint.\n        Args:\n            image (`torch.FloatTensor` or `PIL.Image.Image`):\n                `Image`, or tensor representing an image batch, that will be used as the starting point for the\n                process. This is the image whose masked region will be inpainted.\n            mask_image (`torch.FloatTensor` or `PIL.Image.Image`):\n                `Image`, or tensor representing an image batch, to mask `image`. White pixels in the mask will be\n                replaced by noise and therefore repainted, while black pixels will be preserved. If `mask_image` is a\n                PIL image, it will be converted to a single channel (luminance) before use. If it's a tensor, it should\n                contain one color channel (L) instead of 3, so the expected shape would be `(B, H, W, 1)`.\n            prompt (`str` or `List[str]`):\n                The prompt or prompts to guide the image generation.\n            negative_prompt (`str` or `List[str]`, *optional*):\n                The prompt or prompts not to guide the image generation. Ignored when not using guidance (i.e., ignored\n                if `guidance_scale` is less than `1`).\n            strength (`float`, *optional*, defaults to 0.8):\n                Conceptually, indicates how much to inpaint the masked area. Must be between 0 and 1. When `strength`\n                is 1, the denoising process will be run on the masked area for the full number of iterations specified\n                in `num_inference_steps`. `image` will be used as a reference for the masked area, adding more", "metadata": {"fpath_tuple": ["huggingface_diffusers", "examples", "community", "lpw_stable_diffusion.py"], "line_no": 1101, "task_id": "huggingface_diffusers/18", "start_line_no": 1081, "end_line_no": 1101, "window_size": 20, "context_start_lineno": 983, "repo": "huggingface_diffusers"}}, "top_k_context": [{"context": "        r\"\"\"\n        Function for inpaint.\n        Args:\n            image (`np.ndarray` or `PIL.Image.Image`):\n                `Image`, or tensor representing an image batch, that will be used as the starting point for the\n                process. This is the image whose masked region will be inpainted.\n            mask_image (`np.ndarray` or `PIL.Image.Image`):\n                `Image`, or tensor representing an image batch, to mask `image`. White pixels in the mask will be\n                replaced by noise and therefore repainted, while black pixels will be preserved. If `mask_image` is a\n                PIL image, it will be converted to a single channel (luminance) before use. If it's a tensor, it should\n                contain one color channel (L) instead of 3, so the expected shape would be `(B, H, W, 1)`.\n            prompt (`str` or `List[str]`):\n                The prompt or prompts to guide the image generation.\n            negative_prompt (`str` or `List[str]`, *optional*):\n                The prompt or prompts not to guide the image generation. Ignored when not using guidance (i.e., ignored\n                if `guidance_scale` is less than `1`).\n            strength (`float`, *optional*, defaults to 0.8):\n                Conceptually, indicates how much to inpaint the masked area. Must be between 0 and 1. When `strength`\n                is 1, the denoising process will be run on the masked area for the full number of iterations specified\n                in `num_inference_steps`. `image` will be used as a reference for the masked area, adding more", "metadata": [{"fpath_tuple": ["huggingface_diffusers", "examples", "community", "lpw_stable_diffusion_onnx.py"], "line_no": 1080, "start_line_no": 1070, "end_line_no": 1090, "window_size": 20, "repo": "huggingface_diffusers", "slice_size": 10}], "sim_score": 0.9523809523809523}, {"context": "        **kwargs,\n    ):\n        r\"\"\"\n        Function for inpaint.\n        Args:\n            image (`np.ndarray` or `PIL.Image.Image`):\n                `Image`, or tensor representing an image batch, that will be used as the starting point for the\n                process. This is the image whose masked region will be inpainted.\n            mask_image (`np.ndarray` or `PIL.Image.Image`):\n                `Image`, or tensor representing an image batch, to mask `image`. White pixels in the mask will be\n                replaced by noise and therefore repainted, while black pixels will be preserved. If `mask_image` is a\n                PIL image, it will be converted to a single channel (luminance) before use. If it's a tensor, it should\n                contain one color channel (L) instead of 3, so the expected shape would be `(B, H, W, 1)`.\n            prompt (`str` or `List[str]`):\n                The prompt or prompts to guide the image generation.\n            negative_prompt (`str` or `List[str]`, *optional*):\n                The prompt or prompts not to guide the image generation. Ignored when not using guidance (i.e., ignored\n                if `guidance_scale` is less than `1`).\n            strength (`float`, *optional*, defaults to 0.8):\n                Conceptually, indicates how much to inpaint the masked area. Must be between 0 and 1. When `strength`", "metadata": [{"fpath_tuple": ["huggingface_diffusers", "examples", "community", "lpw_stable_diffusion_onnx.py"], "line_no": 1078, "start_line_no": 1068, "end_line_no": 1088, "window_size": 20, "repo": "huggingface_diffusers", "slice_size": 10}], "sim_score": 0.8323699421965318}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# vizier/_src/pythia/policy.py\n# --------------------------------------------------\n#       default=None,\n#       validator=attr.validators.optional(attr.validators.instance_of(str)))\n# \n#   @property\n#   def study_guid(self) -> str:\n#     return self._study_descriptor.guid\n# \n#   @property\n#   def study_config(self) -> vz.ProblemStatement:\n#     return self._study_descriptor.config\n# \n#   @property\n#   def max_trial_id(self) -> int:\n#     return self._study_descriptor.max_trial_id\n# \n# \n# @attr.define(init=True)\n# class SuggestDecision:\n#   \"\"\"This is the output of the Policy.suggestion() method.\n# \n# --------------------------------------------------\n# the below code fragment can be found in:\n# vizier/_src/pyvizier/shared/parameter_config.py\n# --------------------------------------------------\n#   \"\"\"Holds a reference to (sub) spaces.\"\"\"\n# \n#   # Selected (sub)-spaces.\n#   # TODO: Consider switching the order of SearchSpaceSelector and\n#   # SearchSpace.\n#   _selected: tuple['SearchSpace'] = attr.field(init=True)\n# \n#   def __len__(self) -> int:\n#     return len(self._selected)\n# \n#   def __init__(self, selected: SearchSpaceOrSpaces):\n#     if isinstance(selected, Collection):\n#       self.__attrs_init__(tuple(selected))\n#     else:\n#       self.__attrs_init__(tuple([selected]))\n# \n#   def add_float_param(self,\n#                       name: str,\n#                       min_value: float,\n#                       max_value: float,\n# --------------------------------------------------\n# the below code fragment can be found in:\n# vizier/_src/pyvizier/shared/parameter_config.py\n# --------------------------------------------------\n# \n# \n# ParameterConfigOrConfigs = Union[ParameterConfig, Collection[ParameterConfig]]\n# \n# \n# @attr.define(init=False)\n# class ParameterConfigSelector(Sized):\n#   \"\"\"Holds a reference to ParameterConfigs.\"\"\"\n# \n#   # Selected configs.\n#   _selected: tuple[ParameterConfig] = attr.field(init=True)\n# \n#   def __len__(self) -> int:\n#     return len(self._selected)\n# \n#   def __init__(self, selected: ParameterConfigOrConfigs):\n#     if isinstance(selected, Collection):\n#       self.__attrs_init__(tuple(selected))\n#     else:\n#       self.__attrs_init__(tuple([selected]))\n# --------------------------------------------------\n# the below code fragment can be found in:\n# vizier/_src/pyvizier/shared/parameter_config.py\n# --------------------------------------------------\n# ParameterConfigOrConfigs = Union[ParameterConfig, Collection[ParameterConfig]]\n# \n# \n# @attr.define(init=False)\n# class ParameterConfigSelector(Sized):\n#   \"\"\"Holds a reference to ParameterConfigs.\"\"\"\n# \n#   # Selected configs.\n#   _selected: tuple[ParameterConfig] = attr.field(init=True)\n# \n#   def __len__(self) -> int:\n#     return len(self._selected)\n# \n#   def __init__(self, selected: ParameterConfigOrConfigs):\n#     if isinstance(selected, Collection):\n#       self.__attrs_init__(tuple(selected))\n#     else:\n#       self.__attrs_init__(tuple([selected]))\n# \n#   def select_values(self,\n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\n# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import annotations\n\n\"\"\"HPO-B dataset.\n\nNote that we denote (X,Y) as a batched set of trials (i.e. suggestions X and\nobjectives Y) and (x,y) as a single trial. This is slightly different from (X,y)\nnotation used in the handler to denote batched trials.\n\"\"\"\n# TODO: Replace internal HPOB experimenter with this.\n# pylint:disable=invalid-name\nimport copy\nimport enum\nimport json\nfrom typing import Any, Dict, Iterable, List, Optional, Sequence, Tuple\n\nimport attr\nimport attrs\nimport numpy as np\n\nfrom vizier import pyvizier as vz\nfrom vizier._src.benchmarks.experimenters import experimenter\nfrom vizier._src.benchmarks.experimenters.hpob import handler as handler_lib\n\nimport xgboost as xgb\n\nOpen = open\n\nMETRIC_NAME = 'objective_value'\n# Offset applied to parameter values before the log transformation.\n_TF_OFFSET = 1e-4\n\n\n@attrs.define(auto_attribs=True)\nclass _Dataset:\n  \"\"\"Raw data from HPO-B.\n\n  X and Y are guaranteed to have compatible shapes. A dataset can be sliced like\n  regular numpy arrays but but cannot be indexed at a single point, i.e.\n    `dataset[0]` is not allowed\n    `dataset[0:1]` is allowed\n    `dataset[dataset.Y > 0]` is allowed.\n\n  If the log-transformation is applied to a feature, it's offset by a constant\n    x_log = np.log(x+0.0001)\n\n  Attributes:\n    X: 2-D array of shape (number of observations) * (number of input features).\n      The features may be scaled and log-transformed. _SearchspaceDescriptor\n      holds the necessary information to recover the original values.\n    Y: 2-D array of objective values, of shape (number of observations, 1). The\n      values are not pre-processed.\n  \"\"\"\n\n  X: np.ndarray = attrs.field(converter=np.asarray)\n  Y: np.ndarray = attrs.field(converter=np.asarray)\n\n  def __attrs_post_init__(self) -> None:\n    \"\"\"Performs validation.\"\"\"\n    if len(self.X.shape) != 2:\n      raise ValueError(f'X must be 2-D. Given: {self.X.shape}')\n    if len(self.Y.shape) != 2:\n      raise ValueError(f'Y must be 2-D. Given: {self.Y.shape}')\n\n    if self.X.shape[0] != self.Y.shape[0]:\n      raise ValueError(f'X and y must have same number of rows. '\n                       f'X.shape={self.X.shape}, y.shape={self.Y.shape}')\n\n  def __getitem__(self, idx: slice) -> '_Dataset':\n    return _Dataset(self.X[idx], self.Y[idx])\n\n  def __len__(self) -> int:\n    return self.Y.shape[0]\n\n\n@attr.define(init=True, kw_only=False)\nclass _VariableDescriptor:\n  \"\"\"Variable descriptor.\"\"\"\n  name: str = attrs.field()\n  min_value: Optional[float] = attrs.field(default=None)\n  max_value: Optional[float] = attrs.field(default=None)\n  min_value_before_tf: Optional[float] = attrs.field(default=None)\n  max_value_before_tf: Optional[float] = attrs.field(default=None)\n  apply_log: bool = attrs.field(default=True)\n  optional: bool = attrs.field(default=False)\n  categories: List[str] = attrs.field(factory=list)\n\n  def scale(self, x: np.ndarray) -> np.ndarray:\n    # Raw numbers to HPOB scale. Input and output are 1-D\n    if self.is_categorical:\n      return x\n    if self.apply_log:\n      x = np.log(x)", "metadata": {"task_id": "google_vizier/143", "ground_truth": "    x = (x - self.min_value) / (self.max_value - self.min_value)", "fpath_tuple": ["google_vizier", "vizier", "_src", "benchmarks", "experimenters", "hpob_experimenter.py"], "context_start_lineno": 0, "line_no": 106, "query_window": {"context": "\n\n@attr.define(init=True, kw_only=False)\nclass _VariableDescriptor:\n  \"\"\"Variable descriptor.\"\"\"\n  name: str = attrs.field()\n  min_value: Optional[float] = attrs.field(default=None)\n  max_value: Optional[float] = attrs.field(default=None)\n  min_value_before_tf: Optional[float] = attrs.field(default=None)\n  max_value_before_tf: Optional[float] = attrs.field(default=None)\n  apply_log: bool = attrs.field(default=True)\n  optional: bool = attrs.field(default=False)\n  categories: List[str] = attrs.field(factory=list)\n\n  def scale(self, x: np.ndarray) -> np.ndarray:\n    # Raw numbers to HPOB scale. Input and output are 1-D\n    if self.is_categorical:\n      return x\n    if self.apply_log:\n      x = np.log(x)", "metadata": {"fpath_tuple": ["google_vizier", "vizier", "_src", "benchmarks", "experimenters", "hpob_experimenter.py"], "line_no": 106, "task_id": "google_vizier/143", "start_line_no": 86, "end_line_no": 106, "window_size": 20, "context_start_lineno": 0, "repo": "google_vizier"}}, "top_k_context": [{"context": "\n\nParameterConfigOrConfigs = Union[ParameterConfig, Collection[ParameterConfig]]\n\n\n@attr.define(init=False)\nclass ParameterConfigSelector(Sized):\n  \"\"\"Holds a reference to ParameterConfigs.\"\"\"\n\n  # Selected configs.\n  _selected: tuple[ParameterConfig] = attr.field(init=True)\n\n  def __len__(self) -> int:\n    return len(self._selected)\n\n  def __init__(self, selected: ParameterConfigOrConfigs):\n    if isinstance(selected, Collection):\n      self.__attrs_init__(tuple(selected))\n    else:\n      self.__attrs_init__(tuple([selected]))", "metadata": [{"fpath_tuple": ["google_vizier", "vizier", "_src", "pyvizier", "shared", "parameter_config.py"], "line_no": 636, "start_line_no": 626, "end_line_no": 646, "window_size": 20, "repo": "google_vizier", "slice_size": 10}], "sim_score": 0.30158730158730157}, {"context": "      self._children[value] = SearchSpace(parent_values=[value])\n    return self._children[value]\n\n\nParameterConfigOrConfigs = Union[ParameterConfig, Collection[ParameterConfig]]\n\n\n@attr.define(init=False)\nclass ParameterConfigSelector(Sized):\n  \"\"\"Holds a reference to ParameterConfigs.\"\"\"\n\n  # Selected configs.\n  _selected: tuple[ParameterConfig] = attr.field(init=True)\n\n  def __len__(self) -> int:\n    return len(self._selected)\n\n  def __init__(self, selected: ParameterConfigOrConfigs):\n    if isinstance(selected, Collection):\n      self.__attrs_init__(tuple(selected))", "metadata": [{"fpath_tuple": ["google_vizier", "vizier", "_src", "pyvizier", "shared", "parameter_config.py"], "line_no": 634, "start_line_no": 624, "end_line_no": 644, "window_size": 20, "repo": "google_vizier", "slice_size": 10}], "sim_score": 0.29770992366412213}, {"context": "@attr.define(init=False)\nclass SearchSpaceSelector:\n  \"\"\"Holds a reference to (sub) spaces.\"\"\"\n\n  # Selected (sub)-spaces.\n  # TODO: Consider switching the order of SearchSpaceSelector and\n  # SearchSpace.\n  _selected: tuple['SearchSpace'] = attr.field(init=True)\n\n  def __len__(self) -> int:\n    return len(self._selected)\n\n  def __init__(self, selected: SearchSpaceOrSpaces):\n    if isinstance(selected, Collection):\n      self.__attrs_init__(tuple(selected))\n    else:\n      self.__attrs_init__(tuple([selected]))\n\n  def add_float_param(self,\n                      name: str,", "metadata": [{"fpath_tuple": ["google_vizier", "vizier", "_src", "pyvizier", "shared", "parameter_config.py"], "line_no": 684, "start_line_no": 674, "end_line_no": 694, "window_size": 20, "repo": "google_vizier", "slice_size": 10}], "sim_score": 0.2971014492753623}, {"context": "\n  checkpoint_dir: Optional[str] = attr.field(\n      default=None,\n      validator=attr.validators.optional(attr.validators.instance_of(str)))\n\n  @property\n  def study_guid(self) -> str:\n    return self._study_descriptor.guid\n\n  @property\n  def study_config(self) -> vz.ProblemStatement:\n    return self._study_descriptor.config\n\n  @property\n  def max_trial_id(self) -> int:\n    return self._study_descriptor.max_trial_id\n\n\n@attr.define(init=True)\nclass SuggestDecision:", "metadata": [{"fpath_tuple": ["google_vizier", "vizier", "_src", "pythia", "policy.py"], "line_no": 124, "start_line_no": 114, "end_line_no": 134, "window_size": 20, "repo": "google_vizier", "slice_size": 10}], "sim_score": 0.2916666666666667}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# torchrl/modules/planners/cem.py\n# --------------------------------------------------\n# \n#     A call to the module returns the actions that empirically maximised the\n#     returns given a planning horizon\n# \n#     Args:\n#         env (EnvBase): The environment to perform the planning step on (can be\n#             `ModelBasedEnv` or :obj:`EnvBase`).\n#         planning_horizon (int): The length of the simulated trajectories\n#         optim_steps (int): The number of optimization steps used by the MPC\n#             planner\n#         num_candidates (int): The number of candidates to sample from the\n#             Gaussian distributions.\n#         top_k (int): The number of top candidates to use to\n#             update the mean and standard deviation of the Gaussian distribution.\n#         reward_key (str, optional): The key in the TensorDict to use to\n#             retrieve the reward. Defaults to \"reward\".\n#         action_key (str, optional): The key in the TensorDict to use to store\n#             the action. Defaults to \"action\"\n# \n#     Examples:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# torchrl/modules/planners/cem.py\n# torchrl/modules/planners/mppi.py\n# --------------------------------------------------\n#     returns given a planning horizon\n# \n#     Args:\n#         env (EnvBase): The environment to perform the planning step on (can be\n#             `ModelBasedEnv` or :obj:`EnvBase`).\n#         planning_horizon (int): The length of the simulated trajectories\n#         optim_steps (int): The number of optimization steps used by the MPC\n#             planner\n#         num_candidates (int): The number of candidates to sample from the\n#             Gaussian distributions.\n#         top_k (int): The number of top candidates to use to\n#             update the mean and standard deviation of the Gaussian distribution.\n#         reward_key (str, optional): The key in the TensorDict to use to\n#             retrieve the reward. Defaults to \"reward\".\n#         action_key (str, optional): The key in the TensorDict to use to store\n#             the action. Defaults to \"action\"\n# \n#     Examples:\n#         >>> from tensordict import TensorDict\n#         >>> from torchrl.data import CompositeSpec, UnboundedContinuousTensorSpec\n# --------------------------------------------------\n# the below code fragment can be found in:\n# torchrl/modules/planners/mppi.py\n# --------------------------------------------------\n# \n#     A call to the module returns the actions that empirically maximised the\n#     returns given a planning horizon\n# \n#     Args:\n#         env (EnvBase): The environment to perform the planning step on (can be\n#             `ModelBasedEnv` or :obj:`EnvBase`).\n#         planning_horizon (int): The length of the simulated trajectories\n#         optim_steps (int): The number of optimization steps used by the MPC\n#             planner\n#         num_candidates (int): The number of candidates to sample from the\n#             Gaussian distributions.\n#         top_k (int): The number of top candidates to use to\n#             update the mean and standard deviation of the Gaussian distribution.\n#         reward_key (str, optional): The key in the TensorDict to use to\n#             retrieve the reward. Defaults to \"reward\".\n#         action_key (str, optional): The key in the TensorDict to use to store\n#             the action. Defaults to \"action\"\n# \n#     Examples:\n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\n# Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\nimport abc\nfrom typing import Optional\n\nimport torch\nfrom tensordict.tensordict import TensorDictBase\n\nfrom torchrl.envs import EnvBase\nfrom torchrl.modules import SafeModule\n\n\nclass MPCPlannerBase(SafeModule, metaclass=abc.ABCMeta):\n    \"\"\"MPCPlannerBase abstract Module.\n\n    This class inherits from :obj:`SafeModule`. Provided a :obj:`TensorDict`, this module will perform a Model Predictive Control (MPC) planning step.\n    At the end of the planning step, the :obj:`MPCPlanner` will return a proposed action.\n\n    Args:\n        env (EnvBase): The environment to perform the planning step on (Can be :obj:`ModelBasedEnvBase` or :obj:`EnvBase`).\n        action_key (str, optional): The key that will point to the computed action.\n    \"\"\"\n\n    def __init__(\n        self,\n        env: EnvBase,\n        action_key: str = \"action\",\n    ):\n        # Check if env is stateless\n        if env.batch_locked:", "metadata": {"task_id": "pytorch_rl/118", "ground_truth": "            raise ValueError(", "fpath_tuple": ["pytorch_rl", "torchrl", "modules", "planners", "common.py"], "context_start_lineno": 0, "line_no": 32, "query_window": {"context": "\n\nclass MPCPlannerBase(SafeModule, metaclass=abc.ABCMeta):\n    \"\"\"MPCPlannerBase abstract Module.\n\n    This class inherits from :obj:`SafeModule`. Provided a :obj:`TensorDict`, this module will perform a Model Predictive Control (MPC) planning step.\n    At the end of the planning step, the :obj:`MPCPlanner` will return a proposed action.\n\n    Args:\n        env (EnvBase): The environment to perform the planning step on (Can be :obj:`ModelBasedEnvBase` or :obj:`EnvBase`).\n        action_key (str, optional): The key that will point to the computed action.\n    \"\"\"\n\n    def __init__(\n        self,\n        env: EnvBase,\n        action_key: str = \"action\",\n    ):\n        # Check if env is stateless\n        if env.batch_locked:", "metadata": {"fpath_tuple": ["pytorch_rl", "torchrl", "modules", "planners", "common.py"], "line_no": 32, "task_id": "pytorch_rl/118", "start_line_no": 12, "end_line_no": 32, "window_size": 20, "context_start_lineno": 0, "repo": "pytorch_rl"}}, "top_k_context": [{"context": "    This module will perform a MPPI planning step when given a TensorDict\n    containing initial states.\n\n    A call to the module returns the actions that empirically maximised the\n    returns given a planning horizon\n\n    Args:\n        env (EnvBase): The environment to perform the planning step on (can be\n            `ModelBasedEnv` or :obj:`EnvBase`).\n        planning_horizon (int): The length of the simulated trajectories\n        optim_steps (int): The number of optimization steps used by the MPC\n            planner\n        num_candidates (int): The number of candidates to sample from the\n            Gaussian distributions.\n        top_k (int): The number of top candidates to use to\n            update the mean and standard deviation of the Gaussian distribution.\n        reward_key (str, optional): The key in the TensorDict to use to\n            retrieve the reward. Defaults to \"reward\".\n        action_key (str, optional): The key in the TensorDict to use to store\n            the action. Defaults to \"action\"", "metadata": [{"fpath_tuple": ["pytorch_rl", "torchrl", "modules", "planners", "mppi.py"], "line_no": 32, "start_line_no": 22, "end_line_no": 42, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}], "sim_score": 0.30357142857142855}, {"context": "\n    A call to the module returns the actions that empirically maximised the\n    returns given a planning horizon\n\n    Args:\n        env (EnvBase): The environment to perform the planning step on (can be\n            `ModelBasedEnv` or :obj:`EnvBase`).\n        planning_horizon (int): The length of the simulated trajectories\n        optim_steps (int): The number of optimization steps used by the MPC\n            planner\n        num_candidates (int): The number of candidates to sample from the\n            Gaussian distributions.\n        top_k (int): The number of top candidates to use to\n            update the mean and standard deviation of the Gaussian distribution.\n        reward_key (str, optional): The key in the TensorDict to use to\n            retrieve the reward. Defaults to \"reward\".\n        action_key (str, optional): The key in the TensorDict to use to store\n            the action. Defaults to \"action\"\n\n    Examples:", "metadata": [{"fpath_tuple": ["pytorch_rl", "torchrl", "modules", "planners", "cem.py"], "line_no": 36, "start_line_no": 26, "end_line_no": 46, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}, {"fpath_tuple": ["pytorch_rl", "torchrl", "modules", "planners", "mppi.py"], "line_no": 34, "start_line_no": 24, "end_line_no": 44, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}], "sim_score": 0.29878048780487804}, {"context": "    mean and standard deviation of the actions distribution.\n    The CEM planning step is repeated for a specified number of steps.\n\n    A call to the module returns the actions that empirically maximised the\n    returns given a planning horizon\n\n    Args:\n        env (EnvBase): The environment to perform the planning step on (can be\n            `ModelBasedEnv` or :obj:`EnvBase`).\n        planning_horizon (int): The length of the simulated trajectories\n        optim_steps (int): The number of optimization steps used by the MPC\n            planner\n        num_candidates (int): The number of candidates to sample from the\n            Gaussian distributions.\n        top_k (int): The number of top candidates to use to\n            update the mean and standard deviation of the Gaussian distribution.\n        reward_key (str, optional): The key in the TensorDict to use to\n            retrieve the reward. Defaults to \"reward\".\n        action_key (str, optional): The key in the TensorDict to use to store\n            the action. Defaults to \"action\"", "metadata": [{"fpath_tuple": ["pytorch_rl", "torchrl", "modules", "planners", "cem.py"], "line_no": 34, "start_line_no": 24, "end_line_no": 44, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}], "sim_score": 0.2976190476190476}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# src/evaluate/evaluator/text_classification.py\n# --------------------------------------------------\n# \n# \n# TASK_DOCUMENTATION = r\"\"\"\n#     Examples:\n#     ```python\n#     >>> from evaluate import evaluator\n#     >>> from datasets import load_dataset\n#     >>> task_evaluator = evaluator(\"text-classification\")\n#     >>> data = load_dataset(\"imdb\", split=\"test[:2]\")\n#     >>> results = task_evaluator.compute(\n#     >>>     model_or_pipeline=\"huggingface/prunebert-base-uncased-6-finepruned-w-distil-mnli\",\n#     >>>     data=data,\n#     >>>     metric=\"accuracy\",\n#     >>>     label_mapping={\"LABEL_0\": 0.0, \"LABEL_1\": 1.0},\n#     >>>     strategy=\"bootstrap\",\n#     >>>     n_resamples=10,\n#     >>>     random_state=0\n#     >>> )\n#     ```\n# \"\"\"\n# --------------------------------------------------\n# the below code fragment can be found in:\n# src/evaluate/evaluator/text_classification.py\n# --------------------------------------------------\n# from ..module import EvaluationModule\n# from ..utils.file_utils import add_end_docstrings, add_start_docstrings\n# from .base import EVALUATOR_COMPUTE_RETURN_DOCSTRING, EVALUTOR_COMPUTE_START_DOCSTRING, Evaluator\n# from .utils import DatasetColumnPair\n# \n# \n# TASK_DOCUMENTATION = r\"\"\"\n#     Examples:\n#     ```python\n#     >>> from evaluate import evaluator\n#     >>> from datasets import load_dataset\n#     >>> task_evaluator = evaluator(\"text-classification\")\n#     >>> data = load_dataset(\"imdb\", split=\"test[:2]\")\n#     >>> results = task_evaluator.compute(\n#     >>>     model_or_pipeline=\"huggingface/prunebert-base-uncased-6-finepruned-w-distil-mnli\",\n#     >>>     data=data,\n#     >>>     metric=\"accuracy\",\n#     >>>     label_mapping={\"LABEL_0\": 0.0, \"LABEL_1\": 1.0},\n#     >>>     strategy=\"bootstrap\",\n#     >>>     n_resamples=10,\n# --------------------------------------------------\n# the below code fragment can be found in:\n# src/evaluate/evaluator/text_classification.py\n# --------------------------------------------------\n# from .base import EVALUATOR_COMPUTE_RETURN_DOCSTRING, EVALUTOR_COMPUTE_START_DOCSTRING, Evaluator\n# from .utils import DatasetColumnPair\n# \n# \n# TASK_DOCUMENTATION = r\"\"\"\n#     Examples:\n#     ```python\n#     >>> from evaluate import evaluator\n#     >>> from datasets import load_dataset\n#     >>> task_evaluator = evaluator(\"text-classification\")\n#     >>> data = load_dataset(\"imdb\", split=\"test[:2]\")\n#     >>> results = task_evaluator.compute(\n#     >>>     model_or_pipeline=\"huggingface/prunebert-base-uncased-6-finepruned-w-distil-mnli\",\n#     >>>     data=data,\n#     >>>     metric=\"accuracy\",\n#     >>>     label_mapping={\"LABEL_0\": 0.0, \"LABEL_1\": 1.0},\n#     >>>     strategy=\"bootstrap\",\n#     >>>     n_resamples=10,\n#     >>>     random_state=0\n#     >>> )\n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\n# Copyright 2022 The HuggingFace Evaluate Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom numbers import Number\nfrom typing import Any, Callable, Dict, Optional, Tuple, Union\n\nfrom datasets import Dataset\nfrom typing_extensions import Literal\n\nfrom ..module import EvaluationModule\nfrom ..utils.file_utils import add_end_docstrings, add_start_docstrings\nfrom .base import EVALUATOR_COMPUTE_RETURN_DOCSTRING, EVALUTOR_COMPUTE_START_DOCSTRING, Evaluator\n\n\nTASK_DOCUMENTATION = r\"\"\"\n    Examples:\n    ```python\n    >>> from evaluate import evaluator\n    >>> from datasets import load_dataset\n    >>> task_evaluator = evaluator(\"image-classification\")\n    >>> data = load_dataset(\"beans\", split=\"test[:40]\")\n    >>> results = task_evaluator.compute(\n    >>>     model_or_pipeline=\"nateraw/vit-base-beans\",\n    >>>     data=data,\n    >>>     label_column=\"labels\",\n    >>>     metric=\"accuracy\",\n    >>>     label_mapping={'angular_leaf_spot': 0, 'bean_rust': 1, 'healthy': 2},\n    >>>     strategy=\"bootstrap\"\n    >>> )", "metadata": {"task_id": "huggingface_evaluate/158", "ground_truth": "    ```", "fpath_tuple": ["huggingface_evaluate", "src", "evaluate", "evaluator", "image_classification.py"], "context_start_lineno": 0, "line_no": 40, "query_window": {"context": "from ..module import EvaluationModule\nfrom ..utils.file_utils import add_end_docstrings, add_start_docstrings\nfrom .base import EVALUATOR_COMPUTE_RETURN_DOCSTRING, EVALUTOR_COMPUTE_START_DOCSTRING, Evaluator\n\n\nTASK_DOCUMENTATION = r\"\"\"\n    Examples:\n    ```python\n    >>> from evaluate import evaluator\n    >>> from datasets import load_dataset\n    >>> task_evaluator = evaluator(\"image-classification\")\n    >>> data = load_dataset(\"beans\", split=\"test[:40]\")\n    >>> results = task_evaluator.compute(\n    >>>     model_or_pipeline=\"nateraw/vit-base-beans\",\n    >>>     data=data,\n    >>>     label_column=\"labels\",\n    >>>     metric=\"accuracy\",\n    >>>     label_mapping={'angular_leaf_spot': 0, 'bean_rust': 1, 'healthy': 2},\n    >>>     strategy=\"bootstrap\"\n    >>> )", "metadata": {"fpath_tuple": ["huggingface_evaluate", "src", "evaluate", "evaluator", "image_classification.py"], "line_no": 40, "task_id": "huggingface_evaluate/158", "start_line_no": 20, "end_line_no": 40, "window_size": 20, "context_start_lineno": 0, "repo": "huggingface_evaluate"}}, "top_k_context": [{"context": "from ..module import EvaluationModule\nfrom ..utils.file_utils import add_end_docstrings, add_start_docstrings\nfrom .base import EVALUATOR_COMPUTE_RETURN_DOCSTRING, EVALUTOR_COMPUTE_START_DOCSTRING, Evaluator\nfrom .utils import DatasetColumnPair\n\n\nTASK_DOCUMENTATION = r\"\"\"\n    Examples:\n    ```python\n    >>> from evaluate import evaluator\n    >>> from datasets import load_dataset\n    >>> task_evaluator = evaluator(\"text-classification\")\n    >>> data = load_dataset(\"imdb\", split=\"test[:2]\")\n    >>> results = task_evaluator.compute(\n    >>>     model_or_pipeline=\"huggingface/prunebert-base-uncased-6-finepruned-w-distil-mnli\",\n    >>>     data=data,\n    >>>     metric=\"accuracy\",\n    >>>     label_mapping={\"LABEL_0\": 0.0, \"LABEL_1\": 1.0},\n    >>>     strategy=\"bootstrap\",\n    >>>     n_resamples=10,", "metadata": [{"fpath_tuple": ["huggingface_evaluate", "src", "evaluate", "evaluator", "text_classification.py"], "line_no": 30, "start_line_no": 20, "end_line_no": 40, "window_size": 20, "repo": "huggingface_evaluate", "slice_size": 10}], "sim_score": 0.6296296296296297}, {"context": "from typing_extensions import Literal\n\nfrom ..module import EvaluationModule\nfrom ..utils.file_utils import add_end_docstrings, add_start_docstrings\nfrom .base import EVALUATOR_COMPUTE_RETURN_DOCSTRING, EVALUTOR_COMPUTE_START_DOCSTRING, Evaluator\nfrom .utils import DatasetColumnPair\n\n\nTASK_DOCUMENTATION = r\"\"\"\n    Examples:\n    ```python\n    >>> from evaluate import evaluator\n    >>> from datasets import load_dataset\n    >>> task_evaluator = evaluator(\"text-classification\")\n    >>> data = load_dataset(\"imdb\", split=\"test[:2]\")\n    >>> results = task_evaluator.compute(\n    >>>     model_or_pipeline=\"huggingface/prunebert-base-uncased-6-finepruned-w-distil-mnli\",\n    >>>     data=data,\n    >>>     metric=\"accuracy\",\n    >>>     label_mapping={\"LABEL_0\": 0.0, \"LABEL_1\": 1.0},", "metadata": [{"fpath_tuple": ["huggingface_evaluate", "src", "evaluate", "evaluator", "text_classification.py"], "line_no": 28, "start_line_no": 18, "end_line_no": 38, "window_size": 20, "repo": "huggingface_evaluate", "slice_size": 10}], "sim_score": 0.6073619631901841}, {"context": "from .base import EVALUATOR_COMPUTE_RETURN_DOCSTRING, EVALUTOR_COMPUTE_START_DOCSTRING, Evaluator\nfrom .utils import DatasetColumnPair\n\n\nTASK_DOCUMENTATION = r\"\"\"\n    Examples:\n    ```python\n    >>> from evaluate import evaluator\n    >>> from datasets import load_dataset\n    >>> task_evaluator = evaluator(\"text-classification\")\n    >>> data = load_dataset(\"imdb\", split=\"test[:2]\")\n    >>> results = task_evaluator.compute(\n    >>>     model_or_pipeline=\"huggingface/prunebert-base-uncased-6-finepruned-w-distil-mnli\",\n    >>>     data=data,\n    >>>     metric=\"accuracy\",\n    >>>     label_mapping={\"LABEL_0\": 0.0, \"LABEL_1\": 1.0},\n    >>>     strategy=\"bootstrap\",\n    >>>     n_resamples=10,\n    >>>     random_state=0\n    >>> )", "metadata": [{"fpath_tuple": ["huggingface_evaluate", "src", "evaluate", "evaluator", "text_classification.py"], "line_no": 32, "start_line_no": 22, "end_line_no": 42, "window_size": 20, "repo": "huggingface_evaluate", "slice_size": 10}], "sim_score": 0.5670731707317073}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# src/diffusers/models/unet_2d_blocks.py\n# --------------------------------------------------\n#             for downsampler in self.downsamplers:\n#                 skip_sample = downsampler(skip_sample)\n# \n#             hidden_states = self.skip_conv(skip_sample) + hidden_states\n# \n#             output_states += (hidden_states,)\n# \n#         return hidden_states, output_states, skip_sample\n# \n# \n# class ResnetDownsampleBlock2D(nn.Module):\n#     def __init__(\n#         self,\n#         in_channels: int,\n#         out_channels: int,\n#         temb_channels: int,\n#         dropout: float = 0.0,\n#         num_layers: int = 1,\n#         resnet_eps: float = 1e-6,\n#         resnet_time_scale_shift: str = \"default\",\n# --------------------------------------------------\n# the below code fragment can be found in:\n# src/diffusers/models/unet_1d_blocks.py\n# --------------------------------------------------\n#             hidden_states = self.nonlinearity(hidden_states)\n# \n#         if self.downsample is not None:\n#             hidden_states = self.downsample(hidden_states)\n# \n#         return hidden_states, output_states\n# \n# \n# class UpResnetBlock1D(nn.Module):\n#     def __init__(\n#         self,\n#         in_channels,\n#         out_channels=None,\n#         num_layers=1,\n#         temb_channels=32,\n#         groups=32,\n#         groups_out=None,\n#         non_linearity=None,\n#         time_embedding_norm=\"default\",\n#         output_scale_factor=1.0,\n# --------------------------------------------------\n# the below code fragment can be found in:\n# src/diffusers/models/unet_1d_blocks.py\n# --------------------------------------------------\n# \n#         hidden_states = self.conv_1(hidden_states)\n#         hidden_states = self.group_norm_1(hidden_states)\n#         hidden_states = self.gelu_1(hidden_states)\n#         hidden_states = self.conv_2(hidden_states)\n# \n#         if not self.is_last:\n#             hidden_states = self.group_norm_2(hidden_states)\n#             hidden_states = self.gelu_2(hidden_states)\n# \n#         output = hidden_states + residual\n#         return output\n# \n# \n# class UNetMidBlock1D(nn.Module):\n#     def __init__(self, mid_channels, in_channels, out_channels=None):\n#         super().__init__()\n# \n#         out_channels = in_channels if out_channels is None else out_channels\n# \n# --------------------------------------------------\n# the below code fragment can be found in:\n# src/diffusers/models/unet_2d_blocks.py\n# --------------------------------------------------\n# \n#     def forward(self, hidden_states):\n#         for resnet in self.resnets:\n#             hidden_states = resnet(hidden_states, temb=None)\n# \n#         if self.downsamplers is not None:\n#             for downsampler in self.downsamplers:\n#                 hidden_states = downsampler(hidden_states)\n# \n#         return hidden_states\n# \n# \n# class AttnDownEncoderBlock2D(nn.Module):\n#     def __init__(\n#         self,\n#         in_channels: int,\n#         out_channels: int,\n#         dropout: float = 0.0,\n#         num_layers: int = 1,\n#         resnet_eps: float = 1e-6,\n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\n, W, C]`.\n            weight: Weight tensor of the shape `[filterH, filterW, inChannels,\n                outChannels]`. Grouped convolution can be performed by `inChannels = x.shape[0] // numGroups`.\n            kernel: FIR filter of the shape `[firH, firW]` or `[firN]`\n                (separable). The default is `[1] * factor`, which corresponds to nearest-neighbor upsampling.\n            factor: Integer upsampling factor (default: 2).\n            gain: Scaling factor for signal magnitude (default: 1.0).\n\n        Returns:\n            output: Tensor of the shape `[N, C, H * factor, W * factor]` or `[N, H * factor, W * factor, C]`, and same\n            datatype as `hidden_states`.\n        \"\"\"\n\n        assert isinstance(factor, int) and factor >= 1\n\n        # Setup filter kernel.\n        if kernel is None:\n            kernel = [1] * factor\n\n        # setup kernel\n        kernel = torch.tensor(kernel, dtype=torch.float32)\n        if kernel.ndim == 1:\n            kernel = torch.outer(kernel, kernel)\n        kernel /= torch.sum(kernel)\n\n        kernel = kernel * (gain * (factor**2))\n\n        if self.use_conv:\n            convH = weight.shape[2]\n            convW = weight.shape[3]\n            inC = weight.shape[1]\n\n            pad_value = (kernel.shape[0] - factor) - (convW - 1)\n\n            stride = (factor, factor)\n            # Determine data dimensions.\n            output_shape = (\n                (hidden_states.shape[2] - 1) * factor + convH,\n                (hidden_states.shape[3] - 1) * factor + convW,\n            )\n            output_padding = (\n                output_shape[0] - (hidden_states.shape[2] - 1) * stride[0] - convH,\n                output_shape[1] - (hidden_states.shape[3] - 1) * stride[1] - convW,\n            )\n            assert output_padding[0] >= 0 and output_padding[1] >= 0\n            num_groups = hidden_states.shape[1] // inC\n\n            # Transpose weights.\n            weight = torch.reshape(weight, (num_groups, -1, inC, convH, convW))\n            weight = torch.flip(weight, dims=[3, 4]).permute(0, 2, 1, 3, 4)\n            weight = torch.reshape(weight, (num_groups * inC, -1, convH, convW))\n\n            inverse_conv = F.conv_transpose2d(\n                hidden_states, weight, stride=stride, output_padding=output_padding, padding=0\n            )\n\n            output = upfirdn2d_native(\n                inverse_conv,\n                torch.tensor(kernel, device=inverse_conv.device),\n                pad=((pad_value + 1) // 2 + factor - 1, pad_value // 2 + 1),\n            )\n        else:\n            pad_value = kernel.shape[0] - factor\n            output = upfirdn2d_native(\n                hidden_states,\n                torch.tensor(kernel, device=hidden_states.device),\n                up=factor,\n                pad=((pad_value + 1) // 2 + factor - 1, pad_value // 2),\n            )\n\n        return output\n\n    def forward(self, hidden_states):\n        if self.use_conv:\n            height = self._upsample_2d(hidden_states, self.Conv2d_0.weight, kernel=self.fir_kernel)\n            height = height + self.Conv2d_0.bias.reshape(1, -1, 1, 1)\n        else:\n            height = self._upsample_2d(hidden_states, kernel=self.fir_kernel, factor=2)\n\n        return height\n\n\nclass FirDownsample2D(nn.Module):\n    def __init__(self, channels=None, out_channels=None, use_conv=False, fir_kernel=(1, 3, 3, 1)):\n        super().__init__()\n        out_channels = out_channels if out_channels else channels\n        if use_conv:\n            self.Conv2d_0 = nn.Conv2d(channels, out_channels, kernel_size=3, stride=1, padding=1)\n        self.fir_kernel = fir_kernel\n        self.use_conv = use_conv\n        self.out_channels = out_channels\n\n    def _downsample_2d(self, hidden_states, weight=None, kernel=None, factor=2, gain=1):\n        \"\"\"Fused `Conv2d()` followed by `downsample_2d()`.\n        Padding is performed only once at the beginning, not between the operations. The fused op is considerably more\n        efficient than performing the same calculation using standard TensorFlow ops. It supports gradients of\n        arbitrary order.\n\n        Args:\n            hidden_states: Input tensor of the shape `[N, C, H, W]` or `[N, H, W, C]`.\n            weight:\n                Weight tensor of the shape `[filterH, filterW, inChannels, outChannels]`. Grouped convolution can be\n                performed by `inChannels = x.shape[0] // numGroups`.\n            kernel: FIR filter of the shape `[firH, firW]` or `[firN]` (separable). The default is `[1] *\n            factor`, which corresponds to average pooling.\n            factor: Integer downsampling factor (default: 2).\n            gain: Scaling factor for signal magnitude (default: 1.0).\n\n        Returns:\n            output: Tensor of the shape `[N, C, H // factor, W // factor]` or `[N, H // factor, W // factor, C]`, and\n            same datatype as `x`.\n        \"\"\"\n\n        assert isinstance(factor, int) and factor >= 1\n        if kernel is None:\n            kernel = [1] * factor\n\n        # setup kernel\n        kernel = torch.tensor(kernel, dtype=torch.float32)\n        if kernel.ndim == 1:\n            kernel = torch.outer(kernel, kernel)\n        kernel /= torch.sum(kernel)\n\n        kernel = kernel * gain\n\n        if self.use_conv:\n            _, _, convH, convW = weight.shape\n            pad_value = (kernel.shape[0] - factor) + (convW - 1)\n            stride_value = [factor, factor]\n            upfirdn_input = upfirdn2d_native(\n                hidden_states,\n                torch.tensor(kernel, device=hidden_states.device),\n                pad=((pad_value + 1) // 2, pad_value // 2),\n            )\n            output = F.conv2d(upfirdn_input, weight, stride=stride_value, padding=0)\n        else:\n            pad_value = kernel.shape[0] - factor\n            output = upfirdn2d_native(\n                hidden_states,\n                torch.tensor(kernel, device=hidden_states.device),\n                down=factor,\n                pad=((pad_value + 1) // 2, pad_value // 2),\n            )\n\n        return output\n\n    def forward(self, hidden_states):\n        if self.use_conv:\n            downsample_input = self._downsample_2d(hidden_states, weight=self.Conv2d_0.weight, kernel=self.fir_kernel)\n            hidden_states = downsample_input + self.Conv2d_0.bias.reshape(1, -1, 1, 1)\n        else:\n            hidden_states = self._downsample_2d(hidden_states, kernel=self.fir_kernel, factor=2)\n\n        return hidden_states\n\n\nclass ResnetBlock2D(nn.Module):\n    def __init__(\n        self,\n        *,\n        in_channels,\n        out_channels=None,\n        conv_shortcut=False,\n        dropout=0.0,", "metadata": {"task_id": "huggingface_diffusers/145", "ground_truth": "        temb_channels=512,", "fpath_tuple": ["huggingface_diffusers", "src", "diffusers", "models", "resnet.py"], "context_start_lineno": 210, "line_no": 374, "query_window": {"context": "        return output\n\n    def forward(self, hidden_states):\n        if self.use_conv:\n            downsample_input = self._downsample_2d(hidden_states, weight=self.Conv2d_0.weight, kernel=self.fir_kernel)\n            hidden_states = downsample_input + self.Conv2d_0.bias.reshape(1, -1, 1, 1)\n        else:\n            hidden_states = self._downsample_2d(hidden_states, kernel=self.fir_kernel, factor=2)\n\n        return hidden_states\n\n\nclass ResnetBlock2D(nn.Module):\n    def __init__(\n        self,\n        *,\n        in_channels,\n        out_channels=None,\n        conv_shortcut=False,\n        dropout=0.0,", "metadata": {"fpath_tuple": ["huggingface_diffusers", "src", "diffusers", "models", "resnet.py"], "line_no": 374, "task_id": "huggingface_diffusers/145", "start_line_no": 354, "end_line_no": 374, "window_size": 20, "context_start_lineno": 210, "repo": "huggingface_diffusers"}}, "top_k_context": [{"context": "        else:\n            self.downsamplers = None\n\n    def forward(self, hidden_states):\n        for resnet in self.resnets:\n            hidden_states = resnet(hidden_states, temb=None)\n\n        if self.downsamplers is not None:\n            for downsampler in self.downsamplers:\n                hidden_states = downsampler(hidden_states)\n\n        return hidden_states\n\n\nclass AttnDownEncoderBlock2D(nn.Module):\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        dropout: float = 0.0,", "metadata": [{"fpath_tuple": ["huggingface_diffusers", "src", "diffusers", "models", "unet_2d_blocks.py"], "line_no": 940, "start_line_no": 930, "end_line_no": 950, "window_size": 20, "repo": "huggingface_diffusers", "slice_size": 10}], "sim_score": 0.47368421052631576}, {"context": "    def forward(self, hidden_states):\n        residual = self.conv_skip(hidden_states) if self.has_conv_skip else hidden_states\n\n        hidden_states = self.conv_1(hidden_states)\n        hidden_states = self.group_norm_1(hidden_states)\n        hidden_states = self.gelu_1(hidden_states)\n        hidden_states = self.conv_2(hidden_states)\n\n        if not self.is_last:\n            hidden_states = self.group_norm_2(hidden_states)\n            hidden_states = self.gelu_2(hidden_states)\n\n        output = hidden_states + residual\n        return output\n\n\nclass UNetMidBlock1D(nn.Module):\n    def __init__(self, mid_channels, in_channels, out_channels=None):\n        super().__init__()\n", "metadata": [{"fpath_tuple": ["huggingface_diffusers", "src", "diffusers", "models", "unet_1d_blocks.py"], "line_no": 408, "start_line_no": 398, "end_line_no": 418, "window_size": 20, "repo": "huggingface_diffusers", "slice_size": 10}], "sim_score": 0.46153846153846156}, {"context": "\n        if self.nonlinearity is not None:\n            hidden_states = self.nonlinearity(hidden_states)\n\n        if self.downsample is not None:\n            hidden_states = self.downsample(hidden_states)\n\n        return hidden_states, output_states\n\n\nclass UpResnetBlock1D(nn.Module):\n    def __init__(\n        self,\n        in_channels,\n        out_channels=None,\n        num_layers=1,\n        temb_channels=32,\n        groups=32,\n        groups_out=None,\n        non_linearity=None,", "metadata": [{"fpath_tuple": ["huggingface_diffusers", "src", "diffusers", "models", "unet_1d_blocks.py"], "line_no": 88, "start_line_no": 78, "end_line_no": 98, "window_size": 20, "repo": "huggingface_diffusers", "slice_size": 10}], "sim_score": 0.4606741573033708}, {"context": "        if self.downsamplers is not None:\n            hidden_states = self.resnet_down(hidden_states, temb)\n            for downsampler in self.downsamplers:\n                skip_sample = downsampler(skip_sample)\n\n            hidden_states = self.skip_conv(skip_sample) + hidden_states\n\n            output_states += (hidden_states,)\n\n        return hidden_states, output_states, skip_sample\n\n\nclass ResnetDownsampleBlock2D(nn.Module):\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        temb_channels: int,\n        dropout: float = 0.0,\n        num_layers: int = 1,", "metadata": [{"fpath_tuple": ["huggingface_diffusers", "src", "diffusers", "models", "unet_2d_blocks.py"], "line_no": 1184, "start_line_no": 1174, "end_line_no": 1194, "window_size": 20, "repo": "huggingface_diffusers", "slice_size": 10}], "sim_score": 0.45454545454545453}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# metrics/pearsonr/pearsonr.py\n# --------------------------------------------------\n# \n# \n# _KWARGS_DESCRIPTION = \"\"\"\n# Args:\n#     predictions (`list` of `int`): Predicted class labels, as returned by a model.\n#     references (`list` of `int`): Ground truth labels.\n#     return_pvalue (`boolean`): If `True`, returns the p-value, along with the correlation coefficient. If `False`, returns only the correlation coefficient. Defaults to `False`.\n# \n# Returns:\n#     pearsonr (`float`): Pearson correlation coefficient. Minimum possible value is -1. Maximum possible value is 1. Values of 1 and -1 indicate exact linear positive and negative relationships, respectively. A value of 0 implies no correlation.\n#     p-value (`float`): P-value, which roughly indicates the probability of an The p-value roughly indicates the probability of an uncorrelated system producing datasets that have a Pearson correlation at least as extreme as the one computed from these datasets. Minimum possible value is 0. Maximum possible value is 1. Higher values indicate higher probabilities.\n# \n# Examples:\n# \n#     Example 1-A simple example using only predictions and references.\n#         >>> pearsonr_metric = evaluate.load(\"pearsonr\")\n#         >>> results = pearsonr_metric.compute(predictions=[10, 9, 2.5, 6, 4], references=[1, 2, 3, 4, 5])\n#         >>> print(round(results['pearsonr'], 2))\n#         -0.74\n# \n# --------------------------------------------------\n# the below code fragment can be found in:\n# metrics/pearsonr/pearsonr.py\n# --------------------------------------------------\n# _KWARGS_DESCRIPTION = \"\"\"\n# Args:\n#     predictions (`list` of `int`): Predicted class labels, as returned by a model.\n#     references (`list` of `int`): Ground truth labels.\n#     return_pvalue (`boolean`): If `True`, returns the p-value, along with the correlation coefficient. If `False`, returns only the correlation coefficient. Defaults to `False`.\n# \n# Returns:\n#     pearsonr (`float`): Pearson correlation coefficient. Minimum possible value is -1. Maximum possible value is 1. Values of 1 and -1 indicate exact linear positive and negative relationships, respectively. A value of 0 implies no correlation.\n#     p-value (`float`): P-value, which roughly indicates the probability of an The p-value roughly indicates the probability of an uncorrelated system producing datasets that have a Pearson correlation at least as extreme as the one computed from these datasets. Minimum possible value is 0. Maximum possible value is 1. Higher values indicate higher probabilities.\n# \n# Examples:\n# \n#     Example 1-A simple example using only predictions and references.\n#         >>> pearsonr_metric = evaluate.load(\"pearsonr\")\n#         >>> results = pearsonr_metric.compute(predictions=[10, 9, 2.5, 6, 4], references=[1, 2, 3, 4, 5])\n#         >>> print(round(results['pearsonr'], 2))\n#         -0.74\n# \n#     Example 2-The same as Example 1, but that also returns the `p-value`.\n#         >>> pearsonr_metric = evaluate.load(\"pearsonr\")\n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\n# Copyright 2021 The HuggingFace Datasets Authors and the current dataset script contributor.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Spearman correlation coefficient metric.\"\"\"\n\nimport datasets\nfrom scipy.stats import spearmanr\n\nimport evaluate\n\n\n_DESCRIPTION = \"\"\"\nThe Spearman rank-order correlation coefficient is a measure of the\nrelationship between two datasets. Like other correlation coefficients,\nthis one varies between -1 and +1 with 0 implying no correlation.\nPositive correlations imply that as data in dataset x increases, so\ndoes data in dataset y. Negative correlations imply that as x increases,\ny decreases. Correlations of -1 or +1 imply an exact monotonic relationship.\n\nUnlike the Pearson correlation, the Spearman correlation does not\nassume that both datasets are normally distributed.\n\nThe p-value roughly indicates the probability of an uncorrelated system\nproducing datasets that have a Spearman correlation at least as extreme\nas the one computed from these datasets. The p-values are not entirely\nreliable but are probably reasonable for datasets larger than 500 or so.\n\"\"\"\n\n_KWARGS_DESCRIPTION = \"\"\"\nArgs:\n    predictions (`List[float]`): Predicted labels, as returned by a model.\n    references (`List[float]`): Ground truth labels.\n    return_pvalue (`bool`): If `True`, returns the p-value. If `False`, returns\n            only the spearmanr score. Defaults to `False`.\nReturns:\n    spearmanr (`float`): Spearman correlation coefficient.\n    p-value (`float`): p-value. **Note**: is only returned if `return_pvalue=True` is input.\nExamples:\n    Example 1:\n        >>> spearmanr_metric = evaluate.load(\"spearmanr\")\n        >>> results = spearmanr_metric.compute(references=[1, 2, 3, 4, 5], predictions=[10, 9, 2.5, 6, 4])\n        >>> print(results)\n        {'spearmanr': -0.7}\n\n    Example 2:\n        >>> spearmanr_metric = evaluate.load(\"spearmanr\")\n        >>> results = spearmanr_metric.compute(references=[1, 2, 3, 4, 5],", "metadata": {"task_id": "huggingface_evaluate/169", "ground_truth": "        ...                                     predictions=[10, 9, 2.5, 6, 4],", "fpath_tuple": ["huggingface_evaluate", "metrics", "spearmanr", "spearmanr.py"], "context_start_lineno": 0, "line_no": 57, "query_window": {"context": "\n_KWARGS_DESCRIPTION = \"\"\"\nArgs:\n    predictions (`List[float]`): Predicted labels, as returned by a model.\n    references (`List[float]`): Ground truth labels.\n    return_pvalue (`bool`): If `True`, returns the p-value. If `False`, returns\n            only the spearmanr score. Defaults to `False`.\nReturns:\n    spearmanr (`float`): Spearman correlation coefficient.\n    p-value (`float`): p-value. **Note**: is only returned if `return_pvalue=True` is input.\nExamples:\n    Example 1:\n        >>> spearmanr_metric = evaluate.load(\"spearmanr\")\n        >>> results = spearmanr_metric.compute(references=[1, 2, 3, 4, 5], predictions=[10, 9, 2.5, 6, 4])\n        >>> print(results)\n        {'spearmanr': -0.7}\n\n    Example 2:\n        >>> spearmanr_metric = evaluate.load(\"spearmanr\")\n        >>> results = spearmanr_metric.compute(references=[1, 2, 3, 4, 5],", "metadata": {"fpath_tuple": ["huggingface_evaluate", "metrics", "spearmanr", "spearmanr.py"], "line_no": 57, "task_id": "huggingface_evaluate/169", "start_line_no": 37, "end_line_no": 57, "window_size": 20, "context_start_lineno": 0, "repo": "huggingface_evaluate"}}, "top_k_context": [{"context": "\n\n_KWARGS_DESCRIPTION = \"\"\"\nArgs:\n    predictions (`list` of `int`): Predicted class labels, as returned by a model.\n    references (`list` of `int`): Ground truth labels.\n    return_pvalue (`boolean`): If `True`, returns the p-value, along with the correlation coefficient. If `False`, returns only the correlation coefficient. Defaults to `False`.\n\nReturns:\n    pearsonr (`float`): Pearson correlation coefficient. Minimum possible value is -1. Maximum possible value is 1. Values of 1 and -1 indicate exact linear positive and negative relationships, respectively. A value of 0 implies no correlation.\n    p-value (`float`): P-value, which roughly indicates the probability of an The p-value roughly indicates the probability of an uncorrelated system producing datasets that have a Pearson correlation at least as extreme as the one computed from these datasets. Minimum possible value is 0. Maximum possible value is 1. Higher values indicate higher probabilities.\n\nExamples:\n\n    Example 1-A simple example using only predictions and references.\n        >>> pearsonr_metric = evaluate.load(\"pearsonr\")\n        >>> results = pearsonr_metric.compute(predictions=[10, 9, 2.5, 6, 4], references=[1, 2, 3, 4, 5])\n        >>> print(round(results['pearsonr'], 2))\n        -0.74\n", "metadata": [{"fpath_tuple": ["huggingface_evaluate", "metrics", "pearsonr", "pearsonr.py"], "line_no": 36, "start_line_no": 26, "end_line_no": 46, "window_size": 20, "repo": "huggingface_evaluate", "slice_size": 10}], "sim_score": 0.48295454545454547}, {"context": "The p-value roughly indicates the probability of an uncorrelated system producing datasets that have a Pearson correlation at least as extreme as the one computed from these datasets.\n\"\"\"\n\n\n_KWARGS_DESCRIPTION = \"\"\"\nArgs:\n    predictions (`list` of `int`): Predicted class labels, as returned by a model.\n    references (`list` of `int`): Ground truth labels.\n    return_pvalue (`boolean`): If `True`, returns the p-value, along with the correlation coefficient. If `False`, returns only the correlation coefficient. Defaults to `False`.\n\nReturns:\n    pearsonr (`float`): Pearson correlation coefficient. Minimum possible value is -1. Maximum possible value is 1. Values of 1 and -1 indicate exact linear positive and negative relationships, respectively. A value of 0 implies no correlation.\n    p-value (`float`): P-value, which roughly indicates the probability of an The p-value roughly indicates the probability of an uncorrelated system producing datasets that have a Pearson correlation at least as extreme as the one computed from these datasets. Minimum possible value is 0. Maximum possible value is 1. Higher values indicate higher probabilities.\n\nExamples:\n\n    Example 1-A simple example using only predictions and references.\n        >>> pearsonr_metric = evaluate.load(\"pearsonr\")\n        >>> results = pearsonr_metric.compute(predictions=[10, 9, 2.5, 6, 4], references=[1, 2, 3, 4, 5])\n        >>> print(round(results['pearsonr'], 2))", "metadata": [{"fpath_tuple": ["huggingface_evaluate", "metrics", "pearsonr", "pearsonr.py"], "line_no": 34, "start_line_no": 24, "end_line_no": 44, "window_size": 20, "repo": "huggingface_evaluate", "slice_size": 10}], "sim_score": 0.4745762711864407}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# src/evaluate/evaluator/base.py\n# --------------------------------------------------\n# \n#             if torch.cuda.is_available():\n#                 device = 0  # first GPU\n#             else:\n#                 device = -1  # CPU\n#         except ImportError:\n#             # if not available try TF\n#             try:\n#                 import tensorflow as tf\n# \n#                 if len(tf.config.list_physical_devices(\"GPU\")) > 0:\n#                     device = 0  # first GPU\n#                 else:\n#                     device = -1  # CPU\n#             except ImportError:\n#                 device = -1\n# \n#         if device == -1:\n#             logger.info(\"No GPU found. The default device for pipeline inference is set to CPU.\")\n#         else:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# tests/test_evaluator.py\n# --------------------------------------------------\n#         pt_mock = mock.Mock()\n#         tf_mock = mock.Mock()\n# \n#         # mock import of torch and tensorflow\n#         def import_pt_tf_mock(name, *args):\n#             if name == \"torch\":\n#                 if pt_available:\n#                     return pt_mock\n#                 else:\n#                     raise ImportError\n#             if name == \"tensorflow\":\n#                 if tf_available:\n#                     return tf_mock\n#                 else:\n#                     raise ImportError\n#             return orig_import(name, *args)\n# \n#         with mock.patch(\"builtins.__import__\", side_effect=import_pt_tf_mock):\n#             # neither pt or tf are available\n#             pt_available = False\n# --------------------------------------------------\n# the below code fragment can be found in:\n# tests/test_evaluator.py\n# --------------------------------------------------\n# \n#         # mock import of torch and tensorflow\n#         def import_pt_tf_mock(name, *args):\n#             if name == \"torch\":\n#                 if pt_available:\n#                     return pt_mock\n#                 else:\n#                     raise ImportError\n#             if name == \"tensorflow\":\n#                 if tf_available:\n#                     return tf_mock\n#                 else:\n#                     raise ImportError\n#             return orig_import(name, *args)\n# \n#         with mock.patch(\"builtins.__import__\", side_effect=import_pt_tf_mock):\n#             # neither pt or tf are available\n#             pt_available = False\n#             tf_available = False\n#             self.assertEqual(Evaluator._infer_device(), -1)\n# --------------------------------------------------\n# the below code fragment can be found in:\n# tests/test_evaluator.py\n# --------------------------------------------------\n#         orig_import = __import__\n# \n#         pt_mock = mock.Mock()\n#         tf_mock = mock.Mock()\n# \n#         # mock import of torch and tensorflow\n#         def import_pt_tf_mock(name, *args):\n#             if name == \"torch\":\n#                 if pt_available:\n#                     return pt_mock\n#                 else:\n#                     raise ImportError\n#             if name == \"tensorflow\":\n#                 if tf_available:\n#                     return tf_mock\n#                 else:\n#                     raise ImportError\n#             return orig_import(name, *args)\n# \n#         with mock.patch(\"builtins.__import__\", side_effect=import_pt_tf_mock):\n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\nimport importlib\nimport os\nimport platform\nfrom pathlib import Path\n\nfrom packaging import version\n\nfrom .utils.logging import get_logger\n\n\nlogger = get_logger(__name__)\n\n\n# Metrics\nS3_METRICS_BUCKET_PREFIX = \"https://s3.amazonaws.com/datasets.huggingface.co/datasets/metrics\"\nCLOUDFRONT_METRICS_DISTRIB_PREFIX = \"https://cdn-datasets.huggingface.co/datasets/metric\"\nREPO_METRICS_URL = \"https://raw.githubusercontent.com/huggingface/evaluate/{revision}/metrics/{path}/{name}\"\nREPO_MEASUREMENTS_URL = \"https://raw.githubusercontent.com/huggingface/evaluate/{revision}/measurements/{path}/{name}\"\nREPO_COMPARISONS_URL = \"https://raw.githubusercontent.com/huggingface/evaluate/{revision}/comparisons/{path}/{name}\"\n\n# Evaluation module types\nEVALUATION_MODULE_TYPES = [\"metric\", \"comparison\", \"measurement\"]\n\n# Hub\nHF_ENDPOINT = os.environ.get(\"HF_ENDPOINT\", \"https://huggingface.co\")\nHF_LIST_ENDPOINT = HF_ENDPOINT + \"/api/spaces?filter={type}\"\nHUB_EVALUATE_URL = HF_ENDPOINT + \"/spaces/{path}/resolve/{revision}/{name}\"\nHUB_DEFAULT_VERSION = \"main\"\n\nPY_VERSION = version.parse(platform.python_version())\n\nif PY_VERSION < version.parse(\"3.8\"):\n    import importlib_metadata\nelse:\n    import importlib.metadata as importlib_metadata\n\n# General environment variables accepted values for booleans\nENV_VARS_TRUE_VALUES = {\"1\", \"ON\", \"YES\", \"TRUE\"}\nENV_VARS_TRUE_AND_AUTO_VALUES = ENV_VARS_TRUE_VALUES.union({\"AUTO\"})\n\n\n# Imports\nPANDAS_VERSION = version.parse(importlib_metadata.version(\"pandas\"))\nPYARROW_VERSION = version.parse(importlib_metadata.version(\"pyarrow\"))\n\nUSE_TF = os.environ.get(\"USE_TF\", \"AUTO\").upper()\nUSE_TORCH = os.environ.get(\"USE_TORCH\", \"AUTO\").upper()\nUSE_JAX = os.environ.get(\"USE_JAX\", \"AUTO\").upper()\n\nTORCH_VERSION = \"N/A\"\nTORCH_AVAILABLE = False\n\nif USE_TORCH in ENV_VARS_TRUE_AND_AUTO_VALUES and USE_TF not in ENV_VARS_TRUE_VALUES:\n    TORCH_AVAILABLE = importlib.util.find_spec(\"torch\") is not None\n    if TORCH_AVAILABLE:\n        try:\n            TORCH_VERSION = version.parse(importlib_metadata.version(\"torch\"))\n            logger.info(f\"PyTorch version {TORCH_VERSION} available.\")\n        except importlib_metadata.PackageNotFoundError:\n            pass\nelse:\n    logger.info(\"Disabling PyTorch because USE_TF is set\")\n\nTF_VERSION = \"N/A\"\nTF_AVAILABLE = False\n\nif USE_TF in ENV_VARS_TRUE_AND_AUTO_VALUES and USE_TORCH not in ENV_VARS_TRUE_VALUES:\n    TF_AVAILABLE = importlib.util.find_spec(\"tensorflow\") is not None\n    if TF_AVAILABLE:\n        # For the metadata, we have to look for both tensorflow and tensorflow-cpu\n        for package in [\n            \"tensorflow\",\n            \"tensorflow-cpu\",\n            \"tensorflow-gpu\",\n            \"tf-nightly\",\n            \"tf-nightly-cpu\",\n            \"tf-nightly-gpu\",\n            \"intel-tensorflow\",\n            \"tensorflow-rocm\",\n            \"tensorflow-macos\",\n        ]:\n            try:\n                TF_VERSION = version.parse(importlib_metadata.version(package))\n            except importlib_metadata.PackageNotFoundError:\n                continue\n            else:\n                break\n        else:\n            TF_AVAILABLE = False\n    if TF_AVAILABLE:\n        if TF_VERSION.major < 2:", "metadata": {"task_id": "huggingface_evaluate/137", "ground_truth": "            logger.info(f\"TensorFlow found but with version {TF_VERSION}. `datasets` requires version 2 minimum.\")", "fpath_tuple": ["huggingface_evaluate", "src", "evaluate", "config.py"], "context_start_lineno": 0, "line_no": 91, "query_window": {"context": "            \"tensorflow\",\n            \"tensorflow-cpu\",\n            \"tensorflow-gpu\",\n            \"tf-nightly\",\n            \"tf-nightly-cpu\",\n            \"tf-nightly-gpu\",\n            \"intel-tensorflow\",\n            \"tensorflow-rocm\",\n            \"tensorflow-macos\",\n        ]:\n            try:\n                TF_VERSION = version.parse(importlib_metadata.version(package))\n            except importlib_metadata.PackageNotFoundError:\n                continue\n            else:\n                break\n        else:\n            TF_AVAILABLE = False\n    if TF_AVAILABLE:\n        if TF_VERSION.major < 2:", "metadata": {"fpath_tuple": ["huggingface_evaluate", "src", "evaluate", "config.py"], "line_no": 91, "task_id": "huggingface_evaluate/137", "start_line_no": 71, "end_line_no": 91, "window_size": 20, "context_start_lineno": 0, "repo": "huggingface_evaluate"}}, "top_k_context": [{"context": "\n    def test_device_placement(self):\n        orig_import = __import__\n\n        pt_mock = mock.Mock()\n        tf_mock = mock.Mock()\n\n        # mock import of torch and tensorflow\n        def import_pt_tf_mock(name, *args):\n            if name == \"torch\":\n                if pt_available:\n                    return pt_mock\n                else:\n                    raise ImportError\n            if name == \"tensorflow\":\n                if tf_available:\n                    return tf_mock\n                else:\n                    raise ImportError\n            return orig_import(name, *args)", "metadata": [{"fpath_tuple": ["huggingface_evaluate", "tests", "test_evaluator.py"], "line_no": 152, "start_line_no": 142, "end_line_no": 162, "window_size": 20, "repo": "huggingface_evaluate", "slice_size": 10}], "sim_score": 0.20833333333333334}, {"context": "        pt_mock = mock.Mock()\n        tf_mock = mock.Mock()\n\n        # mock import of torch and tensorflow\n        def import_pt_tf_mock(name, *args):\n            if name == \"torch\":\n                if pt_available:\n                    return pt_mock\n                else:\n                    raise ImportError\n            if name == \"tensorflow\":\n                if tf_available:\n                    return tf_mock\n                else:\n                    raise ImportError\n            return orig_import(name, *args)\n\n        with mock.patch(\"builtins.__import__\", side_effect=import_pt_tf_mock):\n            # neither pt or tf are available\n            pt_available = False", "metadata": [{"fpath_tuple": ["huggingface_evaluate", "tests", "test_evaluator.py"], "line_no": 156, "start_line_no": 146, "end_line_no": 166, "window_size": 20, "repo": "huggingface_evaluate", "slice_size": 10}], "sim_score": 0.20588235294117646}, {"context": "        orig_import = __import__\n\n        pt_mock = mock.Mock()\n        tf_mock = mock.Mock()\n\n        # mock import of torch and tensorflow\n        def import_pt_tf_mock(name, *args):\n            if name == \"torch\":\n                if pt_available:\n                    return pt_mock\n                else:\n                    raise ImportError\n            if name == \"tensorflow\":\n                if tf_available:\n                    return tf_mock\n                else:\n                    raise ImportError\n            return orig_import(name, *args)\n\n        with mock.patch(\"builtins.__import__\", side_effect=import_pt_tf_mock):", "metadata": [{"fpath_tuple": ["huggingface_evaluate", "tests", "test_evaluator.py"], "line_no": 154, "start_line_no": 144, "end_line_no": 164, "window_size": 20, "repo": "huggingface_evaluate", "slice_size": 10}], "sim_score": 0.20202020202020202}, {"context": "        try:\n            import torch\n\n            if torch.cuda.is_available():\n                device = 0  # first GPU\n            else:\n                device = -1  # CPU\n        except ImportError:\n            # if not available try TF\n            try:\n                import tensorflow as tf\n\n                if len(tf.config.list_physical_devices(\"GPU\")) > 0:\n                    device = 0  # first GPU\n                else:\n                    device = -1  # CPU\n            except ImportError:\n                device = -1\n\n        if device == -1:", "metadata": [{"fpath_tuple": ["huggingface_evaluate", "src", "evaluate", "evaluator", "base.py"], "line_no": 194, "start_line_no": 184, "end_line_no": 204, "window_size": 20, "repo": "huggingface_evaluate", "slice_size": 10}], "sim_score": 0.2}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# fortuna/calib_model/calib_model_calibrator.py\n# --------------------------------------------------\n# from fortuna.calibration.state import CalibState\n# from fortuna.data.loader import DataLoader, TargetsLoader\n# from fortuna.training.mixin import (InputValidatorMixin,\n#                                     WithCheckpointingMixin,\n#                                     WithEarlyStoppingMixin)\n# from fortuna.typing import Array, CalibMutable, CalibParams, Path, Status\n# from fortuna.utils.builtins import HashableMixin\n# \n# \n# class CalibModelCalibrator(\n#     HashableMixin,\n#     WithCheckpointingMixin,\n#     WithEarlyStoppingMixin,\n#     InputValidatorMixin,\n#     metaclass=abc.ABCMeta,\n# ):\n#     def __init__(\n#         self,\n#         *args,\n#         calib_outputs: Array,\n# --------------------------------------------------\n# the below code fragment can be found in:\n# fortuna/calib_model/calib_model_calibrator.py\n# --------------------------------------------------\n#                                     WithEarlyStoppingMixin)\n# from fortuna.typing import Array, CalibMutable, CalibParams, Path, Status\n# from fortuna.utils.builtins import HashableMixin\n# \n# \n# class CalibModelCalibrator(\n#     HashableMixin,\n#     WithCheckpointingMixin,\n#     WithEarlyStoppingMixin,\n#     InputValidatorMixin,\n#     metaclass=abc.ABCMeta,\n# ):\n#     def __init__(\n#         self,\n#         *args,\n#         calib_outputs: Array,\n#         calib_targets: Array,\n#         predict_fn: Callable[[jnp.ndarray], jnp.ndarray],\n#         uncertainty_fn: Callable[[jnp.ndarray], jnp.ndarray],\n#         val_outputs: Array,\n# --------------------------------------------------\n# the below code fragment can be found in:\n# fortuna/calib_model/calib_model_calibrator.py\n# --------------------------------------------------\n# from fortuna.training.mixin import (InputValidatorMixin,\n#                                     WithCheckpointingMixin,\n#                                     WithEarlyStoppingMixin)\n# from fortuna.typing import Array, CalibMutable, CalibParams, Path, Status\n# from fortuna.utils.builtins import HashableMixin\n# \n# \n# class CalibModelCalibrator(\n#     HashableMixin,\n#     WithCheckpointingMixin,\n#     WithEarlyStoppingMixin,\n#     InputValidatorMixin,\n#     metaclass=abc.ABCMeta,\n# ):\n#     def __init__(\n#         self,\n#         *args,\n#         calib_outputs: Array,\n#         calib_targets: Array,\n#         predict_fn: Callable[[jnp.ndarray], jnp.ndarray],\n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\nimport abc\nimport collections\nimport logging\nfrom functools import partial\nfrom typing import Any, Callable, Dict, List, Optional, Tuple, Union\n\nimport jax\nimport jax.numpy as jnp\nfrom flax import jax_utils\nfrom flax.training.common_utils import stack_forest\nfrom jax import lax, random, value_and_grad\nfrom jax._src.prng import PRNGKeyArray\nfrom jax.tree_util import tree_map\nfrom tqdm import trange\nfrom tqdm.std import tqdm as TqdmDecorator\n\nfrom fortuna.calibration.state import CalibState\nfrom fortuna.data.loader import DataLoader, TargetsLoader\nfrom fortuna.training.mixin import (InputValidatorMixin,\n                                    WithCheckpointingMixin,\n                                    WithEarlyStoppingMixin)\nfrom fortuna.typing import (Array, Batch, CalibMutable, CalibParams, Path,\n                            Status)\nfrom fortuna.utils.builtins import HashableMixin\n\n\nclass CalibratorABC(\n    HashableMixin,\n    WithCheckpointingMixin,\n    WithEarlyStoppingMixin,\n    InputValidatorMixin,\n    metaclass=abc.ABCMeta,\n):\n    def __init__(\n        self,\n        *args,\n        calib_outputs_loader: TargetsLoader,", "metadata": {"task_id": "awslabs_fortuna/42", "ground_truth": "        predict_fn: Callable[[jnp.ndarray], jnp.ndarray],", "fpath_tuple": ["awslabs_fortuna", "fortuna", "calibration", "calibrator.py"], "context_start_lineno": 0, "line_no": 37, "query_window": {"context": "from fortuna.data.loader import DataLoader, TargetsLoader\nfrom fortuna.training.mixin import (InputValidatorMixin,\n                                    WithCheckpointingMixin,\n                                    WithEarlyStoppingMixin)\nfrom fortuna.typing import (Array, Batch, CalibMutable, CalibParams, Path,\n                            Status)\nfrom fortuna.utils.builtins import HashableMixin\n\n\nclass CalibratorABC(\n    HashableMixin,\n    WithCheckpointingMixin,\n    WithEarlyStoppingMixin,\n    InputValidatorMixin,\n    metaclass=abc.ABCMeta,\n):\n    def __init__(\n        self,\n        *args,\n        calib_outputs_loader: TargetsLoader,", "metadata": {"fpath_tuple": ["awslabs_fortuna", "fortuna", "calibration", "calibrator.py"], "line_no": 37, "task_id": "awslabs_fortuna/42", "start_line_no": 17, "end_line_no": 37, "window_size": 20, "context_start_lineno": 0, "repo": "awslabs_fortuna"}}, "top_k_context": [{"context": "from fortuna.calibration.state import CalibState\nfrom fortuna.data.loader import DataLoader, TargetsLoader\nfrom fortuna.training.mixin import (InputValidatorMixin,\n                                    WithCheckpointingMixin,\n                                    WithEarlyStoppingMixin)\nfrom fortuna.typing import Array, CalibMutable, CalibParams, Path, Status\nfrom fortuna.utils.builtins import HashableMixin\n\n\nclass CalibModelCalibrator(\n    HashableMixin,\n    WithCheckpointingMixin,\n    WithEarlyStoppingMixin,\n    InputValidatorMixin,\n    metaclass=abc.ABCMeta,\n):\n    def __init__(\n        self,\n        *args,\n        calib_outputs: Array,", "metadata": [{"fpath_tuple": ["awslabs_fortuna", "fortuna", "calib_model", "calib_model_calibrator.py"], "line_no": 26, "start_line_no": 16, "end_line_no": 36, "window_size": 20, "repo": "awslabs_fortuna", "slice_size": 10}], "sim_score": 0.8690476190476191}, {"context": "from fortuna.training.mixin import (InputValidatorMixin,\n                                    WithCheckpointingMixin,\n                                    WithEarlyStoppingMixin)\nfrom fortuna.typing import Array, CalibMutable, CalibParams, Path, Status\nfrom fortuna.utils.builtins import HashableMixin\n\n\nclass CalibModelCalibrator(\n    HashableMixin,\n    WithCheckpointingMixin,\n    WithEarlyStoppingMixin,\n    InputValidatorMixin,\n    metaclass=abc.ABCMeta,\n):\n    def __init__(\n        self,\n        *args,\n        calib_outputs: Array,\n        calib_targets: Array,\n        predict_fn: Callable[[jnp.ndarray], jnp.ndarray],", "metadata": [{"fpath_tuple": ["awslabs_fortuna", "fortuna", "calib_model", "calib_model_calibrator.py"], "line_no": 28, "start_line_no": 18, "end_line_no": 38, "window_size": 20, "repo": "awslabs_fortuna", "slice_size": 10}], "sim_score": 0.7391304347826086}, {"context": "from tqdm.std import tqdm as TqdmDecorator\n\nfrom fortuna.calibration.state import CalibState\nfrom fortuna.data.loader import DataLoader, TargetsLoader\nfrom fortuna.training.mixin import (InputValidatorMixin,\n                                    WithCheckpointingMixin,\n                                    WithEarlyStoppingMixin)\nfrom fortuna.typing import Array, CalibMutable, CalibParams, Path, Status\nfrom fortuna.utils.builtins import HashableMixin\n\n\nclass CalibModelCalibrator(\n    HashableMixin,\n    WithCheckpointingMixin,\n    WithEarlyStoppingMixin,\n    InputValidatorMixin,\n    metaclass=abc.ABCMeta,\n):\n    def __init__(\n        self,", "metadata": [{"fpath_tuple": ["awslabs_fortuna", "fortuna", "calib_model", "calib_model_calibrator.py"], "line_no": 24, "start_line_no": 14, "end_line_no": 34, "window_size": 20, "repo": "awslabs_fortuna", "slice_size": 10}], "sim_score": 0.717391304347826}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# tutorials/sphinx-tutorials/coding_dqn.py\n# --------------------------------------------------\n# ###############################################################################\n# \n# # save results\n# torch.save(\n#     {\n#         \"frames\": frames,\n#         \"evals\": evals,\n#         \"mavgs\": mavgs,\n#         \"losses\": losses,\n#         \"values\": values,\n#         \"grad_vals\": grad_vals,\n#         \"traj_lengths_training\": traj_lengths,\n#         \"traj_count\": traj_count,\n#         \"weights\": (params,),\n#     },\n#     \"saved_results_td0.pt\",\n# )\n# \n# ###############################################################################\n# # TD-lambda\n# --------------------------------------------------\n# the below code fragment can be found in:\n# tutorials/sphinx-tutorials/coding_dqn.py\n# tutorials/sphinx-tutorials/coding_dqn.py\n# --------------------------------------------------\n#             plt.plot(frames[-len(mavgs) :], mavgs, label=\"mavg\")\n#             plt.xlabel(\"frames collected\")\n#             plt.ylabel(\"trajectory length (= return)\")\n#             plt.subplot(3, 2, 2)\n#             plt.plot(traj_count[-len(evals) :], evals, label=\"return\")\n#             plt.plot(traj_count[-len(mavgs) :], mavgs, label=\"mavg\")\n#             plt.xlabel(\"trajectories collected\")\n#             plt.legend()\n#             plt.subplot(3, 2, 3)\n#             plt.plot(frames[-len(losses) :], losses)\n#             plt.xlabel(\"frames collected\")\n#             plt.title(\"loss\")\n#             plt.subplot(3, 2, 4)\n#             plt.plot(frames[-len(values) :], values)\n#             plt.xlabel(\"frames collected\")\n#             plt.title(\"value\")\n#             plt.subplot(3, 2, 5)\n#             plt.plot(frames[-len(grad_vals) :], grad_vals)\n#             plt.xlabel(\"frames collected\")\n#             plt.title(\"grad norm\")\n# --------------------------------------------------\n# the below code fragment can be found in:\n# tutorials/sphinx-tutorials/coding_dqn.py\n# tutorials/sphinx-tutorials/coding_dqn.py\n# --------------------------------------------------\n#             plt.ylabel(\"trajectory length (= return)\")\n#             plt.subplot(3, 2, 2)\n#             plt.plot(traj_count[-len(evals) :], evals, label=\"return\")\n#             plt.plot(traj_count[-len(mavgs) :], mavgs, label=\"mavg\")\n#             plt.xlabel(\"trajectories collected\")\n#             plt.legend()\n#             plt.subplot(3, 2, 3)\n#             plt.plot(frames[-len(losses) :], losses)\n#             plt.xlabel(\"frames collected\")\n#             plt.title(\"loss\")\n#             plt.subplot(3, 2, 4)\n#             plt.plot(frames[-len(values) :], values)\n#             plt.xlabel(\"frames collected\")\n#             plt.title(\"value\")\n#             plt.subplot(3, 2, 5)\n#             plt.plot(frames[-len(grad_vals) :], grad_vals)\n#             plt.xlabel(\"frames collected\")\n#             plt.title(\"grad norm\")\n#             if len(traj_lengths):\n#                 plt.subplot(3, 2, 6)\n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\n(gv))\n        traj_lengths_eval.append(eval_rollout.shape[-1])\n        evals.append(eval_rollout[\"reward\"].squeeze(-1).sum(-1).item())\n        if len(mavgs):\n            mavgs.append(evals[-1] * 0.05 + mavgs[-1] * 0.95)\n        else:\n            mavgs.append(evals[-1])\n        losses.append(error.item())\n        values.append(action_value[mask].mean().item())\n        traj_count.append(prev_traj_count + data[\"done\"].sum().item())\n        prev_traj_count = traj_count[-1]\n        # plots\n        if j % 10 == 0:\n            if is_notebook():\n                display.clear_output(wait=True)\n                display.display(plt.gcf())\n            else:\n                plt.clf()\n            plt.figure(figsize=(15, 15))\n            plt.subplot(3, 2, 1)\n            plt.plot(frames[-len(evals) :], evals, label=\"return\")\n            plt.plot(frames[-len(mavgs) :], mavgs, label=\"mavg\")\n            plt.xlabel(\"frames collected\")\n            plt.ylabel(\"trajectory length (= return)\")\n            plt.subplot(3, 2, 2)\n            plt.plot(traj_count[-len(evals) :], evals, label=\"return\")\n            plt.plot(traj_count[-len(mavgs) :], mavgs, label=\"mavg\")\n            plt.xlabel(\"trajectories collected\")\n            plt.legend()\n            plt.subplot(3, 2, 3)\n            plt.plot(frames[-len(losses) :], losses)\n            plt.xlabel(\"frames collected\")\n            plt.title(\"loss\")\n            plt.subplot(3, 2, 4)\n            plt.plot(frames[-len(values) :], values)\n            plt.xlabel(\"frames collected\")\n            plt.title(\"value\")\n            plt.subplot(3, 2, 5)\n            plt.plot(frames[-len(grad_vals) :], grad_vals)\n            plt.xlabel(\"frames collected\")\n            plt.title(\"grad norm\")\n            if len(traj_lengths):\n                plt.subplot(3, 2, 6)\n                plt.plot(traj_lengths)\n                plt.xlabel(\"batches\")\n                plt.title(\"traj length (training)\")\n        plt.savefig(\"dqn_tdlambda.png\")\n        if is_notebook():\n            plt.show()\n\n    # update policy weights\n    data_collector.update_policy_weights_()\n\nif is_notebook():\n    display.clear_output(wait=True)\n    display.display(plt.gcf())\n\n###############################################################################\n# **Note**: As already mentioned above, to get a more reasonable performance,\n# use a greater value for ``total_frames`` e.g. 500000.\n\nplt.figure(figsize=(15, 15))\nplt.imshow(plt.imread(\"dqn_tdlambda.png\"))\nplt.tight_layout()\nplt.axis(\"off\")\n\n###############################################################################\n\n# save results\ntorch.save(\n    {\n        \"frames\": frames,\n        \"evals\": evals,\n        \"mavgs\": mavgs,\n        \"losses\": losses,\n        \"values\": values,\n        \"grad_vals\": grad_vals,\n        \"traj_lengths_training\": traj_lengths,\n        \"traj_count\": traj_count,\n        \"weights\": (params,),\n    },\n    \"saved_results_tdlambda.pt\",\n)\n\n###############################################################################\n# Let's compare the results on a single plot. Because the TD(lambda) version\n# works better, we'll have fewer episodes collected for a given number of\n# frames (as there are more frames per episode).\n#\n# **Note**: As already mentioned above, to get a more reasonable performance,\n# use a greater value for ``total_frames`` e.g. 500000.\n\nload_td0 = torch.load(\"saved_results_td0.pt\")\nload_tdlambda = torch.load(\"saved_results_tdlambda.pt\")\nframes_td0 = load_td0[\"frames\"]\nframes_tdlambda = load_tdlambda[\"frames\"]\nevals_td0 = load_td0[\"evals\"]\nevals_tdlambda = load_tdlambda[\"evals\"]\nmavgs_td0 = load_td0[\"mavgs\"]\nmavgs_tdlambda = load_tdlambda[\"mavgs\"]\nlosses_td0 = load_td0[\"losses\"]\nlosses_tdlambda = load_tdlambda[\"losses\"]\nvalues_td0 = load_td0[\"values\"]\nvalues_tdlambda = load_tdlambda[\"values\"]\ngrad_vals_td0 = load_td0[\"grad_vals\"]\ngrad_vals_tdlambda = load_tdlambda[\"grad_vals\"]\ntraj_lengths_td0 = load_td0[\"traj_lengths_training\"]\ntraj_lengths_tdlambda = load_tdlambda[\"traj_lengths_training\"]\ntraj_count_td0 = load_td0[\"traj_count\"]\ntraj_count_tdlambda = load_tdlambda[\"traj_count\"]\n\nplt.figure(figsize=(15, 15))\nplt.subplot(3, 2, 1)\nplt.plot(frames[-len(evals_td0) :], evals_td0, label=\"return (td0)\", alpha=0.5)\nplt.plot(\n    frames[-len(evals_tdlambda) :],\n    evals_tdlambda,\n    label=\"return (td(lambda))\",\n    alpha=0.5,\n)\nplt.plot(frames[-len(mavgs_td0) :], mavgs_td0, label=\"mavg (td0)\")\nplt.plot(frames[-len(mavgs_tdlambda) :], mavgs_tdlambda, label=\"mavg (td(lambda))\")\nplt.xlabel(\"frames collected\")\nplt.ylabel(\"trajectory length (= return)\")\nplt.subplot(3, 2, 2)\nplt.plot(traj_count_td0[-len(evals_td0) :], evals_td0, label=\"return (td0)\", alpha=0.5)\nplt.plot(\n    traj_count_tdlambda[-len(evals_tdlambda) :],\n    evals_tdlambda,\n    label=\"return (td(lambda))\",\n    alpha=0.5,\n)\nplt.plot(traj_count_td0[-len(mavgs_td0) :], mavgs_td0, label=\"mavg (td0)\")\nplt.plot(\n    traj_count_tdlambda[-len(mavgs_tdlambda) :],\n    mavgs_tdlambda,\n    label=\"mavg (td(lambda))\",\n)\nplt.xlabel(\"trajectories collected\")\nplt.legend()\nplt.subplot(3, 2, 3)\nplt.plot(frames[-len(losses_td0) :], losses_td0, label=\"loss (td0)\")\nplt.plot(frames[-len(losses_tdlambda) :], losses_tdlambda, label=\"loss (td(lambda))\")\nplt.xlabel(\"frames collected\")\nplt.title(\"loss\")\nplt.legend()\nplt.subplot(3, 2, 4)\nplt.plot(frames[-len(values_td0) :], values_td0, label=\"values (td0)\")\nplt.plot(frames[-len(values_tdlambda) :], values_tdlambda, label=\"values (td(lambda))\")\nplt.xlabel(\"frames collected\")", "metadata": {"task_id": "pytorch_rl/189", "ground_truth": "plt.title(\"value\")", "fpath_tuple": ["pytorch_rl", "tutorials", "sphinx-tutorials", "coding_dqn.py"], "context_start_lineno": 675, "line_no": 825, "query_window": {"context": "    alpha=0.5,\n)\nplt.plot(traj_count_td0[-len(mavgs_td0) :], mavgs_td0, label=\"mavg (td0)\")\nplt.plot(\n    traj_count_tdlambda[-len(mavgs_tdlambda) :],\n    mavgs_tdlambda,\n    label=\"mavg (td(lambda))\",\n)\nplt.xlabel(\"trajectories collected\")\nplt.legend()\nplt.subplot(3, 2, 3)\nplt.plot(frames[-len(losses_td0) :], losses_td0, label=\"loss (td0)\")\nplt.plot(frames[-len(losses_tdlambda) :], losses_tdlambda, label=\"loss (td(lambda))\")\nplt.xlabel(\"frames collected\")\nplt.title(\"loss\")\nplt.legend()\nplt.subplot(3, 2, 4)\nplt.plot(frames[-len(values_td0) :], values_td0, label=\"values (td0)\")\nplt.plot(frames[-len(values_tdlambda) :], values_tdlambda, label=\"values (td(lambda))\")\nplt.xlabel(\"frames collected\")", "metadata": {"fpath_tuple": ["pytorch_rl", "tutorials", "sphinx-tutorials", "coding_dqn.py"], "line_no": 825, "task_id": "pytorch_rl/189", "start_line_no": 805, "end_line_no": 825, "window_size": 20, "context_start_lineno": 675, "repo": "pytorch_rl"}}, "top_k_context": [{"context": "            plt.plot(frames[-len(mavgs) :], mavgs, label=\"mavg\")\n            plt.xlabel(\"frames collected\")\n            plt.ylabel(\"trajectory length (= return)\")\n            plt.subplot(3, 2, 2)\n            plt.plot(traj_count[-len(evals) :], evals, label=\"return\")\n            plt.plot(traj_count[-len(mavgs) :], mavgs, label=\"mavg\")\n            plt.xlabel(\"trajectories collected\")\n            plt.legend()\n            plt.subplot(3, 2, 3)\n            plt.plot(frames[-len(losses) :], losses)\n            plt.xlabel(\"frames collected\")\n            plt.title(\"loss\")\n            plt.subplot(3, 2, 4)\n            plt.plot(frames[-len(values) :], values)\n            plt.xlabel(\"frames collected\")\n            plt.title(\"value\")\n            plt.subplot(3, 2, 5)\n            plt.plot(frames[-len(grad_vals) :], grad_vals)\n            plt.xlabel(\"frames collected\")\n            plt.title(\"grad norm\")", "metadata": [{"fpath_tuple": ["pytorch_rl", "tutorials", "sphinx-tutorials", "coding_dqn.py"], "line_no": 464, "start_line_no": 454, "end_line_no": 474, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}, {"fpath_tuple": ["pytorch_rl", "tutorials", "sphinx-tutorials", "coding_dqn.py"], "line_no": 706, "start_line_no": 696, "end_line_no": 716, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}], "sim_score": 0.6}, {"context": "            plt.subplot(3, 2, 1)\n            plt.plot(frames[-len(evals) :], evals, label=\"return\")\n            plt.plot(frames[-len(mavgs) :], mavgs, label=\"mavg\")\n            plt.xlabel(\"frames collected\")\n            plt.ylabel(\"trajectory length (= return)\")\n            plt.subplot(3, 2, 2)\n            plt.plot(traj_count[-len(evals) :], evals, label=\"return\")\n            plt.plot(traj_count[-len(mavgs) :], mavgs, label=\"mavg\")\n            plt.xlabel(\"trajectories collected\")\n            plt.legend()\n            plt.subplot(3, 2, 3)\n            plt.plot(frames[-len(losses) :], losses)\n            plt.xlabel(\"frames collected\")\n            plt.title(\"loss\")\n            plt.subplot(3, 2, 4)\n            plt.plot(frames[-len(values) :], values)\n            plt.xlabel(\"frames collected\")\n            plt.title(\"value\")\n            plt.subplot(3, 2, 5)\n            plt.plot(frames[-len(grad_vals) :], grad_vals)", "metadata": [{"fpath_tuple": ["pytorch_rl", "tutorials", "sphinx-tutorials", "coding_dqn.py"], "line_no": 462, "start_line_no": 452, "end_line_no": 472, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}, {"fpath_tuple": ["pytorch_rl", "tutorials", "sphinx-tutorials", "coding_dqn.py"], "line_no": 704, "start_line_no": 694, "end_line_no": 714, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}], "sim_score": 0.6}, {"context": "plt.axis(\"off\")\n\n###############################################################################\n\n# save results\ntorch.save(\n    {\n        \"frames\": frames,\n        \"evals\": evals,\n        \"mavgs\": mavgs,\n        \"losses\": losses,\n        \"values\": values,\n        \"grad_vals\": grad_vals,\n        \"traj_lengths_training\": traj_lengths,\n        \"traj_count\": traj_count,\n        \"weights\": (params,),\n    },\n    \"saved_results_td0.pt\",\n)\n", "metadata": [{"fpath_tuple": ["pytorch_rl", "tutorials", "sphinx-tutorials", "coding_dqn.py"], "line_no": 508, "start_line_no": 498, "end_line_no": 518, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}], "sim_score": 0.32222222222222224}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# fortuna/training/trainer.py\n# --------------------------------------------------\n#         grad = lax.pmean(grads, axis_name=\"batch\")\n#         loss = lax.pmean(loss, axis_name=\"batch\")\n#         return grad, loss\n# \n#     def save_checkpoint(\n#         self,\n#         state: TrainState,\n#         save_checkpoint_dir: Path,\n#         keep: int = 1,\n#         force_save: bool = False,\n#         prefix: str = \"checkpoint_\",\n#     ) -> None:\n#         state = self.sync_mutable(state)\n#         state = jax.device_get(tree_map(lambda x: x[0], state))\n#         return super(MultiDeviceMixin, self).save_checkpoint(\n#             state, save_checkpoint_dir, keep, force_save, prefix\n#         )\n# \n#     def on_train_start(\n#         self, state: TrainState, dataloaders: List[DataLoader], rng: PRNGKeyArray\n# --------------------------------------------------\n# the below code fragment can be found in:\n# fortuna/calibration/calibrator.py\n# --------------------------------------------------\n#         grad = lax.pmean(grads, axis_name=\"batch\")\n#         loss = lax.pmean(loss, axis_name=\"batch\")\n#         return grad, loss\n# \n#     def save_checkpoint(\n#         self,\n#         state: CalibState,\n#         save_checkpoint_dir: Path,\n#         keep: int = 1,\n#         force_save: bool = False,\n#         prefix: str = \"checkpoint_\",\n#     ) -> None:\n#         state = self.sync_mutable(state)\n#         state = jax.device_get(tree_map(lambda x: x[0], state))\n#         return super(MultiDeviceMixin, self).save_checkpoint(\n#             state, save_checkpoint_dir, keep, force_save, prefix\n#         )\n# \n#     def on_train_start(\n#         self,\n# --------------------------------------------------\n# the below code fragment can be found in:\n# fortuna/calibration/calibrator.py\n# --------------------------------------------------\n#         return grad, loss\n# \n#     def save_checkpoint(\n#         self,\n#         state: CalibState,\n#         save_checkpoint_dir: Path,\n#         keep: int = 1,\n#         force_save: bool = False,\n#         prefix: str = \"checkpoint_\",\n#     ) -> None:\n#         state = self.sync_mutable(state)\n#         state = jax.device_get(tree_map(lambda x: x[0], state))\n#         return super(MultiDeviceMixin, self).save_checkpoint(\n#             state, save_checkpoint_dir, keep, force_save, prefix\n#         )\n# \n#     def on_train_start(\n#         self,\n#         state: CalibState,\n#         data_loaders: List[DataLoader],\n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\nmetrics_step(aux, targets, metrics)\n        return {\"val_loss\": val_loss, **val_metrics}\n\n    def val_loss_step(\n        self,\n        state: CalibState,\n        targets: Array,\n        outputs: Array,\n        fun: Callable,\n        rng: PRNGKeyArray,\n    ) -> Tuple[jnp.ndarray, Dict[str, jnp.ndarray]]:\n        log_joint_probs, aux = fun(\n            params=state.params,\n            targets=targets,\n            outputs=outputs,\n            mutable=state.mutable,\n            rng=rng,\n            return_aux=[\"outputs\"],\n        )\n        return -log_joint_probs, aux\n\n    def val_metrics_step(\n        self,\n        aux: Dict[str, jnp.ndarray],\n        targets: Array,\n        metrics: Optional[\n            Tuple[Callable[[jnp.ndarray, jnp.ndarray, Array], Array], ...]\n        ] = None,\n    ) -> Dict[str, jnp.ndarray]:\n        if metrics is not None:\n            val_metrics = self.compute_metrics(\n                self.predict_fn(aux[\"outputs\"]),\n                self.uncertainty_fn(aux[\"outputs\"]),\n                targets,\n                metrics,\n            )\n            return {f\"val_{m}\": v for m, v in val_metrics.items()}\n        else:\n            return {}\n\n    def training_epoch_end(\n        self, training_losses_and_metrics_current_epoch: List[Dict[str, jnp.ndarray]]\n    ) -> Dict[str, float]:\n        return self._get_mean_losses_and_metrics(\n            training_losses_and_metrics_current_epoch\n        )\n\n    def val_epoch_end(\n        self,\n        val_losses_and_metrics_current_epoch: List[Dict[str, jnp.ndarray]],\n        state: CalibState,\n    ) -> Dict[str, float]:\n        val_losses_and_metrics_current_epoch = self._get_mean_losses_and_metrics(\n            val_losses_and_metrics_current_epoch\n        )\n        # early stopping\n        improved = self.early_stopping_update(val_losses_and_metrics_current_epoch)\n        if improved and self.save_checkpoint_dir:\n            self.save_checkpoint(state, self.save_checkpoint_dir, force_save=True)\n        return val_losses_and_metrics_current_epoch\n\n    def _get_mean_losses_and_metrics(\n        self, losses_and_metrics: List[Dict[str, jnp.ndarray]]\n    ) -> Dict[str, float]:\n        losses_and_metrics = stack_forest(losses_and_metrics)\n        losses_and_metrics = tree_map(lambda x: x.mean(), losses_and_metrics)\n        return losses_and_metrics\n\n    def should_perform_validation(\n        self, val_data_loader: Optional[DataLoader], epoch: int\n    ) -> bool:\n        return (\n            val_data_loader is not None\n            and self.eval_every_n_epochs > 0\n            and epoch % self.eval_every_n_epochs == 0\n        )\n\n    @staticmethod\n    def sync_gradients_and_loss(\n        grad: jnp.ndarray, loss: jnp.ndarray\n    ) -> Tuple[jnp.ndarray, jnp.ndarray]:\n        return grad, loss\n\n    def on_train_start(\n        self,\n        state: CalibState,\n        targets: List[Array],\n        outputs: List[Array],\n        rng: PRNGKeyArray,\n    ) -> Tuple[CalibState, List[Array], List[Array], PRNGKeyArray]:\n        return state, targets, outputs, rng\n\n    def on_train_end(self, state: CalibState) -> CalibState:\n        self.save_checkpoint(\n            state,\n            save_checkpoint_dir=self.save_checkpoint_dir,\n            keep=self.keep_top_n_checkpoints,\n            force_save=True,\n        )\n        return state\n\n    def on_val_start(self, state: CalibState) -> CalibState:\n        return state\n\n    def compute_metrics(\n        self,\n        preds: Array,\n        uncertainties: Array,\n        targets: Array,\n        metrics: Optional[\n            Tuple[Callable[[jnp.ndarray, jnp.ndarray, Array], Array], ...]\n        ],\n    ) -> Dict[str, Array]:\n        metrics_vals = {}\n        for metric in metrics:\n            metrics_vals[metric.__name__] = metric(preds, uncertainties, targets)\n        return metrics_vals\n\n\nclass JittedMixin:\n    @partial(jax.jit, static_argnums=(0, 4))\n    def training_step(\n        self,\n        state: CalibState,\n        targets: Array,\n        outputs: Array,\n        fun: Callable,\n        rng: PRNGKeyArray,\n    ) -> Tuple[CalibState, Dict[str, Any]]:\n        return super().training_step(state, targets, outputs, fun, rng)\n\n    @partial(jax.jit, static_argnums=(0, 4))\n    def val_loss_step(\n        self,\n        state: CalibState,\n        targets: Array,\n        outputs: Array,\n        fun: Callable,\n        rng: PRNGKeyArray,\n    ) -> Dict[str, jnp.ndarray]:\n        return super().val_loss_step(state, targets, outputs, fun, rng)\n\n\nclass MultiDeviceMixin:\n    all_reduce_mean = jax.pmap(lambda x: lax.pmean(x, \"x\"), \"x\")\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.multi_device = True\n\n    @staticmethod\n    def _add_device_dim_to_array(arr: Array) -> Array:\n        n_devices = jax.local_device_count()\n        if arr.shape[0] % n_devices != 0:\n            raise ValueError(\n                f\"The number of data points of all outputs and targets must be a multiple of {n_devices}, \"\n                f\"that is the number of available devices. However, {arr.shape[0]} were found.\"\n            )\n        return arr.reshape((n_devices, -1) + arr.shape[1:]) if arr is not None else arr\n\n    @staticmethod\n    def sync_mutable(state: CalibState) -> CalibState:\n        return (\n            state.replace(mutable=MultiDeviceMixin.all_reduce_mean(state.mutable))\n            if state.mutable[\"output_calibrator\"] is not None\n            else state\n        )\n\n    @staticmethod\n    def sync_gradients_and_loss(\n        grads: jnp.ndarray, loss: jnp.ndarray\n    ) -> Tuple[jnp.ndarray, jnp.ndarray]:\n        grad = lax.pmean(grads, axis_name=\"batch\")\n        loss = lax.pmean(loss, axis_name=\"batch\")\n        return grad, loss\n\n    def save_checkpoint(\n        self,\n        state: CalibState,\n        save_checkpoint_dir: Path,\n        keep: int = 1,\n        force_save: bool = False,\n        prefix: str = \"checkpoint_\",\n    ) -> None:\n        state = self.sync_mutable(state)\n        state = jax.device_get(tree_map(lambda x: x[0], state))\n        return super(MultiDeviceMixin, self).save_checkpoint(\n            state, save_checkpoint_dir, keep, force_save, prefix\n        )\n\n    def on_train_start(\n        self,", "metadata": {"task_id": "awslabs_fortuna/159", "ground_truth": "        state: CalibState,", "fpath_tuple": ["awslabs_fortuna", "fortuna", "calib_model", "calib_model_calibrator.py"], "context_start_lineno": 355, "line_no": 547, "query_window": {"context": "        grad = lax.pmean(grads, axis_name=\"batch\")\n        loss = lax.pmean(loss, axis_name=\"batch\")\n        return grad, loss\n\n    def save_checkpoint(\n        self,\n        state: CalibState,\n        save_checkpoint_dir: Path,\n        keep: int = 1,\n        force_save: bool = False,\n        prefix: str = \"checkpoint_\",\n    ) -> None:\n        state = self.sync_mutable(state)\n        state = jax.device_get(tree_map(lambda x: x[0], state))\n        return super(MultiDeviceMixin, self).save_checkpoint(\n            state, save_checkpoint_dir, keep, force_save, prefix\n        )\n\n    def on_train_start(\n        self,", "metadata": {"fpath_tuple": ["awslabs_fortuna", "fortuna", "calib_model", "calib_model_calibrator.py"], "line_no": 547, "task_id": "awslabs_fortuna/159", "start_line_no": 527, "end_line_no": 547, "window_size": 20, "context_start_lineno": 355, "repo": "awslabs_fortuna"}}, "top_k_context": [{"context": "        grad = lax.pmean(grads, axis_name=\"batch\")\n        loss = lax.pmean(loss, axis_name=\"batch\")\n        return grad, loss\n\n    def save_checkpoint(\n        self,\n        state: CalibState,\n        save_checkpoint_dir: Path,\n        keep: int = 1,\n        force_save: bool = False,\n        prefix: str = \"checkpoint_\",\n    ) -> None:\n        state = self.sync_mutable(state)\n        state = jax.device_get(tree_map(lambda x: x[0], state))\n        return super(MultiDeviceMixin, self).save_checkpoint(\n            state, save_checkpoint_dir, keep, force_save, prefix\n        )\n\n    def on_train_start(\n        self,", "metadata": [{"fpath_tuple": ["awslabs_fortuna", "fortuna", "calibration", "calibrator.py"], "line_no": 576, "start_line_no": 566, "end_line_no": 586, "window_size": 20, "repo": "awslabs_fortuna", "slice_size": 10}], "sim_score": 1.0}, {"context": "        grads: jnp.ndarray, loss: jnp.ndarray\n    ) -> Tuple[jnp.ndarray, jnp.ndarray]:\n        grad = lax.pmean(grads, axis_name=\"batch\")\n        loss = lax.pmean(loss, axis_name=\"batch\")\n        return grad, loss\n\n    def save_checkpoint(\n        self,\n        state: CalibState,\n        save_checkpoint_dir: Path,\n        keep: int = 1,\n        force_save: bool = False,\n        prefix: str = \"checkpoint_\",\n    ) -> None:\n        state = self.sync_mutable(state)\n        state = jax.device_get(tree_map(lambda x: x[0], state))\n        return super(MultiDeviceMixin, self).save_checkpoint(\n            state, save_checkpoint_dir, keep, force_save, prefix\n        )\n", "metadata": [{"fpath_tuple": ["awslabs_fortuna", "fortuna", "calibration", "calibrator.py"], "line_no": 574, "start_line_no": 564, "end_line_no": 584, "window_size": 20, "repo": "awslabs_fortuna", "slice_size": 10}], "sim_score": 0.8690476190476191}, {"context": "        grads: jnp.ndarray, loss: jnp.ndarray\n    ) -> Tuple[jnp.ndarray, jnp.ndarray]:\n        grad = lax.pmean(grads, axis_name=\"batch\")\n        loss = lax.pmean(loss, axis_name=\"batch\")\n        return grad, loss\n\n    def save_checkpoint(\n        self,\n        state: TrainState,\n        save_checkpoint_dir: Path,\n        keep: int = 1,\n        force_save: bool = False,\n        prefix: str = \"checkpoint_\",\n    ) -> None:\n        state = self.sync_mutable(state)\n        state = jax.device_get(tree_map(lambda x: x[0], state))\n        return super(MultiDeviceMixin, self).save_checkpoint(\n            state, save_checkpoint_dir, keep, force_save, prefix\n        )\n", "metadata": [{"fpath_tuple": ["awslabs_fortuna", "fortuna", "training", "trainer.py"], "line_no": 506, "start_line_no": 496, "end_line_no": 516, "window_size": 20, "repo": "awslabs_fortuna", "slice_size": 10}], "sim_score": 0.8352941176470589}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# ding/hpc_rl/tests/test_tdlambda.py\n# --------------------------------------------------\n#         ori_loss.backward()\n#         if use_cuda:\n#             torch.cuda.synchronize()\n#         print('epoch: {}, original td cost time: {}'.format(i, time.time() - t))\n# \n#     hpc_value.requires_grad_(True)\n#     for i in range(times):\n#         t = time.time()\n#         hpc_loss = hpc_td(hpc_value, hpc_reward, hpc_weight)\n#         hpc_loss = hpc_loss.mean()\n#         hpc_loss.backward()\n#         if use_cuda:\n#             torch.cuda.synchronize()\n#         print('epoch: {}, hpc td cost time: {}'.format(i, time.time() - t))\n# \n# \n# if __name__ == '__main__':\n#     print(\"target problem: T = {}, B = {}\".format(T, B))\n#     print(\"================run td validation test================\")\n#     td_val()\n# --------------------------------------------------\n# the below code fragment can be found in:\n# ding/hpc_rl/tests/test_vtrace.py\n# --------------------------------------------------\n#         ori_loss.backward()\n#         if use_cuda:\n#             torch.cuda.synchronize()\n#         print('epoch: {}, original vtrace cost time: {}'.format(i, time.time() - t))\n# \n#     hpc_target_output.requires_grad_(True)\n#     hpc_value.requires_grad_(True)\n#     for i in range(times):\n#         t = time.time()\n#         hpc_loss = hpc_vtrace(hpc_target_output, hpc_behaviour_output, hpc_action, hpc_value, hpc_reward)\n#         hpc_loss = sum(hpc_loss)\n#         hpc_loss.backward()\n#         if use_cuda:\n#             torch.cuda.synchronize()\n#         print('epoch: {}, hpc vtrace cost time: {}'.format(i, time.time() - t))\n# \n# \n# if __name__ == '__main__':\n#     print(\"target problem: T = {}, B = {}, N = {}\".format(T, B, N))\n#     print(\"================run vtrace validation test================\")\n# --------------------------------------------------\n# the below code fragment can be found in:\n# ding/hpc_rl/tests/test_upgo.py\n# --------------------------------------------------\n#         ori_loss = ori_loss.mean()\n#         ori_loss.backward()\n#         if use_cuda:\n#             torch.cuda.synchronize()\n#         print('epoch: {}, original upgo cost time: {}'.format(i, time.time() - t))\n# \n#     hpc_target_output.requires_grad_(True)\n#     for i in range(times):\n#         t = time.time()\n#         hpc_loss = hpc_upgo(hpc_target_output, hpc_rhos, hpc_action, hpc_rewards, hpc_bootstrap_values)\n#         hpc_loss = hpc_loss.mean()\n#         hpc_loss.backward()\n#         if use_cuda:\n#             torch.cuda.synchronize()\n#         print('epoch: {}, hpc upgo cost time: {}'.format(i, time.time() - t))\n# \n# \n# if __name__ == '__main__':\n#     print(\"target problem: T = {}, B = {}, N = {}\".format(T, B, N))\n#     print(\"================run upgo validation test================\")\n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\nimport time\nimport torch\nfrom typing import Tuple\nfrom hpc_rll.origin.scatter_connection import ScatterConnection\nfrom hpc_rll.torch_utils.network.scatter_connection import ScatterConnection as HPCScatterConnection\nfrom testbase import mean_relative_error, times\n\nassert torch.cuda.is_available()\nuse_cuda = True\n\nB = 256\nM = 256\nN = 256\nH = 16\nW = 16\n\n\n# Note: origin gpu version of cover mode is not determinate, thus validation test use origin cpu version instead\ndef scatter_val():\n    for scatter_type in ['add', 'cover']:\n        ori_input = torch.randn(B, M, N)\n        h = torch.randint(\n            low=0, high=H, size=(\n                B,\n                M,\n            )\n        ).unsqueeze(dim=2)\n        w = torch.randint(\n            low=0, high=W, size=(\n                B,\n                M,\n            )\n        ).unsqueeze(dim=2)\n        ori_location = torch.cat([h, w], dim=2)\n        ori_scatter = ScatterConnection(scatter_type)\n\n        hpc_input = ori_input.clone().detach()\n        hpc_location = ori_location.clone().detach()\n        hpc_scatter = HPCScatterConnection(B, M, N, H, W, scatter_type)\n\n        if use_cuda:\n            #ori_input = ori_input.cuda()\n            #ori_location = ori_location.cuda()\n            #ori_scatter = ori_scatter.cuda()\n\n            hpc_input = hpc_input.cuda()\n            hpc_location = hpc_location.cuda()\n            hpc_scatter = hpc_scatter.cuda()\n\n        ori_input.requires_grad_(True)\n        ori_output = ori_scatter(ori_input, (H, W), ori_location)\n        ori_loss = ori_output * ori_output\n        ori_loss = ori_loss.mean()\n        ori_loss.backward()\n        if use_cuda:\n            torch.cuda.synchronize()\n\n        hpc_input.requires_grad_(True)\n        hpc_output = hpc_scatter(hpc_input, hpc_location)\n        hpc_loss = hpc_output * hpc_output\n        hpc_loss = hpc_loss.mean()\n        hpc_loss.backward()\n        if use_cuda:\n            torch.cuda.synchronize()\n\n        mre = mean_relative_error(\n            torch.flatten(ori_loss).cpu().detach().numpy(),\n            torch.flatten(hpc_loss).cpu().detach().numpy()\n        )\n        print(\"scatter type {} fp mean_relative_error: {}\".format(scatter_type, str(mre)))\n        mre = mean_relative_error(\n            torch.flatten(ori_input.grad).cpu().detach().numpy(),\n            torch.flatten(hpc_input.grad).cpu().detach().numpy()\n        )\n        print(\"scatter type {} bp mean_relative_error: {}\".format(scatter_type, str(mre)))\n\n\n# Note: performance test use origin gpu version\ndef scatter_perf():\n    for scatter_type in ['add', 'cover']:\n        ori_input = torch.randn(B, M, N)\n        h = torch.randint(\n            low=0, high=H, size=(\n                B,\n                M,\n            )\n        ).unsqueeze(dim=2)\n        w = torch.randint(\n            low=0, high=W, size=(\n                B,\n                M,\n            )\n        ).unsqueeze(dim=2)\n        ori_location = torch.cat([h, w], dim=2)\n        ori_scatter = ScatterConnection(scatter_type)\n\n        hpc_input = ori_input.clone().detach()\n        hpc_location = ori_location.clone().detach()\n        hpc_scatter = HPCScatterConnection(B, M, N, H, W, scatter_type)\n\n        if use_cuda:\n            ori_input = ori_input.cuda()\n            ori_location = ori_location.cuda()\n            ori_scatter = ori_scatter.cuda()\n\n            hpc_input = hpc_input.cuda()\n            hpc_location = hpc_location.cuda()\n            hpc_scatter = hpc_scatter.cuda()\n\n        for i in range(times):\n            t = time.time()\n            ori_input.requires_grad_(True)\n            ori_output = ori_scatter(ori_input, (H, W), ori_location)\n            ori_loss = ori_output * ori_output\n            ori_loss = ori_loss.mean()\n            ori_loss.backward()\n            if use_cuda:\n                torch.cuda.synchronize()\n            print('epoch: {}, original scatter type {} cost time: {}'.format(i, scatter_type, time.time() - t))\n\n        for i in range(times):\n            t = time.time()\n            hpc_input.requires_grad_(True)\n            hpc_output = hpc_scatter(hpc_input, hpc_location)\n            hpc_loss = hpc_output * hpc_output\n            hpc_loss = hpc_loss.mean()\n            hpc_loss.backward()\n            if use_cuda:\n                torch.cuda.synchronize()\n            print('epoch: {}, hpc scatter type {} cost time: {}'.format(i, scatter_type, time.time() - t))\n\n\nif __name__ == '__main__':", "metadata": {"task_id": "opendilab_ACE/43", "ground_truth": "    print(\"target problem: B = {}, M = {}, N = {}, H = {}, W = {}\".format(B, M, N, H, W))", "fpath_tuple": ["opendilab_ACE", "ding", "hpc_rl", "tests", "test_scatter.py"], "context_start_lineno": 0, "line_no": 133, "query_window": {"context": "            ori_loss = ori_output * ori_output\n            ori_loss = ori_loss.mean()\n            ori_loss.backward()\n            if use_cuda:\n                torch.cuda.synchronize()\n            print('epoch: {}, original scatter type {} cost time: {}'.format(i, scatter_type, time.time() - t))\n\n        for i in range(times):\n            t = time.time()\n            hpc_input.requires_grad_(True)\n            hpc_output = hpc_scatter(hpc_input, hpc_location)\n            hpc_loss = hpc_output * hpc_output\n            hpc_loss = hpc_loss.mean()\n            hpc_loss.backward()\n            if use_cuda:\n                torch.cuda.synchronize()\n            print('epoch: {}, hpc scatter type {} cost time: {}'.format(i, scatter_type, time.time() - t))\n\n\nif __name__ == '__main__':", "metadata": {"fpath_tuple": ["opendilab_ACE", "ding", "hpc_rl", "tests", "test_scatter.py"], "line_no": 133, "task_id": "opendilab_ACE/43", "start_line_no": 113, "end_line_no": 133, "window_size": 20, "context_start_lineno": 0, "repo": "opendilab_ACE"}}, "top_k_context": [{"context": "        t = time.time()\n        ori_loss = upgo_loss(ori_target_output, ori_rhos, ori_action, ori_rewards, ori_bootstrap_values)\n        ori_loss = ori_loss.mean()\n        ori_loss.backward()\n        if use_cuda:\n            torch.cuda.synchronize()\n        print('epoch: {}, original upgo cost time: {}'.format(i, time.time() - t))\n\n    hpc_target_output.requires_grad_(True)\n    for i in range(times):\n        t = time.time()\n        hpc_loss = hpc_upgo(hpc_target_output, hpc_rhos, hpc_action, hpc_rewards, hpc_bootstrap_values)\n        hpc_loss = hpc_loss.mean()\n        hpc_loss.backward()\n        if use_cuda:\n            torch.cuda.synchronize()\n        print('epoch: {}, hpc upgo cost time: {}'.format(i, time.time() - t))\n\n\nif __name__ == '__main__':", "metadata": [{"fpath_tuple": ["opendilab_ACE", "ding", "hpc_rl", "tests", "test_upgo.py"], "line_no": 118, "start_line_no": 108, "end_line_no": 128, "window_size": 20, "repo": "opendilab_ACE", "slice_size": 10}], "sim_score": 0.735632183908046}, {"context": "        )\n        ori_loss = sum(ori_loss)\n        ori_loss.backward()\n        if use_cuda:\n            torch.cuda.synchronize()\n        print('epoch: {}, original vtrace cost time: {}'.format(i, time.time() - t))\n\n    hpc_target_output.requires_grad_(True)\n    hpc_value.requires_grad_(True)\n    for i in range(times):\n        t = time.time()\n        hpc_loss = hpc_vtrace(hpc_target_output, hpc_behaviour_output, hpc_action, hpc_value, hpc_reward)\n        hpc_loss = sum(hpc_loss)\n        hpc_loss.backward()\n        if use_cuda:\n            torch.cuda.synchronize()\n        print('epoch: {}, hpc vtrace cost time: {}'.format(i, time.time() - t))\n\n\nif __name__ == '__main__':", "metadata": [{"fpath_tuple": ["opendilab_ACE", "ding", "hpc_rl", "tests", "test_vtrace.py"], "line_no": 128, "start_line_no": 118, "end_line_no": 138, "window_size": 20, "repo": "opendilab_ACE", "slice_size": 10}], "sim_score": 0.7325581395348837}, {"context": "        ori_loss = td_lambda_error(td_lambda_data(ori_value, ori_reward, ori_weight))\n        ori_loss = ori_loss.mean()\n        ori_loss.backward()\n        if use_cuda:\n            torch.cuda.synchronize()\n        print('epoch: {}, original td cost time: {}'.format(i, time.time() - t))\n\n    hpc_value.requires_grad_(True)\n    for i in range(times):\n        t = time.time()\n        hpc_loss = hpc_td(hpc_value, hpc_reward, hpc_weight)\n        hpc_loss = hpc_loss.mean()\n        hpc_loss.backward()\n        if use_cuda:\n            torch.cuda.synchronize()\n        print('epoch: {}, hpc td cost time: {}'.format(i, time.time() - t))\n\n\nif __name__ == '__main__':\n    print(\"target problem: T = {}, B = {}\".format(T, B))", "metadata": [{"fpath_tuple": ["opendilab_ACE", "ding", "hpc_rl", "tests", "test_tdlambda.py"], "line_no": 92, "start_line_no": 82, "end_line_no": 102, "window_size": 20, "repo": "opendilab_ACE", "slice_size": 10}], "sim_score": 0.7}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# test/mocking_classes.py\n# --------------------------------------------------\n#                     )\n#                 ),\n#                 shape=batch_size,\n#             )\n#         if reward_spec is None:\n#             reward_spec = UnboundedContinuousTensorSpec(\n#                 (\n#                     *batch_size,\n#                     1,\n#                 )\n#             )\n#         cls._reward_spec = reward_spec\n#         cls._observation_spec = observation_spec\n#         cls._input_spec = input_spec\n#         return super().__new__(\n#             cls,\n#             *args,\n#             **kwargs,\n#         )\n# \n# --------------------------------------------------\n# the below code fragment can be found in:\n# test/mocking_classes.py\n# --------------------------------------------------\n#         if input_spec is None:\n#             input_spec = CompositeSpec(action=action_spec, shape=batch_size)\n#         cls._reward_spec = reward_spec\n#         cls._observation_spec = observation_spec\n#         cls._input_spec = input_spec\n#         return super().__new__(*args, **kwargs)\n# \n#     def __init__(self, device):\n#         super(MockSerialEnv, self).__init__(device=device)\n#         self.is_closed = False\n# \n#     def _set_seed(self, seed: Optional[int]):\n#         assert seed >= 1\n#         self.seed = seed\n#         self.counter = seed % 17  # make counter a small number\n#         self.max_val = max(self.counter + 100, self.counter * 2)\n# \n#     def _step(self, tensordict):\n#         self.counter += 1\n#         n = torch.tensor(\n# --------------------------------------------------\n# the below code fragment can be found in:\n# test/mocking_classes.py\n# --------------------------------------------------\n#         cls._reward_spec = reward_spec\n#         cls._observation_spec = observation_spec\n#         cls._input_spec = input_spec\n#         return super().__new__(*args, **kwargs)\n# \n#     def __init__(self, device):\n#         super(MockSerialEnv, self).__init__(device=device)\n#         self.is_closed = False\n# \n#     def _set_seed(self, seed: Optional[int]):\n#         assert seed >= 1\n#         self.seed = seed\n#         self.counter = seed % 17  # make counter a small number\n#         self.max_val = max(self.counter + 100, self.counter * 2)\n# \n#     def _step(self, tensordict):\n#         self.counter += 1\n#         n = torch.tensor(\n#             [self.counter], device=self.device, dtype=torch.get_default_dtype()\n#         )\n# --------------------------------------------------\n# the below code fragment can be found in:\n# test/mocking_classes.py\n# --------------------------------------------------\n#                 ),\n#                 shape=batch_size,\n#             )\n#         if reward_spec is None:\n#             reward_spec = UnboundedContinuousTensorSpec(\n#                 (\n#                     *batch_size,\n#                     1,\n#                 )\n#             )\n#         if input_spec is None:\n#             input_spec = CompositeSpec(action=action_spec, shape=batch_size)\n#         cls._reward_spec = reward_spec\n#         cls._observation_spec = observation_spec\n#         cls._input_spec = input_spec\n#         return super().__new__(*args, **kwargs)\n# \n#     def __init__(self, device):\n#         super(MockSerialEnv, self).__init__(device=device)\n#         self.is_closed = False\n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\nlen(self.batch_size)]\n                if tensordict is not None\n                else []\n            )\n        else:\n            leading_batch_size = tensordict.shape if tensordict is not None else []\n\n        n = (\n            torch.full(\n                [*leading_batch_size, *self.observation_spec[\"observation\"].shape],\n                self.counter,\n            )\n            .to(self.device)\n            .to(torch.get_default_dtype())\n        )\n        done = self.counter >= self.max_val\n        done = torch.full(\n            (*leading_batch_size, *batch_size, 1),\n            done,\n            dtype=torch.bool,\n            device=self.device,\n        )\n        return TensorDict(\n            {\"reward\": n, \"done\": done, \"observation\": n},\n            [\n                *leading_batch_size,\n                *batch_size,\n            ],\n            device=self.device,\n        )\n\n\nclass MockBatchedUnLockedEnv(MockBatchedLockedEnv):\n    \"\"\"Mocks an env whose batch_size does not define the size of the output tensordict.\n\n    The size of the output tensordict is defined by the input tensordict itself.\n\n    \"\"\"\n\n    def __init__(self, device, batch_size=None):\n        super(MockBatchedUnLockedEnv, self).__init__(\n            batch_size=batch_size, device=device\n        )\n\n    @classmethod\n    def __new__(cls, *args, **kwargs):\n        return super().__new__(cls, *args, _batch_locked=False, **kwargs)\n\n\nclass DiscreteActionVecMockEnv(_MockEnv):\n    @classmethod\n    def __new__(\n        cls,\n        *args,\n        observation_spec=None,\n        action_spec=None,\n        input_spec=None,\n        reward_spec=None,\n        from_pixels=False,\n        categorical_action_encoding=False,\n        **kwargs,\n    ):\n        batch_size = kwargs.setdefault(\"batch_size\", torch.Size([]))\n        size = cls.size = 7\n        if observation_spec is None:\n            cls.out_key = \"observation\"\n            observation_spec = CompositeSpec(\n                observation=UnboundedContinuousTensorSpec(\n                    shape=torch.Size([*batch_size, size])\n                ),\n                observation_orig=UnboundedContinuousTensorSpec(\n                    shape=torch.Size([*batch_size, size])\n                ),\n                shape=batch_size,\n            )\n        if action_spec is None:\n            action_spec_cls = (\n                DiscreteTensorSpec\n                if categorical_action_encoding\n                else OneHotDiscreteTensorSpec\n            )\n            action_spec = action_spec_cls(*batch_size, 7)\n        if reward_spec is None:\n            reward_spec = UnboundedContinuousTensorSpec(shape=(1,))\n\n        if input_spec is None:\n            cls._out_key = \"observation_orig\"\n            input_spec = CompositeSpec(\n                **{\n                    cls._out_key: observation_spec[\"observation\"],\n                    \"action\": action_spec,\n                }\n            )\n        cls._reward_spec = reward_spec\n        cls._observation_spec = observation_spec\n        cls._input_spec = input_spec\n        cls.from_pixels = from_pixels\n        cls.categorical_action_encoding = categorical_action_encoding\n        return super().__new__(*args, **kwargs)\n\n    def _get_in_obs(self, obs):\n        return obs\n\n    def _get_out_obs(self, obs):\n        return obs\n\n    def _reset(self, tensordict: TensorDictBase = None) -> TensorDictBase:\n        self.counter += 1\n        state = torch.zeros(self.size) + self.counter\n        if tensordict is None:\n            tensordict = TensorDict({}, self.batch_size, device=self.device)\n        tensordict = tensordict.select().set(self.out_key, self._get_out_obs(state))\n        tensordict = tensordict.set(self._out_key, self._get_out_obs(state))\n        tensordict.set(\"done\", torch.zeros(*tensordict.shape, 1, dtype=torch.bool))\n        return tensordict\n\n    def _step(\n        self,\n        tensordict: TensorDictBase,\n    ) -> TensorDictBase:\n        tensordict = tensordict.to(self.device)\n        a = tensordict.get(\"action\")\n\n        if not self.categorical_action_encoding:\n            assert (a.sum(-1) == 1).all()\n\n        obs = self._get_in_obs(tensordict.get(self._out_key)) + a / self.maxstep\n        tensordict = tensordict.select()  # empty tensordict\n\n        tensordict.set(self.out_key, self._get_out_obs(obs))\n        tensordict.set(self._out_key, self._get_out_obs(obs))\n\n        done = torch.isclose(obs, torch.ones_like(obs) * (self.counter + 1))\n        reward = done.any(-1).unsqueeze(-1)\n        # set done to False\n        done = torch.zeros_like(done).all(-1).unsqueeze(-1)\n        tensordict.set(\"reward\", reward.to(torch.get_default_dtype()))\n        tensordict.set(\"done\", done)\n        return tensordict\n\n\nclass ContinuousActionVecMockEnv(_MockEnv):\n    @classmethod\n    def __new__(\n        cls,\n        *args,\n        observation_spec=None,\n        action_spec=None,\n        input_spec=None,\n        reward_spec=None,\n        from_pixels=False,\n        **kwargs,\n    ):\n        batch_size = kwargs.setdefault(\"batch_size\", torch.Size([]))\n        size = cls.size = 7\n        if observation_spec is None:\n            cls.out_key = \"observation\"\n            observation_spec = CompositeSpec(\n                observation=UnboundedContinuousTensorSpec(\n                    shape=torch.Size([*batch_size, size])\n                ),\n                observation_orig=UnboundedContinuousTensorSpec(\n                    shape=torch.Size([*batch_size, size])\n                ),\n                shape=batch_size,\n            )\n        if action_spec is None:\n            action_spec = BoundedTensorSpec(\n                -1,\n                1,\n                (\n                    *batch_size,\n                    7,\n                ),\n            )\n        if reward_spec is None:\n            reward_spec = UnboundedContinuousTensorSpec(shape=(*batch_size, 1))\n\n        if input_spec is None:\n            cls._out_key = \"observation_orig\"\n            input_spec = CompositeSpec(\n                **{\n                    cls._out_key: observation_spec[\"observation\"],\n                    \"action\": action_spec,\n                },\n                shape=batch_size,\n            )\n        cls._reward_spec = reward_spec\n        cls._observation_spec = observation_spec\n        cls._input_spec = input_spec\n        cls.from_pixels = from_pixels\n        return super().__new__(*args, **kwargs)\n\n    def _get_in_obs(self, obs):\n        return obs\n\n    def _get_out_obs(self, obs):\n        return obs\n\n    def _reset(self, tensordict: TensorDictBase) -> TensorDictBase:\n        self.counter += 1\n        self.step_count = 0\n        # state = torch.zeros(self.size) + self.counter\n        if tensordict is None:", "metadata": {"task_id": "pytorch_rl/10", "ground_truth": "            tensordict = TensorDict({}, self.batch_size, device=self.device)", "fpath_tuple": ["pytorch_rl", "test", "mocking_classes.py"], "context_start_lineno": 281, "line_no": 485, "query_window": {"context": "                },\n                shape=batch_size,\n            )\n        cls._reward_spec = reward_spec\n        cls._observation_spec = observation_spec\n        cls._input_spec = input_spec\n        cls.from_pixels = from_pixels\n        return super().__new__(*args, **kwargs)\n\n    def _get_in_obs(self, obs):\n        return obs\n\n    def _get_out_obs(self, obs):\n        return obs\n\n    def _reset(self, tensordict: TensorDictBase) -> TensorDictBase:\n        self.counter += 1\n        self.step_count = 0\n        # state = torch.zeros(self.size) + self.counter\n        if tensordict is None:", "metadata": {"fpath_tuple": ["pytorch_rl", "test", "mocking_classes.py"], "line_no": 485, "task_id": "pytorch_rl/10", "start_line_no": 465, "end_line_no": 485, "window_size": 20, "context_start_lineno": 281, "repo": "pytorch_rl"}}, "top_k_context": [{"context": "                        1,\n                    )\n                ),\n                shape=batch_size,\n            )\n        if reward_spec is None:\n            reward_spec = UnboundedContinuousTensorSpec(\n                (\n                    *batch_size,\n                    1,\n                )\n            )\n        if input_spec is None:\n            input_spec = CompositeSpec(action=action_spec, shape=batch_size)\n        cls._reward_spec = reward_spec\n        cls._observation_spec = observation_spec\n        cls._input_spec = input_spec\n        return super().__new__(*args, **kwargs)\n\n    def __init__(self, device):", "metadata": [{"fpath_tuple": ["pytorch_rl", "test", "mocking_classes.py"], "line_no": 132, "start_line_no": 122, "end_line_no": 142, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}], "sim_score": 0.4895833333333333}, {"context": "        if input_spec is None:\n            input_spec = CompositeSpec(action=action_spec, shape=batch_size)\n        cls._reward_spec = reward_spec\n        cls._observation_spec = observation_spec\n        cls._input_spec = input_spec\n        return super().__new__(*args, **kwargs)\n\n    def __init__(self, device):\n        super(MockSerialEnv, self).__init__(device=device)\n        self.is_closed = False\n\n    def _set_seed(self, seed: Optional[int]):\n        assert seed >= 1\n        self.seed = seed\n        self.counter = seed % 17  # make counter a small number\n        self.max_val = max(self.counter + 100, self.counter * 2)\n\n    def _step(self, tensordict):\n        self.counter += 1\n        n = torch.tensor(", "metadata": [{"fpath_tuple": ["pytorch_rl", "test", "mocking_classes.py"], "line_no": 144, "start_line_no": 134, "end_line_no": 154, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}], "sim_score": 0.475}, {"context": "                )\n            )\n        if input_spec is None:\n            input_spec = CompositeSpec(action=action_spec, shape=batch_size)\n        cls._reward_spec = reward_spec\n        cls._observation_spec = observation_spec\n        cls._input_spec = input_spec\n        return super().__new__(*args, **kwargs)\n\n    def __init__(self, device):\n        super(MockSerialEnv, self).__init__(device=device)\n        self.is_closed = False\n\n    def _set_seed(self, seed: Optional[int]):\n        assert seed >= 1\n        self.seed = seed\n        self.counter = seed % 17  # make counter a small number\n        self.max_val = max(self.counter + 100, self.counter * 2)\n\n    def _step(self, tensordict):", "metadata": [{"fpath_tuple": ["pytorch_rl", "test", "mocking_classes.py"], "line_no": 142, "start_line_no": 132, "end_line_no": 152, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}], "sim_score": 0.4745762711864407}, {"context": "                        *batch_size,\n                        1,\n                    )\n                ),\n                shape=batch_size,\n            )\n        if reward_spec is None:\n            reward_spec = UnboundedContinuousTensorSpec(\n                (\n                    *batch_size,\n                    1,\n                )\n            )\n        cls._reward_spec = reward_spec\n        cls._observation_spec = observation_spec\n        cls._input_spec = input_spec\n        return super().__new__(\n            cls,\n            *args,\n            **kwargs,", "metadata": [{"fpath_tuple": ["pytorch_rl", "test", "mocking_classes.py"], "line_no": 220, "start_line_no": 210, "end_line_no": 230, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}], "sim_score": 0.43956043956043955}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# torchrl/objectives/deprecated.py\n# torchrl/objectives/redq.py\n# --------------------------------------------------\n#             self.register_parameter(\n#                 \"log_alpha\",\n#                 torch.nn.Parameter(torch.tensor(math.log(alpha_init), device=device)),\n#             )\n# \n#         if target_entropy == \"auto\":\n#             if actor_network.spec[\"action\"] is None:\n#                 raise RuntimeError(\n#                     \"Cannot infer the dimensionality of the action. Consider providing \"\n#                     \"the target entropy explicitely or provide the spec of the \"\n#                     \"action tensor in the actor network.\"\n#                 )\n#             target_entropy = -float(np.prod(actor_network.spec[\"action\"].shape))\n#         self.register_buffer(\n#             \"target_entropy\", torch.tensor(target_entropy, device=device)\n#         )\n#         self.gSDE = gSDE\n# \n#     @property\n#     def alpha(self):\n# --------------------------------------------------\n# the below code fragment can be found in:\n# torchrl/objectives/sac.py\n# --------------------------------------------------\n#             )\n#         else:\n#             self.register_parameter(\n#                 \"log_alpha\",\n#                 torch.nn.Parameter(torch.tensor(math.log(alpha_init), device=device)),\n#             )\n# \n#         if target_entropy == \"auto\":\n#             if actor_network.spec is None:\n#                 raise RuntimeError(\n#                     \"Cannot infer the dimensionality of the action. Consider providing \"\n#                     \"the target entropy explicitely or provide the spec of the \"\n#                     \"action tensor in the actor network.\"\n#                 )\n#             target_entropy = -float(np.prod(actor_network.spec[\"action\"].shape))\n#         self.register_buffer(\n#             \"target_entropy\", torch.tensor(target_entropy, device=device)\n#         )\n#         if self._version == 1:\n#             self.actor_critic = ActorCriticWrapper(\n# --------------------------------------------------\n# the below code fragment can be found in:\n# torchrl/objectives/deprecated.py\n# torchrl/objectives/redq.py\n# --------------------------------------------------\n#             )\n#         else:\n#             self.register_parameter(\n#                 \"log_alpha\",\n#                 torch.nn.Parameter(torch.tensor(math.log(alpha_init), device=device)),\n#             )\n# \n#         if target_entropy == \"auto\":\n#             if actor_network.spec[\"action\"] is None:\n#                 raise RuntimeError(\n#                     \"Cannot infer the dimensionality of the action. Consider providing \"\n#                     \"the target entropy explicitely or provide the spec of the \"\n#                     \"action tensor in the actor network.\"\n#                 )\n#             target_entropy = -float(np.prod(actor_network.spec[\"action\"].shape))\n#         self.register_buffer(\n#             \"target_entropy\", torch.tensor(target_entropy, device=device)\n#         )\n#         self.gSDE = gSDE\n# \n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\n# Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport math\nfrom numbers import Number\nfrom typing import Union\n\nimport numpy as np\nimport torch\n\nfrom tensordict.nn import TensorDictSequential\nfrom tensordict.tensordict import TensorDict, TensorDictBase\nfrom torch import Tensor\n\nfrom torchrl.envs.utils import set_exploration_mode, step_mdp\nfrom torchrl.modules import SafeModule\nfrom torchrl.objectives.common import LossModule\nfrom torchrl.objectives.utils import (\n    distance_loss,\n    next_state_value as get_next_state_value,\n)\n\ntry:\n    from functorch import vmap\n\n    FUNCTORCH_ERR = \"\"\n    _has_functorch = True\nexcept ImportError as err:\n    FUNCTORCH_ERR = str(err)\n    _has_functorch = False\n\n\nclass REDQLoss(LossModule):\n    \"\"\"REDQ Loss module.\n\n    REDQ (RANDOMIZED ENSEMBLED DOUBLE Q-LEARNING: LEARNING FAST WITHOUT A MODEL\n    https://openreview.net/pdf?id=AY8zfZm0tDd) generalizes the idea of using an ensemble of Q-value functions to\n    train a SAC-like algorithm.\n\n    Args:\n        actor_network (SafeModule): the actor to be trained\n        qvalue_network (SafeModule): a single Q-value network that will be multiplicated as many times as needed.\n        num_qvalue_nets (int, optional): Number of Q-value networks to be trained. Default is 10.\n        sub_sample_len (int, optional): number of Q-value networks to be subsampled to evaluate the next state value\n            Default is 2.\n        gamma (Number, optional): gamma decay factor. Default is 0.99.\n        priotity_key (str, optional): Key where to write the priority value for prioritized replay buffers. Default is\n            `\"td_error\"`.\n        loss_function (str, optional): loss function to be used for the Q-value. Can be one of  `\"smooth_l1\"`, \"l2\",\n            \"l1\", Default is \"smooth_l1\".\n        alpha_init (float, optional): initial entropy multiplier.\n            Default is 1.0.\n        min_alpha (float, optional): min value of alpha.\n            Default is 0.1.\n        max_alpha (float, optional): max value of alpha.\n            Default is 10.0.\n        fixed_alpha (bool, optional): whether alpha should be trained to match a target entropy. Default is :obj:`False`.\n        target_entropy (Union[str, Number], optional): Target entropy for the stochastic policy. Default is \"auto\".\n        delay_qvalue (bool, optional): Whether to separate the target Q value networks from the Q value networks used\n            for data collection. Default is :obj:`False`.\n        gSDE (bool, optional): Knowing if gSDE is used is necessary to create random noise variables.\n            Default is False\n\n    \"\"\"\n\n    delay_actor: bool = False\n\n    def __init__(\n        self,\n        actor_network: SafeModule,\n        qvalue_network: SafeModule,\n        num_qvalue_nets: int = 10,\n        sub_sample_len: int = 2,\n        gamma: Number = 0.99,\n        priotity_key: str = \"td_error\",\n        loss_function: str = \"smooth_l1\",\n        alpha_init: float = 1.0,\n        min_alpha: float = 0.1,\n        max_alpha: float = 10.0,\n        fixed_alpha: bool = False,\n        target_entropy: Union[str, Number] = \"auto\",\n        delay_qvalue: bool = True,\n        gSDE: bool = False,\n    ):\n        if not _has_functorch:\n            raise ImportError(\"Failed to import functorch.\") from FUNCTORCH_ERR\n\n        super().__init__()\n        self.convert_to_functional(\n            actor_network,\n            \"actor_network\",\n            create_target_params=self.delay_actor,\n            funs_to_decorate=[\"forward\", \"get_dist_params\"],\n        )\n\n        # let's make sure that actor_network has `return_log_prob` set to True\n        self.actor_network.return_log_prob = True\n\n        self.delay_qvalue = delay_qvalue\n        self.convert_to_functional(\n            qvalue_network,\n            \"qvalue_network\",\n            num_qvalue_nets,\n            create_target_params=self.delay_qvalue,\n            compare_against=list(actor_network.parameters()),\n        )\n        self.num_qvalue_nets = num_qvalue_nets\n        self.sub_sample_len = max(1, min(sub_sample_len, num_qvalue_nets - 1))\n        self.register_buffer(\"gamma\", torch.tensor(gamma))\n        self.priority_key = priotity_key\n        self.loss_function = loss_function\n\n        try:\n            device = next(self.parameters()).device\n        except AttributeError:\n            device = torch.device(\"cpu\")\n\n        self.register_buffer(\"alpha_init\", torch.tensor(alpha_init, device=device))\n        self.register_buffer(\n            \"min_log_alpha\", torch.tensor(min_alpha, device=device).log()\n        )\n        self.register_buffer(\n            \"max_log_alpha\", torch.tensor(max_alpha, device=device).log()\n        )\n        self.fixed_alpha = fixed_alpha\n        if fixed_alpha:\n            self.register_buffer(\n                \"log_alpha\", torch.tensor(math.log(alpha_init), device=device)\n            )\n        else:\n            self.register_parameter(\n                \"log_alpha\",\n                torch.nn.Parameter(torch.tensor(math.log(alpha_init), device=device)),\n            )\n\n        if target_entropy == \"auto\":\n            if actor_network.spec[\"action\"] is None:\n                raise RuntimeError(\n                    \"Cannot infer the dimensionality of the action. Consider providing \"\n                    \"the target entropy explicitely or provide the spec of the \"\n                    \"action tensor in the actor network.\"\n                )\n            target_entropy = -float(np.prod(actor_network.spec[\"action\"].shape))\n        self.register_buffer(\n            \"target_entropy\", torch.tensor(target_entropy, device=device)\n        )", "metadata": {"task_id": "pytorch_rl/144", "ground_truth": "        self.gSDE = gSDE", "fpath_tuple": ["pytorch_rl", "torchrl", "objectives", "redq.py"], "context_start_lineno": 0, "line_no": 148, "query_window": {"context": "            self.register_buffer(\n                \"log_alpha\", torch.tensor(math.log(alpha_init), device=device)\n            )\n        else:\n            self.register_parameter(\n                \"log_alpha\",\n                torch.nn.Parameter(torch.tensor(math.log(alpha_init), device=device)),\n            )\n\n        if target_entropy == \"auto\":\n            if actor_network.spec[\"action\"] is None:\n                raise RuntimeError(\n                    \"Cannot infer the dimensionality of the action. Consider providing \"\n                    \"the target entropy explicitely or provide the spec of the \"\n                    \"action tensor in the actor network.\"\n                )\n            target_entropy = -float(np.prod(actor_network.spec[\"action\"].shape))\n        self.register_buffer(\n            \"target_entropy\", torch.tensor(target_entropy, device=device)\n        )", "metadata": {"fpath_tuple": ["pytorch_rl", "torchrl", "objectives", "redq.py"], "line_no": 148, "task_id": "pytorch_rl/144", "start_line_no": 128, "end_line_no": 148, "window_size": 20, "context_start_lineno": 0, "repo": "pytorch_rl"}}, "top_k_context": [{"context": "            self.register_buffer(\n                \"log_alpha\", torch.tensor(math.log(alpha_init), device=device)\n            )\n        else:\n            self.register_parameter(\n                \"log_alpha\",\n                torch.nn.Parameter(torch.tensor(math.log(alpha_init), device=device)),\n            )\n\n        if target_entropy == \"auto\":\n            if actor_network.spec[\"action\"] is None:\n                raise RuntimeError(\n                    \"Cannot infer the dimensionality of the action. Consider providing \"\n                    \"the target entropy explicitely or provide the spec of the \"\n                    \"action tensor in the actor network.\"\n                )\n            target_entropy = -float(np.prod(actor_network.spec[\"action\"].shape))\n        self.register_buffer(\n            \"target_entropy\", torch.tensor(target_entropy, device=device)\n        )", "metadata": [{"fpath_tuple": ["pytorch_rl", "torchrl", "objectives", "deprecated.py"], "line_no": 132, "start_line_no": 122, "end_line_no": 142, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}, {"fpath_tuple": ["pytorch_rl", "torchrl", "objectives", "redq.py"], "line_no": 138, "start_line_no": 128, "end_line_no": 148, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}], "sim_score": 1.0}, {"context": "            self.register_buffer(\n                \"log_alpha\", torch.tensor(math.log(alpha_init), device=device)\n            )\n        else:\n            self.register_parameter(\n                \"log_alpha\",\n                torch.nn.Parameter(torch.tensor(math.log(alpha_init), device=device)),\n            )\n\n        if target_entropy == \"auto\":\n            if actor_network.spec is None:\n                raise RuntimeError(\n                    \"Cannot infer the dimensionality of the action. Consider providing \"\n                    \"the target entropy explicitely or provide the spec of the \"\n                    \"action tensor in the actor network.\"\n                )\n            target_entropy = -float(np.prod(actor_network.spec[\"action\"].shape))\n        self.register_buffer(\n            \"target_entropy\", torch.tensor(target_entropy, device=device)\n        )", "metadata": [{"fpath_tuple": ["pytorch_rl", "torchrl", "objectives", "sac.py"], "line_no": 164, "start_line_no": 154, "end_line_no": 174, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}], "sim_score": 0.9888888888888889}, {"context": "            )\n        else:\n            self.register_parameter(\n                \"log_alpha\",\n                torch.nn.Parameter(torch.tensor(math.log(alpha_init), device=device)),\n            )\n\n        if target_entropy == \"auto\":\n            if actor_network.spec[\"action\"] is None:\n                raise RuntimeError(\n                    \"Cannot infer the dimensionality of the action. Consider providing \"\n                    \"the target entropy explicitely or provide the spec of the \"\n                    \"action tensor in the actor network.\"\n                )\n            target_entropy = -float(np.prod(actor_network.spec[\"action\"].shape))\n        self.register_buffer(\n            \"target_entropy\", torch.tensor(target_entropy, device=device)\n        )\n        self.gSDE = gSDE\n", "metadata": [{"fpath_tuple": ["pytorch_rl", "torchrl", "objectives", "deprecated.py"], "line_no": 134, "start_line_no": 124, "end_line_no": 144, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}, {"fpath_tuple": ["pytorch_rl", "torchrl", "objectives", "redq.py"], "line_no": 140, "start_line_no": 130, "end_line_no": 150, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}], "sim_score": 0.9574468085106383}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# torchrl/data/replay_buffers/storages.py\n# --------------------------------------------------\n#             \"_len\": self._len,\n#         }\n# \n#     def load_state_dict(self, state_dict):\n#         _storage = copy(state_dict[\"_storage\"])\n#         if isinstance(_storage, torch.Tensor):\n#             if isinstance(self._storage, torch.Tensor):\n#                 self._storage.copy_(_storage)\n#             elif self._storage is None:\n#                 self._storage = _storage\n#             else:\n#                 raise RuntimeError(\n#                     f\"Cannot copy a storage of type {type(_storage)} onto another of type {type(self._storage)}\"\n#                 )\n#         elif isinstance(_storage, (dict, OrderedDict)):\n#             if isinstance(self._storage, TensorDictBase):\n#                 self._storage.load_state_dict(_storage)\n#             elif self._storage is None:\n#                 batch_size = _storage.pop(\"__batch_size\")\n#                 device = _storage.pop(\"__device\")\n# --------------------------------------------------\n# the below code fragment can be found in:\n# torchrl/data/replay_buffers/storages.py\n# --------------------------------------------------\n#         _storage = self._storage\n#         if isinstance(_storage, torch.Tensor):\n#             pass\n#         elif isinstance(_storage, TensorDictBase):\n#             _storage = _storage.state_dict()\n#         elif _storage is None:\n#             _storage = {}\n#         else:\n#             raise TypeError(\n#                 f\"Objects of type {type(_storage)} are not supported by LazyTensorStorage.state_dict\"\n#             )\n#         return {\n#             \"_storage\": _storage,\n#             \"initialized\": self.initialized,\n#             \"_len\": self._len,\n#         }\n# \n#     def load_state_dict(self, state_dict):\n#         _storage = copy(state_dict[\"_storage\"])\n#         if isinstance(_storage, torch.Tensor):\n# --------------------------------------------------\n# the below code fragment can be found in:\n# torchrl/collectors/collectors.py\n# --------------------------------------------------\n#             if td.device == torch.device(\"cpu\") and self.pin_memory:\n#                 td.pin_memory()\n#             self._td_policy.update(td, inplace=True)\n#         return self._td_policy\n# \n#     def _cast_to_env(\n#         self, td: TensorDictBase, dest: Optional[TensorDictBase] = None\n#     ) -> TensorDictBase:\n#         env_device = self.env_device\n#         if dest is None:\n#             if self._td_env is None:\n#                 self._td_env = td.to(env_device)\n#             else:\n#                 self._td_env.update(td, inplace=True)\n#             return self._td_env\n#         else:\n#             return dest.update(td, inplace=True)\n# \n#     def _reset_if_necessary(self) -> None:\n#         done = self._tensordict.get(\"done\")\n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\nsteps_ops = []\n        self._post_steps_log_ops = []\n        self._pre_steps_log_ops = []\n        self._post_optim_log_ops = []\n        self._pre_optim_ops = []\n        self._post_loss_ops = []\n        self._optimizer_ops = []\n        self._process_optim_batch_ops = []\n        self._post_optim_ops = []\n        self._modules = {}\n\n        if self.optimizer is not None:\n            optimizer_hook = OptimizerHook(self.optimizer)\n            optimizer_hook.register(self)\n\n    def register_module(self, module_name: str, module: Any) -> None:\n        if module_name in self._modules:\n            raise RuntimeError(\n                f\"{module_name} is already registered, choose a different name.\"\n            )\n        self._modules[module_name] = module\n\n    def _get_state(self):\n        if _CKPT_BACKEND == \"torchsnapshot\":\n            state = StateDict(\n                collected_frames=self.collected_frames,\n                _last_log=self._last_log,\n                _last_save=self._last_save,\n                _optim_count=self._optim_count,\n            )\n        else:\n            state = OrderedDict(\n                collected_frames=self.collected_frames,\n                _last_log=self._last_log,\n                _last_save=self._last_save,\n                _optim_count=self._optim_count,\n            )\n        return state\n\n    @property\n    def app_state(self):\n        self._app_state = {\n            \"state\": StateDict(**self._get_state()),\n            \"collector\": self.collector,\n            \"loss_module\": self.loss_module,\n            **{k: item for k, item in self._modules.items()},\n        }\n        return self._app_state\n\n    def state_dict(self) -> Dict:\n        state = self._get_state()\n        state_dict = OrderedDict(\n            collector=self.collector.state_dict(),\n            loss_module=self.loss_module.state_dict(),\n            state=state,\n            **{k: item.state_dict() for k, item in self._modules.items()},\n        )\n        return state_dict\n\n    def load_state_dict(self, state_dict: Dict) -> None:\n        model_state_dict = state_dict[\"loss_module\"]\n        collector_state_dict = state_dict[\"collector\"]\n\n        self.loss_module.load_state_dict(model_state_dict)\n        self.collector.load_state_dict(collector_state_dict)\n        for key, item in self._modules.items():\n            item.load_state_dict(state_dict[key])\n\n        self.collected_frames = state_dict[\"state\"][\"collected_frames\"]\n        self._last_log = state_dict[\"state\"][\"_last_log\"]\n        self._last_save = state_dict[\"state\"][\"_last_save\"]\n        self._optim_count = state_dict[\"state\"][\"_optim_count\"]\n\n    def _save_trainer(self) -> None:\n        if _CKPT_BACKEND == \"torchsnapshot\":\n            if not _has_ts:\n                raise ImportError(\n                    \"torchsnapshot not found. Consider installing torchsnapshot or \"\n                    \"using the torch checkpointing backend (`CKPT_BACKEND=torch`)\"\n                )\n            Snapshot.take(app_state=self.app_state, path=self.save_trainer_file)\n        elif _CKPT_BACKEND == \"torch\":\n            torch.save(self.state_dict(), self.save_trainer_file)\n        else:\n            raise NotImplementedError(\n                f\"CKPT_BACKEND should be one of {_CKPT_BACKEND.backends}, got {_CKPT_BACKEND}.\"\n            )\n\n    def save_trainer(self, force_save: bool = False) -> None:\n        _save = force_save\n        if self.save_trainer_file is not None:\n            if (self.collected_frames - self._last_save) > self.save_trainer_interval:\n                self._last_save = self.collected_frames\n                _save = True\n        if _save and self.save_trainer_file:\n            self._save_trainer()\n\n    def load_from_file(self, file: Union[str, pathlib.Path]) -> Trainer:\n        if _CKPT_BACKEND == \"torchsnapshot\":\n            snapshot = Snapshot(path=file)\n            snapshot.restore(app_state=self.app_state)\n        elif _CKPT_BACKEND == \"torch\":\n            loaded_dict: OrderedDict = torch.load(file)\n            self.load_state_dict(loaded_dict)\n        return self\n\n    def set_seed(self):\n        seed = self.collector.set_seed(self.seed, static_seed=False)\n        torch.manual_seed(seed)\n        np.random.seed(seed)\n\n    @property\n    def collector(self) -> _DataCollector:\n        return self._collector\n\n    @collector.setter\n    def collector(self, collector: _DataCollector) -> None:\n        self._collector = collector\n\n    def register_op(self, dest: str, op: Callable, **kwargs) -> None:\n        if dest == \"batch_process\":\n            _check_input_output_typehint(\n                op, input=TensorDictBase, output=TensorDictBase\n            )\n            self._batch_process_ops.append((op, kwargs))\n\n        elif dest == \"pre_optim_steps\":\n            _check_input_output_typehint(op, input=None, output=None)\n            self._pre_optim_ops.append((op, kwargs))\n\n        elif dest == \"process_optim_batch\":\n            _check_input_output_typehint(\n                op, input=TensorDictBase, output=TensorDictBase\n            )\n            self._process_optim_batch_ops.append((op, kwargs))\n\n        elif dest == \"post_loss\":\n            _check_input_output_typehint(\n                op, input=TensorDictBase, output=TensorDictBase\n            )\n            self._post_loss_ops.append((op, kwargs))\n\n        elif dest == \"optimizer\":\n            _check_input_output_typehint(\n                op, input=[TensorDictBase, bool, float, int], output=TensorDictBase\n            )\n            self._optimizer_ops.append((op, kwargs))\n\n        elif dest == \"post_steps\":\n            _check_input_output_typehint(op, input=None, output=None)\n            self._post_steps_ops.append((op, kwargs))\n\n        elif dest == \"post_optim\":\n            _check_input_output_typehint(op, input=None, output=None)\n            self._post_optim_ops.append((op, kwargs))\n\n        elif dest == \"pre_steps_log\":\n            _check_input_output_typehint(\n                op, input=TensorDictBase, output=Tuple[str, float]\n            )\n            self._pre_steps_log_ops.append((op, kwargs))\n\n        elif dest == \"post_steps_log\":\n            _check_input_output_typehint(\n                op, input=TensorDictBase, output=Tuple[str, float]\n            )\n            self._post_steps_log_ops.append((op, kwargs))\n\n        elif dest == \"post_optim_log\":\n            _check_input_output_typehint(\n                op, input=TensorDictBase, output=Tuple[str, float]\n            )\n            self._post_optim_log_ops.append((op, kwargs))\n\n        else:\n            raise RuntimeError(", "metadata": {"task_id": "pytorch_rl/163", "ground_truth": "                f\"The hook collection {dest} is not recognised. Choose from:\"", "fpath_tuple": ["pytorch_rl", "torchrl", "trainers", "trainers.py"], "context_start_lineno": 175, "line_no": 351, "query_window": {"context": "        elif dest == \"pre_steps_log\":\n            _check_input_output_typehint(\n                op, input=TensorDictBase, output=Tuple[str, float]\n            )\n            self._pre_steps_log_ops.append((op, kwargs))\n\n        elif dest == \"post_steps_log\":\n            _check_input_output_typehint(\n                op, input=TensorDictBase, output=Tuple[str, float]\n            )\n            self._post_steps_log_ops.append((op, kwargs))\n\n        elif dest == \"post_optim_log\":\n            _check_input_output_typehint(\n                op, input=TensorDictBase, output=Tuple[str, float]\n            )\n            self._post_optim_log_ops.append((op, kwargs))\n\n        else:\n            raise RuntimeError(", "metadata": {"fpath_tuple": ["pytorch_rl", "torchrl", "trainers", "trainers.py"], "line_no": 351, "task_id": "pytorch_rl/163", "start_line_no": 331, "end_line_no": 351, "window_size": 20, "context_start_lineno": 175, "repo": "pytorch_rl"}}, "top_k_context": [{"context": "            self._td_policy = td.to(policy_device)\n        else:\n            if td.device == torch.device(\"cpu\") and self.pin_memory:\n                td.pin_memory()\n            self._td_policy.update(td, inplace=True)\n        return self._td_policy\n\n    def _cast_to_env(\n        self, td: TensorDictBase, dest: Optional[TensorDictBase] = None\n    ) -> TensorDictBase:\n        env_device = self.env_device\n        if dest is None:\n            if self._td_env is None:\n                self._td_env = td.to(env_device)\n            else:\n                self._td_env.update(td, inplace=True)\n            return self._td_env\n        else:\n            return dest.update(td, inplace=True)\n", "metadata": [{"fpath_tuple": ["pytorch_rl", "torchrl", "collectors", "collectors.py"], "line_no": 588, "start_line_no": 578, "end_line_no": 598, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}], "sim_score": 0.2840909090909091}, {"context": "\n    def state_dict(self) -> Dict[str, Any]:\n        _storage = self._storage\n        if isinstance(_storage, torch.Tensor):\n            pass\n        elif isinstance(_storage, TensorDictBase):\n            _storage = _storage.state_dict()\n        elif _storage is None:\n            _storage = {}\n        else:\n            raise TypeError(\n                f\"Objects of type {type(_storage)} are not supported by LazyTensorStorage.state_dict\"\n            )\n        return {\n            \"_storage\": _storage,\n            \"initialized\": self.initialized,\n            \"_len\": self._len,\n        }\n\n    def load_state_dict(self, state_dict):", "metadata": [{"fpath_tuple": ["pytorch_rl", "torchrl", "data", "replay_buffers", "storages.py"], "line_no": 188, "start_line_no": 178, "end_line_no": 198, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}], "sim_score": 0.2815533980582524}, {"context": "            \"_storage\": _storage,\n            \"initialized\": self.initialized,\n            \"_len\": self._len,\n        }\n\n    def load_state_dict(self, state_dict):\n        _storage = copy(state_dict[\"_storage\"])\n        if isinstance(_storage, torch.Tensor):\n            if isinstance(self._storage, torch.Tensor):\n                self._storage.copy_(_storage)\n            elif self._storage is None:\n                self._storage = _storage\n            else:\n                raise RuntimeError(\n                    f\"Cannot copy a storage of type {type(_storage)} onto another of type {type(self._storage)}\"\n                )\n        elif isinstance(_storage, (dict, OrderedDict)):\n            if isinstance(self._storage, TensorDictBase):\n                self._storage.load_state_dict(_storage)\n            elif self._storage is None:", "metadata": [{"fpath_tuple": ["pytorch_rl", "torchrl", "data", "replay_buffers", "storages.py"], "line_no": 202, "start_line_no": 192, "end_line_no": 212, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}], "sim_score": 0.27722772277227725}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# test/test_loggers.py\n# --------------------------------------------------\n# \n# \n# @pytest.mark.skipif(not _has_tb, reason=\"TensorBoard not installed\")\n# class TestTensorboard:\n#     @pytest.mark.parametrize(\"steps\", [None, [1, 10, 11]])\n#     def test_log_scalar(self, steps):\n#         torch.manual_seed(0)\n#         with tempfile.TemporaryDirectory() as log_dir:\n#             exp_name = \"ramala\"\n#             logger = TensorboardLogger(log_dir=log_dir, exp_name=exp_name)\n# \n#             values = torch.rand(3)\n#             for i in range(3):\n#                 scalar_name = \"foo\"\n#                 scalar_value = values[i].item()\n#                 logger.log_scalar(\n#                     value=scalar_value,\n#                     name=scalar_name,\n#                     step=steps[i] if steps else None,\n#                 )\n# --------------------------------------------------\n# the below code fragment can be found in:\n# test/test_loggers.py\n# --------------------------------------------------\n#     def test_log_video(self, steps):\n#         torch.manual_seed(0)\n#         with tempfile.TemporaryDirectory() as log_dir:\n#             exp_name = \"ramala\"\n#             logger = TensorboardLogger(log_dir=log_dir, exp_name=exp_name)\n# \n#             # creating a sample video (T, C, H, W), where T - number of frames,\n#             # C - number of image channels (e.g. 3 for RGB), H, W - image dimensions.\n#             # the first 64 frames are black and the next 64 are white\n#             video = torch.cat(\n#                 (torch.zeros(64, 1, 32, 32), torch.full((64, 1, 32, 32), 255))\n#             )\n#             video = video[None, :]\n#             for i in range(3):\n#                 logger.log_video(\n#                     name=\"foo\",\n#                     video=video,\n#                     step=steps[i] if steps else None,\n#                     fps=6,  # we can't test for the difference between fps, because the result is an encoded_string\n#                 )\n# --------------------------------------------------\n# the below code fragment can be found in:\n# test/test_loggers.py\n# --------------------------------------------------\n# @pytest.mark.skipif(not _has_tb, reason=\"TensorBoard not installed\")\n# class TestTensorboard:\n#     @pytest.mark.parametrize(\"steps\", [None, [1, 10, 11]])\n#     def test_log_scalar(self, steps):\n#         torch.manual_seed(0)\n#         with tempfile.TemporaryDirectory() as log_dir:\n#             exp_name = \"ramala\"\n#             logger = TensorboardLogger(log_dir=log_dir, exp_name=exp_name)\n# \n#             values = torch.rand(3)\n#             for i in range(3):\n#                 scalar_name = \"foo\"\n#                 scalar_value = values[i].item()\n#                 logger.log_scalar(\n#                     value=scalar_value,\n#                     name=scalar_name,\n#                     step=steps[i] if steps else None,\n#                 )\n# \n#             sleep(0.01)  # wait until events are registered\n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\n                    name=scalar_name,\n                    step=steps[i] if steps else None,\n                )\n\n            with open(\n                os.path.join(log_dir, exp_name, \"scalars\", \"foo.csv\"), \"r\"\n            ) as file:\n                for i, row in enumerate(file.readlines()):\n                    step = steps[i] if steps else i\n                    assert row == f\"{step},{values[i].item()}\\n\"\n\n    @pytest.mark.parametrize(\"steps\", [None, [1, 10, 11]])\n    def test_log_video(self, steps):\n        torch.manual_seed(0)\n        with tempfile.TemporaryDirectory() as log_dir:\n            exp_name = \"ramala\"\n            logger = CSVLogger(log_dir=log_dir, exp_name=exp_name)\n\n            # creating a sample video (T, C, H, W), where T - number of frames,\n            # C - number of image channels (e.g. 3 for RGB), H, W - image dimensions.\n            # the first 64 frames are black and the next 64 are white\n            video = torch.cat(\n                (torch.zeros(64, 1, 32, 32), torch.full((64, 1, 32, 32), 255))\n            )\n            video = video[None, :]\n            for i in range(3):\n                logger.log_video(\n                    name=\"foo\",\n                    video=video,\n                    step=steps[i] if steps else None,\n                )\n            sleep(0.01)  # wait until events are registered\n\n            # check that the logged videos are the same as the initial video\n            video_file_name = \"foo_\" + (\"0\" if not steps else str(steps[0])) + \".pt\"\n            logged_video = torch.load(\n                os.path.join(log_dir, exp_name, \"videos\", video_file_name)\n            )\n            assert torch.equal(video, logged_video), logged_video\n\n            # check that we catch the error in case the format of the tensor is wrong\n            video_wrong_format = torch.zeros(64, 2, 32, 32)\n            video_wrong_format = video_wrong_format[None, :]\n            with pytest.raises(Exception):\n                logger.log_video(\n                    name=\"foo\",\n                    video=video_wrong_format,\n                    step=steps[i] if steps else None,\n                )\n\n\n@pytest.mark.skipif(not _has_wandb, reason=\"Wandb not installed\")\nclass TestWandbLogger:\n    @pytest.mark.parametrize(\"steps\", [None, [1, 10, 11]])\n    def test_log_scalar(self, steps):\n        torch.manual_seed(0)\n        with tempfile.TemporaryDirectory() as log_dir:\n            exp_name = \"ramala\"\n            logger = WandbLogger(log_dir=log_dir, exp_name=exp_name, offline=True)\n\n            values = torch.rand(3)\n            for i in range(3):\n                scalar_name = \"foo\"\n                scalar_value = values[i].item()\n                logger.log_scalar(\n                    value=scalar_value,\n                    name=scalar_name,\n                    step=steps[i] if steps else None,\n                )\n\n            assert logger.experiment.summary[\"foo\"] == values[-1].item()\n            assert logger.experiment.summary[\"_step\"] == i if not steps else steps[i]\n\n            logger.experiment.finish()\n            del logger\n\n    def test_log_video(self):\n        torch.manual_seed(0)\n        with tempfile.TemporaryDirectory() as log_dir:\n            exp_name = \"ramala\"\n            logger = WandbLogger(log_dir=log_dir, exp_name=exp_name, offline=True)\n\n            # creating a sample video (T, C, H, W), where T - number of frames,\n            # C - number of image channels (e.g. 3 for RGB), H, W - image dimensions.\n            # the first 64 frames are black and the next 64 are white\n            video = torch.cat(\n                (torch.zeros(64, 1, 32, 32), torch.full((64, 1, 32, 32), 255))\n            )\n            video = video[None, :]\n            logger.log_video(\n                name=\"foo\",\n                video=video,\n                fps=6,\n            )\n            logger.log_video(\n                name=\"foo_12fps\",\n                video=video,\n                fps=24,\n            )\n            sleep(0.01)  # wait until events are registered\n\n            # check that fps can be passed and that it has impact on the length of the video\n            video_6fps_size = logger.experiment.summary[\"foo\"][\"size\"]\n            video_24fps_size = logger.experiment.summary[\"foo_12fps\"][\"size\"]\n            assert video_6fps_size > video_24fps_size, video_6fps_size\n\n            # check that we catch the error in case the format of the tensor is wrong\n            video_wrong_format = torch.zeros(64, 2, 32, 32)\n            video_wrong_format = video_wrong_format[None, :]\n            with pytest.raises(Exception):\n                logger.log_video(\n                    name=\"foo\",\n                    video=video_wrong_format,\n                )\n\n            logger.experiment.finish()\n            del logger\n\n\n@pytest.fixture\ndef mlflow_fixture():\n    torch.manual_seed(0)\n\n    with tempfile.TemporaryDirectory() as log_dir:\n        exp_name = \"ramala\"\n        log_dir_uri = pathlib.Path(log_dir).as_uri()\n        logger = MLFlowLogger(exp_name=exp_name, tracking_uri=log_dir_uri)\n        client = mlflow.MlflowClient()\n        yield logger, client\n        mlflow.end_run()\n\n\n@pytest.mark.skipif(not _has_mlflow, reason=\"MLFlow not installed\")\nclass TestMLFlowLogger:\n    @pytest.mark.parametrize(\"steps\", [None, [1, 10, 11]])\n    def test_log_scalar(self, steps, mlflow_fixture):\n\n        logger, client = mlflow_fixture\n        values = torch.rand(3)\n        for i in range(3):\n            scalar_name = \"foo\"\n            scalar_value = values[i].item()\n            logger.log_scalar(\n                value=scalar_value,\n                name=scalar_name,\n                step=steps[i] if steps else None,\n            )\n        run_id = mlflow.active_run().info.run_id\n        for i, metric in enumerate(client.get_metric_history(run_id, \"foo\")):\n            assert metric.key == \"foo\"\n            assert metric.step == (steps[i] if steps else 0)\n            assert metric.value == values[i].item()\n\n    @pytest.mark.parametrize(\"steps\", [None, [1, 10, 11]])\n    @pytest.mark.skipif(not _has_tv, reason=\"torchvision not installed\")\n    def test_log_video(self, steps, mlflow_fixture):\n\n        logger, client = mlflow_fixture\n        videos = torch.cat(\n            (torch.full((3, 64, 3, 32, 32), 255), torch.zeros(3, 64, 3, 32, 32)),\n            dim=1,\n        )\n        fps = 6\n        for i in range(3):\n            logger.log_video(\n                name=\"test_video\",\n                video=videos[i],\n                fps=fps,\n                step=steps[i] if steps else None,\n            )\n        run_id = mlflow.active_run().info.run_id\n        with tempfile.TemporaryDirectory() as artifacts_dir:\n            videos_dir = client.download_artifacts(run_id, \"videos\", artifacts_dir)", "metadata": {"task_id": "pytorch_rl/64", "ground_truth": "            for i, video_name in enumerate(os.listdir(videos_dir)):", "fpath_tuple": ["pytorch_rl", "test", "test_loggers.py"], "context_start_lineno": 117, "line_no": 290, "query_window": {"context": "    @pytest.mark.parametrize(\"steps\", [None, [1, 10, 11]])\n    @pytest.mark.skipif(not _has_tv, reason=\"torchvision not installed\")\n    def test_log_video(self, steps, mlflow_fixture):\n\n        logger, client = mlflow_fixture\n        videos = torch.cat(\n            (torch.full((3, 64, 3, 32, 32), 255), torch.zeros(3, 64, 3, 32, 32)),\n            dim=1,\n        )\n        fps = 6\n        for i in range(3):\n            logger.log_video(\n                name=\"test_video\",\n                video=videos[i],\n                fps=fps,\n                step=steps[i] if steps else None,\n            )\n        run_id = mlflow.active_run().info.run_id\n        with tempfile.TemporaryDirectory() as artifacts_dir:\n            videos_dir = client.download_artifacts(run_id, \"videos\", artifacts_dir)", "metadata": {"fpath_tuple": ["pytorch_rl", "test", "test_loggers.py"], "line_no": 290, "task_id": "pytorch_rl/64", "start_line_no": 270, "end_line_no": 290, "window_size": 20, "context_start_lineno": 117, "repo": "pytorch_rl"}}, "top_k_context": [{"context": "\n\n@pytest.mark.skipif(not _has_tb, reason=\"TensorBoard not installed\")\nclass TestTensorboard:\n    @pytest.mark.parametrize(\"steps\", [None, [1, 10, 11]])\n    def test_log_scalar(self, steps):\n        torch.manual_seed(0)\n        with tempfile.TemporaryDirectory() as log_dir:\n            exp_name = \"ramala\"\n            logger = TensorboardLogger(log_dir=log_dir, exp_name=exp_name)\n\n            values = torch.rand(3)\n            for i in range(3):\n                scalar_name = \"foo\"\n                scalar_value = values[i].item()\n                logger.log_scalar(\n                    value=scalar_value,\n                    name=scalar_name,\n                    step=steps[i] if steps else None,\n                )", "metadata": [{"fpath_tuple": ["pytorch_rl", "test", "test_loggers.py"], "line_no": 42, "start_line_no": 32, "end_line_no": 52, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}], "sim_score": 0.49324324324324326}, {"context": "\n    @pytest.mark.parametrize(\"steps\", [None, [1, 10, 11]])\n    def test_log_video(self, steps):\n        torch.manual_seed(0)\n        with tempfile.TemporaryDirectory() as log_dir:\n            exp_name = \"ramala\"\n            logger = TensorboardLogger(log_dir=log_dir, exp_name=exp_name)\n\n            # creating a sample video (T, C, H, W), where T - number of frames,\n            # C - number of image channels (e.g. 3 for RGB), H, W - image dimensions.\n            # the first 64 frames are black and the next 64 are white\n            video = torch.cat(\n                (torch.zeros(64, 1, 32, 32), torch.full((64, 1, 32, 32), 255))\n            )\n            video = video[None, :]\n            for i in range(3):\n                logger.log_video(\n                    name=\"foo\",\n                    video=video,\n                    step=steps[i] if steps else None,", "metadata": [{"fpath_tuple": ["pytorch_rl", "test", "test_loggers.py"], "line_no": 72, "start_line_no": 62, "end_line_no": 82, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}], "sim_score": 0.4906832298136646}, {"context": "if _has_mlflow:\n    import mlflow\n\n\n@pytest.mark.skipif(not _has_tb, reason=\"TensorBoard not installed\")\nclass TestTensorboard:\n    @pytest.mark.parametrize(\"steps\", [None, [1, 10, 11]])\n    def test_log_scalar(self, steps):\n        torch.manual_seed(0)\n        with tempfile.TemporaryDirectory() as log_dir:\n            exp_name = \"ramala\"\n            logger = TensorboardLogger(log_dir=log_dir, exp_name=exp_name)\n\n            values = torch.rand(3)\n            for i in range(3):\n                scalar_name = \"foo\"\n                scalar_value = values[i].item()\n                logger.log_scalar(\n                    value=scalar_value,\n                    name=scalar_name,", "metadata": [{"fpath_tuple": ["pytorch_rl", "test", "test_loggers.py"], "line_no": 40, "start_line_no": 30, "end_line_no": 50, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}], "sim_score": 0.4666666666666667}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# torchrl/modules/models/models.py\n# --------------------------------------------------\n#         self.mlp1 = MLP(device=device, **mlp1_net_default_kwargs)\n# \n#         mlp2_net_default_kwargs = {\n#             \"in_features\": None,\n#             \"out_features\": 1,\n#             \"num_cells\": [\n#                 300,\n#             ],\n#             \"activation_class\": nn.ELU,\n#             \"bias_last_layer\": True,\n#         }\n#         mlp_net_kwargs_net2 = (\n#             mlp_net_kwargs_net2 if mlp_net_kwargs_net2 is not None else {}\n#         )\n#         mlp2_net_default_kwargs.update(mlp_net_kwargs_net2)\n#         self.mlp2 = MLP(device=device, **mlp2_net_default_kwargs)\n#         ddpg_init_last_layer(self.mlp2[-1], 6e-3, device=device)\n# \n#     def forward(self, observation: torch.Tensor, action: torch.Tensor) -> torch.Tensor:\n#         value = self.mlp2(torch.cat([self.mlp1(observation), action], -1))\n# --------------------------------------------------\n# the below code fragment can be found in:\n# torchrl/modules/models/models.py\n# --------------------------------------------------\n#             \"activation_class\": nn.ELU,\n#             \"bias_last_layer\": True,\n#             \"activate_last_layer\": True,\n#         }\n#         mlp_net_kwargs_net1: Dict = (\n#             mlp_net_kwargs_net1 if mlp_net_kwargs_net1 is not None else {}\n#         )\n#         mlp1_net_default_kwargs.update(mlp_net_kwargs_net1)\n#         self.mlp1 = MLP(device=device, **mlp1_net_default_kwargs)\n# \n#         mlp2_net_default_kwargs = {\n#             \"in_features\": None,\n#             \"out_features\": 1,\n#             \"num_cells\": [\n#                 300,\n#             ],\n#             \"activation_class\": nn.ELU,\n#             \"bias_last_layer\": True,\n#         }\n#         mlp_net_kwargs_net2 = (\n# --------------------------------------------------\n# the below code fragment can be found in:\n# torchrl/modules/models/models.py\n# --------------------------------------------------\n#         )\n#         mlp1_net_default_kwargs.update(mlp_net_kwargs_net1)\n#         self.mlp1 = MLP(device=device, **mlp1_net_default_kwargs)\n# \n#         mlp2_net_default_kwargs = {\n#             \"in_features\": None,\n#             \"out_features\": 1,\n#             \"num_cells\": [\n#                 300,\n#             ],\n#             \"activation_class\": nn.ELU,\n#             \"bias_last_layer\": True,\n#         }\n#         mlp_net_kwargs_net2 = (\n#             mlp_net_kwargs_net2 if mlp_net_kwargs_net2 is not None else {}\n#         )\n#         mlp2_net_default_kwargs.update(mlp_net_kwargs_net2)\n#         self.mlp2 = MLP(device=device, **mlp2_net_default_kwargs)\n#         ddpg_init_last_layer(self.mlp2[-1], 6e-3, device=device)\n# \n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\n must be cast. Default is \"cpu\".\n\n    Returns:\n         An actor and a value operators for DDPG.\n\n    For more details on DDPG, refer to \"CONTINUOUS CONTROL WITH DEEP REINFORCEMENT LEARNING\",\n    https://arxiv.org/pdf/1509.02971.pdf.\n\n    Examples:\n        >>> from torchrl.trainers.helpers.models import make_ddpg_actor\n        >>> from torchrl.envs.libs.gym import GymEnv\n        >>> from torchrl.envs.transforms import CatTensors, TransformedEnv, DoubleToFloat, Compose\n        >>> import hydra\n        >>> from hydra.core.config_store import ConfigStore\n        >>> import dataclasses\n        >>> proof_environment = TransformedEnv(GymEnv(\"HalfCheetah-v2\"), Compose(DoubleToFloat([\"observation\"]),\n        ...    CatTensors([\"observation\"], \"observation_vector\")))\n        >>> device = torch.device(\"cpu\")\n        >>> config_fields = [(config_field.name, config_field.type, config_field) for config_cls in\n        ...                    (DDPGModelConfig, EnvConfig)\n        ...                   for config_field in dataclasses.fields(config_cls)]\n        >>> Config = dataclasses.make_dataclass(cls_name=\"Config\", fields=config_fields)\n        >>> cs = ConfigStore.instance()\n        >>> cs.store(name=\"config\", node=Config)\n        >>> with initialize(config_path=None):\n        >>>     cfg = compose(config_name=\"config\")\n        >>> actor, value = make_ddpg_actor(\n        ...     proof_environment,\n        ...     device=device,\n        ...     cfg=cfg)\n        >>> td = proof_environment.reset()\n        >>> print(actor(td))\n        TensorDict(\n            fields={\n                done: Tensor(torch.Size([1]), dtype=torch.bool),\n                observation_vector: Tensor(torch.Size([17]), dtype=torch.float32),\n                param: Tensor(torch.Size([6]), dtype=torch.float32),\n                action: Tensor(torch.Size([6]), dtype=torch.float32)},\n            batch_size=torch.Size([]),\n            device=cpu,\n            is_shared=False)\n        >>> print(value(td))\n        TensorDict(\n            fields={\n                done: Tensor(torch.Size([1]), dtype=torch.bool),\n                observation_vector: Tensor(torch.Size([17]), dtype=torch.float32),\n                param: Tensor(torch.Size([6]), dtype=torch.float32),\n                action: Tensor(torch.Size([6]), dtype=torch.float32),\n                state_action_value: Tensor(torch.Size([1]), dtype=torch.float32)},\n            batch_size=torch.Size([]),\n            device=cpu,\n            is_shared=False)\n    \"\"\"\n    # TODO: https://arxiv.org/pdf/1804.08617.pdf\n\n    from_pixels = cfg.from_pixels\n    noisy = cfg.noisy\n\n    actor_net_kwargs = actor_net_kwargs if actor_net_kwargs is not None else {}\n    value_net_kwargs = value_net_kwargs if value_net_kwargs is not None else {}\n\n    linear_layer_class = torch.nn.Linear if not noisy else NoisyLinear\n\n    env_specs = proof_environment.specs\n    out_features = env_specs[\"action_spec\"].shape[0]\n\n    actor_net_default_kwargs = {\n        \"action_dim\": out_features,\n        \"mlp_net_kwargs\": {\n            \"layer_class\": linear_layer_class,\n            \"activation_class\": ACTIVATIONS[cfg.activation],\n        },\n    }\n    actor_net_default_kwargs.update(actor_net_kwargs)\n    if from_pixels:\n        in_keys = [\"pixels\"]\n        actor_net_default_kwargs[\"conv_net_kwargs\"] = {\n            \"activation_class\": ACTIVATIONS[cfg.activation]\n        }\n        actor_net = DdpgCnnActor(**actor_net_default_kwargs)\n        gSDE_state_key = \"hidden\"\n        out_keys = [\"param\", \"hidden\"]\n    else:\n        in_keys = [\"observation_vector\"]\n        actor_net = DdpgMlpActor(**actor_net_default_kwargs)\n        gSDE_state_key = \"observation_vector\"\n        out_keys = [\"param\"]\n    actor_module = SafeModule(actor_net, in_keys=in_keys, out_keys=out_keys)\n\n    if cfg.gSDE:\n        min = env_specs[\"action_spec\"].space.minimum\n        max = env_specs[\"action_spec\"].space.maximum\n        transform = SafeTanhTransform()\n        if (min != -1).any() or (max != 1).any():\n            transform = d.ComposeTransform(\n                transform, d.AffineTransform(loc=(max + min) / 2, scale=(max - min) / 2)\n            )\n        actor_module = SafeSequential(\n            actor_module,\n            SafeModule(\n                LazygSDEModule(transform=transform, learn_sigma=False),\n                in_keys=[\"param\", gSDE_state_key, \"_eps_gSDE\"],\n                out_keys=[\"loc\", \"scale\", \"action\", \"_eps_gSDE\"],\n            ),\n        )\n\n    # We use a ProbabilisticActor to make sure that we map the network output to the right space using a TanhDelta\n    # distribution.\n    actor = ProbabilisticActor(\n        module=actor_module,\n        in_keys=[\"param\"],\n        spec=CompositeSpec(action=env_specs[\"action_spec\"]),\n        safe=True,\n        distribution_class=TanhDelta,\n        distribution_kwargs={\n            \"min\": env_specs[\"action_spec\"].space.minimum,\n            \"max\": env_specs[\"action_spec\"].space.maximum,\n        },\n    )\n\n    state_class = ValueOperator\n    if from_pixels:\n        value_net_default_kwargs = {\n            \"mlp_net_kwargs\": {\n                \"layer_class\": linear_layer_class,\n                \"activation_class\": ACTIVATIONS[cfg.activation],\n            }\n        }\n        value_net_default_kwargs.update(value_net_kwargs)\n\n        in_keys = [\"pixels\", \"action\"]\n        out_keys = [\"state_action_value\"]\n        q_net = DdpgCnnQNet(**value_net_default_kwargs)\n    else:\n        value_net_default_kwargs1 = {\"activation_class\": ACTIVATIONS[cfg.activation]}\n        value_net_default_kwargs1.update(\n            value_net_kwargs.get(\n                \"mlp_net_kwargs_net1\",\n                {\n                    \"layer_class\": linear_layer_class,\n                    \"activation_class\": ACTIVATIONS[cfg.activation],\n                    \"bias_last_layer\": True,\n                },\n            )\n        )\n        value_net_default_kwargs2 = {\n            \"num_cells\": [400, 300],\n            \"activation_class\": ACTIVATIONS[cfg.activation],\n            \"bias_last_layer\": True,\n        }\n        value_net_default_kwargs2.update(\n            value_net_kwargs.get(\n                \"mlp_net_kwargs_net2\",\n                {\n                    \"layer_class\": linear_layer_class,\n                },\n            )\n        )\n        in_keys = [\"observation_vector\", \"action\"]\n        out_keys = [\"state_action_value\"]\n        q_net = DdpgMlpQNet(\n            mlp_net_kwargs_net1=value_net_default_kwargs1,", "metadata": {"task_id": "pytorch_rl/117", "ground_truth": "            mlp_net_kwargs_net2=value_net_default_kwargs2,", "fpath_tuple": ["pytorch_rl", "torchrl", "trainers", "helpers", "models.py"], "context_start_lineno": 232, "line_no": 394, "query_window": {"context": "                },\n            )\n        )\n        value_net_default_kwargs2 = {\n            \"num_cells\": [400, 300],\n            \"activation_class\": ACTIVATIONS[cfg.activation],\n            \"bias_last_layer\": True,\n        }\n        value_net_default_kwargs2.update(\n            value_net_kwargs.get(\n                \"mlp_net_kwargs_net2\",\n                {\n                    \"layer_class\": linear_layer_class,\n                },\n            )\n        )\n        in_keys = [\"observation_vector\", \"action\"]\n        out_keys = [\"state_action_value\"]\n        q_net = DdpgMlpQNet(\n            mlp_net_kwargs_net1=value_net_default_kwargs1,", "metadata": {"fpath_tuple": ["pytorch_rl", "torchrl", "trainers", "helpers", "models.py"], "line_no": 394, "task_id": "pytorch_rl/117", "start_line_no": 374, "end_line_no": 394, "window_size": 20, "context_start_lineno": 232, "repo": "pytorch_rl"}}, "top_k_context": [{"context": "        mlp_net_kwargs_net1: Dict = (\n            mlp_net_kwargs_net1 if mlp_net_kwargs_net1 is not None else {}\n        )\n        mlp1_net_default_kwargs.update(mlp_net_kwargs_net1)\n        self.mlp1 = MLP(device=device, **mlp1_net_default_kwargs)\n\n        mlp2_net_default_kwargs = {\n            \"in_features\": None,\n            \"out_features\": 1,\n            \"num_cells\": [\n                300,\n            ],\n            \"activation_class\": nn.ELU,\n            \"bias_last_layer\": True,\n        }\n        mlp_net_kwargs_net2 = (\n            mlp_net_kwargs_net2 if mlp_net_kwargs_net2 is not None else {}\n        )\n        mlp2_net_default_kwargs.update(mlp_net_kwargs_net2)\n        self.mlp2 = MLP(device=device, **mlp2_net_default_kwargs)", "metadata": [{"fpath_tuple": ["pytorch_rl", "torchrl", "modules", "models", "models.py"], "line_no": 958, "start_line_no": 948, "end_line_no": 968, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}], "sim_score": 0.39361702127659576}, {"context": "            \"depth\": 0,\n            \"num_cells\": [],\n            \"activation_class\": nn.ELU,\n            \"bias_last_layer\": True,\n            \"activate_last_layer\": True,\n        }\n        mlp_net_kwargs_net1: Dict = (\n            mlp_net_kwargs_net1 if mlp_net_kwargs_net1 is not None else {}\n        )\n        mlp1_net_default_kwargs.update(mlp_net_kwargs_net1)\n        self.mlp1 = MLP(device=device, **mlp1_net_default_kwargs)\n\n        mlp2_net_default_kwargs = {\n            \"in_features\": None,\n            \"out_features\": 1,\n            \"num_cells\": [\n                300,\n            ],\n            \"activation_class\": nn.ELU,\n            \"bias_last_layer\": True,", "metadata": [{"fpath_tuple": ["pytorch_rl", "torchrl", "modules", "models", "models.py"], "line_no": 952, "start_line_no": 942, "end_line_no": 962, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}], "sim_score": 0.3917525773195876}, {"context": "        )\n        mlp1_net_default_kwargs.update(mlp_net_kwargs_net1)\n        self.mlp1 = MLP(device=device, **mlp1_net_default_kwargs)\n\n        mlp2_net_default_kwargs = {\n            \"in_features\": None,\n            \"out_features\": 1,\n            \"num_cells\": [\n                300,\n            ],\n            \"activation_class\": nn.ELU,\n            \"bias_last_layer\": True,\n        }\n        mlp_net_kwargs_net2 = (\n            mlp_net_kwargs_net2 if mlp_net_kwargs_net2 is not None else {}\n        )\n        mlp2_net_default_kwargs.update(mlp_net_kwargs_net2)\n        self.mlp2 = MLP(device=device, **mlp2_net_default_kwargs)\n        ddpg_init_last_layer(self.mlp2[-1], 6e-3, device=device)\n", "metadata": [{"fpath_tuple": ["pytorch_rl", "torchrl", "modules", "models", "models.py"], "line_no": 960, "start_line_no": 950, "end_line_no": 970, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}], "sim_score": 0.39}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# examples/research_projects/intel_opts/textual_inversion/textual_inversion_bf16.py\n# examples/textual_inversion/textual_inversion.py\n# examples/textual_inversion/textual_inversion_flax.py\n# --------------------------------------------------\n#     \"a cool painting in the style of {}\",\n#     \"a close-up painting in the style of {}\",\n#     \"a bright painting in the style of {}\",\n#     \"a cropped painting in the style of {}\",\n#     \"a good painting in the style of {}\",\n#     \"a close-up painting in the style of {}\",\n#     \"a rendition in the style of {}\",\n#     \"a nice painting in the style of {}\",\n#     \"a small painting in the style of {}\",\n#     \"a weird painting in the style of {}\",\n#     \"a large painting in the style of {}\",\n# ]\n# \n# \n# class TextualInversionDataset(Dataset):\n#     def __init__(\n#         self,\n#         data_root,\n#         tokenizer,\n#         learnable_property=\"object\",  # [object, style]\n# --------------------------------------------------\n# the below code fragment can be found in:\n# examples/research_projects/intel_opts/textual_inversion/textual_inversion_bf16.py\n# examples/textual_inversion/textual_inversion.py\n# examples/textual_inversion/textual_inversion_flax.py\n# --------------------------------------------------\n#     \"a good painting in the style of {}\",\n#     \"a close-up painting in the style of {}\",\n#     \"a rendition in the style of {}\",\n#     \"a nice painting in the style of {}\",\n#     \"a small painting in the style of {}\",\n#     \"a weird painting in the style of {}\",\n#     \"a large painting in the style of {}\",\n# ]\n# \n# \n# class TextualInversionDataset(Dataset):\n#     def __init__(\n#         self,\n#         data_root,\n#         tokenizer,\n#         learnable_property=\"object\",  # [object, style]\n#         size=512,\n#         repeats=100,\n#         interpolation=\"bicubic\",\n#         flip_p=0.5,\n# --------------------------------------------------\n# the below code fragment can be found in:\n# examples/research_projects/intel_opts/textual_inversion/textual_inversion_bf16.py\n# examples/textual_inversion/textual_inversion.py\n# examples/textual_inversion/textual_inversion_flax.py\n# --------------------------------------------------\n#     \"a bright painting in the style of {}\",\n#     \"a cropped painting in the style of {}\",\n#     \"a good painting in the style of {}\",\n#     \"a close-up painting in the style of {}\",\n#     \"a rendition in the style of {}\",\n#     \"a nice painting in the style of {}\",\n#     \"a small painting in the style of {}\",\n#     \"a weird painting in the style of {}\",\n#     \"a large painting in the style of {}\",\n# ]\n# \n# \n# class TextualInversionDataset(Dataset):\n#     def __init__(\n#         self,\n#         data_root,\n#         tokenizer,\n#         learnable_property=\"object\",  # [object, style]\n#         size=512,\n#         repeats=100,\n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\nname\",\n    )\n    parser.add_argument(\n        \"--train_data_dir\", type=str, default=None, required=True, help=\"A folder containing the training data.\"\n    )\n    parser.add_argument(\n        \"--placeholder_token\",\n        type=str,\n        default=None,\n        required=True,\n        help=\"A token to use as a placeholder for the concept.\",\n    )\n    parser.add_argument(\n        \"--initializer_token\", type=str, default=None, required=True, help=\"A token to use as initializer word.\"\n    )\n    parser.add_argument(\"--learnable_property\", type=str, default=\"object\", help=\"Choose between 'object' and 'style'\")\n    parser.add_argument(\"--repeats\", type=int, default=100, help=\"How many times to repeat the training data.\")\n    parser.add_argument(\n        \"--output_dir\",\n        type=str,\n        default=\"text-inversion-model\",\n        help=\"The output directory where the model predictions and checkpoints will be written.\",\n    )\n    parser.add_argument(\"--seed\", type=int, default=None, help=\"A seed for reproducible training.\")\n    parser.add_argument(\n        \"--resolution\",\n        type=int,\n        default=512,\n        help=(\n            \"The resolution for input images, all the images in the train/validation dataset will be resized to this\"\n            \" resolution\"\n        ),\n    )\n    parser.add_argument(\n        \"--center_crop\", action=\"store_true\", help=\"Whether to center crop images before resizing to resolution.\"\n    )\n    parser.add_argument(\n        \"--train_batch_size\", type=int, default=16, help=\"Batch size (per device) for the training dataloader.\"\n    )\n    parser.add_argument(\"--num_train_epochs\", type=int, default=100)\n    parser.add_argument(\n        \"--max_train_steps\",\n        type=int,\n        default=5000,\n        help=\"Total number of training steps to perform.  If provided, overrides num_train_epochs.\",\n    )\n    parser.add_argument(\n        \"--gradient_accumulation_steps\",\n        type=int,\n        default=1,\n        help=\"Number of updates steps to accumulate before performing a backward/update pass.\",\n    )\n    parser.add_argument(\n        \"--learning_rate\",\n        type=float,\n        default=1e-4,\n        help=\"Initial learning rate (after the potential warmup period) to use.\",\n    )\n    parser.add_argument(\n        \"--scale_lr\",\n        action=\"store_true\",\n        default=True,\n        help=\"Scale the learning rate by the number of GPUs, gradient accumulation steps, and batch size.\",\n    )\n    parser.add_argument(\n        \"--lr_scheduler\",\n        type=str,\n        default=\"constant\",\n        help=(\n            'The scheduler type to use. Choose between [\"linear\", \"cosine\", \"cosine_with_restarts\", \"polynomial\",'\n            ' \"constant\", \"constant_with_warmup\"]'\n        ),\n    )\n    parser.add_argument(\n        \"--lr_warmup_steps\", type=int, default=500, help=\"Number of steps for the warmup in the lr scheduler.\"\n    )\n    parser.add_argument(\"--adam_beta1\", type=float, default=0.9, help=\"The beta1 parameter for the Adam optimizer.\")\n    parser.add_argument(\"--adam_beta2\", type=float, default=0.999, help=\"The beta2 parameter for the Adam optimizer.\")\n    parser.add_argument(\"--adam_weight_decay\", type=float, default=1e-2, help=\"Weight decay to use.\")\n    parser.add_argument(\"--adam_epsilon\", type=float, default=1e-08, help=\"Epsilon value for the Adam optimizer\")\n    parser.add_argument(\"--push_to_hub\", action=\"store_true\", help=\"Whether or not to push the model to the Hub.\")\n    parser.add_argument(\"--hub_token\", type=str, default=None, help=\"The token to use to push to the Model Hub.\")\n    parser.add_argument(\n        \"--hub_model_id\",\n        type=str,\n        default=None,\n        help=\"The name of the repository to keep in sync with the local `output_dir`.\",\n    )\n    parser.add_argument(\n        \"--logging_dir\",\n        type=str,\n        default=\"logs\",\n        help=(\n            \"[TensorBoard](https://www.tensorflow.org/tensorboard) log directory. Will default to\"\n            \" *output_dir/runs/**CURRENT_DATETIME_HOSTNAME***.\"\n        ),\n    )\n    parser.add_argument(\n        \"--mixed_precision\",\n        type=str,\n        default=\"no\",\n        choices=[\"no\", \"fp16\", \"bf16\"],\n        help=(\n            \"Whether to use mixed precision. Choose\"\n            \"between fp16 and bf16 (bfloat16). Bf16 requires PyTorch >= 1.10.\"\n            \"and an Nvidia Ampere GPU.\"\n        ),\n    )\n    parser.add_argument(\"--local_rank\", type=int, default=-1, help=\"For distributed training: local_rank\")\n\n    args = parser.parse_args()\n    env_local_rank = int(os.environ.get(\"LOCAL_RANK\", -1))\n    if env_local_rank != -1 and env_local_rank != args.local_rank:\n        args.local_rank = env_local_rank\n\n    if args.train_data_dir is None:\n        raise ValueError(\"You must specify a train data directory.\")\n\n    return args\n\n\nimagenet_templates_small = [\n    \"a photo of a {}\",\n    \"a rendering of a {}\",\n    \"a cropped photo of the {}\",\n    \"the photo of a {}\",\n    \"a photo of a clean {}\",\n    \"a photo of a dirty {}\",\n    \"a dark photo of the {}\",\n    \"a photo of my {}\",\n    \"a photo of the cool {}\",\n    \"a close-up photo of a {}\",\n    \"a bright photo of the {}\",\n    \"a cropped photo of a {}\",\n    \"a photo of the {}\",\n    \"a good photo of the {}\",\n    \"a photo of one {}\",\n    \"a close-up photo of the {}\",\n    \"a rendition of the {}\",\n    \"a photo of the clean {}\",\n    \"a rendition of a {}\",\n    \"a photo of a nice {}\",\n    \"a good photo of a {}\",\n    \"a photo of the nice {}\",\n    \"a photo of the small {}\",\n    \"a photo of the weird {}\",\n    \"a photo of the large {}\",\n    \"a photo of a cool {}\",\n    \"a photo of a small {}\",\n]\n\nimagenet_style_templates_small = [\n    \"a painting in the style of {}\",\n    \"a rendering in the style of {}\",\n    \"a cropped painting in the style of {}\",\n    \"the painting in the style of {}\",\n    \"a clean painting in the style of {}\",\n    \"a dirty painting in the style of {}\",\n    \"a dark painting in the style of {}\",\n    \"a picture in the style of {}\",\n    \"a cool painting in the style of {}\",\n    \"a close-up painting in the style of {}\",\n    \"a bright painting in the style of {}\",\n    \"a cropped painting in the style of {}\",\n    \"a good painting in the style of {}\",\n    \"a close-up painting in the style of {}\",\n    \"a rendition in the style of {}\",\n    \"a nice painting in the style of {}\",\n    \"a small painting in the style of {}\",\n    \"a weird painting in the style of {}\",\n    \"a large painting in the style of {}\",\n]\n\n\nclass TextualInversionDataset(Dataset):\n    def __init__(\n        self,\n        data_root,\n        tokenizer,\n        learnable_property=\"object\",  # [object, style]", "metadata": {"task_id": "huggingface_diffusers/162", "ground_truth": "        size=512,", "fpath_tuple": ["huggingface_diffusers", "examples", "research_projects", "intel_opts", "textual_inversion", "textual_inversion_bf16.py"], "context_start_lineno": 98, "line_no": 278, "query_window": {"context": "    \"a cool painting in the style of {}\",\n    \"a close-up painting in the style of {}\",\n    \"a bright painting in the style of {}\",\n    \"a cropped painting in the style of {}\",\n    \"a good painting in the style of {}\",\n    \"a close-up painting in the style of {}\",\n    \"a rendition in the style of {}\",\n    \"a nice painting in the style of {}\",\n    \"a small painting in the style of {}\",\n    \"a weird painting in the style of {}\",\n    \"a large painting in the style of {}\",\n]\n\n\nclass TextualInversionDataset(Dataset):\n    def __init__(\n        self,\n        data_root,\n        tokenizer,\n        learnable_property=\"object\",  # [object, style]", "metadata": {"fpath_tuple": ["huggingface_diffusers", "examples", "research_projects", "intel_opts", "textual_inversion", "textual_inversion_bf16.py"], "line_no": 278, "task_id": "huggingface_diffusers/162", "start_line_no": 258, "end_line_no": 278, "window_size": 20, "context_start_lineno": 98, "repo": "huggingface_diffusers"}}, "top_k_context": [{"context": "    \"a cool painting in the style of {}\",\n    \"a close-up painting in the style of {}\",\n    \"a bright painting in the style of {}\",\n    \"a cropped painting in the style of {}\",\n    \"a good painting in the style of {}\",\n    \"a close-up painting in the style of {}\",\n    \"a rendition in the style of {}\",\n    \"a nice painting in the style of {}\",\n    \"a small painting in the style of {}\",\n    \"a weird painting in the style of {}\",\n    \"a large painting in the style of {}\",\n]\n\n\nclass TextualInversionDataset(Dataset):\n    def __init__(\n        self,\n        data_root,\n        tokenizer,\n        learnable_property=\"object\",  # [object, style]", "metadata": [{"fpath_tuple": ["huggingface_diffusers", "examples", "research_projects", "intel_opts", "textual_inversion", "textual_inversion_bf16.py"], "line_no": 268, "start_line_no": 258, "end_line_no": 278, "window_size": 20, "repo": "huggingface_diffusers", "slice_size": 10}, {"fpath_tuple": ["huggingface_diffusers", "examples", "textual_inversion", "textual_inversion.py"], "line_no": 364, "start_line_no": 354, "end_line_no": 374, "window_size": 20, "repo": "huggingface_diffusers", "slice_size": 10}, {"fpath_tuple": ["huggingface_diffusers", "examples", "textual_inversion", "textual_inversion_flax.py"], "line_no": 238, "start_line_no": 228, "end_line_no": 248, "window_size": 20, "repo": "huggingface_diffusers", "slice_size": 10}], "sim_score": 1.0}, {"context": "    \"a bright painting in the style of {}\",\n    \"a cropped painting in the style of {}\",\n    \"a good painting in the style of {}\",\n    \"a close-up painting in the style of {}\",\n    \"a rendition in the style of {}\",\n    \"a nice painting in the style of {}\",\n    \"a small painting in the style of {}\",\n    \"a weird painting in the style of {}\",\n    \"a large painting in the style of {}\",\n]\n\n\nclass TextualInversionDataset(Dataset):\n    def __init__(\n        self,\n        data_root,\n        tokenizer,\n        learnable_property=\"object\",  # [object, style]\n        size=512,\n        repeats=100,", "metadata": [{"fpath_tuple": ["huggingface_diffusers", "examples", "research_projects", "intel_opts", "textual_inversion", "textual_inversion_bf16.py"], "line_no": 270, "start_line_no": 260, "end_line_no": 280, "window_size": 20, "repo": "huggingface_diffusers", "slice_size": 10}, {"fpath_tuple": ["huggingface_diffusers", "examples", "textual_inversion", "textual_inversion.py"], "line_no": 366, "start_line_no": 356, "end_line_no": 376, "window_size": 20, "repo": "huggingface_diffusers", "slice_size": 10}, {"fpath_tuple": ["huggingface_diffusers", "examples", "textual_inversion", "textual_inversion_flax.py"], "line_no": 240, "start_line_no": 230, "end_line_no": 250, "window_size": 20, "repo": "huggingface_diffusers", "slice_size": 10}], "sim_score": 0.9}, {"context": "    \"a dark painting in the style of {}\",\n    \"a picture in the style of {}\",\n    \"a cool painting in the style of {}\",\n    \"a close-up painting in the style of {}\",\n    \"a bright painting in the style of {}\",\n    \"a cropped painting in the style of {}\",\n    \"a good painting in the style of {}\",\n    \"a close-up painting in the style of {}\",\n    \"a rendition in the style of {}\",\n    \"a nice painting in the style of {}\",\n    \"a small painting in the style of {}\",\n    \"a weird painting in the style of {}\",\n    \"a large painting in the style of {}\",\n]\n\n\nclass TextualInversionDataset(Dataset):\n    def __init__(\n        self,\n        data_root,", "metadata": [{"fpath_tuple": ["huggingface_diffusers", "examples", "research_projects", "intel_opts", "textual_inversion", "textual_inversion_bf16.py"], "line_no": 266, "start_line_no": 256, "end_line_no": 276, "window_size": 20, "repo": "huggingface_diffusers", "slice_size": 10}, {"fpath_tuple": ["huggingface_diffusers", "examples", "textual_inversion", "textual_inversion.py"], "line_no": 362, "start_line_no": 352, "end_line_no": 372, "window_size": 20, "repo": "huggingface_diffusers", "slice_size": 10}, {"fpath_tuple": ["huggingface_diffusers", "examples", "textual_inversion", "textual_inversion_flax.py"], "line_no": 236, "start_line_no": 226, "end_line_no": 246, "window_size": 20, "repo": "huggingface_diffusers", "slice_size": 10}], "sim_score": 0.7894736842105263}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# federatedscope/nlp/hetero_tasks/dataset/newsqa.py\n# --------------------------------------------------\n#                     'answer_start': spans[i][0]\n#                 } for i in range(len(spans))]\n# \n#         start_pos, end_pos, context_tokens = get_char_to_word_positions(\n#             context, train_answer, start_char_pos, is_impossible)\n#         examples.append(\n#             NewsQAExample(qa_id, question, context, train_answer, val_answer,\n#                           start_pos, end_pos, context_tokens, is_impossible))\n#     return examples\n# \n# \n# def process_newsqa_dataset(data,\n#                            split,\n#                            tokenizer,\n#                            max_seq_len,\n#                            max_query_len,\n#                            trunc_stride,\n#                            cache_dir='',\n#                            client_id=None,\n#                            pretrain=False,\n# --------------------------------------------------\n# the below code fragment can be found in:\n# federatedscope/nlp/hetero_tasks/dataset/newsqa.py\n# --------------------------------------------------\n# \n#         start_pos, end_pos, context_tokens = get_char_to_word_positions(\n#             context, train_answer, start_char_pos, is_impossible)\n#         examples.append(\n#             NewsQAExample(qa_id, question, context, train_answer, val_answer,\n#                           start_pos, end_pos, context_tokens, is_impossible))\n#     return examples\n# \n# \n# def process_newsqa_dataset(data,\n#                            split,\n#                            tokenizer,\n#                            max_seq_len,\n#                            max_query_len,\n#                            trunc_stride,\n#                            cache_dir='',\n#                            client_id=None,\n#                            pretrain=False,\n#                            is_debug=False,\n#                            **kwargs):\n# --------------------------------------------------\n# the below code fragment can be found in:\n# federatedscope/nlp/hetero_tasks/dataset/newsqa.py\n# --------------------------------------------------\n#             context, train_answer, start_char_pos, is_impossible)\n#         examples.append(\n#             NewsQAExample(qa_id, question, context, train_answer, val_answer,\n#                           start_pos, end_pos, context_tokens, is_impossible))\n#     return examples\n# \n# \n# def process_newsqa_dataset(data,\n#                            split,\n#                            tokenizer,\n#                            max_seq_len,\n#                            max_query_len,\n#                            trunc_stride,\n#                            cache_dir='',\n#                            client_id=None,\n#                            pretrain=False,\n#                            is_debug=False,\n#                            **kwargs):\n#     if pretrain:\n#         return process_newsqa_dataset_for_pretrain(data, split, tokenizer,\n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\n = context\n        self.train_answer = train_answer\n        self.val_answer = val_answer\n        self.start_position = start_pos\n        self.end_position = end_pos\n        self.context_tokens = context_tokens\n        self.is_impossible = is_impossible\n\n\nclass SquadEncodedInput(object):\n    def __init__(self, token_ids, token_type_ids, attention_mask,\n                 overflow_token_ids):\n        self.token_ids = token_ids\n        self.token_type_ids = token_type_ids\n        self.attention_mask = attention_mask\n        self.overflow_token_ids = overflow_token_ids\n\n\nclass SquadResult(object):\n    def __init__(self, unique_id, start_logits, end_logits):\n        self.unique_id = unique_id\n        self.start_logits = start_logits\n        self.end_logits = end_logits\n\n\ndef refine_subtoken_position(context_subtokens, subtoken_start_pos,\n                             subtoken_end_pos, tokenizer, annotated_answer):\n    subtoken_answer = ' '.join(tokenizer.tokenize(annotated_answer))\n    for new_st in range(subtoken_start_pos, subtoken_end_pos + 1):\n        for new_ed in range(subtoken_end_pos, subtoken_start_pos - 1, -1):\n            text_span = ' '.join(context_subtokens[new_st:(new_ed + 1)])\n            if text_span == subtoken_answer:\n                return new_st, new_ed\n    return subtoken_start_pos, subtoken_end_pos\n\n\ndef get_char_to_word_positions(context, answer, start_char_pos, is_impossible):\n    context_tokens = []\n    char_to_word_offset = []\n    is_prev_whitespace = True\n    for c in context:\n        is_whitespace = (c == ' ' or c == '\\t' or c == '\\r' or c == '\\n'\n                         or ord(c) == 0x202F)\n        if is_whitespace:\n            is_prev_whitespace = True\n        else:\n            if is_prev_whitespace:\n                context_tokens.append(c)\n            else:\n                context_tokens[-1] += c\n            is_prev_whitespace = False\n        char_to_word_offset.append(len(context_tokens) - 1)\n\n    start_pos, end_pos = 0, 0\n    if start_char_pos is not None and not is_impossible:\n        start_pos = char_to_word_offset[start_char_pos]\n        end_pos = char_to_word_offset[start_char_pos + len(answer) - 1]\n    return start_pos, end_pos, context_tokens\n\n\ndef check_max_context_token(all_spans, cur_span_idx, pos):\n    best_score, best_span_idx = None, None\n    for span_idx, span in enumerate(all_spans):\n        end = span.context_start_position + span.context_len - 1\n        if pos < span.context_start_position or pos > end:\n            continue\n        num_left_context = pos - span.context_start_position\n        num_right_context = end - pos\n        score = \\\n            min(num_left_context, num_right_context) + 0.01 * span.context_len\n        if best_score is None or score > best_score:\n            best_score = score\n            best_span_idx = span_idx\n    return cur_span_idx == best_span_idx\n\n\ndef encode(tokenizer, text_a, text_b, max_seq_len, max_query_len,\n           added_trunc_size):\n    def _get_token_ids(text):\n        if isinstance(text, str):\n            return tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text))\n        elif isinstance(text, (list, tuple)) and len(text) > 0 and \\\n                isinstance(text[0], str):\n            return tokenizer.convert_tokens_to_ids(text)\n        elif isinstance(text, (list, tuple)) and len(text) > 0 and \\\n                isinstance(text[0], int):\n            return text\n        else:\n            raise ValueError('Input is not valid, should be a string, '\n                             'a list/tuple of strings or a list/tuple of '\n                             'integers.')\n\n    token_ids_a = _get_token_ids(text_a)\n    token_ids_b = _get_token_ids(text_b)\n\n    # Truncate\n    overflow_token_ids = None\n    len_a = len(token_ids_a) + 2\n    total_len = len(token_ids_a) + len(token_ids_b) + 3\n    if len_a > max_query_len:\n        num_remove = len_a - max_query_len\n        token_ids_a = token_ids_a[:-num_remove]\n    if total_len > max_seq_len:\n        num_remove = total_len - max_seq_len\n        trunc_size = min(len(token_ids_b), added_trunc_size + num_remove)\n        overflow_token_ids = token_ids_b[-trunc_size:]\n        token_ids_b = token_ids_b[:-num_remove]\n\n    # Combine and pad\n    token_ids = \\\n        [tokenizer.cls_token_id] + token_ids_a + [tokenizer.sep_token_id]\n    token_type_ids = [0] * len(token_ids)\n    token_ids += token_ids_b + [tokenizer.sep_token_id]\n    token_type_ids += [1] * (len(token_ids_b) + 1)\n    attention_mask = [1] * len(token_ids)\n    if len(token_ids) < max_seq_len:\n        dif = max_seq_len - len(token_ids)\n        token_ids += [tokenizer.pad_token_id] * dif\n        token_type_ids += [0] * dif\n        attention_mask += [0] * dif\n\n    return SquadEncodedInput(token_ids, token_type_ids, attention_mask,\n                             overflow_token_ids)\n\n\ndef get_squad_examples(data, split, is_debug=False):\n    if is_debug:\n        data = data[:NUM_DEBUG]\n    examples = []\n    for para in data:\n        context = para['context']\n        qa = para['qa']\n        qa_id = qa['id']\n        question = qa['question']\n        start_char_pos = None\n        train_answer = None\n        val_answer = []\n\n        is_impossible = qa['is_impossible'] if 'is_impossible' in qa else False\n        if not is_impossible:\n            if split == 'train':\n                train_answer = qa['answers'][0]['text']\n                start_char_pos = qa['answers'][0]['answer_start']\n            else:\n                val_answer = qa['answers']\n\n        start_pos, end_pos, context_tokens = get_char_to_word_positions(\n            context, train_answer, start_char_pos, is_impossible)\n        examples.append(\n            SquadExample(qa_id, question, context, train_answer, val_answer,\n                         start_pos, end_pos, context_tokens, is_impossible))\n    return examples\n\n\ndef process_squad_dataset(data,\n                          split,\n                          tokenizer,\n                          max_seq_len,\n                          max_query_len,\n                          trunc_stride,\n                          cache_dir='',\n                          client_id=None,", "metadata": {"task_id": "alibaba_FederatedScope/111", "ground_truth": "                          pretrain=False,", "fpath_tuple": ["alibaba_FederatedScope", "federatedscope", "nlp", "hetero_tasks", "dataset", "squad.py"], "context_start_lineno": 15, "line_no": 177, "query_window": {"context": "                start_char_pos = qa['answers'][0]['answer_start']\n            else:\n                val_answer = qa['answers']\n\n        start_pos, end_pos, context_tokens = get_char_to_word_positions(\n            context, train_answer, start_char_pos, is_impossible)\n        examples.append(\n            SquadExample(qa_id, question, context, train_answer, val_answer,\n                         start_pos, end_pos, context_tokens, is_impossible))\n    return examples\n\n\ndef process_squad_dataset(data,\n                          split,\n                          tokenizer,\n                          max_seq_len,\n                          max_query_len,\n                          trunc_stride,\n                          cache_dir='',\n                          client_id=None,", "metadata": {"fpath_tuple": ["alibaba_FederatedScope", "federatedscope", "nlp", "hetero_tasks", "dataset", "squad.py"], "line_no": 177, "task_id": "alibaba_FederatedScope/111", "start_line_no": 157, "end_line_no": 177, "window_size": 20, "context_start_lineno": 15, "repo": "alibaba_FederatedScope"}}, "top_k_context": [{"context": "\n        start_pos, end_pos, context_tokens = get_char_to_word_positions(\n            context, train_answer, start_char_pos, is_impossible)\n        examples.append(\n            NewsQAExample(qa_id, question, context, train_answer, val_answer,\n                          start_pos, end_pos, context_tokens, is_impossible))\n    return examples\n\n\ndef process_newsqa_dataset(data,\n                           split,\n                           tokenizer,\n                           max_seq_len,\n                           max_query_len,\n                           trunc_stride,\n                           cache_dir='',\n                           client_id=None,\n                           pretrain=False,\n                           is_debug=False,\n                           **kwargs):", "metadata": [{"fpath_tuple": ["alibaba_FederatedScope", "federatedscope", "nlp", "hetero_tasks", "dataset", "newsqa.py"], "line_no": 176, "start_line_no": 166, "end_line_no": 186, "window_size": 20, "repo": "alibaba_FederatedScope", "slice_size": 10}], "sim_score": 0.6559139784946236}, {"context": "                    'answer_start': spans[i][0]\n                } for i in range(len(spans))]\n\n        start_pos, end_pos, context_tokens = get_char_to_word_positions(\n            context, train_answer, start_char_pos, is_impossible)\n        examples.append(\n            NewsQAExample(qa_id, question, context, train_answer, val_answer,\n                          start_pos, end_pos, context_tokens, is_impossible))\n    return examples\n\n\ndef process_newsqa_dataset(data,\n                           split,\n                           tokenizer,\n                           max_seq_len,\n                           max_query_len,\n                           trunc_stride,\n                           cache_dir='',\n                           client_id=None,\n                           pretrain=False,", "metadata": [{"fpath_tuple": ["alibaba_FederatedScope", "federatedscope", "nlp", "hetero_tasks", "dataset", "newsqa.py"], "line_no": 174, "start_line_no": 164, "end_line_no": 184, "window_size": 20, "repo": "alibaba_FederatedScope", "slice_size": 10}], "sim_score": 0.6534653465346535}, {"context": "                val_answer = [{\n                    'text': context[spans[i][0]:spans[i][1] + 1],\n                    'answer_start': spans[i][0]\n                } for i in range(len(spans))]\n\n        start_pos, end_pos, context_tokens = get_char_to_word_positions(\n            context, train_answer, start_char_pos, is_impossible)\n        examples.append(\n            NewsQAExample(qa_id, question, context, train_answer, val_answer,\n                          start_pos, end_pos, context_tokens, is_impossible))\n    return examples\n\n\ndef process_newsqa_dataset(data,\n                           split,\n                           tokenizer,\n                           max_seq_len,\n                           max_query_len,\n                           trunc_stride,\n                           cache_dir='',", "metadata": [{"fpath_tuple": ["alibaba_FederatedScope", "federatedscope", "nlp", "hetero_tasks", "dataset", "newsqa.py"], "line_no": 172, "start_line_no": 162, "end_line_no": 182, "window_size": 20, "repo": "alibaba_FederatedScope", "slice_size": 10}], "sim_score": 0.5943396226415094}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# ding/entry/tests/test_serial_entry.py\n# --------------------------------------------------\n#     config = [deepcopy(cartpole_r2d2_config), deepcopy(cartpole_r2d2_create_config)]\n#     config[0].policy.learn.update_per_collect = 1\n#     try:\n#         serial_pipeline(config, seed=0, max_iterations=1)\n#     except Exception:\n#         assert False, \"pipeline fail\"\n# \n# \n# @pytest.mark.unittest\n# def test_a2c_with_nstep_return():\n#     config = [deepcopy(cartpole_a2c_config), deepcopy(cartpole_a2c_create_config)]\n#     config[0].policy.learn.update_per_collect = 1\n#     config[0].policy.learn.nstep_return = config[0].policy.collect.nstep_return = True\n#     config[0].policy.collect.discount_factor = 0.9\n#     config[0].policy.collect.nstep = 3\n#     try:\n#         serial_pipeline(config, seed=0, max_iterations=1)\n#     except Exception:\n#         assert False, \"pipeline fail\"\n# \n# --------------------------------------------------\n# the below code fragment can be found in:\n# ding/entry/tests/test_serial_entry.py\n# --------------------------------------------------\n# @pytest.mark.unittest\n# def test_r2d2():\n#     config = [deepcopy(cartpole_r2d2_config), deepcopy(cartpole_r2d2_create_config)]\n#     config[0].policy.learn.update_per_collect = 1\n#     try:\n#         serial_pipeline(config, seed=0, max_iterations=1)\n#     except Exception:\n#         assert False, \"pipeline fail\"\n# \n# \n# @pytest.mark.unittest\n# def test_a2c_with_nstep_return():\n#     config = [deepcopy(cartpole_a2c_config), deepcopy(cartpole_a2c_create_config)]\n#     config[0].policy.learn.update_per_collect = 1\n#     config[0].policy.learn.nstep_return = config[0].policy.collect.nstep_return = True\n#     config[0].policy.collect.discount_factor = 0.9\n#     config[0].policy.collect.nstep = 3\n#     try:\n#         serial_pipeline(config, seed=0, max_iterations=1)\n#     except Exception:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# ding/entry/tests/test_serial_entry.py\n# --------------------------------------------------\n#     try:\n#         serial_pipeline(config, seed=0, max_iterations=1)\n#     except Exception:\n#         assert False, \"pipeline fail\"\n# \n# \n# @pytest.mark.unittest\n# def test_a2c_with_nstep_return():\n#     config = [deepcopy(cartpole_a2c_config), deepcopy(cartpole_a2c_create_config)]\n#     config[0].policy.learn.update_per_collect = 1\n#     config[0].policy.learn.nstep_return = config[0].policy.collect.nstep_return = True\n#     config[0].policy.collect.discount_factor = 0.9\n#     config[0].policy.collect.nstep = 3\n#     try:\n#         serial_pipeline(config, seed=0, max_iterations=1)\n#     except Exception:\n#         assert False, \"pipeline fail\"\n# \n# \n# @pytest.mark.unittest\n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\nip_dqn_main\nfrom dizoo.multiagent_particle.config import cooperative_navigation_qmix_config, cooperative_navigation_qmix_create_config  # noqa\nfrom dizoo.multiagent_particle.config import cooperative_navigation_wqmix_config, cooperative_navigation_wqmix_create_config  # noqa\nfrom dizoo.multiagent_particle.config import cooperative_navigation_vdn_config, cooperative_navigation_vdn_create_config  # noqa\nfrom dizoo.multiagent_particle.config import cooperative_navigation_coma_config, cooperative_navigation_coma_create_config  # noqa\nfrom dizoo.multiagent_particle.config import cooperative_navigation_collaq_config, cooperative_navigation_collaq_create_config  # noqa\nfrom dizoo.multiagent_particle.config import cooperative_navigation_atoc_config, cooperative_navigation_atoc_create_config  # noqa\nfrom dizoo.league_demo.league_demo_ppo_config import league_demo_ppo_config\nfrom dizoo.league_demo.selfplay_demo_ppo_main import main as selfplay_main\nfrom dizoo.league_demo.league_demo_ppo_main import main as league_main\nfrom dizoo.classic_control.pendulum.config.pendulum_sac_data_generation_default_config import pendulum_sac_data_genearation_default_config, pendulum_sac_data_genearation_default_create_config  # noqa\nfrom dizoo.classic_control.pendulum.config.pendulum_cql_config import pendulum_cql_default_config, pendulum_cql_default_create_config  # noqa\nfrom dizoo.classic_control.cartpole.config.cartpole_qrdqn_generation_data_config import cartpole_qrdqn_generation_data_config, cartpole_qrdqn_generation_data_create_config  # noqa\nfrom dizoo.classic_control.cartpole.config.cartpole_cql_config import cartpole_discrete_cql_config, cartpole_discrete_cql_create_config  # noqa\n\nwith open(\"./algo_record.log\", \"w+\") as f:\n    f.write(\"ALGO TEST STARTS\\n\")\n\n\n@pytest.mark.algotest\ndef test_dqn():\n    config = [deepcopy(cartpole_dqn_config), deepcopy(cartpole_dqn_create_config)]\n    try:\n        serial_pipeline(config, seed=0)\n    except Exception:\n        assert False, \"pipeline fail\"\n    with open(\"./algo_record.log\", \"a+\") as f:\n        f.write(\"1. dqn\\n\")\n\n\n@pytest.mark.algotest\ndef test_ddpg():\n    config = [deepcopy(pendulum_ddpg_config), deepcopy(pendulum_ddpg_create_config)]\n    try:\n        serial_pipeline(config, seed=0)\n    except Exception:\n        assert False, \"pipeline fail\"\n    with open(\"./algo_record.log\", \"a+\") as f:\n        f.write(\"2. ddpg\\n\")\n\n\n@pytest.mark.algotest\ndef test_td3():\n    config = [deepcopy(pendulum_td3_config), deepcopy(pendulum_td3_create_config)]\n    try:\n        serial_pipeline(config, seed=0)\n    except Exception:\n        assert False, \"pipeline fail\"\n    with open(\"./algo_record.log\", \"a+\") as f:\n        f.write(\"3. td3\\n\")\n\n\n@pytest.mark.algotest\ndef test_a2c():\n    config = [deepcopy(cartpole_a2c_config), deepcopy(cartpole_a2c_create_config)]\n    try:\n        serial_pipeline(config, seed=0)\n    except Exception:\n        assert False, \"pipeline fail\"\n    with open(\"./algo_record.log\", \"a+\") as f:\n        f.write(\"4. a2c\\n\")\n\n\n@pytest.mark.algotest\ndef test_rainbow():\n    config = [deepcopy(cartpole_rainbow_config), deepcopy(cartpole_rainbow_create_config)]\n    try:\n        serial_pipeline(config, seed=0)\n    except Exception:\n        assert False, \"pipeline fail\"\n    with open(\"./algo_record.log\", \"a+\") as f:\n        f.write(\"5. rainbow\\n\")\n\n\n@pytest.mark.algotest\ndef test_ppo():\n    config = [deepcopy(cartpole_ppo_config), deepcopy(cartpole_ppo_create_config)]\n    try:\n        ppo_main(config[0], seed=0)\n    except Exception:\n        assert False, \"pipeline fail\"\n    with open(\"./algo_record.log\", \"a+\") as f:\n        f.write(\"6. ppo\\n\")\n\n\n# @pytest.mark.algotest\ndef test_collaq():\n    config = [deepcopy(cooperative_navigation_collaq_config), deepcopy(cooperative_navigation_collaq_create_config)]\n    try:\n        serial_pipeline(config, seed=0)\n    except Exception:\n        assert False, \"pipeline fail\"\n    with open(\"./algo_record.log\", \"a+\") as f:\n        f.write(\"7. collaq\\n\")\n\n\n# @pytest.mark.algotest\ndef test_coma():\n    config = [deepcopy(cooperative_navigation_coma_config), deepcopy(cooperative_navigation_coma_create_config)]\n    try:\n        serial_pipeline(config, seed=0)\n    except Exception:\n        assert False, \"pipeline fail\"\n    with open(\"./algo_record.log\", \"a+\") as f:\n        f.write(\"8. coma\\n\")\n\n\n@pytest.mark.algotest\ndef test_sac():\n    config = [deepcopy(pendulum_sac_config), deepcopy(pendulum_sac_create_config)]\n    try:\n        serial_pipeline(config, seed=0)\n    except Exception:\n        assert False, \"pipeline fail\"\n    with open(\"./algo_record.log\", \"a+\") as f:\n        f.write(\"9. sac\\n\")\n\n\n@pytest.mark.algotest\ndef test_c51():\n    config = [deepcopy(cartpole_c51_config), deepcopy(cartpole_c51_create_config)]\n    try:\n        serial_pipeline(config, seed=0)\n    except Exception:\n        assert False, \"pipeline fail\"\n    with open(\"./algo_record.log\", \"a+\") as f:\n        f.write(\"10. c51\\n\")\n\n\n@pytest.mark.algotest\ndef test_r2d2():\n    config = [deepcopy(cartpole_r2d2_config), deepcopy(cartpole_r2d2_create_config)]\n    try:\n        serial_pipeline(config, seed=0)\n    except Exception:\n        assert False, \"pipeline fail\"\n    with open(\"./algo_record.log\", \"a+\") as f:\n        f.write(\"11. r2d2\\n\")\n\n\n@pytest.mark.algotest\ndef test_a2c_with_nstep_return():\n    config = [deepcopy(cartpole_a2c_config), deepcopy(cartpole_a2c_create_config)]\n    config[0].policy.learn.nstep_return = config[0].policy.collect.nstep_return = True\n    config[0].policy.collect.discount_factor = 0.9\n    config[0].policy.collect.nstep = 3\n    try:\n        serial_pipeline(config, seed=0)", "metadata": {"task_id": "opendilab_ACE/100", "ground_truth": "    except Exception:", "fpath_tuple": ["opendilab_ACE", "ding", "entry", "tests", "test_serial_entry_algo.py"], "context_start_lineno": 30, "line_no": 178, "query_window": {"context": "\n@pytest.mark.algotest\ndef test_r2d2():\n    config = [deepcopy(cartpole_r2d2_config), deepcopy(cartpole_r2d2_create_config)]\n    try:\n        serial_pipeline(config, seed=0)\n    except Exception:\n        assert False, \"pipeline fail\"\n    with open(\"./algo_record.log\", \"a+\") as f:\n        f.write(\"11. r2d2\\n\")\n\n\n@pytest.mark.algotest\ndef test_a2c_with_nstep_return():\n    config = [deepcopy(cartpole_a2c_config), deepcopy(cartpole_a2c_create_config)]\n    config[0].policy.learn.nstep_return = config[0].policy.collect.nstep_return = True\n    config[0].policy.collect.discount_factor = 0.9\n    config[0].policy.collect.nstep = 3\n    try:\n        serial_pipeline(config, seed=0)", "metadata": {"fpath_tuple": ["opendilab_ACE", "ding", "entry", "tests", "test_serial_entry_algo.py"], "line_no": 178, "task_id": "opendilab_ACE/100", "start_line_no": 158, "end_line_no": 178, "window_size": 20, "context_start_lineno": 30, "repo": "opendilab_ACE"}}, "top_k_context": [{"context": "    config = [deepcopy(cartpole_r2d2_config), deepcopy(cartpole_r2d2_create_config)]\n    config[0].policy.learn.update_per_collect = 1\n    try:\n        serial_pipeline(config, seed=0, max_iterations=1)\n    except Exception:\n        assert False, \"pipeline fail\"\n\n\n@pytest.mark.unittest\ndef test_a2c_with_nstep_return():\n    config = [deepcopy(cartpole_a2c_config), deepcopy(cartpole_a2c_create_config)]\n    config[0].policy.learn.update_per_collect = 1\n    config[0].policy.learn.nstep_return = config[0].policy.collect.nstep_return = True\n    config[0].policy.collect.discount_factor = 0.9\n    config[0].policy.collect.nstep = 3\n    try:\n        serial_pipeline(config, seed=0, max_iterations=1)\n    except Exception:\n        assert False, \"pipeline fail\"\n", "metadata": [{"fpath_tuple": ["opendilab_ACE", "ding", "entry", "tests", "test_serial_entry.py"], "line_no": 168, "start_line_no": 158, "end_line_no": 178, "window_size": 20, "repo": "opendilab_ACE", "slice_size": 10}], "sim_score": 0.6947368421052632}, {"context": "\n\n@pytest.mark.unittest\ndef test_r2d2():\n    config = [deepcopy(cartpole_r2d2_config), deepcopy(cartpole_r2d2_create_config)]\n    config[0].policy.learn.update_per_collect = 1\n    try:\n        serial_pipeline(config, seed=0, max_iterations=1)\n    except Exception:\n        assert False, \"pipeline fail\"\n\n\n@pytest.mark.unittest\ndef test_a2c_with_nstep_return():\n    config = [deepcopy(cartpole_a2c_config), deepcopy(cartpole_a2c_create_config)]\n    config[0].policy.learn.update_per_collect = 1\n    config[0].policy.learn.nstep_return = config[0].policy.collect.nstep_return = True\n    config[0].policy.collect.discount_factor = 0.9\n    config[0].policy.collect.nstep = 3\n    try:", "metadata": [{"fpath_tuple": ["opendilab_ACE", "ding", "entry", "tests", "test_serial_entry.py"], "line_no": 164, "start_line_no": 154, "end_line_no": 174, "window_size": 20, "repo": "opendilab_ACE", "slice_size": 10}], "sim_score": 0.6947368421052632}, {"context": "@pytest.mark.unittest\ndef test_r2d2():\n    config = [deepcopy(cartpole_r2d2_config), deepcopy(cartpole_r2d2_create_config)]\n    config[0].policy.learn.update_per_collect = 1\n    try:\n        serial_pipeline(config, seed=0, max_iterations=1)\n    except Exception:\n        assert False, \"pipeline fail\"\n\n\n@pytest.mark.unittest\ndef test_a2c_with_nstep_return():\n    config = [deepcopy(cartpole_a2c_config), deepcopy(cartpole_a2c_create_config)]\n    config[0].policy.learn.update_per_collect = 1\n    config[0].policy.learn.nstep_return = config[0].policy.collect.nstep_return = True\n    config[0].policy.collect.discount_factor = 0.9\n    config[0].policy.collect.nstep = 3\n    try:\n        serial_pipeline(config, seed=0, max_iterations=1)\n    except Exception:", "metadata": [{"fpath_tuple": ["opendilab_ACE", "ding", "entry", "tests", "test_serial_entry.py"], "line_no": 166, "start_line_no": 156, "end_line_no": 176, "window_size": 20, "repo": "opendilab_ACE", "slice_size": 10}], "sim_score": 0.6947368421052632}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# src/diffusers/utils/dynamic_modules_utils.py\n# --------------------------------------------------\n#     if not dynamic_module_path.parent.exists():\n#         create_dynamic_module(dynamic_module_path.parent)\n#     os.makedirs(dynamic_module_path, exist_ok=True)\n#     init_path = dynamic_module_path / \"__init__.py\"\n#     if not init_path.exists():\n#         init_path.touch()\n# \n# \n# def get_relative_imports(module_file):\n#     \"\"\"\n#     Get the list of modules that are relatively imported in a module file.\n# \n#     Args:\n#         module_file (`str` or `os.PathLike`): The module file to inspect.\n#     \"\"\"\n#     with open(module_file, \"r\", encoding=\"utf-8\") as f:\n#         content = f.read()\n# \n#     # Imports of the form `import .xxx`\n#     relative_imports = re.findall(\"^\\s*import\\s+\\.(\\S+)\\s*$\", content, flags=re.MULTILINE)\n# --------------------------------------------------\n# the below code fragment can be found in:\n# utils/overwrite_expected_slice.py\n# --------------------------------------------------\n# \n# def main(correct, fail=None):\n#     if fail is not None:\n#         with open(fail, \"r\") as f:\n#             test_failures = set([l.strip() for l in f.readlines()])\n#     else:\n#         test_failures = None\n# \n#     with open(correct, \"r\") as f:\n#         correct_lines = f.readlines()\n# \n#     done_tests = defaultdict(int)\n#     for line in correct_lines:\n#         file, class_name, test_name, correct_line = line.split(\";\")\n#         if test_failures is None or \"::\".join([file, class_name, test_name]) in test_failures:\n#             overwrite_file(file, class_name, test_name, correct_line, done_tests)\n# \n# \n# if __name__ == \"__main__\":\n#     parser = argparse.ArgumentParser()\n# --------------------------------------------------\n# the below code fragment can be found in:\n# src/diffusers/utils/dynamic_modules_utils.py\n# --------------------------------------------------\n#     dynamic_module_path = Path(HF_MODULES_CACHE) / name\n#     # If the parent module does not exist yet, recursively create it.\n#     if not dynamic_module_path.parent.exists():\n#         create_dynamic_module(dynamic_module_path.parent)\n#     os.makedirs(dynamic_module_path, exist_ok=True)\n#     init_path = dynamic_module_path / \"__init__.py\"\n#     if not init_path.exists():\n#         init_path.touch()\n# \n# \n# def get_relative_imports(module_file):\n#     \"\"\"\n#     Get the list of modules that are relatively imported in a module file.\n# \n#     Args:\n#         module_file (`str` or `os.PathLike`): The module file to inspect.\n#     \"\"\"\n#     with open(module_file, \"r\", encoding=\"utf-8\") as f:\n#         content = f.read()\n# \n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\nimport inspect\nimport logging\nimport os\nimport random\nimport re\nimport unittest\nimport urllib.parse\nfrom distutils.util import strtobool\nfrom io import BytesIO, StringIO\nfrom pathlib import Path\nfrom typing import Optional, Union\n\nimport numpy as np\n\nimport PIL.Image\nimport PIL.ImageOps\nimport requests\nfrom packaging import version\n\nfrom .import_utils import is_flax_available, is_onnx_available, is_torch_available\n\n\nglobal_rng = random.Random()\n\n\nif is_torch_available():\n    import torch\n\n    torch_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    is_torch_higher_equal_than_1_12 = version.parse(version.parse(torch.__version__).base_version) >= version.parse(\n        \"1.12\"\n    )\n\n    if is_torch_higher_equal_than_1_12:\n        # Some builds of torch 1.12 don't have the mps backend registered. See #892 for more details\n        mps_backend_registered = hasattr(torch.backends, \"mps\")\n        torch_device = \"mps\" if (mps_backend_registered and torch.backends.mps.is_available()) else torch_device\n\n\ndef torch_all_close(a, b, *args, **kwargs):\n    if not is_torch_available():\n        raise ValueError(\"PyTorch needs to be installed to use this function.\")\n    if not torch.allclose(a, b, *args, **kwargs):\n        assert False, f\"Max diff is absolute {(a - b).abs().max()}. Diff tensor is {(a - b).abs()}.\"\n    return True\n\n\ndef print_tensor_test(tensor, filename=\"test_corrections.txt\", expected_tensor_name=\"expected_slice\"):\n    test_name = os.environ.get(\"PYTEST_CURRENT_TEST\")\n    if not torch.is_tensor(tensor):\n        tensor = torch.from_numpy(tensor)\n\n    tensor_str = str(tensor.detach().cpu().flatten().to(torch.float32)).replace(\"\\n\", \"\")\n    # format is usually:\n    # expected_slice = np.array([-0.5713, -0.3018, -0.9814, 0.04663, -0.879, 0.76, -1.734, 0.1044, 1.161])\n    output_str = tensor_str.replace(\"tensor\", f\"{expected_tensor_name} = np.array\")\n    test_file, test_class, test_fn = test_name.split(\"::\")\n    test_fn = test_fn.split()[0]\n    with open(filename, \"a\") as f:\n        print(\";\".join([test_file, test_class, test_fn, output_str]), file=f)\n\n\ndef get_tests_dir(append_path=None):\n    \"\"\"\n    Args:\n        append_path: optional path to append to the tests dir path\n    Return:\n        The full path to the `tests` dir, so that the tests can be invoked from anywhere. Optionally `append_path` is\n        joined after the `tests` dir the former is provided.\n    \"\"\"\n    # this function caller's __file__\n    caller__file__ = inspect.stack()[1][1]\n    tests_dir = os.path.abspath(os.path.dirname(caller__file__))\n\n    while not tests_dir.endswith(\"tests\"):\n        tests_dir = os.path.dirname(tests_dir)\n\n    if append_path:", "metadata": {"task_id": "huggingface_diffusers/146", "ground_truth": "        return os.path.join(tests_dir, append_path)", "fpath_tuple": ["huggingface_diffusers", "src", "diffusers", "utils", "testing_utils.py"], "context_start_lineno": 0, "line_no": 78, "query_window": {"context": "    with open(filename, \"a\") as f:\n        print(\";\".join([test_file, test_class, test_fn, output_str]), file=f)\n\n\ndef get_tests_dir(append_path=None):\n    \"\"\"\n    Args:\n        append_path: optional path to append to the tests dir path\n    Return:\n        The full path to the `tests` dir, so that the tests can be invoked from anywhere. Optionally `append_path` is\n        joined after the `tests` dir the former is provided.\n    \"\"\"\n    # this function caller's __file__\n    caller__file__ = inspect.stack()[1][1]\n    tests_dir = os.path.abspath(os.path.dirname(caller__file__))\n\n    while not tests_dir.endswith(\"tests\"):\n        tests_dir = os.path.dirname(tests_dir)\n\n    if append_path:", "metadata": {"fpath_tuple": ["huggingface_diffusers", "src", "diffusers", "utils", "testing_utils.py"], "line_no": 78, "task_id": "huggingface_diffusers/146", "start_line_no": 58, "end_line_no": 78, "window_size": 20, "context_start_lineno": 0, "repo": "huggingface_diffusers"}}, "top_k_context": [{"context": "    \"\"\"\n    init_hf_modules()\n    dynamic_module_path = Path(HF_MODULES_CACHE) / name\n    # If the parent module does not exist yet, recursively create it.\n    if not dynamic_module_path.parent.exists():\n        create_dynamic_module(dynamic_module_path.parent)\n    os.makedirs(dynamic_module_path, exist_ok=True)\n    init_path = dynamic_module_path / \"__init__.py\"\n    if not init_path.exists():\n        init_path.touch()\n\n\ndef get_relative_imports(module_file):\n    \"\"\"\n    Get the list of modules that are relatively imported in a module file.\n\n    Args:\n        module_file (`str` or `os.PathLike`): The module file to inspect.\n    \"\"\"\n    with open(module_file, \"r\", encoding=\"utf-8\") as f:", "metadata": [{"fpath_tuple": ["huggingface_diffusers", "src", "diffusers", "utils", "dynamic_modules_utils.py"], "line_no": 76, "start_line_no": 66, "end_line_no": 86, "window_size": 20, "repo": "huggingface_diffusers", "slice_size": 10}], "sim_score": 0.26506024096385544}, {"context": "            f.write(line)\n\n\ndef main(correct, fail=None):\n    if fail is not None:\n        with open(fail, \"r\") as f:\n            test_failures = set([l.strip() for l in f.readlines()])\n    else:\n        test_failures = None\n\n    with open(correct, \"r\") as f:\n        correct_lines = f.readlines()\n\n    done_tests = defaultdict(int)\n    for line in correct_lines:\n        file, class_name, test_name, correct_line = line.split(\";\")\n        if test_failures is None or \"::\".join([file, class_name, test_name]) in test_failures:\n            overwrite_file(file, class_name, test_name, correct_line, done_tests)\n\n", "metadata": [{"fpath_tuple": ["huggingface_diffusers", "utils", "overwrite_expected_slice.py"], "line_no": 72, "start_line_no": 62, "end_line_no": 82, "window_size": 20, "repo": "huggingface_diffusers", "slice_size": 10}], "sim_score": 0.26119402985074625}, {"context": "    dynamic_module_path = Path(HF_MODULES_CACHE) / name\n    # If the parent module does not exist yet, recursively create it.\n    if not dynamic_module_path.parent.exists():\n        create_dynamic_module(dynamic_module_path.parent)\n    os.makedirs(dynamic_module_path, exist_ok=True)\n    init_path = dynamic_module_path / \"__init__.py\"\n    if not init_path.exists():\n        init_path.touch()\n\n\ndef get_relative_imports(module_file):\n    \"\"\"\n    Get the list of modules that are relatively imported in a module file.\n\n    Args:\n        module_file (`str` or `os.PathLike`): The module file to inspect.\n    \"\"\"\n    with open(module_file, \"r\", encoding=\"utf-8\") as f:\n        content = f.read()\n", "metadata": [{"fpath_tuple": ["huggingface_diffusers", "src", "diffusers", "utils", "dynamic_modules_utils.py"], "line_no": 78, "start_line_no": 68, "end_line_no": 88, "window_size": 20, "repo": "huggingface_diffusers", "slice_size": 10}], "sim_score": 0.25903614457831325}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# vizier/_src/benchmarks/analyzers/convergence_curve.py\n# --------------------------------------------------\n# \n# def build_convergence_curve(baseline_curve: Sequence[float],\n#                             compared_curve: Sequence[float]) -> List[float]:\n#   \"\"\"Builds a relative convergence curve (see returns for definition).\n# \n#   Args:\n#     baseline_curve: Baseline maximization convergence curve.\n#     compared_curve: Compared maximization convergence curve.\n# \n#   Returns:\n#     A list of numbers where i-th (zero-index) element is the smallest \"j\" such\n#     that baseline_curve[i] <= compared_curve[j]\n#   \"\"\"\n#   convergence_curve = []\n#   t1 = 0\n#   for t0 in range(len(baseline_curve)):\n#     while t1 < len(compared_curve) and compared_curve[t1] < baseline_curve[t0]:\n#       t1 += 1\n#     convergence_curve.append(float('inf') if t1 >= len(compared_curve) else t1)\n#   return convergence_curve\n# --------------------------------------------------\n# the below code fragment can be found in:\n# vizier/_src/benchmarks/analyzers/convergence_curve.py\n# --------------------------------------------------\n# \n# def build_convergence_curve(baseline_curve: Sequence[float],\n#                             compared_curve: Sequence[float]) -> List[float]:\n#   \"\"\"Builds a relative convergence curve (see returns for definition).\n# \n#   Args:\n#     baseline_curve: Baseline maximization convergence curve.\n#     compared_curve: Compared maximization convergence curve.\n# \n#   Returns:\n#     A list of numbers where i-th (zero-index) element is the smallest \"j\" such\n#     that baseline_curve[i] <= compared_curve[j]\n#   \"\"\"\n#   convergence_curve = []\n#   t1 = 0\n#   for t0 in range(len(baseline_curve)):\n#     while t1 < len(compared_curve) and compared_curve[t1] < baseline_curve[t0]:\n#       t1 += 1\n#     convergence_curve.append(float('inf') if t1 >= len(compared_curve) else t1)\n#   return convergence_curve\n# --------------------------------------------------\n# the below code fragment can be found in:\n# vizier/_src/benchmarks/analyzers/convergence_curve.py\n# --------------------------------------------------\n# \n# def build_convergence_curve(baseline_curve: Sequence[float],\n#                             compared_curve: Sequence[float]) -> List[float]:\n#   \"\"\"Builds a relative convergence curve (see returns for definition).\n# \n#   Args:\n#     baseline_curve: Baseline maximization convergence curve.\n#     compared_curve: Compared maximization convergence curve.\n# \n#   Returns:\n#     A list of numbers where i-th (zero-index) element is the smallest \"j\" such\n#     that baseline_curve[i] <= compared_curve[j]\n#   \"\"\"\n#   convergence_curve = []\n#   t1 = 0\n#   for t0 in range(len(baseline_curve)):\n#     while t1 < len(compared_curve) and compared_curve[t1] < baseline_curve[t0]:\n#       t1 += 1\n#     convergence_curve.append(float('inf') if t1 >= len(compared_curve) else t1)\n#   return convergence_curve\n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\n# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import annotations\n\n\"\"\"Common classes shared between Study and Trial.\"\"\"\n\nimport collections\nfrom collections import abc\nfrom typing import DefaultDict, Dict, overload, Iterator\nfrom typing import Iterable, List, Optional, Tuple, TypeVar, Union, Type\n\nfrom absl import logging\nimport attr\n\nfrom google.protobuf import any_pb2\nfrom google.protobuf.message import Message\n\nM = TypeVar('M', bound=Message)\nT = TypeVar('T')\nT1 = TypeVar('T1')\nT2 = TypeVar('T2')\nMetadataValue = Union[str, any_pb2.Any, Message]\n\n# Namespace Encoding.\n#\n# By definition, \u2200 ns \u2208 Namespace, Namespace.decode(ns.encode()) == ns.\n# The tricky part of that definition is handling namespaces with components\n# that are empty strings.  Notably, we want to make sure that\n# Namespace(()).encode() != Namespace(('',)).encode().\n# So, we set up the mapping:\n# Namespace(()).encode() -> ''\n# Namespace((s,)).encode() -> ':s'\n# Namespace((s, s)).encode() -> ':s:s',\n# et cetera, and note that every tuple gets a unique encoding, even if $s is the\n# empty string.  (As long as we escape colons properly.)\n#\n# So, ns.encode() is a bijection, therefore it has an inverse which we call\n# Namespace.decode(s).\n\n\ndef _parse(arg: str) -> Tuple[str, ...]:\n  \"\"\"Parses an encoded namespace string into a namespace tuple.\"\"\"\n  # The tricky part here is that arg.split('') has a length of 1, so it can't\n  # generate a zero-length tuple; we handle that corner case manually.\n  if not arg:\n    return ()\n  # And, then, once we've handled the case of _parse(''), we note that all the\n  # other encoded strings begin with a colon.  It thus contains no information\n  # and we can remove it.\n  # TODO: Once we're on Python 3.9, use: arg = arg.removeprefix(':')\n  if arg.startswith(':'):\n    arg = arg[1:]\n  # The rest of the algorithm is that we split on all colons, both\n  # escaped and unescaped.  Then, we walk through the list of fragments and\n  # join back together the colons that were preceeded by an escape character,\n  # dropping the escape character as we go.\n  fragments = arg.split(':')\n  output = []\n  join = False\n  for frag in fragments:\n    if join and frag and frag[-1] == '\\\\':\n      output[-1] += ':' + frag[:-1]\n      join = True\n    elif join:  # Doesn't end in an escape character.\n      output[-1] += ':' + frag\n      join = False\n    elif frag and frag[-1] == '\\\\':  # Don't join to previous.\n      output.append(frag[:-1])\n      join = True\n    else:  # Don't join to previous and doesn't end in an escape.\n      output.append(frag)\n      join = False", "metadata": {"task_id": "google_vizier/161", "ground_truth": "  return tuple(output)", "fpath_tuple": ["google_vizier", "vizier", "_src", "pyvizier", "shared", "common.py"], "context_start_lineno": 0, "line_no": 84, "query_window": {"context": "  # The rest of the algorithm is that we split on all colons, both\n  # escaped and unescaped.  Then, we walk through the list of fragments and\n  # join back together the colons that were preceeded by an escape character,\n  # dropping the escape character as we go.\n  fragments = arg.split(':')\n  output = []\n  join = False\n  for frag in fragments:\n    if join and frag and frag[-1] == '\\\\':\n      output[-1] += ':' + frag[:-1]\n      join = True\n    elif join:  # Doesn't end in an escape character.\n      output[-1] += ':' + frag\n      join = False\n    elif frag and frag[-1] == '\\\\':  # Don't join to previous.\n      output.append(frag[:-1])\n      join = True\n    else:  # Don't join to previous and doesn't end in an escape.\n      output.append(frag)\n      join = False", "metadata": {"fpath_tuple": ["google_vizier", "vizier", "_src", "pyvizier", "shared", "common.py"], "line_no": 84, "task_id": "google_vizier/161", "start_line_no": 64, "end_line_no": 84, "window_size": 20, "context_start_lineno": 0, "repo": "google_vizier"}}, "top_k_context": [{"context": "  Returns:\n    A list of numbers where i-th (zero-index) element is the smallest \"j\" such\n    that baseline_curve[i] <= compared_curve[j]\n  \"\"\"\n  convergence_curve = []\n  t1 = 0\n  for t0 in range(len(baseline_curve)):\n    while t1 < len(compared_curve) and compared_curve[t1] < baseline_curve[t0]:\n      t1 += 1\n    convergence_curve.append(float('inf') if t1 >= len(compared_curve) else t1)\n  return convergence_curve", "metadata": [{"fpath_tuple": ["google_vizier", "vizier", "_src", "benchmarks", "analyzers", "convergence_curve.py"], "line_no": 368, "start_line_no": 358, "end_line_no": 369, "window_size": 20, "repo": "google_vizier", "slice_size": 10}], "sim_score": 0.20610687022900764}, {"context": "    compared_curve: Compared maximization convergence curve.\n\n  Returns:\n    A list of numbers where i-th (zero-index) element is the smallest \"j\" such\n    that baseline_curve[i] <= compared_curve[j]\n  \"\"\"\n  convergence_curve = []\n  t1 = 0\n  for t0 in range(len(baseline_curve)):\n    while t1 < len(compared_curve) and compared_curve[t1] < baseline_curve[t0]:\n      t1 += 1\n    convergence_curve.append(float('inf') if t1 >= len(compared_curve) else t1)\n  return convergence_curve", "metadata": [{"fpath_tuple": ["google_vizier", "vizier", "_src", "benchmarks", "analyzers", "convergence_curve.py"], "line_no": 366, "start_line_no": 356, "end_line_no": 369, "window_size": 20, "repo": "google_vizier", "slice_size": 10}], "sim_score": 0.19852941176470587}, {"context": "  Args:\n    baseline_curve: Baseline maximization convergence curve.\n    compared_curve: Compared maximization convergence curve.\n\n  Returns:\n    A list of numbers where i-th (zero-index) element is the smallest \"j\" such\n    that baseline_curve[i] <= compared_curve[j]\n  \"\"\"\n  convergence_curve = []\n  t1 = 0\n  for t0 in range(len(baseline_curve)):\n    while t1 < len(compared_curve) and compared_curve[t1] < baseline_curve[t0]:\n      t1 += 1\n    convergence_curve.append(float('inf') if t1 >= len(compared_curve) else t1)\n  return convergence_curve", "metadata": [{"fpath_tuple": ["google_vizier", "vizier", "_src", "benchmarks", "analyzers", "convergence_curve.py"], "line_no": 364, "start_line_no": 354, "end_line_no": 369, "window_size": 20, "repo": "google_vizier", "slice_size": 10}], "sim_score": 0.19424460431654678}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# tests/test_trainer_evaluator_parity.py\n# --------------------------------------------------\n# \n#         task_evaluator = evaluator(task=\"text-classification\")\n#         evaluator_results = task_evaluator.compute(\n#             model_or_pipeline=pipe,\n#             data=eval_dataset,\n#             metric=\"accuracy\",\n#             input_column=\"sentence\",\n#             label_column=\"label\",\n#             label_mapping={\"negative\": 0, \"positive\": 1},\n#             strategy=\"simple\",\n#         )\n# \n#         self.assertEqual(transformers_results[\"eval_accuracy\"], evaluator_results[\"accuracy\"])\n# \n#     @slow\n#     def test_text_classification_parity_two_columns(self):\n#         model_name = \"prajjwal1/bert-tiny-mnli\"\n#         max_eval_samples = 150\n# \n#         subprocess.run(\n# --------------------------------------------------\n# the below code fragment can be found in:\n# tests/test_trainer_evaluator_parity.py\n# --------------------------------------------------\n#             os.path.join(self.dir_path, \"tokenclassification_conll2003_transformers\", \"eval_results.json\"), \"r\"\n#         ) as f:\n#             transformers_results = json.load(f)\n# \n#         eval_dataset = load_dataset(\"conll2003\", split=f\"validation[:{n_samples}]\")\n# \n#         pipe = pipeline(task=\"token-classification\", model=model_name)\n# \n#         e = evaluator(task=\"token-classification\")\n#         evaluator_results = e.compute(\n#             model_or_pipeline=pipe,\n#             data=eval_dataset,\n#             metric=\"seqeval\",\n#             input_column=\"tokens\",\n#             label_column=\"ner_tags\",\n#             strategy=\"simple\",\n#         )\n# \n#         self.assertEqual(transformers_results[\"eval_accuracy\"], evaluator_results[\"overall_accuracy\"])\n#         self.assertEqual(transformers_results[\"eval_f1\"], evaluator_results[\"overall_f1\"])\n# --------------------------------------------------\n# the below code fragment can be found in:\n# tests/test_trainer_evaluator_parity.py\n# --------------------------------------------------\n#         evaluator_results = task_evaluator.compute(\n#             model_or_pipeline=pipe,\n#             data=eval_dataset,\n#             metric=\"accuracy\",\n#             input_column=\"sentence\",\n#             label_column=\"label\",\n#             label_mapping={\"negative\": 0, \"positive\": 1},\n#             strategy=\"simple\",\n#         )\n# \n#         self.assertEqual(transformers_results[\"eval_accuracy\"], evaluator_results[\"accuracy\"])\n# \n#     @slow\n#     def test_text_classification_parity_two_columns(self):\n#         model_name = \"prajjwal1/bert-tiny-mnli\"\n#         max_eval_samples = 150\n# \n#         subprocess.run(\n#             \"git sparse-checkout set examples/pytorch/text-classification\",\n#             shell=True,\n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\n, Evaluator._infer_device(), self.pipe\n            )\n\n            # tf accelerator found and pipeline instantiated on CPU\n            pt_available = False\n            tf_available = True\n            self.assertRaises(\n                ValueError, Evaluator.check_for_mismatch_in_device_setup, Evaluator._infer_device(), self.pipe\n            )\n\n    def test_pipe_init(self):\n        self.evaluator.compute(\n            model_or_pipeline=self.pipe,\n            data=self.data,\n            input_column=\"text\",\n            label_column=\"label\",\n            label_mapping=self.label_mapping,\n        )\n\n    def test_model_init(self):\n        self.evaluator.compute(\n            model_or_pipeline=self.default_model,\n            tokenizer=self.default_tokenizer,\n            data=self.data,\n            input_column=\"text\",\n            label_column=\"label\",\n            label_mapping=self.label_mapping,\n        )\n\n    def test_model_str_init(self):\n        self.evaluator.compute(\n            model_or_pipeline=self.default_ckpt,\n            data=self.data,\n            input_column=\"text\",\n            label_column=\"label\",\n            label_mapping=self.label_mapping,\n        )\n\n\nclass TestTextClassificationEvaluator(TestCase):\n    def setUp(self):\n        self.data = Dataset.from_dict({\"label\": [1, 0], \"text\": [\"great movie\", \"horrible movie\"]})\n        self.default_model = \"lvwerra/distilbert-imdb\"\n        self.input_column = \"text\"\n        self.label_column = \"label\"\n        self.pipe = DummyTextClassificationPipeline()\n        self.perf_pipe = DummyTextClassificationPipeline(sleep_time=0.1)\n        self.evaluator = evaluator(\"text-classification\")\n        self.label_mapping = {\"NEGATIVE\": 0.0, \"POSITIVE\": 1.0}\n\n    def test_pipe_init(self):\n        results = self.evaluator.compute(\n            model_or_pipeline=self.pipe,\n            data=self.data,\n            input_column=\"text\",\n            label_column=\"label\",\n            label_mapping=self.label_mapping,\n        )\n        self.assertEqual(results[\"accuracy\"], 1.0)\n\n    @slow\n    def test_model_init(self):\n        results = self.evaluator.compute(\n            model_or_pipeline=self.default_model,\n            data=self.data,\n            metric=\"accuracy\",\n            input_column=self.input_column,\n            label_column=self.label_column,\n            label_mapping=self.label_mapping,\n        )\n\n        model = AutoModelForSequenceClassification.from_pretrained(self.default_model)\n        tokenizer = AutoTokenizer.from_pretrained(self.default_model)\n\n        self.assertEqual(results[\"accuracy\"], 1.0)\n        results = self.evaluator.compute(\n            model_or_pipeline=model,\n            data=self.data,\n            metric=\"accuracy\",\n            tokenizer=tokenizer,\n            label_mapping=self.label_mapping,\n        )\n        self.assertEqual(results[\"accuracy\"], 1.0)\n\n    def test_class_init(self):\n        evaluator = TextClassificationEvaluator()\n        self.assertEqual(evaluator.task, \"text-classification\")\n        self.assertIsNone(evaluator.default_metric_name)\n\n        results = evaluator.compute(\n            model_or_pipeline=self.pipe,\n            data=self.data,\n            metric=\"f1\",\n            label_mapping=self.label_mapping,\n        )\n        self.assertEqual(results[\"f1\"], 1.0)\n\n    @slow\n    def test_default_pipe_init(self):\n        results = self.evaluator.compute(\n            data=self.data,\n            label_mapping=self.label_mapping,\n        )\n        self.assertEqual(results[\"accuracy\"], 1.0)\n\n    def test_data_loading(self):\n\n        # Test passing in dataset by name with split\n        data = self.evaluator.load_data(\"evaluate/imdb-ci\", split=\"test[:1]\")\n        self.evaluator.prepare_data(data=data, input_column=\"text\", label_column=\"label\", second_input_column=None)\n\n        # Test passing in dataset by name without split and inferring the optimal split\n        data = self.evaluator.load_data(\"evaluate/imdb-ci\")\n        self.evaluator.prepare_data(data=data, input_column=\"text\", label_column=\"label\", second_input_column=None)\n\n        # Test that it chooses the correct one (e.g. imdb only has train and test, but no validation)\n        self.assertEqual(data.split, \"test\")\n\n        # Test that the data point returned is correct; this maps to the first example in the dataset\n        self.assertEqual(data[0][\"text\"], \"I love movies about whales!\")\n\n        # Test loading subset of a dataset with the `name` field\n        data = self.evaluator.load_data(\"evaluate/glue-ci\", subset=\"cola\", split=\"test\")\n        self.assertEqual(isinstance(data, Dataset), True)\n\n        # Test loading subset of a dataset with the `name` field and having it infer the split\n        data = self.evaluator.load_data(\"evaluate/glue-ci\", subset=\"cola\")\n        self.assertEqual(isinstance(data, Dataset), True)\n\n    def test_overwrite_default_metric(self):\n        accuracy = load(\"accuracy\")\n        results = self.evaluator.compute(\n            model_or_pipeline=self.pipe,\n            data=self.data,\n            metric=accuracy,\n            label_mapping=self.label_mapping,\n        )\n        self.assertEqual(results[\"accuracy\"], 1.0)\n        results = self.evaluator.compute(\n            model_or_pipeline=self.pipe,\n            data=self.data,\n            metric=\"accuracy\",\n            label_mapping=self.label_mapping,\n        )\n        self.assertEqual(results[\"accuracy\"], 1.0)\n\n    def test_bootstrap(self):\n        data = Dataset.from_dict({\"label\": [1, 0, 0], \"text\": [\"great movie\", \"great movie\", \"horrible movie\"]})\n\n        results = self.evaluator.compute(\n            model_or_pipeline=self.pipe,\n            data=data,\n            metric=\"accuracy\",\n            label_mapping=self.label_mapping,\n            strategy=\"bootstrap\",\n            n_resamples=10,\n            random_state=0,\n        )\n        self.assertAlmostEqual(results[\"accuracy\"][\"score\"], 0.666666, 5)\n        self.assertAlmostEqual(results[\"accuracy\"][\"confidence_interval\"][0], 0.33333, 5)\n        self.assertAlmostEqual(results[\"accuracy\"][\"confidence_interval\"][1], 0.666666, 5)\n        self.assertAlmostEqual(results[\"accuracy\"][\"standard_error\"], 0.22498, 5)\n\n    def test_perf(self):\n        results = self.evaluator.compute(\n            model_or_pipeline=self.perf_pipe,\n            data=self.data,\n            metric=\"accuracy\",\n            input_column=self.input_column,\n            label_column=self.label_column,\n            label_mapping=self.label_mapping,\n            n_resamples=10,\n            random_state=0,\n        )\n        self.assertEqual(results[\"accuracy\"], 1.0)", "metadata": {"task_id": "huggingface_evaluate/188", "ground_truth": "        self.assertAlmostEqual(results[\"total_time_in_seconds\"], 0.1, 1)", "fpath_tuple": ["huggingface_evaluate", "tests", "test_evaluator.py"], "context_start_lineno": 191, "line_no": 366, "query_window": {"context": "            n_resamples=10,\n            random_state=0,\n        )\n        self.assertAlmostEqual(results[\"accuracy\"][\"score\"], 0.666666, 5)\n        self.assertAlmostEqual(results[\"accuracy\"][\"confidence_interval\"][0], 0.33333, 5)\n        self.assertAlmostEqual(results[\"accuracy\"][\"confidence_interval\"][1], 0.666666, 5)\n        self.assertAlmostEqual(results[\"accuracy\"][\"standard_error\"], 0.22498, 5)\n\n    def test_perf(self):\n        results = self.evaluator.compute(\n            model_or_pipeline=self.perf_pipe,\n            data=self.data,\n            metric=\"accuracy\",\n            input_column=self.input_column,\n            label_column=self.label_column,\n            label_mapping=self.label_mapping,\n            n_resamples=10,\n            random_state=0,\n        )\n        self.assertEqual(results[\"accuracy\"], 1.0)", "metadata": {"fpath_tuple": ["huggingface_evaluate", "tests", "test_evaluator.py"], "line_no": 366, "task_id": "huggingface_evaluate/188", "start_line_no": 346, "end_line_no": 366, "window_size": 20, "context_start_lineno": 191, "repo": "huggingface_evaluate"}}, "top_k_context": [{"context": "\n        task_evaluator = evaluator(task=\"text-classification\")\n        evaluator_results = task_evaluator.compute(\n            model_or_pipeline=pipe,\n            data=eval_dataset,\n            metric=\"accuracy\",\n            input_column=\"sentence\",\n            label_column=\"label\",\n            label_mapping={\"negative\": 0, \"positive\": 1},\n            strategy=\"simple\",\n        )\n\n        self.assertEqual(transformers_results[\"eval_accuracy\"], evaluator_results[\"accuracy\"])\n\n    @slow\n    def test_text_classification_parity_two_columns(self):\n        model_name = \"prajjwal1/bert-tiny-mnli\"\n        max_eval_samples = 150\n\n        subprocess.run(", "metadata": [{"fpath_tuple": ["huggingface_evaluate", "tests", "test_trainer_evaluator_parity.py"], "line_no": 74, "start_line_no": 64, "end_line_no": 84, "window_size": 20, "repo": "huggingface_evaluate", "slice_size": 10}], "sim_score": 0.42276422764227645}, {"context": "        evaluator_results = e.compute(\n            model_or_pipeline=pipe,\n            data=eval_dataset,\n            metric=\"seqeval\",\n            input_column=\"tokens\",\n            label_column=\"ner_tags\",\n            strategy=\"simple\",\n        )\n\n        self.assertEqual(transformers_results[\"eval_accuracy\"], evaluator_results[\"overall_accuracy\"])\n        self.assertEqual(transformers_results[\"eval_f1\"], evaluator_results[\"overall_f1\"])", "metadata": [{"fpath_tuple": ["huggingface_evaluate", "tests", "test_trainer_evaluator_parity.py"], "line_no": 312, "start_line_no": 302, "end_line_no": 313, "window_size": 20, "repo": "huggingface_evaluate", "slice_size": 10}], "sim_score": 0.422680412371134}, {"context": "\n        pipe = pipeline(task=\"text-classification\", model=model_name, tokenizer=model_name)\n\n        task_evaluator = evaluator(task=\"text-classification\")\n        evaluator_results = task_evaluator.compute(\n            model_or_pipeline=pipe,\n            data=eval_dataset,\n            metric=\"accuracy\",\n            input_column=\"sentence\",\n            label_column=\"label\",\n            label_mapping={\"negative\": 0, \"positive\": 1},\n            strategy=\"simple\",\n        )\n\n        self.assertEqual(transformers_results[\"eval_accuracy\"], evaluator_results[\"accuracy\"])\n\n    @slow\n    def test_text_classification_parity_two_columns(self):\n        model_name = \"prajjwal1/bert-tiny-mnli\"\n        max_eval_samples = 150", "metadata": [{"fpath_tuple": ["huggingface_evaluate", "tests", "test_trainer_evaluator_parity.py"], "line_no": 72, "start_line_no": 62, "end_line_no": 82, "window_size": 20, "repo": "huggingface_evaluate", "slice_size": 10}], "sim_score": 0.416}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# fortuna/prob_model/prob_model_calibrator.py\n# --------------------------------------------------\n#             def __iter__(self):\n#                 outputs_loader = map(\n#                     lambda batch: tree_map(_reshape_batch, batch), self._outputs_loader\n#                 )\n#                 outputs_loader = jax_utils.prefetch_to_device(outputs_loader, 2)\n#                 yield from outputs_loader\n# \n#         return (\n#             TargetsLoaderWrapper(outputs_loader)\n#             if outputs_loader is not None\n#             else outputs_loader\n#         )\n# \n# \n# class JittedProbModelCalibrator(JittedMixin, ProbModelCalibrator):\n#     pass\n# \n# \n# class MultiDeviceProbModelCalibrator(ProbModelMultiDeviceMixin, ProbModelCalibrator):\n#     pass\n# --------------------------------------------------\n# the below code fragment can be found in:\n# fortuna/prob_model/prob_model_calibrator.py\n# --------------------------------------------------\n#                 self._outputs_loader = outputs_loader\n# \n#             def __iter__(self):\n#                 outputs_loader = map(\n#                     lambda batch: tree_map(_reshape_batch, batch), self._outputs_loader\n#                 )\n#                 outputs_loader = jax_utils.prefetch_to_device(outputs_loader, 2)\n#                 yield from outputs_loader\n# \n#         return (\n#             TargetsLoaderWrapper(outputs_loader)\n#             if outputs_loader is not None\n#             else outputs_loader\n#         )\n# \n# \n# class JittedProbModelCalibrator(JittedMixin, ProbModelCalibrator):\n#     pass\n# \n# \n# --------------------------------------------------\n# the below code fragment can be found in:\n# fortuna/calibration/calibrator.py\n# --------------------------------------------------\n#                 self._data_loader = data_loader\n# \n#             def __iter__(self):\n#                 data_loader = map(\n#                     lambda batch: tree_map(_reshape_batch, batch), self._data_loader\n#                 )\n#                 data_loader = jax_utils.prefetch_to_device(data_loader, 2)\n#                 yield from data_loader\n# \n#         return (\n#             DataLoaderWrapper(data_loader) if data_loader is not None else data_loader\n#         )\n# \n#     @staticmethod\n#     def _add_device_dim_to_outputs_loader(\n#         outputs_loader: TargetsLoader,\n#     ) -> TargetsLoader:\n#         def _reshape_batch(batch):\n#             n_devices = jax.local_device_count()\n#             if batch.shape[0] % n_devices != 0:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# fortuna/prob_model/prob_model_calibrator.py\n# --------------------------------------------------\n#         class TargetsLoaderWrapper:\n#             def __init__(self, outputs_loader: TargetsLoader):\n#                 self._outputs_loader = outputs_loader\n# \n#             def __iter__(self):\n#                 outputs_loader = map(\n#                     lambda batch: tree_map(_reshape_batch, batch), self._outputs_loader\n#                 )\n#                 outputs_loader = jax_utils.prefetch_to_device(outputs_loader, 2)\n#                 yield from outputs_loader\n# \n#         return (\n#             TargetsLoaderWrapper(outputs_loader)\n#             if outputs_loader is not None\n#             else outputs_loader\n#         )\n# \n# \n# class JittedProbModelCalibrator(JittedMixin, ProbModelCalibrator):\n#     pass\n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\n x_batch, y_batch in zip(x_batches, y_batches):\n                    yield x_batch, y_batch\n\n            yield from PrefetchedGenerator(make_gen()) if self._prefetch else make_gen()\n\n\nclass FromCallableIterableToDataLoader:\n    def __init__(\n        self, fun: Callable[[], Iterable[Batch],],\n    ):\n        self._fun = fun\n\n    def __call__(self, *args, **kwargs):\n        return self._fun()\n\n\nclass FromCallableIterableToInputsLoader:\n    def __init__(\n        self, fun: Callable[[], Iterable[Array]],\n    ):\n        self._fun = fun\n\n    def __call__(self, *args, **kwargs):\n        return self._fun()\n\n\nclass FromCallableIterableToTargetsLoader:\n    def __init__(\n        self, fun: Callable[[], Iterable[Array]],\n    ):\n        self._fun = fun\n\n    def __call__(self, *args, **kwargs):\n        return self._fun()\n\n\nclass FromIterableToDataLoader:\n    def __init__(\n        self, batched_data: Iterable[Batch],\n    ):\n        self._batched_data = batched_data\n\n    def __call__(self, *args, **kwargs):\n        return self._batched_data\n\n\nclass FromIterableToInputsLoader:\n    def __init__(\n        self, batched_inputs: Iterable[Array],\n    ):\n        self._batched_inputs = batched_inputs\n\n    def __call__(self, *args, **kwargs):\n        return self._batched_inputs\n\n\nclass FromIterableToTargetsLoader:\n    def __init__(\n        self, batched_targets: Iterable[Array],\n    ):\n        self._batched_targets = batched_targets\n\n    def __call__(self, *args, **kwargs):\n        return self._batched_targets\n\n\nclass FromTensorFlowDataLoaderToDataLoader:\n    def __init__(self, tf_data_loader):\n        self._tf_data_loader = tf_data_loader\n\n    def __call__(self, *args, **kwargs):\n        for batch_inputs, batch_targets in self._tf_data_loader:\n            yield batch_inputs.numpy(), batch_targets.numpy()\n\n\nclass FromTorchDataLoaderToDataLoader:\n    def __init__(self, torch_data_loader):\n        self._torch_data_loader = torch_data_loader\n\n    def __call__(self, *args, **kwargs):\n        for batch_inputs, batch_targets in self._torch_data_loader:\n            yield batch_inputs.numpy(), batch_targets.numpy()\n\n\nclass FromDataLoaderToArrayInputs:\n    def __init__(self, data_loader: DataLoader):\n        self._data_loader = data_loader\n\n    def __call__(self, *args, **kwargs):\n        inputs = []\n        for batch_inputs, batch_targets in self._data_loader:\n            inputs.append(batch_inputs)\n        return np.concatenate(inputs, 0)\n\n\nclass FromDataLoaderToArrayTargets:\n    def __init__(self, data_loader: DataLoader):\n        self._data_loader = data_loader\n\n    def __call__(self, *args, **kwargs):\n        targets = []\n        for batch_inputs, batch_targets in self._data_loader:\n            targets.append(batch_targets)\n        return np.concatenate(targets, 0)\n\n\nclass FromInputsLoaderToArrayInputs:\n    def __init__(self, inputs_loader: InputsLoader):\n        self._inputs_loader = inputs_loader\n\n    def __call__(self, *args, **kwargs):\n        inputs = []\n        for batch_inputs in self._inputs_loader:\n            inputs.append(batch_inputs)\n        return np.concatenate(inputs, 0)\n\n\nclass FromArrayInputsToInputsLoader:\n    def __init__(\n        self,\n        inputs: Array,\n        batch_size: Optional[int] = None,\n        shuffle: bool = False,\n        prefetch: bool = False,\n    ):\n        self._inputs = inputs\n        self._batch_size = batch_size\n        self._shuflle = shuffle\n        self._prefetch = prefetch\n\n    def __call__(self, *args, **kwargs):\n        if self._shuflle:\n            perm = np.random.choice(\n                self._inputs.shape[0], self._inputs.shape[0], replace=False\n            )\n        if self._batch_size is None:\n            yield self._inputs\n        else:\n            x_batches = np.split(\n                self._inputs[perm] if self._shuflle else self._inputs,\n                np.arange(self._batch_size, self._inputs.shape[0], self._batch_size),\n                axis=0,\n            )\n\n            def make_gen():\n                for x_batch in x_batches:\n                    yield x_batch\n\n            yield from PrefetchedGenerator(make_gen()) if self._prefetch else make_gen()\n\n\nclass FromDataLoaderToInputsLoader:\n    def __init__(\n        self, data_loader: DataLoader,\n    ):\n        self._data_loader = data_loader\n\n    def __call__(self, *args, **kwargs):\n        for inputs, targets in self._data_loader:\n            yield inputs\n\n\nclass FromDataLoaderToTargetsLoader:\n    def __init__(\n        self, data_loader: DataLoader,\n    ):\n        self._data_loader = data_loader\n\n    def __call__(self, *args, **kwargs):\n        for inputs, targets in self._data_loader:\n            yield targets\n\n\nclass FromTargetsLoaderToArrayTargets:\n    def __init__(self, targets_loader: TargetsLoader):\n        self._targets_loader = targets_loader\n\n    def __call__(self, *args, **kwargs):\n        targets = []\n        for batch_targets in self._targets_loader:\n            targets.append(batch_targets)\n        return np.concatenate(targets, 0)\n\n\nclass FromArrayTargetsToTargetsLoader:\n    def __init__(\n        self,\n        targets: Array,\n        batch_size: Optional[int] = None,\n        shuffle: bool = False,\n        prefetch: bool = False,\n    ):\n        self._targets = targets\n        self._batch_size = batch_size\n        self._shuflle = shuffle\n        self._prefetch = prefetch\n\n    def __call__(self, *args, **kwargs):\n        if self._shuflle:\n            perm = np.random.choice(\n                self._targets.shape[0], self._targets.shape[0], replace=False\n            )\n        if self._batch_size is None:\n            yield self._targets\n        else:\n            x_batches = np.split(\n                self._targets[perm] if self._shuflle else self._targets,\n                np.arange(self._batch_size, self._targets.shape[0], self._batch_size),\n                axis=0,\n            )\n\n            def make_gen():\n                for x_batch in x_batches:\n                    yield x_batch\n\n            yield from PrefetchedGenerator(make_gen()) if self._prefetch else make_gen()\n\n\nclass PrefetchedGenerator:\n    def __init__(self, generator):\n        self._batch = generator.__next__()\n        self._generator = generator\n        self._ready = True\n\n    def __iter__(self):\n        return self\n\n    def __next__(self):\n        if not self._ready:\n            self._prefetch()\n        self._ready = False\n        return self._batch\n\n    def _prefetch(self):", "metadata": {"task_id": "awslabs_fortuna/165", "ground_truth": "        if not self._ready:", "fpath_tuple": ["awslabs_fortuna", "fortuna", "data", "loader.py"], "context_start_lineno": 578, "line_no": 812, "query_window": {"context": "\n            yield from PrefetchedGenerator(make_gen()) if self._prefetch else make_gen()\n\n\nclass PrefetchedGenerator:\n    def __init__(self, generator):\n        self._batch = generator.__next__()\n        self._generator = generator\n        self._ready = True\n\n    def __iter__(self):\n        return self\n\n    def __next__(self):\n        if not self._ready:\n            self._prefetch()\n        self._ready = False\n        return self._batch\n\n    def _prefetch(self):", "metadata": {"fpath_tuple": ["awslabs_fortuna", "fortuna", "data", "loader.py"], "line_no": 812, "task_id": "awslabs_fortuna/165", "start_line_no": 792, "end_line_no": 812, "window_size": 20, "context_start_lineno": 578, "repo": "awslabs_fortuna"}}, "top_k_context": [{"context": "            )\n\n        class TargetsLoaderWrapper:\n            def __init__(self, outputs_loader: TargetsLoader):\n                self._outputs_loader = outputs_loader\n\n            def __iter__(self):\n                outputs_loader = map(\n                    lambda batch: tree_map(_reshape_batch, batch), self._outputs_loader\n                )\n                outputs_loader = jax_utils.prefetch_to_device(outputs_loader, 2)\n                yield from outputs_loader\n\n        return (\n            TargetsLoaderWrapper(outputs_loader)\n            if outputs_loader is not None\n            else outputs_loader\n        )\n\n", "metadata": [{"fpath_tuple": ["awslabs_fortuna", "fortuna", "prob_model", "prob_model_calibrator.py"], "line_no": 102, "start_line_no": 92, "end_line_no": 112, "window_size": 20, "repo": "awslabs_fortuna", "slice_size": 10}], "sim_score": 0.3670886075949367}, {"context": "        class DataLoaderWrapper:\n            def __init__(self, data_loader: DataLoader):\n                self._data_loader = data_loader\n\n            def __iter__(self):\n                data_loader = map(\n                    lambda batch: tree_map(_reshape_batch, batch), self._data_loader\n                )\n                data_loader = jax_utils.prefetch_to_device(data_loader, 2)\n                yield from data_loader\n\n        return (\n            DataLoaderWrapper(data_loader) if data_loader is not None else data_loader\n        )\n\n    @staticmethod\n    def _add_device_dim_to_outputs_loader(\n        outputs_loader: TargetsLoader,\n    ) -> TargetsLoader:\n        def _reshape_batch(batch):", "metadata": [{"fpath_tuple": ["awslabs_fortuna", "fortuna", "calibration", "calibrator.py"], "line_no": 518, "start_line_no": 508, "end_line_no": 528, "window_size": 20, "repo": "awslabs_fortuna", "slice_size": 10}], "sim_score": 0.3522727272727273}, {"context": "        class TargetsLoaderWrapper:\n            def __init__(self, outputs_loader: TargetsLoader):\n                self._outputs_loader = outputs_loader\n\n            def __iter__(self):\n                outputs_loader = map(\n                    lambda batch: tree_map(_reshape_batch, batch), self._outputs_loader\n                )\n                outputs_loader = jax_utils.prefetch_to_device(outputs_loader, 2)\n                yield from outputs_loader\n\n        return (\n            TargetsLoaderWrapper(outputs_loader)\n            if outputs_loader is not None\n            else outputs_loader\n        )\n\n\nclass JittedProbModelCalibrator(JittedMixin, ProbModelCalibrator):\n    pass", "metadata": [{"fpath_tuple": ["awslabs_fortuna", "fortuna", "prob_model", "prob_model_calibrator.py"], "line_no": 104, "start_line_no": 94, "end_line_no": 114, "window_size": 20, "repo": "awslabs_fortuna", "slice_size": 10}], "sim_score": 0.3516483516483517}, {"context": "                self._outputs_loader = outputs_loader\n\n            def __iter__(self):\n                outputs_loader = map(\n                    lambda batch: tree_map(_reshape_batch, batch), self._outputs_loader\n                )\n                outputs_loader = jax_utils.prefetch_to_device(outputs_loader, 2)\n                yield from outputs_loader\n\n        return (\n            TargetsLoaderWrapper(outputs_loader)\n            if outputs_loader is not None\n            else outputs_loader\n        )\n\n\nclass JittedProbModelCalibrator(JittedMixin, ProbModelCalibrator):\n    pass\n\n", "metadata": [{"fpath_tuple": ["awslabs_fortuna", "fortuna", "prob_model", "prob_model_calibrator.py"], "line_no": 106, "start_line_no": 96, "end_line_no": 116, "window_size": 20, "repo": "awslabs_fortuna", "slice_size": 10}], "sim_score": 0.34444444444444444}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# torchrl/data/tensor_specs.py\n# --------------------------------------------------\n#         if any(val < 0 for val in shape):\n#             raise ValueError(\n#                 f\"{self.__class__.__name__}.extend does not support negative shapes.\"\n#             )\n#         if any(s1 != s2 and s2 != 1 for s1, s2 in zip(shape[-self.ndim :], self.shape)):\n#             raise ValueError(\n#                 f\"The last {self.ndim} of the extended shape must match the\"\n#                 f\"shape of the CompositeSpec in CompositeSpec.extend.\"\n#             )\n#         return self.__class__(\n#             n=shape[-1], shape=shape, device=self.device, dtype=self.dtype\n#         )\n# \n#     def to(self, dest: Union[torch.dtype, DEVICE_TYPING]) -> CompositeSpec:\n#         if isinstance(dest, torch.dtype):\n#             dest_dtype = dest\n#             dest_device = self.device\n#         else:\n#             dest_dtype = self.dtype\n#             dest_device = torch.device(dest)\n# --------------------------------------------------\n# the below code fragment can be found in:\n# torchrl/data/tensor_specs.py\n# --------------------------------------------------\n#         if any(s1 != s2 and s2 != 1 for s1, s2 in zip(shape[-self.ndim :], self.shape)):\n#             raise ValueError(\n#                 f\"The last {self.ndim} of the extended shape must match the\"\n#                 f\"shape of the CompositeSpec in CompositeSpec.extend.\"\n#             )\n#         return self.__class__(\n#             n=shape[-1], shape=shape, device=self.device, dtype=self.dtype\n#         )\n# \n#     def to(self, dest: Union[torch.dtype, DEVICE_TYPING]) -> CompositeSpec:\n#         if isinstance(dest, torch.dtype):\n#             dest_dtype = dest\n#             dest_device = self.device\n#         else:\n#             dest_dtype = self.dtype\n#             dest_device = torch.device(dest)\n#         return self.__class__(\n#             n=self.space.n, shape=self.shape, device=dest_device, dtype=dest_dtype\n#         )\n# \n# --------------------------------------------------\n# the below code fragment can be found in:\n# torchrl/data/tensor_specs.py\n# --------------------------------------------------\n#                 f\"{self.__class__.__name__}.extend does not support negative shapes.\"\n#             )\n#         if any(s1 != s2 and s2 != 1 for s1, s2 in zip(shape[-self.ndim :], self.shape)):\n#             raise ValueError(\n#                 f\"The last {self.ndim} of the extended shape must match the\"\n#                 f\"shape of the CompositeSpec in CompositeSpec.extend.\"\n#             )\n#         return self.__class__(\n#             n=shape[-1], shape=shape, device=self.device, dtype=self.dtype\n#         )\n# \n#     def to(self, dest: Union[torch.dtype, DEVICE_TYPING]) -> CompositeSpec:\n#         if isinstance(dest, torch.dtype):\n#             dest_dtype = dest\n#             dest_device = self.device\n#         else:\n#             dest_dtype = self.dtype\n#             dest_device = torch.device(dest)\n#         return self.__class__(\n#             n=self.space.n, shape=self.shape, device=dest_device, dtype=dest_dtype\n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\n.split(split_sizes, dim=-1)\n\n    def to_numpy(self, val: torch.Tensor, safe: bool = True) -> np.ndarray:\n        if safe:\n            self.assert_is_in(val)\n        vals = self._split(val)\n        out = torch.stack([val.argmax(-1) for val in vals], -1).numpy()\n        return out\n\n    def index(self, index: INDEX_TYPING, tensor_to_index: torch.Tensor) -> torch.Tensor:\n        if not isinstance(index, torch.Tensor):\n            raise ValueError(\n                f\"Only tensors are allowed for indexing using\"\n                f\" {self.__class__.__name__}.index(...)\"\n            )\n        indices = self._split(index)\n        tensor_to_index = self._split(tensor_to_index)\n\n        out = []\n        for _index, _tensor_to_index in zip(indices, tensor_to_index):\n            _index = _index.nonzero().squeeze()\n            _index = _index.expand(*_tensor_to_index.shape[:-1], _index.shape[-1])\n            out.append(_tensor_to_index.gather(-1, _index))\n        return torch.cat(out, -1)\n\n    def is_in(self, val: torch.Tensor) -> bool:\n        vals = self._split(val)\n        if vals is None:\n            return False\n        return all(\n            super(MultiOneHotDiscreteTensorSpec, self).is_in(_val) for _val in vals\n        )\n\n    def _project(self, val: torch.Tensor) -> torch.Tensor:\n        vals = self._split(val)\n        return torch.cat([super()._project(_val) for _val in vals], -1)\n\n    def to_categorical(self) -> MultiDiscreteTensorSpec:\n\n        return MultiDiscreteTensorSpec(\n            [_space.n for _space in self.space],\n            device=self.device,\n            dtype=self.dtype,\n            shape=[*self.shape[:-1], len(self.space)],\n        )\n\n    def expand(self, *shape):\n        nvecs = [space.n for space in self.space]\n        if len(shape) == 1 and isinstance(shape[0], (tuple, list, torch.Size)):\n            shape = shape[0]\n        if any(val < 0 for val in shape):\n            raise ValueError(\n                f\"{self.__class__.__name__}.extend does not support negative shapes.\"\n            )\n        if any(s1 != s2 and s2 != 1 for s1, s2 in zip(shape[-self.ndim :], self.shape)):\n            raise ValueError(\n                f\"The last {self.ndim} of the extended shape must match the\"\n                f\"shape of the CompositeSpec in CompositeSpec.extend.\"\n            )\n        return self.__class__(\n            nvec=nvecs, shape=shape, device=self.device, dtype=self.dtype\n        )\n\n\nclass DiscreteTensorSpec(TensorSpec):\n    \"\"\"A discrete tensor spec.\n\n    An alternative to OneHotTensorSpec for categorical variables in TorchRL. Instead of\n    using multiplication, categorical variables perform indexing which can speed up\n    computation and reduce memory cost for large categorical variables.\n\n    Example:\n        >>> batch, size = 3, 4\n        >>> action_value = torch.arange(batch*size)\n        >>> action_value = action_value.view(batch, size).to(torch.float)\n        >>> action = torch.argmax(action_value, dim=-1).to(torch.long)\n        >>> chosen_action_value = action_value[range(batch), action]\n        >>> print(chosen_action_value)\n        tensor([ 3.,  7., 11.])\n\n    Args:\n        n (int): number of possible outcomes.\n        shape: (torch.Size, optional): shape of the variable, default is \"torch.Size([])\".\n        device (str, int or torch.device, optional): device of the tensors.\n        dtype (str or torch.dtype, optional): dtype of the tensors.\n\n    \"\"\"\n\n    shape: torch.Size\n    space: DiscreteBox\n    device: torch.device = torch.device(\"cpu\")\n    dtype: torch.dtype = torch.float\n    domain: str = \"\"\n\n    def __init__(\n        self,\n        n: int,\n        shape: Optional[torch.Size] = None,\n        device: Optional[DEVICE_TYPING] = None,\n        dtype: Optional[Union[str, torch.dtype]] = torch.long,\n    ):\n        if shape is None:\n            shape = torch.Size([])\n        dtype, device = _default_dtype_and_device(dtype, device)\n        space = DiscreteBox(n)\n        super().__init__(shape, space, device, dtype, domain=\"discrete\")\n\n    def rand(self, shape=None) -> torch.Tensor:\n        if shape is None:\n            shape = torch.Size([])\n        return torch.randint(\n            0,\n            self.space.n,\n            torch.Size([*shape, *self.shape]),\n            device=self.device,\n            dtype=self.dtype,\n        )\n\n    def _project(self, val: torch.Tensor) -> torch.Tensor:\n        if val.dtype not in (torch.int, torch.long):\n            val = torch.round(val)\n        return val.clamp_(min=0, max=self.space.n - 1)\n\n    def is_in(self, val: torch.Tensor) -> bool:\n        return (0 <= val).all() and (val < self.space.n).all()\n\n    def __eq__(self, other):\n        return (\n            type(self) == type(other)\n            and self.shape == other.shape\n            and self.space == other.space\n            and self.device == other.device\n            and self.dtype == other.dtype\n            and self.domain == other.domain\n        )\n\n    def to_numpy(self, val: TensorDict, safe: bool = True) -> dict:\n        return super().to_numpy(val, safe)\n\n    def to_onehot(self) -> OneHotDiscreteTensorSpec:\n        # if len(self.shape) > 1:\n        #     raise RuntimeError(\n        #         f\"DiscreteTensorSpec with shape that has several dimensions can't be converted to \"\n        #         f\"OneHotDiscreteTensorSpec. Got shape={self.shape}.\"\n        #     )\n        shape = [*self.shape, self.space.n]\n        return OneHotDiscreteTensorSpec(\n            n=self.space.n, shape=shape, device=self.device, dtype=self.dtype\n        )\n\n    def expand(self, *shape):\n        if len(shape) == 1 and isinstance(shape[0], (tuple, list, torch.Size)):\n            shape = shape[0]\n        if any(val < 0 for val in shape):\n            raise ValueError(\n                f\"{self.__class__.__name__}.extend does not support negative shapes.\"\n            )\n        if any(s1 != s2 and s2 != 1 for s1, s2 in zip(shape[-self.ndim :], self.shape)):\n            raise ValueError(\n                f\"The last {self.ndim} of the extended shape must match the\"\n                f\"shape of the CompositeSpec in CompositeSpec.extend.\"\n            )\n        return self.__class__(\n            n=self.space.n, shape=shape, device=self.device, dtype=self.dtype\n        )\n\n    def to(self, dest: Union[torch.dtype, DEVICE_TYPING]) -> CompositeSpec:\n        if isinstance(dest, torch.dtype):\n            dest_dtype = dest\n            dest_device = self.device\n        else:\n            dest_dtype = self.dtype", "metadata": {"task_id": "pytorch_rl/127", "ground_truth": "            dest_device = torch.device(dest)", "fpath_tuple": ["pytorch_rl", "torchrl", "data", "tensor_specs.py"], "context_start_lineno": 1171, "line_no": 1343, "query_window": {"context": "            shape = shape[0]\n        if any(val < 0 for val in shape):\n            raise ValueError(\n                f\"{self.__class__.__name__}.extend does not support negative shapes.\"\n            )\n        if any(s1 != s2 and s2 != 1 for s1, s2 in zip(shape[-self.ndim :], self.shape)):\n            raise ValueError(\n                f\"The last {self.ndim} of the extended shape must match the\"\n                f\"shape of the CompositeSpec in CompositeSpec.extend.\"\n            )\n        return self.__class__(\n            n=self.space.n, shape=shape, device=self.device, dtype=self.dtype\n        )\n\n    def to(self, dest: Union[torch.dtype, DEVICE_TYPING]) -> CompositeSpec:\n        if isinstance(dest, torch.dtype):\n            dest_dtype = dest\n            dest_device = self.device\n        else:\n            dest_dtype = self.dtype", "metadata": {"fpath_tuple": ["pytorch_rl", "torchrl", "data", "tensor_specs.py"], "line_no": 1343, "task_id": "pytorch_rl/127", "start_line_no": 1323, "end_line_no": 1343, "window_size": 20, "context_start_lineno": 1171, "repo": "pytorch_rl"}}, "top_k_context": [{"context": "        if any(val < 0 for val in shape):\n            raise ValueError(\n                f\"{self.__class__.__name__}.extend does not support negative shapes.\"\n            )\n        if any(s1 != s2 and s2 != 1 for s1, s2 in zip(shape[-self.ndim :], self.shape)):\n            raise ValueError(\n                f\"The last {self.ndim} of the extended shape must match the\"\n                f\"shape of the CompositeSpec in CompositeSpec.extend.\"\n            )\n        return self.__class__(\n            n=shape[-1], shape=shape, device=self.device, dtype=self.dtype\n        )\n\n    def to(self, dest: Union[torch.dtype, DEVICE_TYPING]) -> CompositeSpec:\n        if isinstance(dest, torch.dtype):\n            dest_dtype = dest\n            dest_device = self.device\n        else:\n            dest_dtype = self.dtype\n            dest_device = torch.device(dest)", "metadata": [{"fpath_tuple": ["pytorch_rl", "torchrl", "data", "tensor_specs.py"], "line_no": 1036, "start_line_no": 1026, "end_line_no": 1046, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}], "sim_score": 0.96}, {"context": "                f\"{self.__class__.__name__}.extend does not support negative shapes.\"\n            )\n        if any(s1 != s2 and s2 != 1 for s1, s2 in zip(shape[-self.ndim :], self.shape)):\n            raise ValueError(\n                f\"The last {self.ndim} of the extended shape must match the\"\n                f\"shape of the CompositeSpec in CompositeSpec.extend.\"\n            )\n        return self.__class__(\n            n=shape[-1], shape=shape, device=self.device, dtype=self.dtype\n        )\n\n    def to(self, dest: Union[torch.dtype, DEVICE_TYPING]) -> CompositeSpec:\n        if isinstance(dest, torch.dtype):\n            dest_dtype = dest\n            dest_device = self.device\n        else:\n            dest_dtype = self.dtype\n            dest_device = torch.device(dest)\n        return self.__class__(\n            n=self.space.n, shape=self.shape, device=dest_device, dtype=dest_dtype", "metadata": [{"fpath_tuple": ["pytorch_rl", "torchrl", "data", "tensor_specs.py"], "line_no": 1038, "start_line_no": 1028, "end_line_no": 1048, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}], "sim_score": 0.94}, {"context": "        if len(shape) == 1 and isinstance(shape[0], (tuple, list, torch.Size)):\n            shape = shape[0]\n        if any(val < 0 for val in shape):\n            raise ValueError(\n                f\"{self.__class__.__name__}.extend does not support negative shapes.\"\n            )\n        if any(s1 != s2 and s2 != 1 for s1, s2 in zip(shape[-self.ndim :], self.shape)):\n            raise ValueError(\n                f\"The last {self.ndim} of the extended shape must match the\"\n                f\"shape of the CompositeSpec in CompositeSpec.extend.\"\n            )\n        return self.__class__(\n            n=shape[-1], shape=shape, device=self.device, dtype=self.dtype\n        )\n\n    def to(self, dest: Union[torch.dtype, DEVICE_TYPING]) -> CompositeSpec:\n        if isinstance(dest, torch.dtype):\n            dest_dtype = dest\n            dest_device = self.device\n        else:", "metadata": [{"fpath_tuple": ["pytorch_rl", "torchrl", "data", "tensor_specs.py"], "line_no": 1034, "start_line_no": 1024, "end_line_no": 1044, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}], "sim_score": 0.9158878504672897}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# ding/hpc_rl/tests/test_scatter.py\n# --------------------------------------------------\n# \n# assert torch.cuda.is_available()\n# use_cuda = True\n# \n# B = 256\n# M = 256\n# N = 256\n# H = 16\n# W = 16\n# \n# \n# # Note: origin gpu version of cover mode is not determinate, thus validation test use origin cpu version instead\n# def scatter_val():\n#     for scatter_type in ['add', 'cover']:\n#         ori_input = torch.randn(B, M, N)\n#         h = torch.randint(\n#             low=0, high=H, size=(\n#                 B,\n#                 M,\n#             )\n# --------------------------------------------------\n# the below code fragment can be found in:\n# ding/hpc_rl/tests/test_qntd_rescale.py\n# --------------------------------------------------\n# import time\n# import torch\n# from hpc_rll.origin.td import q_nstep_td_error_with_rescale, q_nstep_td_data\n# from hpc_rll.rl_utils.td import QNStepTDRescale\n# from testbase import mean_relative_error, times\n# \n# assert torch.cuda.is_available()\n# use_cuda = True\n# \n# T = 1024\n# B = 64\n# N = 64\n# --------------------------------------------------\n# the below code fragment can be found in:\n# ding/hpc_rl/tests/test_qntd_rescale.py\n# --------------------------------------------------\n# from hpc_rll.origin.td import q_nstep_td_error_with_rescale, q_nstep_td_data\n# from hpc_rll.rl_utils.td import QNStepTDRescale\n# from testbase import mean_relative_error, times\n# \n# assert torch.cuda.is_available()\n# use_cuda = True\n# \n# T = 1024\n# B = 64\n# N = 64\n# gamma = 0.95\n# \n# \n# def qntd_rescale_val():\n#     ori_q = torch.randn(B, N)\n#     ori_next_n_q = torch.randn(B, N)\n#     ori_action = torch.randint(0, N, size=(B, ))\n#     ori_next_n_action = torch.randint(0, N, size=(B, ))\n#     ori_reward = torch.randn(T, B)\n#     ori_done = torch.randn(B)\n# --------------------------------------------------\n# the below code fragment can be found in:\n# ding/hpc_rl/tests/test_qntd_rescale.py\n# --------------------------------------------------\n# import time\n# import torch\n# from hpc_rll.origin.td import q_nstep_td_error_with_rescale, q_nstep_td_data\n# from hpc_rll.rl_utils.td import QNStepTDRescale\n# from testbase import mean_relative_error, times\n# \n# assert torch.cuda.is_available()\n# use_cuda = True\n# \n# T = 1024\n# B = 64\n# N = 64\n# gamma = 0.95\n# \n# \n# def qntd_rescale_val():\n#     ori_q = torch.randn(B, N)\n#     ori_next_n_q = torch.randn(B, N)\n#     ori_action = torch.randint(0, N, size=(B, ))\n#     ori_next_n_action = torch.randint(0, N, size=(B, ))\n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\nimport importlib\nimport logging\nfrom collections import OrderedDict\nfrom functools import wraps\nimport ding\n'''\nOverview:\n    `hpc_wrapper` is the wrapper for functions which are supported by hpc. If a function is wrapped by it, we will\n    search for its hpc type and return the function implemented by hpc.\n    We will use the following code as a sample to introduce `hpc_wrapper`:\n    ```\n    @hpc_wrapper(shape_fn=shape_fn_dntd, namedtuple_data=True, include_args=[0,1,2,3],\n                 include_kwargs=['data', 'gamma', 'v_min', 'v_max'], is_cls_method=False)\n    def dist_nstep_td_error(\n            data: namedtuple,\n            gamma: float,\n            v_min: float,\n            v_max: float,\n            n_atom: int,\n            nstep: int = 1,\n    ) -> torch.Tensor:\n    ...\n    ```\nParameters:\n    - shape_fn (:obj:`function`): a function which return the shape needed by hpc function. In fact, it returns\n        all args that the hpc function needs.\n    - nametuple_data (:obj:`bool`): If True, when hpc function is called, it will be called as hpc_function(*nametuple).\n        If False, nametuple data will remain its `nametuple` type.\n    - include_args (:obj:`list`): a list of index of the args need to be set in hpc function. As shown in the sample,\n        include_args=[0,1,2,3], which means `data`, `gamma`, `v_min` and `v_max` will be set in hpc function.\n    - include_kwargs (:obj:`list`): a list of key of the kwargs need to be set in hpc function. As shown in the sample,\n        include_kwargs=['data', 'gamma', 'v_min', 'v_max'], which means `data`, `gamma`, `v_min` and `v_max` will be\n        set in hpc function.\n    - is_cls_method (:obj:`bool`): If True, it means the function we wrap is a method of a class. `self` will be put\n        into args. We will get rid of `self` in args. Besides, we will use its classname as its fn_name.\n        If False, it means the function is a simple method.\nQ&A:\n    - Q: Is `include_args` and `include_kwargs` need to be set at the same time?\n    - A: Yes. `include_args` and `include_kwargs` can deal with all type of input, such as (data, gamma, v_min=v_min,\n        v_max=v_max) and (data, gamma, v_min, v_max).\n    - Q: What is `hpc_fns`?\n    - A: Here we show a normal `hpc_fns`:\n         ```\n         hpc_fns = {\n             'fn_name1': {\n                 'runtime_name1': hpc_fn1,\n                 'runtime_name2': hpc_fn2,\n                 ...\n             },\n             ...\n         }\n         ```\n         Besides, `per_fn_limit` means the max length of `hpc_fns[fn_name]`. When new function comes, the oldest\n         function will be popped from `hpc_fns[fn_name]`.\n'''\n\nhpc_fns = {}\nper_fn_limit = 3\n\n\ndef register_runtime_fn(fn_name, runtime_name, shape):\n    fn_name_mapping = {\n        'gae': ['hpc_rll.rl_utils.gae', 'GAE'],\n        'dist_nstep_td_error': ['hpc_rll.rl_utils.td', 'DistNStepTD'],\n        'LSTM': ['hpc_rll.torch_utils.network.rnn', 'LSTM'],\n        'ppo_error': ['hpc_rll.rl_utils.ppo', 'PPO'],\n        'q_nstep_td_error': ['hpc_rll.rl_utils.td', 'QNStepTD'],\n        'q_nstep_td_error_with_rescale': ['hpc_rll.rl_utils.td', 'QNStepTDRescale'],\n        'ScatterConnection': ['hpc_rll.torch_utils.network.scatter_connection', 'ScatterConnection'],\n        'td_lambda_error': ['hpc_rll.rl_utils.td', 'TDLambda'],\n        'upgo_loss': ['hpc_rll.rl_utils.upgo', 'UPGO'],\n        'vtrace_error': ['hpc_rll.rl_utils.vtrace', 'VTrace'],\n    }\n    fn_str = fn_name_mapping[fn_name]\n    cls = getattr(importlib.import_module(fn_str[0]), fn_str[1])\n    hpc_fn = cls(*shape).cuda()\n    if fn_name not in hpc_fns:\n        hpc_fns[fn_name] = OrderedDict()\n    hpc_fns[fn_name][runtime_name] = hpc_fn", "metadata": {"task_id": "opendilab_ACE/15", "ground_truth": "    while len(hpc_fns[fn_name]) > per_fn_limit:", "fpath_tuple": ["opendilab_ACE", "ding", "hpc_rl", "wrapper.py"], "context_start_lineno": 0, "line_no": 79, "query_window": {"context": "\ndef register_runtime_fn(fn_name, runtime_name, shape):\n    fn_name_mapping = {\n        'gae': ['hpc_rll.rl_utils.gae', 'GAE'],\n        'dist_nstep_td_error': ['hpc_rll.rl_utils.td', 'DistNStepTD'],\n        'LSTM': ['hpc_rll.torch_utils.network.rnn', 'LSTM'],\n        'ppo_error': ['hpc_rll.rl_utils.ppo', 'PPO'],\n        'q_nstep_td_error': ['hpc_rll.rl_utils.td', 'QNStepTD'],\n        'q_nstep_td_error_with_rescale': ['hpc_rll.rl_utils.td', 'QNStepTDRescale'],\n        'ScatterConnection': ['hpc_rll.torch_utils.network.scatter_connection', 'ScatterConnection'],\n        'td_lambda_error': ['hpc_rll.rl_utils.td', 'TDLambda'],\n        'upgo_loss': ['hpc_rll.rl_utils.upgo', 'UPGO'],\n        'vtrace_error': ['hpc_rll.rl_utils.vtrace', 'VTrace'],\n    }\n    fn_str = fn_name_mapping[fn_name]\n    cls = getattr(importlib.import_module(fn_str[0]), fn_str[1])\n    hpc_fn = cls(*shape).cuda()\n    if fn_name not in hpc_fns:\n        hpc_fns[fn_name] = OrderedDict()\n    hpc_fns[fn_name][runtime_name] = hpc_fn", "metadata": {"fpath_tuple": ["opendilab_ACE", "ding", "hpc_rl", "wrapper.py"], "line_no": 79, "task_id": "opendilab_ACE/15", "start_line_no": 59, "end_line_no": 79, "window_size": 20, "context_start_lineno": 0, "repo": "opendilab_ACE"}}, "top_k_context": [{"context": "import time\nimport torch\nfrom hpc_rll.origin.td import q_nstep_td_error_with_rescale, q_nstep_td_data\nfrom hpc_rll.rl_utils.td import QNStepTDRescale\nfrom testbase import mean_relative_error, times\n\nassert torch.cuda.is_available()\nuse_cuda = True\n\nT = 1024\nB = 64\nN = 64\ngamma = 0.95\n\n\ndef qntd_rescale_val():\n    ori_q = torch.randn(B, N)\n    ori_next_n_q = torch.randn(B, N)", "metadata": [{"fpath_tuple": ["opendilab_ACE", "ding", "hpc_rl", "tests", "test_qntd_rescale.py"], "line_no": 8, "start_line_no": 0, "end_line_no": 18, "window_size": 20, "repo": "opendilab_ACE", "slice_size": 10}], "sim_score": 0.21333333333333335}, {"context": "import time\nimport torch\nfrom hpc_rll.origin.td import q_nstep_td_error_with_rescale, q_nstep_td_data\nfrom hpc_rll.rl_utils.td import QNStepTDRescale\nfrom testbase import mean_relative_error, times\n\nassert torch.cuda.is_available()\nuse_cuda = True\n\nT = 1024\nB = 64\nN = 64\ngamma = 0.95\n\n\ndef qntd_rescale_val():\n    ori_q = torch.randn(B, N)\n    ori_next_n_q = torch.randn(B, N)\n    ori_action = torch.randint(0, N, size=(B, ))\n    ori_next_n_action = torch.randint(0, N, size=(B, ))", "metadata": [{"fpath_tuple": ["opendilab_ACE", "ding", "hpc_rl", "tests", "test_qntd_rescale.py"], "line_no": 10, "start_line_no": 0, "end_line_no": 20, "window_size": 20, "repo": "opendilab_ACE", "slice_size": 10}], "sim_score": 0.2129032258064516}, {"context": "import time\nimport torch\nfrom hpc_rll.origin.td import q_nstep_td_error_with_rescale, q_nstep_td_data\nfrom hpc_rll.rl_utils.td import QNStepTDRescale\nfrom testbase import mean_relative_error, times\n\nassert torch.cuda.is_available()\nuse_cuda = True\n\nT = 1024", "metadata": [{"fpath_tuple": ["opendilab_ACE", "ding", "hpc_rl", "tests", "test_qntd_rescale.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 10, "window_size": 20, "repo": "opendilab_ACE", "slice_size": 10}], "sim_score": 0.208955223880597}, {"context": "from hpc_rll.torch_utils.network.scatter_connection import ScatterConnection as HPCScatterConnection\nfrom testbase import mean_relative_error, times\n\nassert torch.cuda.is_available()\nuse_cuda = True\n\nB = 256\nM = 256\nN = 256\nH = 16\nW = 16\n\n\n# Note: origin gpu version of cover mode is not determinate, thus validation test use origin cpu version instead\ndef scatter_val():\n    for scatter_type in ['add', 'cover']:\n        ori_input = torch.randn(B, M, N)\n        h = torch.randint(\n            low=0, high=H, size=(\n                B,", "metadata": [{"fpath_tuple": ["opendilab_ACE", "ding", "hpc_rl", "tests", "test_scatter.py"], "line_no": 14, "start_line_no": 4, "end_line_no": 24, "window_size": 20, "repo": "opendilab_ACE", "slice_size": 10}], "sim_score": 0.20786516853932585}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# federatedscope/core/trainers/context.py\n# --------------------------------------------------\n#             super(LifecycleDict, self).__setitem__(key, value.obj)\n#         else:\n#             super(LifecycleDict, self).__setitem__(key, value)\n# \n#     def clear(self, lifecycle):\n#         keys = list(self.lifecycles[lifecycle])\n#         for key in keys:\n#             if key in self:\n#                 del self[key]\n#             self.lifecycles[lifecycle].remove(key)\n# \n# \n# class Context(LifecycleDict):\n#     \"\"\"\n#     Record and pass variables among different hook functions.\n# \n#     Arguments:\n#         model: training model\n#         cfg: config\n#         data (dict): a dict contains train/val/test dataset or dataloader\n# --------------------------------------------------\n# the below code fragment can be found in:\n# federatedscope/nlp/hetero_tasks/trainer/trainer.py\n# --------------------------------------------------\n# \n#         if raw_num_train_epoch is not None and raw_num_train_batch is not None:\n#             self.ctx.num_train_epoch = raw_num_train_epoch\n#             self.ctx.num_train_batch = raw_num_train_batch\n#             self.ctx.num_train_batch_last_epoch = self.ctx.num_train_batch\n#             self.ctx.num_total_train_batch = \\\n#                 self.ctx.num_train_epoch * self.ctx.num_train_batch\n# \n#         return self.ctx.num_samples\n# \n#     @lifecycle(LIFECYCLE.BATCH)\n#     def _run_batch(self, hooks_set):\n#         for batch_i in tqdm(range(\n#                 getattr(self.ctx, f\"num_{self.ctx.cur_split}_batch\", None)),\n#                             disable=not (self._in_contrast_prepare\n#                                          or self.ctx.cur_split == \"test\")):\n#             self.ctx.cur_batch_i = CtxVar(batch_i, LIFECYCLE.BATCH)\n# \n#             for hook in hooks_set[\"on_batch_start\"]:\n#                 hook(self.ctx)\n# --------------------------------------------------\n# the below code fragment can be found in:\n# federatedscope/nlp/hetero_tasks/trainer/trainer.py\n# --------------------------------------------------\n#             self.ctx.num_train_epoch = raw_num_train_epoch\n#             self.ctx.num_train_batch = raw_num_train_batch\n#             self.ctx.num_train_batch_last_epoch = self.ctx.num_train_batch\n#             self.ctx.num_total_train_batch = \\\n#                 self.ctx.num_train_epoch * self.ctx.num_train_batch\n# \n#         return self.ctx.num_samples\n# \n#     @lifecycle(LIFECYCLE.BATCH)\n#     def _run_batch(self, hooks_set):\n#         for batch_i in tqdm(range(\n#                 getattr(self.ctx, f\"num_{self.ctx.cur_split}_batch\", None)),\n#                             disable=not (self._in_contrast_prepare\n#                                          or self.ctx.cur_split == \"test\")):\n#             self.ctx.cur_batch_i = CtxVar(batch_i, LIFECYCLE.BATCH)\n# \n#             for hook in hooks_set[\"on_batch_start\"]:\n#                 hook(self.ctx)\n# \n#             for hook in hooks_set[\"on_batch_forward\"]:\n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\n `regularizer` @property and cached\n            #  to compare whether changes happen\n            self.criterion = get_criterion(self.cfg.criterion.type,\n                                           self.device)\n            self.regularizer = get_regularizer(self.cfg.regularizer.type)\n            self.grad_clip = self.cfg.grad.grad_clip\n        elif self.cfg.backend == 'tensorflow':\n            self.trainable_para_names = self.model.trainable_variables()\n            self.criterion = None\n            self.regularizer = None\n            self.optimizer = None\n            self.grad_clip = None\n\n    # Train related property, query from `cfg` if not set\n    @property\n    def num_train_batch(self):\n        if self.get('num_train_batch'):\n            return self.get('num_train_batch')\n        return self._calculate_batch_epoch_num(mode='train')[0]\n\n    @property\n    def num_train_batch_last_epoch(self):\n        if self.get('num_train_batch_last_epoch'):\n            return self.get('num_train_batch_last_epoch')\n        return self._calculate_batch_epoch_num(mode='train')[1]\n\n    @property\n    def num_train_epoch(self):\n        if self.get('num_train_epoch'):\n            return self.get('num_train_epoch')\n        return self._calculate_batch_epoch_num(mode='train')[2]\n\n    @property\n    def num_total_train_batch(self):\n        if self.get('num_total_train_batch'):\n            return self.get('num_total_train_batch')\n        return self._calculate_batch_epoch_num(mode='train')[3]\n\n    # Val related property, query from `cfg` if not set\n    @property\n    def num_val_batch(self):\n        if self.get('num_val_batch'):\n            return self.get('num_val_batch')\n        return self._calculate_batch_epoch_num(mode='val')[0]\n\n    @property\n    def num_val_epoch(self):\n        if self.get('num_val_epoch'):\n            return self.get('num_val_epoch')\n        return self._calculate_batch_epoch_num(mode='val')[2]\n\n    # Test related property, query from `cfg` if not set\n    @property\n    def num_test_batch(self):\n        if self.get('num_test_batch'):\n            return self.get('num_test_batch')\n        return self._calculate_batch_epoch_num(mode='test')[0]\n\n    @property\n    def num_test_epoch(self):\n        if self.get('num_test_epoch'):\n            return self.get('num_test_epoch')\n        return self._calculate_batch_epoch_num(mode='test')[2]\n\n    def _calculate_batch_epoch_num(self, mode='train'):\n        if self.cur_mode is not None and self.cur_mode != mode:\n            logger.warning(\n                f'cur_mode `{self.cur_mode}` mismatch mode `{mode}`, '\n                f'will use `{mode}` to calculate `ctx.var`.')\n        if self.cur_split is None:\n            logger.warning(\n                f'cur_split `{self.cur_split}` not found in data_split, '\n                f'will use `train` split to calculate `ctx.var`.')\n            cur_split = 'train'\n        else:\n            cur_split = self.cur_split\n\n        num_batch_last_epoch, num_total_batch = None, None\n        if mode in ['train', 'finetune']:\n            num_batch, num_batch_last_epoch, num_epoch, num_total_batch = \\\n                calculate_batch_epoch_num(\n                    self.cfg.train.local_update_steps *\n                    self.cfg.grad.grad_accum_count,\n                    self.cfg.train.batch_or_epoch,\n                    self.get(f'num_{cur_split}_data'),\n                    self.cfg.dataloader.batch_size,\n                    self.cfg.dataloader.drop_last)\n        elif mode in ['val', 'test']:\n            num_epoch = 1\n            num_batch = self.get(f'num_{cur_split}_data'\n                                 ) // self.cfg.dataloader.batch_size + int(\n                                     not self.cfg.dataloader.drop_last\n                                     and bool(\n                                         self.get(f'num_{cur_split}_data') %\n                                         self.cfg.dataloader.batch_size))\n        else:\n            raise ValueError(f'Invalid mode {mode}.')\n\n        return num_batch, num_batch_last_epoch, num_epoch, num_total_batch\n\n    def track_mode(self, mode):\n        self.mode_stack.append(mode)\n        self.cur_mode = self.mode_stack[-1]\n        self.change_mode(self.cur_mode)\n\n    def reset_mode(self):\n        self.mode_stack.pop()\n        self.cur_mode = self.mode_stack[-1] if len(\n            self.mode_stack) != 0 else None\n        if len(self.mode_stack) != 0:\n            self.change_mode(self.cur_mode)\n\n    def change_mode(self, mode):\n        # change state\n        if self.cfg.backend == 'torch':\n            getattr(\n                self.model, 'train'\n                if mode == MODE.TRAIN or mode == MODE.FINETUNE else 'eval')()\n        else:\n            pass\n\n    def track_split(self, dataset):\n        # stack-style to enable mixture usage such as evaluation on train\n        # dataset\n        self.split_stack.append(dataset)\n        self.cur_split = self.split_stack[-1]\n\n    def reset_split(self):\n        self.split_stack.pop()\n        self.cur_split = self.split_stack[-1] if \\\n            len(self.split_stack) != 0 else None\n\n    def check_split(self, target_split_name, skip=False):\n        if self.get(f\"{target_split_name}_data\") is None and self.get(\n                f\"{target_split_name}_loader\") is None:\n            if skip:\n                logger.warning(\n                    f\"No {target_split_name}_data or\"\n                    f\" {target_split_name}_loader in the trainer, \"\n                    f\"will skip evaluation.\"\n                    f\"If this is not the case you want, please check \"\n                    f\"whether there is typo for the name\")\n                return False\n            else:\n                raise ValueError(f\"No {target_split_name}_data or\"\n                                 f\" {target_split_name}_loader in the trainer\")\n        else:\n            return True\n\n    def merge_from_dict(self, other_dict):\n        for key, value in other_dict.items():\n            setattr(self, key, value)\n\n\nclass CtxVar(object):\n    \"\"\"\n    Basic variable class\n\n    Arguments:\n        lifecycle: specific lifecycle of the attribute\n    \"\"\"\n\n    LIFECYCLES = [\"batch\", \"epoch\", \"routine\", None]\n\n    def __init__(self, obj, lifecycle=None):\n        assert lifecycle in CtxVar.LIFECYCLES\n        self.obj = obj\n        self.lifecycle = lifecycle\n\n\ndef lifecycle(lifecycle):\n    \"\"\"\n    Manage the lifecycle of the variables within context, \\\n    and blind these operations from user.\n\n    Arguments:\n        lifecycle: the type of lifecycle, choose from \"batch/epoch/routine\"\n    \"\"\"\n    if lifecycle == \"routine\":\n\n        def decorate(func):", "metadata": {"task_id": "alibaba_FederatedScope/157", "ground_truth": "            def wrapper(self, mode, hooks_set, dataset_name=None):", "fpath_tuple": ["alibaba_FederatedScope", "federatedscope", "core", "trainers", "context.py"], "context_start_lineno": 157, "line_no": 338, "query_window": {"context": "\n    LIFECYCLES = [\"batch\", \"epoch\", \"routine\", None]\n\n    def __init__(self, obj, lifecycle=None):\n        assert lifecycle in CtxVar.LIFECYCLES\n        self.obj = obj\n        self.lifecycle = lifecycle\n\n\ndef lifecycle(lifecycle):\n    \"\"\"\n    Manage the lifecycle of the variables within context, \\\n    and blind these operations from user.\n\n    Arguments:\n        lifecycle: the type of lifecycle, choose from \"batch/epoch/routine\"\n    \"\"\"\n    if lifecycle == \"routine\":\n\n        def decorate(func):", "metadata": {"fpath_tuple": ["alibaba_FederatedScope", "federatedscope", "core", "trainers", "context.py"], "line_no": 338, "task_id": "alibaba_FederatedScope/157", "start_line_no": 318, "end_line_no": 338, "window_size": 20, "context_start_lineno": 157, "repo": "alibaba_FederatedScope"}}, "top_k_context": [{"context": "\n        if raw_num_train_epoch is not None and raw_num_train_batch is not None:\n            self.ctx.num_train_epoch = raw_num_train_epoch\n            self.ctx.num_train_batch = raw_num_train_batch\n            self.ctx.num_train_batch_last_epoch = self.ctx.num_train_batch\n            self.ctx.num_total_train_batch = \\\n                self.ctx.num_train_epoch * self.ctx.num_train_batch\n\n        return self.ctx.num_samples\n\n    @lifecycle(LIFECYCLE.BATCH)\n    def _run_batch(self, hooks_set):\n        for batch_i in tqdm(range(\n                getattr(self.ctx, f\"num_{self.ctx.cur_split}_batch\", None)),\n                            disable=not (self._in_contrast_prepare\n                                         or self.ctx.cur_split == \"test\")):\n            self.ctx.cur_batch_i = CtxVar(batch_i, LIFECYCLE.BATCH)\n\n            for hook in hooks_set[\"on_batch_start\"]:\n                hook(self.ctx)", "metadata": [{"fpath_tuple": ["alibaba_FederatedScope", "federatedscope", "nlp", "hetero_tasks", "trainer", "trainer.py"], "line_no": 264, "start_line_no": 254, "end_line_no": 274, "window_size": 20, "repo": "alibaba_FederatedScope", "slice_size": 10}], "sim_score": 0.2748091603053435}, {"context": "        for hook in hooks_set[\"on_fit_end\"]:\n            hook(self.ctx)\n\n        if raw_num_train_epoch is not None and raw_num_train_batch is not None:\n            self.ctx.num_train_epoch = raw_num_train_epoch\n            self.ctx.num_train_batch = raw_num_train_batch\n            self.ctx.num_train_batch_last_epoch = self.ctx.num_train_batch\n            self.ctx.num_total_train_batch = \\\n                self.ctx.num_train_epoch * self.ctx.num_train_batch\n\n        return self.ctx.num_samples\n\n    @lifecycle(LIFECYCLE.BATCH)\n    def _run_batch(self, hooks_set):\n        for batch_i in tqdm(range(\n                getattr(self.ctx, f\"num_{self.ctx.cur_split}_batch\", None)),\n                            disable=not (self._in_contrast_prepare\n                                         or self.ctx.cur_split == \"test\")):\n            self.ctx.cur_batch_i = CtxVar(batch_i, LIFECYCLE.BATCH)\n", "metadata": [{"fpath_tuple": ["alibaba_FederatedScope", "federatedscope", "nlp", "hetero_tasks", "trainer", "trainer.py"], "line_no": 262, "start_line_no": 252, "end_line_no": 272, "window_size": 20, "repo": "alibaba_FederatedScope", "slice_size": 10}], "sim_score": 0.2727272727272727}, {"context": "        if isinstance(value, CtxVar):\n            self.lifecycles[value.lifecycle].add(key)\n            super(LifecycleDict, self).__setitem__(key, value.obj)\n        else:\n            super(LifecycleDict, self).__setitem__(key, value)\n\n    def clear(self, lifecycle):\n        keys = list(self.lifecycles[lifecycle])\n        for key in keys:\n            if key in self:\n                del self[key]\n            self.lifecycles[lifecycle].remove(key)\n\n\nclass Context(LifecycleDict):\n    \"\"\"\n    Record and pass variables among different hook functions.\n\n    Arguments:\n        model: training model", "metadata": [{"fpath_tuple": ["alibaba_FederatedScope", "federatedscope", "core", "trainers", "context.py"], "line_no": 42, "start_line_no": 32, "end_line_no": 52, "window_size": 20, "repo": "alibaba_FederatedScope", "slice_size": 10}], "sim_score": 0.26548672566371684}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# fortuna/prob_model/posterior/posterior_mixin.py\n# --------------------------------------------------\n# from typing import Optional\n# \n# from fortuna.prob_model.posterior.name_to_posterior_state import \\\n#     NameToPosteriorState\n# from fortuna.prob_model.posterior.state import PosteriorState\n# from fortuna.training.mixin import WithCheckpointingMixin\n# from fortuna.typing import OptaxOptimizer, Path\n# \n# \n# class WithPosteriorCheckpointingMixin(WithCheckpointingMixin):\n#     def restore_checkpoint(\n#         self,\n# --------------------------------------------------\n# the below code fragment can be found in:\n# fortuna/prob_model/posterior/posterior_state_repository.py\n# --------------------------------------------------\n# from typing import Dict, Optional\n# \n# from fortuna.prob_model.posterior.posterior_mixin import \\\n#     WithPosteriorCheckpointingMixin\n# from fortuna.training.train_state_repository import TrainStateRepository\n# from fortuna.typing import Path\n# \n# \n# class PosteriorStateRepository(WithPosteriorCheckpointingMixin, TrainStateRepository):\n#     def extract_calib_keys(\n#         self,\n#         checkpoint_path: Optional[Path] = None,\n#         prefix: str = \"checkpoint_\",\n#         **kwargs\n# --------------------------------------------------\n# the below code fragment can be found in:\n# fortuna/training/mixin.py\n# --------------------------------------------------\n# import logging\n# import os\n# from typing import Dict, Optional\n# \n# from flax.training import checkpoints\n# from flax.training.early_stopping import EarlyStopping\n# \n# from fortuna.training.name_to_train_state import NameToTrainState\n# from fortuna.training.train_state import TrainState\n# from fortuna.typing import OptaxOptimizer, Path\n# \n# logger = logging.getLogger(__name__)\n# --------------------------------------------------\n# the below code fragment can be found in:\n# fortuna/training/mixin.py\n# --------------------------------------------------\n# import logging\n# import os\n# from typing import Dict, Optional\n# \n# from flax.training import checkpoints\n# from flax.training.early_stopping import EarlyStopping\n# \n# from fortuna.training.name_to_train_state import NameToTrainState\n# from fortuna.training.train_state import TrainState\n# from fortuna.typing import OptaxOptimizer, Path\n# \n# logger = logging.getLogger(__name__)\n# \n# \n# class WithCheckpointingMixin:\n#     def __init__(\n#         self, **kwargs,\n#     ):\n# --------------------------------------------------\n# the below code fragment can be found in:\n# fortuna/prob_model/posterior/posterior_state_repository.py\n# --------------------------------------------------\n# from typing import Dict, Optional\n# \n# from fortuna.prob_model.posterior.posterior_mixin import \\\n#     WithPosteriorCheckpointingMixin\n# from fortuna.training.train_state_repository import TrainStateRepository\n# from fortuna.typing import Path\n# \n# \n# class PosteriorStateRepository(WithPosteriorCheckpointingMixin, TrainStateRepository):\n#     def extract_calib_keys(\n#         self,\n#         checkpoint_path: Optional[Path] = None,\n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\nimport os\nfrom copy import deepcopy\nfrom typing import Dict, List, Optional, Union\n\nfrom fortuna.training.mixin import WithCheckpointingMixin\nfrom fortuna.training.train_state import TrainState", "metadata": {"task_id": "awslabs_fortuna/63", "ground_truth": "from fortuna.typing import OptaxOptimizer, Path", "fpath_tuple": ["awslabs_fortuna", "fortuna", "training", "train_state_repository.py"], "context_start_lineno": 0, "line_no": 6, "query_window": {"context": "import os\nfrom copy import deepcopy\nfrom typing import Dict, List, Optional, Union\n\nfrom fortuna.training.mixin import WithCheckpointingMixin\nfrom fortuna.training.train_state import TrainState", "metadata": {"fpath_tuple": ["awslabs_fortuna", "fortuna", "training", "train_state_repository.py"], "line_no": 6, "task_id": "awslabs_fortuna/63", "start_line_no": 0, "end_line_no": 6, "window_size": 20, "context_start_lineno": 0, "repo": "awslabs_fortuna"}}, "top_k_context": [{"context": "from typing import Dict, Optional\n\nfrom fortuna.prob_model.posterior.posterior_mixin import \\\n    WithPosteriorCheckpointingMixin\nfrom fortuna.training.train_state_repository import TrainStateRepository\nfrom fortuna.typing import Path\n\n\nclass PosteriorStateRepository(WithPosteriorCheckpointingMixin, TrainStateRepository):\n    def extract_calib_keys(", "metadata": [{"fpath_tuple": ["awslabs_fortuna", "fortuna", "prob_model", "posterior", "posterior_state_repository.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 10, "window_size": 20, "repo": "awslabs_fortuna", "slice_size": 10}], "sim_score": 0.4067796610169492}, {"context": "import logging\nimport os\nfrom typing import Dict, Optional\n\nfrom flax.training import checkpoints\nfrom flax.training.early_stopping import EarlyStopping\n\nfrom fortuna.training.name_to_train_state import NameToTrainState\nfrom fortuna.training.train_state import TrainState\nfrom fortuna.typing import OptaxOptimizer, Path\n\nlogger = logging.getLogger(__name__)\n\n\nclass WithCheckpointingMixin:\n    def __init__(", "metadata": [{"fpath_tuple": ["awslabs_fortuna", "fortuna", "training", "mixin.py"], "line_no": 6, "start_line_no": 0, "end_line_no": 16, "window_size": 20, "repo": "awslabs_fortuna", "slice_size": 10}], "sim_score": 0.373134328358209}, {"context": "import logging\nimport os\nfrom typing import Dict, Optional\n\nfrom flax.training import checkpoints\nfrom flax.training.early_stopping import EarlyStopping\n\nfrom fortuna.training.name_to_train_state import NameToTrainState\nfrom fortuna.training.train_state import TrainState\nfrom fortuna.typing import OptaxOptimizer, Path", "metadata": [{"fpath_tuple": ["awslabs_fortuna", "fortuna", "training", "mixin.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 10, "window_size": 20, "repo": "awslabs_fortuna", "slice_size": 10}], "sim_score": 0.36538461538461536}, {"context": "from typing import Dict, Optional\n\nfrom fortuna.prob_model.posterior.posterior_mixin import \\\n    WithPosteriorCheckpointingMixin\nfrom fortuna.training.train_state_repository import TrainStateRepository\nfrom fortuna.typing import Path\n\n\nclass PosteriorStateRepository(WithPosteriorCheckpointingMixin, TrainStateRepository):\n    def extract_calib_keys(\n        self,\n        checkpoint_path: Optional[Path] = None,", "metadata": [{"fpath_tuple": ["awslabs_fortuna", "fortuna", "prob_model", "posterior", "posterior_state_repository.py"], "line_no": 2, "start_line_no": 0, "end_line_no": 12, "window_size": 20, "repo": "awslabs_fortuna", "slice_size": 10}], "sim_score": 0.34782608695652173}, {"context": "from typing import Optional\n\nfrom fortuna.prob_model.posterior.name_to_posterior_state import \\\n    NameToPosteriorState\nfrom fortuna.prob_model.posterior.state import PosteriorState\nfrom fortuna.training.mixin import WithCheckpointingMixin\nfrom fortuna.typing import OptaxOptimizer, Path\n\n\nclass WithPosteriorCheckpointingMixin(WithCheckpointingMixin):", "metadata": [{"fpath_tuple": ["awslabs_fortuna", "fortuna", "prob_model", "posterior", "posterior_mixin.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 10, "window_size": 20, "repo": "awslabs_fortuna", "slice_size": 10}], "sim_score": 0.3448275862068966}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# ding/model/template/q_learning.py\n# --------------------------------------------------\n# class C51DQN(nn.Module):\n# \n#     def __init__(\n#         self,\n#         obs_shape: Union[int, SequenceType],\n#         action_shape: Union[int, SequenceType],\n#         encoder_hidden_size_list: SequenceType = [128, 128, 64],\n#         head_hidden_size: int = None,\n#         head_layer_num: int = 1,\n#         activation: Optional[nn.Module] = nn.ReLU(),\n#         norm_type: Optional[str] = None,\n#         v_min: Optional[float] = -10,\n#         v_max: Optional[float] = 10,\n#         n_atom: Optional[int] = 51,\n#     ) -> None:\n#         r\"\"\"\n#         Overview:\n#             Init the C51 Model according to input arguments.\n#         Arguments:\n#             - obs_shape (:obj:`Union[int, SequenceType]`): Observation's space.\n# --------------------------------------------------\n# the below code fragment can be found in:\n# ding/model/template/q_learning.py\n# --------------------------------------------------\n#     def __init__(\n#         self,\n#         obs_shape: Union[int, SequenceType],\n#         action_shape: Union[int, SequenceType],\n#         encoder_hidden_size_list: SequenceType = [128, 128, 64],\n#         head_hidden_size: int = None,\n#         head_layer_num: int = 1,\n#         activation: Optional[nn.Module] = nn.ReLU(),\n#         norm_type: Optional[str] = None,\n#         v_min: Optional[float] = -10,\n#         v_max: Optional[float] = 10,\n#         n_atom: Optional[int] = 51,\n#     ) -> None:\n#         r\"\"\"\n#         Overview:\n#             Init the C51 Model according to input arguments.\n#         Arguments:\n#             - obs_shape (:obj:`Union[int, SequenceType]`): Observation's space.\n#             - action_shape (:obj:`Union[int, SequenceType]`): Action's space.\n#             - encoder_hidden_size_list (:obj:`SequenceType`): Collection of ``hidden_size`` to pass to ``Encoder``\n# --------------------------------------------------\n# the below code fragment can be found in:\n# ding/model/template/q_learning.py\n# --------------------------------------------------\n# \n# @MODEL_REGISTRY.register('c51dqn')\n# class C51DQN(nn.Module):\n# \n#     def __init__(\n#         self,\n#         obs_shape: Union[int, SequenceType],\n#         action_shape: Union[int, SequenceType],\n#         encoder_hidden_size_list: SequenceType = [128, 128, 64],\n#         head_hidden_size: int = None,\n#         head_layer_num: int = 1,\n#         activation: Optional[nn.Module] = nn.ReLU(),\n#         norm_type: Optional[str] = None,\n#         v_min: Optional[float] = -10,\n#         v_max: Optional[float] = 10,\n#         n_atom: Optional[int] = 51,\n#     ) -> None:\n#         r\"\"\"\n#         Overview:\n#             Init the C51 Model according to input arguments.\n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\n tau (:obj:`torch.Tensor`): tau tensor of size ``(B, N, 1)``\n        Shapes:\n            - x (:obj:`torch.Tensor`): :math:`(B, N)`, where B is batch size and N is head_hidden_size.\n            - logit (:obj:`torch.FloatTensor`): :math:`(B, M)`, where M is action_shape.\n            - tau (:obj:`torch.Tensor`):  :math:`(B, M, 1)`\n\n        Examples:\n            >>> model = QRDQN(64, 64)\n            >>> inputs = torch.randn(4, 64)\n            >>> outputs = model(inputs)\n            >>> assert isinstance(outputs, dict)\n            >>> assert outputs['logit'].shape == torch.Size([4, 64])\n            >>> # default num_quantiles : int = 32\n            >>> assert outputs['q'].shape == torch.Size([4, 64, 32])\n            >>> assert outputs['tau'].shape == torch.Size([4, 32, 1])\n        \"\"\"\n        x = self.encoder(x)\n        x = self.head(x)\n        return x\n\n\n@MODEL_REGISTRY.register('iqn')\nclass IQN(nn.Module):\n\n    def __init__(\n            self,\n            obs_shape: Union[int, SequenceType],\n            action_shape: Union[int, SequenceType],\n            encoder_hidden_size_list: SequenceType = [128, 128, 64],\n            head_hidden_size: Optional[int] = None,\n            head_layer_num: int = 1,\n            num_quantiles: int = 32,\n            quantile_embedding_size: int = 128,\n            activation: Optional[nn.Module] = nn.ReLU(),\n            norm_type: Optional[str] = None\n    ) -> None:\n        r\"\"\"\n        Overview:\n            Init the IQN Model according to input arguments.\n        Arguments:\n            - obs_shape (:obj:`Union[int, SequenceType]`): Observation space shape.\n            - action_shape (:obj:`Union[int, SequenceType]`): Action space shape.\n            - encoder_hidden_size_list (:obj:`SequenceType`): Collection of ``hidden_size`` to pass to ``Encoder``\n            - head_hidden_size (:obj:`Optional[int]`): The ``hidden_size`` to pass to ``Head``.\n            - head_layer_num (:obj:`int`): The num of layers used in the network to compute Q value output\n            - num_quantiles (:obj:`int`): Number of quantiles in the prediction distribution.\n            - activation (:obj:`Optional[nn.Module]`):\n                The type of activation function to use in ``MLP`` the after ``layer_fn``,\n                if ``None`` then default set to ``nn.ReLU()``\n            - norm_type (:obj:`Optional[str]`):\n                The type of normalization to use, see ``ding.torch_utils.fc_block`` for more details.\n        \"\"\"\n        super(IQN, self).__init__()\n        # For compatibility: 1, (1, ), [4, 32, 32]\n        obs_shape, action_shape = squeeze(obs_shape), squeeze(action_shape)\n        if head_hidden_size is None:\n            head_hidden_size = encoder_hidden_size_list[-1]\n        # FC Encoder\n        if isinstance(obs_shape, int) or len(obs_shape) == 1:\n            self.encoder = FCEncoder(obs_shape, encoder_hidden_size_list, activation=activation, norm_type=norm_type)\n        # Conv Encoder\n        elif len(obs_shape) == 3:\n            self.encoder = ConvEncoder(obs_shape, encoder_hidden_size_list, activation=activation, norm_type=norm_type)\n        else:\n            raise RuntimeError(\n                \"not support obs_shape for pre-defined encoder: {}, please customize your own IQN\".format(obs_shape)\n            )\n        # Head Type\n        head_cls = QuantileHead\n        multi_head = not isinstance(action_shape, int)\n        if multi_head:\n            self.head = MultiHead(\n                head_cls,\n                head_hidden_size,\n                action_shape,\n                layer_num=head_layer_num,\n                num_quantiles=num_quantiles,\n                quantile_embedding_size=quantile_embedding_size,\n                activation=activation,\n                norm_type=norm_type\n            )\n        else:\n            self.head = head_cls(\n                head_hidden_size,\n                action_shape,\n                head_layer_num,\n                activation=activation,\n                norm_type=norm_type,\n                num_quantiles=num_quantiles,\n                quantile_embedding_size=quantile_embedding_size,\n            )\n\n    def forward(self, x: torch.Tensor) -> Dict:\n        r\"\"\"\n        Overview:\n            Use encoded embedding tensor to predict IQN's output.\n            Parameter updates with IQN's MLPs forward setup.\n        Arguments:\n            - x (:obj:`torch.Tensor`):\n                The encoded embedding tensor with ``(B, N=hidden_size)``.\n        Returns:\n            - outputs (:obj:`Dict`):\n                Run with encoder and head. Return the result prediction dictionary.\n\n        ReturnsKeys:\n            - logit (:obj:`torch.Tensor`): Logit tensor with same size as input ``x``.\n            - q (:obj:`torch.Tensor`): Q valye tensor tensor of size ``(num_quantiles, N, B)``\n            - quantiles (:obj:`torch.Tensor`): quantiles tensor of size ``(quantile_embedding_size, 1)``\n        Shapes:\n            - x (:obj:`torch.Tensor`): :math:`(B, N)`, where B is batch size and N is head_hidden_size.\n            - logit (:obj:`torch.FloatTensor`): :math:`(B, M)`, where M is action_shape\n            - quantiles (:obj:`torch.Tensor`):  :math:`(P, 1)`, where P is quantile_embedding_size.\n        Examples:\n            >>> model = IQN(64, 64) # arguments: 'obs_shape' and 'action_shape'\n            >>> inputs = torch.randn(4, 64)\n            >>> outputs = model(inputs)\n            >>> assert isinstance(outputs, dict)\n            >>> assert outputs['logit'].shape == torch.Size([4, 64])\n            >>> # default num_quantiles: int = 32\n            >>> assert outputs['q'].shape == torch.Size([32, 4, 64]\n            >>> # default quantile_embedding_size: int = 128\n            >>> assert outputs['quantiles'].shape == torch.Size([128, 1])\n        \"\"\"\n        x = self.encoder(x)\n        x = self.head(x)\n        return x\n\n\n@MODEL_REGISTRY.register('rainbowdqn')\nclass RainbowDQN(nn.Module):\n    \"\"\"\n    Overview:\n        RainbowDQN network (C51 + Dueling + Noisy Block)\n\n    .. note::\n        RainbowDQN contains dueling architecture by default\n    \"\"\"\n\n    def __init__(\n        self,\n        obs_shape: Union[int, SequenceType],\n        action_shape: Union[int, SequenceType],\n        encoder_hidden_size_list: SequenceType = [128, 128, 64],\n        head_hidden_size: Optional[int] = None,\n        head_layer_num: int = 1,\n        activation: Optional[nn.Module] = nn.ReLU(),\n        norm_type: Optional[str] = None,\n        v_min: Optional[float] = -10,\n        v_max: Optional[float] = 10,\n        n_atom: Optional[int] = 51,\n    ) -> None:\n        \"\"\"\n        Overview:", "metadata": {"task_id": "opendilab_ACE/8", "ground_truth": "            Init the Rainbow Model according to arguments.", "fpath_tuple": ["opendilab_ACE", "ding", "model", "template", "q_learning.py"], "context_start_lineno": 289, "line_no": 442, "query_window": {"context": "\n    .. note::\n        RainbowDQN contains dueling architecture by default\n    \"\"\"\n\n    def __init__(\n        self,\n        obs_shape: Union[int, SequenceType],\n        action_shape: Union[int, SequenceType],\n        encoder_hidden_size_list: SequenceType = [128, 128, 64],\n        head_hidden_size: Optional[int] = None,\n        head_layer_num: int = 1,\n        activation: Optional[nn.Module] = nn.ReLU(),\n        norm_type: Optional[str] = None,\n        v_min: Optional[float] = -10,\n        v_max: Optional[float] = 10,\n        n_atom: Optional[int] = 51,\n    ) -> None:\n        \"\"\"\n        Overview:", "metadata": {"fpath_tuple": ["opendilab_ACE", "ding", "model", "template", "q_learning.py"], "line_no": 442, "task_id": "opendilab_ACE/8", "start_line_no": 422, "end_line_no": 442, "window_size": 20, "context_start_lineno": 289, "repo": "opendilab_ACE"}}, "top_k_context": [{"context": "        return x\n\n\n@MODEL_REGISTRY.register('c51dqn')\nclass C51DQN(nn.Module):\n\n    def __init__(\n        self,\n        obs_shape: Union[int, SequenceType],\n        action_shape: Union[int, SequenceType],\n        encoder_hidden_size_list: SequenceType = [128, 128, 64],\n        head_hidden_size: int = None,\n        head_layer_num: int = 1,\n        activation: Optional[nn.Module] = nn.ReLU(),\n        norm_type: Optional[str] = None,\n        v_min: Optional[float] = -10,\n        v_max: Optional[float] = 10,\n        n_atom: Optional[int] = 51,\n    ) -> None:\n        r\"\"\"", "metadata": [{"fpath_tuple": ["opendilab_ACE", "ding", "model", "template", "q_learning.py"], "line_no": 106, "start_line_no": 96, "end_line_no": 116, "window_size": 20, "repo": "opendilab_ACE", "slice_size": 10}], "sim_score": 0.6804123711340206}, {"context": "class C51DQN(nn.Module):\n\n    def __init__(\n        self,\n        obs_shape: Union[int, SequenceType],\n        action_shape: Union[int, SequenceType],\n        encoder_hidden_size_list: SequenceType = [128, 128, 64],\n        head_hidden_size: int = None,\n        head_layer_num: int = 1,\n        activation: Optional[nn.Module] = nn.ReLU(),\n        norm_type: Optional[str] = None,\n        v_min: Optional[float] = -10,\n        v_max: Optional[float] = 10,\n        n_atom: Optional[int] = 51,\n    ) -> None:\n        r\"\"\"\n        Overview:\n            Init the C51 Model according to input arguments.\n        Arguments:\n            - obs_shape (:obj:`Union[int, SequenceType]`): Observation's space.", "metadata": [{"fpath_tuple": ["opendilab_ACE", "ding", "model", "template", "q_learning.py"], "line_no": 110, "start_line_no": 100, "end_line_no": 120, "window_size": 20, "repo": "opendilab_ACE", "slice_size": 10}], "sim_score": 0.6568627450980392}, {"context": "\n@MODEL_REGISTRY.register('c51dqn')\nclass C51DQN(nn.Module):\n\n    def __init__(\n        self,\n        obs_shape: Union[int, SequenceType],\n        action_shape: Union[int, SequenceType],\n        encoder_hidden_size_list: SequenceType = [128, 128, 64],\n        head_hidden_size: int = None,\n        head_layer_num: int = 1,\n        activation: Optional[nn.Module] = nn.ReLU(),\n        norm_type: Optional[str] = None,\n        v_min: Optional[float] = -10,\n        v_max: Optional[float] = 10,\n        n_atom: Optional[int] = 51,\n    ) -> None:\n        r\"\"\"\n        Overview:\n            Init the C51 Model according to input arguments.", "metadata": [{"fpath_tuple": ["opendilab_ACE", "ding", "model", "template", "q_learning.py"], "line_no": 108, "start_line_no": 98, "end_line_no": 118, "window_size": 20, "repo": "opendilab_ACE", "slice_size": 10}], "sim_score": 0.6504854368932039}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# templates/{{ cookiecutter.module_slug }}/{{ cookiecutter.module_slug }}.py\n# --------------------------------------------------\n# # TODO: Add description of the module here\n# _DESCRIPTION = \"\"\"\\\n# This new module is designed to solve this great ML task and is crafted with a lot of care.\n# \"\"\"\n# \n# \n# # TODO: Add description of the arguments of the module here\n# _KWARGS_DESCRIPTION = \"\"\"\n# Calculates how good are predictions given some references, using certain scores\n# Args:\n#     predictions: list of predictions to score. Each predictions\n#         should be a string with tokens separated by spaces.\n#     references: list of reference for each prediction. Each\n#         reference should be a string with tokens separated by spaces.\n# Returns:\n#     accuracy: description of the first score,\n#     another_score: description of the second score,\n# Examples:\n#     Examples should be written in doctest format, and should illustrate how\n#     to use the function.\n# --------------------------------------------------\n# the below code fragment can be found in:\n# templates/{{ cookiecutter.module_slug }}/{{ cookiecutter.module_slug }}.py\n# --------------------------------------------------\n# This new module is designed to solve this great ML task and is crafted with a lot of care.\n# \"\"\"\n# \n# \n# # TODO: Add description of the arguments of the module here\n# _KWARGS_DESCRIPTION = \"\"\"\n# Calculates how good are predictions given some references, using certain scores\n# Args:\n#     predictions: list of predictions to score. Each predictions\n#         should be a string with tokens separated by spaces.\n#     references: list of reference for each prediction. Each\n#         reference should be a string with tokens separated by spaces.\n# Returns:\n#     accuracy: description of the first score,\n#     another_score: description of the second score,\n# Examples:\n#     Examples should be written in doctest format, and should illustrate how\n#     to use the function.\n# \n#     >>> my_new_module = evaluate.load(\"my_new_module\")\n# --------------------------------------------------\n# the below code fragment can be found in:\n# templates/{{ cookiecutter.module_slug }}/{{ cookiecutter.module_slug }}.py\n# --------------------------------------------------\n# \"\"\"\n# \n# # TODO: Add description of the module here\n# _DESCRIPTION = \"\"\"\\\n# This new module is designed to solve this great ML task and is crafted with a lot of care.\n# \"\"\"\n# \n# \n# # TODO: Add description of the arguments of the module here\n# _KWARGS_DESCRIPTION = \"\"\"\n# Calculates how good are predictions given some references, using certain scores\n# Args:\n#     predictions: list of predictions to score. Each predictions\n#         should be a string with tokens separated by spaces.\n#     references: list of reference for each prediction. Each\n#         reference should be a string with tokens separated by spaces.\n# Returns:\n#     accuracy: description of the first score,\n#     another_score: description of the second score,\n# Examples:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# templates/{{ cookiecutter.module_slug }}/{{ cookiecutter.module_slug }}.py\n# --------------------------------------------------\n# year={2020}\n# }\n# \"\"\"\n# \n# # TODO: Add description of the module here\n# _DESCRIPTION = \"\"\"\\\n# This new module is designed to solve this great ML task and is crafted with a lot of care.\n# \"\"\"\n# \n# \n# # TODO: Add description of the arguments of the module here\n# _KWARGS_DESCRIPTION = \"\"\"\n# Calculates how good are predictions given some references, using certain scores\n# Args:\n#     predictions: list of predictions to score. Each predictions\n#         should be a string with tokens separated by spaces.\n#     references: list of reference for each prediction. Each\n#         reference should be a string with tokens separated by spaces.\n# Returns:\n#     accuracy: description of the first score,\n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\n# Copyright 2020 The HuggingFace Evaluate Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\" ROUGE metric from Google Research github repo. \"\"\"\n\n# The dependencies in https://github.com/google-research/google-research/blob/master/rouge/requirements.txt\nimport absl  # Here to have a nice missing dependency error message early on\nimport datasets\nimport nltk  # Here to have a nice missing dependency error message early on\nimport numpy  # Here to have a nice missing dependency error message early on\nimport six  # Here to have a nice missing dependency error message early on\nfrom rouge_score import rouge_scorer, scoring\n\nimport evaluate\n\n\n_CITATION = \"\"\"\\\n@inproceedings{lin-2004-rouge,\n    title = \"{ROUGE}: A Package for Automatic Evaluation of Summaries\",\n    author = \"Lin, Chin-Yew\",\n    booktitle = \"Text Summarization Branches Out\",\n    month = jul,\n    year = \"2004\",\n    address = \"Barcelona, Spain\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://www.aclweb.org/anthology/W04-1013\",\n    pages = \"74--81\",\n}\n\"\"\"\n\n_DESCRIPTION = \"\"\"\\\nROUGE, or Recall-Oriented Understudy for Gisting Evaluation, is a set of metrics and a software package used for\nevaluating automatic summarization and machine translation software in natural language processing.\nThe metrics compare an automatically produced summary or translation against a reference or a set of references (human-produced) summary or translation.\n\nNote that ROUGE is case insensitive, meaning that upper case letters are treated the same way as lower case letters.\n\nThis metrics is a wrapper around Google Research reimplementation of ROUGE:\nhttps://github.com/google-research/google-research/tree/master/rouge\n\"\"\"\n\n_KWARGS_DESCRIPTION = \"\"\"\nCalculates average rouge scores for a list of hypotheses and references\nArgs:\n    predictions: list of predictions to score. Each prediction\n        should be a string with tokens separated by spaces.\n    references: list of reference for each prediction. Each\n        reference should be a string with tokens separated by spaces.\n    rouge_types: A list of rouge types to calculate.\n        Valid names:\n        `\"rouge{n}\"` (e.g. `\"rouge1\"`, `\"rouge2\"`) where: {n} is the n-gram based scoring,\n        `\"rougeL\"`: Longest common subsequence based scoring.\n        `\"rougeLsum\"`: rougeLsum splits text using `\"\\n\"`.\n        See details in https://github.com/huggingface/datasets/issues/617\n    use_stemmer: Bool indicating whether Porter stemmer should be used to strip word suffixes.\n    use_aggregator: Return aggregates if this is set to True\nReturns:\n    rouge1: rouge_1 (f1),\n    rouge2: rouge_2 (f1),\n    rougeL: rouge_l (f1),", "metadata": {"task_id": "huggingface_evaluate/73", "ground_truth": "    rougeLsum: rouge_lsum (f1)", "fpath_tuple": ["huggingface_evaluate", "metrics", "rouge", "rouge.py"], "context_start_lineno": 0, "line_no": 70, "query_window": {"context": "\n_KWARGS_DESCRIPTION = \"\"\"\nCalculates average rouge scores for a list of hypotheses and references\nArgs:\n    predictions: list of predictions to score. Each prediction\n        should be a string with tokens separated by spaces.\n    references: list of reference for each prediction. Each\n        reference should be a string with tokens separated by spaces.\n    rouge_types: A list of rouge types to calculate.\n        Valid names:\n        `\"rouge{n}\"` (e.g. `\"rouge1\"`, `\"rouge2\"`) where: {n} is the n-gram based scoring,\n        `\"rougeL\"`: Longest common subsequence based scoring.\n        `\"rougeLsum\"`: rougeLsum splits text using `\"\\n\"`.\n        See details in https://github.com/huggingface/datasets/issues/617\n    use_stemmer: Bool indicating whether Porter stemmer should be used to strip word suffixes.\n    use_aggregator: Return aggregates if this is set to True\nReturns:\n    rouge1: rouge_1 (f1),\n    rouge2: rouge_2 (f1),\n    rougeL: rouge_l (f1),", "metadata": {"fpath_tuple": ["huggingface_evaluate", "metrics", "rouge", "rouge.py"], "line_no": 70, "task_id": "huggingface_evaluate/73", "start_line_no": 50, "end_line_no": 70, "window_size": 20, "context_start_lineno": 0, "repo": "huggingface_evaluate"}}, "top_k_context": [{"context": "title = {A great new module},\nauthors={huggingface, Inc.},\nyear={2020}\n}\n\"\"\"\n\n# TODO: Add description of the module here\n_DESCRIPTION = \"\"\"\\\nThis new module is designed to solve this great ML task and is crafted with a lot of care.\n\"\"\"\n\n\n# TODO: Add description of the arguments of the module here\n_KWARGS_DESCRIPTION = \"\"\"\nCalculates how good are predictions given some references, using certain scores\nArgs:\n    predictions: list of predictions to score. Each predictions\n        should be a string with tokens separated by spaces.\n    references: list of reference for each prediction. Each\n        reference should be a string with tokens separated by spaces.", "metadata": [{"fpath_tuple": ["huggingface_evaluate", "templates", "{{ cookiecutter.module_slug }}", "{{ cookiecutter.module_slug }}.py"], "line_no": 32, "start_line_no": 22, "end_line_no": 42, "window_size": 20, "repo": "huggingface_evaluate", "slice_size": 10}], "sim_score": 0.31097560975609756}, {"context": "year={2020}\n}\n\"\"\"\n\n# TODO: Add description of the module here\n_DESCRIPTION = \"\"\"\\\nThis new module is designed to solve this great ML task and is crafted with a lot of care.\n\"\"\"\n\n\n# TODO: Add description of the arguments of the module here\n_KWARGS_DESCRIPTION = \"\"\"\nCalculates how good are predictions given some references, using certain scores\nArgs:\n    predictions: list of predictions to score. Each predictions\n        should be a string with tokens separated by spaces.\n    references: list of reference for each prediction. Each\n        reference should be a string with tokens separated by spaces.\nReturns:\n    accuracy: description of the first score,", "metadata": [{"fpath_tuple": ["huggingface_evaluate", "templates", "{{ cookiecutter.module_slug }}", "{{ cookiecutter.module_slug }}.py"], "line_no": 34, "start_line_no": 24, "end_line_no": 44, "window_size": 20, "repo": "huggingface_evaluate", "slice_size": 10}], "sim_score": 0.2919254658385093}, {"context": "# TODO: Add description of the module here\n_DESCRIPTION = \"\"\"\\\nThis new module is designed to solve this great ML task and is crafted with a lot of care.\n\"\"\"\n\n\n# TODO: Add description of the arguments of the module here\n_KWARGS_DESCRIPTION = \"\"\"\nCalculates how good are predictions given some references, using certain scores\nArgs:\n    predictions: list of predictions to score. Each predictions\n        should be a string with tokens separated by spaces.\n    references: list of reference for each prediction. Each\n        reference should be a string with tokens separated by spaces.\nReturns:\n    accuracy: description of the first score,\n    another_score: description of the second score,\nExamples:\n    Examples should be written in doctest format, and should illustrate how\n    to use the function.", "metadata": [{"fpath_tuple": ["huggingface_evaluate", "templates", "{{ cookiecutter.module_slug }}", "{{ cookiecutter.module_slug }}.py"], "line_no": 38, "start_line_no": 28, "end_line_no": 48, "window_size": 20, "repo": "huggingface_evaluate", "slice_size": 10}], "sim_score": 0.2916666666666667}, {"context": "\"\"\"\n\n# TODO: Add description of the module here\n_DESCRIPTION = \"\"\"\\\nThis new module is designed to solve this great ML task and is crafted with a lot of care.\n\"\"\"\n\n\n# TODO: Add description of the arguments of the module here\n_KWARGS_DESCRIPTION = \"\"\"\nCalculates how good are predictions given some references, using certain scores\nArgs:\n    predictions: list of predictions to score. Each predictions\n        should be a string with tokens separated by spaces.\n    references: list of reference for each prediction. Each\n        reference should be a string with tokens separated by spaces.\nReturns:\n    accuracy: description of the first score,\n    another_score: description of the second score,\nExamples:", "metadata": [{"fpath_tuple": ["huggingface_evaluate", "templates", "{{ cookiecutter.module_slug }}", "{{ cookiecutter.module_slug }}.py"], "line_no": 36, "start_line_no": 26, "end_line_no": 46, "window_size": 20, "repo": "huggingface_evaluate", "slice_size": 10}], "sim_score": 0.2839506172839506}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# ding/utils/loader/tests/loader/test_number.py\n# --------------------------------------------------\n#             _loader(3.6)\n#         with pytest.raises(ValueError):\n#             _loader(4.0)\n# \n#     def test_interval_right_close(self):\n#         _loader = interval(right=3.5)\n#         assert _loader(0.5) == 0.5\n#         assert _loader(1.0) == 1.0\n#         assert _loader(1.5) == 1.5\n#         assert _loader(3.4) == 3.4\n#         assert _loader(3.499) == 3.499\n#         assert _loader(3.5) == 3.5\n#         with pytest.raises(ValueError):\n#             _loader(3.501)\n#         with pytest.raises(ValueError):\n#             _loader(3.6)\n#         with pytest.raises(ValueError):\n#             _loader(4.0)\n# \n#     def test_interval_right_close_eps(self):\n# --------------------------------------------------\n# the below code fragment can be found in:\n# ding/utils/loader/tests/loader/test_mapping.py\n# --------------------------------------------------\n#         assert _loader({'a': 1, 'b': 2, 'sum': 3, 'sdk': 4}) == {'a', 'b', 'sum', 'sdk'}\n# \n#     def test_mpvalues(self):\n#         _loader = mpvalues()\n#         assert _loader({'a': 1, 'b': 2, 'sum': 3, 'sdk': 4}) == {1, 2, 3, 4}\n# \n#     def test_mpitems(self):\n#         _loader = mpitems()\n#         assert _loader({'a': 1, 'b': 2, 'sum': 3, 'sdk': 4}) == {('a', 1), ('b', 2), ('sum', 3), ('sdk', 4)}\n# \n#     def test_item(self):\n#         _loader = item('a') | item('b')\n#         assert _loader({'a': 1}) == 1\n#         assert _loader({'b': 2}) == 2\n#         assert _loader({'a': 3, 'b': -2}) == 3\n# \n#     def test_item_or(self):\n#         _loader = item_or('a', 0)\n#         assert _loader({'a': 1}) == 1\n#         assert _loader({'b': 2}) == 0\n# --------------------------------------------------\n# the below code fragment can be found in:\n# ding/utils/loader/tests/loader/test_types.py\n# --------------------------------------------------\n#             return x * y\n# \n#         assert _loader(_func_1)(y=6) == 8\n#         assert _loader(_func_2)(y=6) == 12\n# \n#     def test_func_complex_case_1(self):\n#         _loader = fpartial(x=1) >> ((fcall(y=1) >> interval(0, None)) | (fcall(y=2) >> interval(None, 0) >> negative()))\n# \n#         def _func_1(x, y):\n#             return x + y\n# \n#         def _func_2(x, y):\n#             return 5 * x - 4 * y\n# \n#         def _func_3(x, y):\n#             return -5 * x - 4 * y\n# \n#         assert _loader(_func_1) == 2\n#         assert _loader(_func_2) == 1\n#         assert _loader(_func_3) == 13\n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\n1.1) == 1.1\n        assert _loader(1.5) == 1.5\n        assert _loader(3.4) == 3.4\n        assert _loader(3.499) == 3.499\n        assert _loader(3.5) == 3.5\n        assert _loader(3.501) == 3.501\n        with pytest.raises(ValueError):\n            _loader(3.6)\n        with pytest.raises(ValueError):\n            _loader(4.0)\n\n    def test_interval_both_close_open(self):\n        _loader = interval(1.0, 3.5, right_ok=False)\n        with pytest.raises(ValueError):\n            _loader(0.5)\n        with pytest.raises(ValueError):\n            _loader(0.9)\n        with pytest.raises(ValueError):\n            _loader(0.999)\n        assert _loader(1.0) == 1.0\n        assert _loader(1.001) == 1.001\n        assert _loader(1.1) == 1.1\n        assert _loader(1.5) == 1.5\n        assert _loader(3.4) == 3.4\n        assert _loader(3.499) == 3.499\n        with pytest.raises(ValueError):\n            _loader(3.5)\n        with pytest.raises(ValueError):\n            _loader(3.501)\n        with pytest.raises(ValueError):\n            _loader(3.6)\n        with pytest.raises(ValueError):\n            _loader(4.0)\n\n    # noinspection DuplicatedCode\n    def test_interval_both_close_open_eps(self):\n        _loader = interval(1.0, 3.5, right_ok=False, eps=0.01)\n        with pytest.raises(ValueError):\n            _loader(0.5)\n        with pytest.raises(ValueError):\n            _loader(0.9)\n        assert _loader(0.999) == 0.999\n        assert _loader(1.0) == 1.0\n        assert _loader(1.001) == 1.001\n        assert _loader(1.1) == 1.1\n        assert _loader(1.5) == 1.5\n        assert _loader(3.4) == 3.4\n        with pytest.raises(ValueError):\n            _loader(3.499)\n        with pytest.raises(ValueError):\n            _loader(3.5)\n        with pytest.raises(ValueError):\n            _loader(3.501)\n        with pytest.raises(ValueError):\n            _loader(3.6)\n        with pytest.raises(ValueError):\n            _loader(4.0)\n\n    # noinspection DuplicatedCode\n    def test_interval_both_close_close(self):\n        _loader = interval(1.0, 3.5)\n        with pytest.raises(ValueError):\n            _loader(0.5)\n        with pytest.raises(ValueError):\n            _loader(0.9)\n        with pytest.raises(ValueError):\n            _loader(0.999)\n        assert _loader(1.0) == 1.0\n        assert _loader(1.001) == 1.001\n        assert _loader(1.1) == 1.1\n        assert _loader(1.5) == 1.5\n        assert _loader(3.4) == 3.4\n        assert _loader(3.499) == 3.499\n        assert _loader(3.5) == 3.5\n        with pytest.raises(ValueError):\n            _loader(3.501)\n        with pytest.raises(ValueError):\n            _loader(3.6)\n        with pytest.raises(ValueError):\n            _loader(4.0)\n\n    # noinspection DuplicatedCode\n    def test_interval_both_close_close_eps(self):\n        _loader = interval(1.0, 3.5, eps=0.01)\n        with pytest.raises(ValueError):\n            _loader(0.5)\n        with pytest.raises(ValueError):\n            _loader(0.9)\n        assert _loader(0.999) == 0.999\n        assert _loader(1.0) == 1.0\n        assert _loader(1.001) == 1.001\n        assert _loader(1.1) == 1.1\n        assert _loader(1.5) == 1.5\n        assert _loader(3.4) == 3.4\n        assert _loader(3.499) == 3.499\n        assert _loader(3.5) == 3.5\n        assert _loader(3.501) == 3.501\n        with pytest.raises(ValueError):\n            _loader(3.6)\n        with pytest.raises(ValueError):\n            _loader(4.0)\n\n    def test_interval_invalid(self):\n        with pytest.raises(ValueError):\n            interval(1.0, 0.9)\n\n    def test_interval_complex_1(self):\n        _loader = float & (interval(1, 4, left_ok=False, right_ok=False) | interval(10.2, 13.4, eps=0.01))\n        with pytest.raises(ValueError):\n            _loader(0.9)\n        with pytest.raises(ValueError):\n            _loader(1.0)\n        assert _loader(1.1) == 1.1\n        with pytest.raises(TypeError):\n            _loader(2)\n        assert _loader(2.0) == 2.0\n        assert _loader(3.9) == 3.9\n        with pytest.raises(ValueError):\n            _loader(4.0)\n        with pytest.raises(ValueError):\n            _loader(4.1)\n        with pytest.raises(ValueError):\n            _loader(10.1)\n        assert _loader(10.199) == 10.199\n        assert _loader(10.2) == 10.2\n        with pytest.raises(TypeError):\n            _loader(11)\n        assert _loader(11.0) == 11.0\n        assert _loader(13.4) == 13.4\n        assert _loader(13.401) == 13.401\n        with pytest.raises(ValueError):\n            _loader(13.5)\n        with pytest.raises(TypeError):\n            _loader(None)\n        with pytest.raises(TypeError):\n            _loader('string')\n\n    def test_negative(self):\n        _loader = negative()\n        assert _loader(1) == -1\n        assert _loader(-2) == 2\n\n    def test_positive(self):\n        _loader = positive()\n        assert _loader(1) == 1\n        assert _loader(0) == 0\n        assert _loader(-1) == -1\n\n    def test_plus(self):\n        _loader = plus(1)\n        assert _loader(1) == 2\n        assert _loader(-2) == -1\n\n        _loader = plus(negative())\n        assert _loader(1) == 0\n        assert _loader(-2) == 0\n\n    def test_minus(self):\n        _loader = minus(2)\n        assert _loader(1) == -1\n        assert _loader(-2) == -4\n\n        _loader = minus(negative())\n        assert _loader(1) == 2\n        assert _loader(-2) == -4\n\n    def test_minus_with(self):\n        _loader = minus_with(2)\n        assert _loader(1) == 1\n        assert _loader(-2) == 4\n\n        _loader = minus_with(negative())\n        assert _loader(1) == -2\n        assert _loader(-2) == 4\n\n    def test_multi(self):\n        _loader = multi(2)\n        assert _loader(1) == 2\n        assert _loader(-2) == -4\n\n        _loader = multi(keep())\n        assert _loader(1) == 1\n        assert _loader(-2) == 4\n        assert _loader(-3) == 9\n\n    def test_divide(self):\n        _loader = divide(2)", "metadata": {"task_id": "opendilab_ACE/180", "ground_truth": "        assert _loader(1) == 0.5", "fpath_tuple": ["opendilab_ACE", "ding", "utils", "loader", "tests", "loader", "test_number.py"], "context_start_lineno": 370, "line_no": 557, "query_window": {"context": "        _loader = minus_with(2)\n        assert _loader(1) == 1\n        assert _loader(-2) == 4\n\n        _loader = minus_with(negative())\n        assert _loader(1) == -2\n        assert _loader(-2) == 4\n\n    def test_multi(self):\n        _loader = multi(2)\n        assert _loader(1) == 2\n        assert _loader(-2) == -4\n\n        _loader = multi(keep())\n        assert _loader(1) == 1\n        assert _loader(-2) == 4\n        assert _loader(-3) == 9\n\n    def test_divide(self):\n        _loader = divide(2)", "metadata": {"fpath_tuple": ["opendilab_ACE", "ding", "utils", "loader", "tests", "loader", "test_number.py"], "line_no": 557, "task_id": "opendilab_ACE/180", "start_line_no": 537, "end_line_no": 557, "window_size": 20, "context_start_lineno": 370, "repo": "opendilab_ACE"}}, "top_k_context": [{"context": "        def _func_1(x, y):\n            return x + y\n\n        def _func_2(x, y):\n            return 5 * x - 4 * y\n\n        def _func_3(x, y):\n            return -5 * x - 4 * y\n\n        assert _loader(_func_1) == 2\n        assert _loader(_func_2) == 1\n        assert _loader(_func_3) == 13", "metadata": [{"fpath_tuple": ["opendilab_ACE", "ding", "utils", "loader", "tests", "loader", "test_types.py"], "line_no": 108, "start_line_no": 98, "end_line_no": 110, "window_size": 20, "repo": "opendilab_ACE", "slice_size": 10}], "sim_score": 0.3877551020408163}, {"context": "\n    def test_item(self):\n        _loader = item('a') | item('b')\n        assert _loader({'a': 1}) == 1\n        assert _loader({'b': 2}) == 2\n        assert _loader({'a': 3, 'b': -2}) == 3\n\n    def test_item_or(self):\n        _loader = item_or('a', 0)\n        assert _loader({'a': 1}) == 1\n        assert _loader({'b': 2}) == 0", "metadata": [{"fpath_tuple": ["opendilab_ACE", "ding", "utils", "loader", "tests", "loader", "test_mapping.py"], "line_no": 52, "start_line_no": 42, "end_line_no": 53, "window_size": 20, "repo": "opendilab_ACE", "slice_size": 10}], "sim_score": 0.37735849056603776}, {"context": "            _loader(3.501)\n        with pytest.raises(ValueError):\n            _loader(3.6)\n        with pytest.raises(ValueError):\n            _loader(4.0)\n\n    def test_interval_right_close(self):\n        _loader = interval(right=3.5)\n        assert _loader(0.5) == 0.5\n        assert _loader(1.0) == 1.0\n        assert _loader(1.5) == 1.5\n        assert _loader(3.4) == 3.4\n        assert _loader(3.499) == 3.499\n        assert _loader(3.5) == 3.5\n        with pytest.raises(ValueError):\n            _loader(3.501)\n        with pytest.raises(ValueError):\n            _loader(3.6)\n        with pytest.raises(ValueError):\n            _loader(4.0)", "metadata": [{"fpath_tuple": ["opendilab_ACE", "ding", "utils", "loader", "tests", "loader", "test_number.py"], "line_no": 256, "start_line_no": 246, "end_line_no": 266, "window_size": 20, "repo": "opendilab_ACE", "slice_size": 10}], "sim_score": 0.3448275862068966}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# ding/worker/coordinator/comm_coordinator.py\n# --------------------------------------------------\n# \n#                 success, _, message, _ = self._operator_server.post_replicas_failed(\n#                     learners=list(learner_conn), collectors=list(collector_conn)\n#                 )\n#                 if success:\n#                     # do not update collector or learner instantly, update at /GET replicas\n#                     self._failed_collector_conn.clear()\n#                     self._failed_learner_conn.clear()\n#                 else:\n#                     self._logger.error(\"Failed to send failed list to server, message: {}\".format(message))\n# \n#             # get list from server\n#             success, _, message, data = self._operator_server.get_replicas()\n#             if success:\n#                 cur_collectors = data[\"collectors\"]\n#                 cur_learners = data[\"learners\"]\n#                 # self._logger.info(\"current list:\", cur_collectors, cur_learners)\n#                 self._update_connection_collector(cur_collectors)\n#                 self._update_connection_learner(cur_learners)\n#             else:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# ding/worker/coordinator/comm_coordinator.py\n# --------------------------------------------------\n#                     dns_name = replica_conn.split(\":\")[0]\n#                     pod_name_list = dns_name.split(\".\")[:-1]\n#                     pod_name = \".\".join(pod_name_list)\n#                     learner_conn.append(pod_name)\n# \n#                 success, _, message, _ = self._operator_server.post_replicas_failed(\n#                     learners=list(learner_conn), collectors=list(collector_conn)\n#                 )\n#                 if success:\n#                     # do not update collector or learner instantly, update at /GET replicas\n#                     self._failed_collector_conn.clear()\n#                     self._failed_learner_conn.clear()\n#                 else:\n#                     self._logger.error(\"Failed to send failed list to server, message: {}\".format(message))\n# \n#             # get list from server\n#             success, _, message, data = self._operator_server.get_replicas()\n#             if success:\n#                 cur_collectors = data[\"collectors\"]\n#                 cur_learners = data[\"learners\"]\n# --------------------------------------------------\n# the below code fragment can be found in:\n# ding/worker/coordinator/comm_coordinator.py\n# --------------------------------------------------\n#                     pod_name = \".\".join(pod_name_list)\n#                     learner_conn.append(pod_name)\n# \n#                 success, _, message, _ = self._operator_server.post_replicas_failed(\n#                     learners=list(learner_conn), collectors=list(collector_conn)\n#                 )\n#                 if success:\n#                     # do not update collector or learner instantly, update at /GET replicas\n#                     self._failed_collector_conn.clear()\n#                     self._failed_learner_conn.clear()\n#                 else:\n#                     self._logger.error(\"Failed to send failed list to server, message: {}\".format(message))\n# \n#             # get list from server\n#             success, _, message, data = self._operator_server.get_replicas()\n#             if success:\n#                 cur_collectors = data[\"collectors\"]\n#                 cur_learners = data[\"learners\"]\n#                 # self._logger.info(\"current list:\", cur_collectors, cur_learners)\n#                 self._update_connection_collector(cur_collectors)\n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\n) -> dict:\n        if len(self._learner_connection) == 0:\n            raise TaskFail(message='no connected learner', result={'message': 'no connected learner'})\n        name = task['name']\n        start_task = {}\n        for k, v in self._learner_connection.items():\n            start_task[k] = v.new_task({'name': name, 'task_info': task['task_info']})\n            start_task[k].start()\n        for k, v in start_task.items():\n            v.join()\n        task_status = [v.status for v in start_task.values()]\n        if any([s != TaskStatus.COMPLETED for s in task_status]):\n            # TODO(nyz) dynamic learner gpu add/remove\n            message = \"one of learner can't start_task\"\n            raise TaskFail(message=message, result={'message': message})\n        return {'message': 'learner task has started'}\n\n    def deal_with_get_data(self, task: dict) -> dict:\n        data_task = {}\n        for k, v in self._learner_connection.items():\n            data_task[k] = v.new_task({'name': task['name']})\n            data_task[k].start()\n        for k, v in data_task.items():\n            v.join()\n        # TODO deal with task fail\n        self._data_demand = {k: v.result for k, v in data_task.items()}\n        demand_list = list(self._data_demand.values())\n        # Merge data demand info by adding up all learners' demand batch size.\n        merged_demand = copy.deepcopy(demand_list[0])\n        merged_demand['batch_size'] = sum([d['batch_size'] for d in demand_list])\n        return merged_demand\n\n    def deal_with_learn(self, task: dict) -> dict:\n        learn_task = {}\n        merged_data = task['data']\n        # Split training data for each learner according to ``self._data_demand``.\n        split_data = []\n        start = 0\n        for item in self._data_demand.values():\n            end = item['batch_size'] + start\n            split_data.append(merged_data[start:end])\n            start = end\n        for (k, v), d in zip(self._learner_connection.items(), split_data):\n            learn_task[k] = v.new_task({'name': task['name'], 'data': d})\n            learn_task[k].start()\n        for k, v in learn_task.items():\n            v.join()\n        # TODO deal with task fail\n        info_list = [v.result for v in learn_task.values()]\n        # Merge learn info through ``merge_info`` method.\n        merged_info = self.merge_info(info_list)\n        return merged_info\n\n    @staticmethod\n    def merge_info(info: list) -> dict:\n        homogeneous_keys = ['learner_step', 'buffer_id', 'task_id', 'learner_done']\n        elem = info[0]\n        if elem is None:\n            return info\n        elif isinstance(elem, numbers.Integral) or isinstance(elem, str) or isinstance(elem, float):\n            return info\n        elif isinstance(elem, list) or isinstance(elem, tuple):\n            return list(reduce(lambda x, y: x + y, info))\n        elif isinstance(elem, dict):\n            ret = {}\n            for k in elem.keys():\n                if k in homogeneous_keys:\n                    ret[k] = elem[k]\n                else:\n                    ret[k] = LearnerAggregator.merge_info([e[k] for e in info])\n            return ret\n        else:\n            raise TypeError(\"not support type: {}\".format(type(elem)))\n\n    def _new_connection_learner(self, learner_id: str, learner_host: str, learner_port: int) -> None:\n        start_time = time.time()\n        conn = None\n        while time.time() - start_time <= self._max_retry_second and not self._end_flag:\n            try:\n                if conn is None or not conn.is_connected:\n                    conn = self._master.new_connection(learner_id, learner_host, learner_port)\n                    conn.connect()\n                    assert conn.is_connected\n                    self._learner_connection[learner_id] = conn\n                    self._world_size += 1\n                    break\n            except Exception as e:\n                self._logger.error(\n                    f\"learner({learner_id}) connection start error:\\n\" + ''.join(traceback.format_tb(e.__traceback__)) +\n                    repr(e) + '\\nAuto Retry...'\n                )\n                time.sleep(2)\n\n        if learner_id in self._learner_connection:\n            self._logger.info(f\"Succeed to connect to learner({learner_id})\")\n        else:\n            self._logger.info(f\"Fail to connect to learner({learner_id})\")\n            self._failed_learner_conn.add(learner_id)\n\n    def _update_connection_learner(self, cur_learners) -> None:\n        conn_learners = list(self._learner_connection.keys())\n        new_c = set(cur_learners) - set(conn_learners)\n        del_c = set(conn_learners) - (set(cur_learners) | self._failed_learner_conn)\n        # conns which have terminated in server side, clear up\n        self._failed_learner_conn = self._failed_learner_conn & set(cur_learners)\n\n        # connect to each new learner\n        for learner_id in new_c:\n            learner_host, learner_port = learner_id.split(':')\n            self._new_connection_learner(learner_id, learner_host, int(learner_port))\n\n        for learner_id in del_c:\n            if learner_id in conn_learners:\n                if self._connection_learner[learner_id].is_connected:\n                    conn = self._connection_learner.pop(learner_id)\n                    conn.disconnect()\n                    assert not conn.is_connected\n                else:\n                    # ignore the operation of disconnect, since the pod will be terminated by server,\n                    # just throw the connection\n                    self._connection_learner.pop(learner_id)\n\n    def _period_sync_with_server(self) -> None:\n        while not self._end_flag:\n            # First: send failed list to notify server which replicas are failed, then terminate such replicas.\n            if len(self._failed_learner_conn) > 0:\n                learner_conn = []\n                for replica_conn in self._failed_learner_conn:\n                    dns_name = replica_conn.split(\":\")[0]\n                    pod_name_list = dns_name.split(\".\")[:-1]\n                    pod_name = \".\".join(pod_name_list)\n                    if pod_name not in learner_conn:\n                        learner_conn.append(pod_name)\n                success, _, message, _ = self._operator_server.post_replicas_failed(learners=list(learner_conn))\n                if success:\n                    # do not update learner instantly, update at /GET replicas\n                    self._failed_learner_conn.clear()\n                else:\n                    self._logger.error(\"Failed to send failed list to server, message: {}\".format(message))\n\n            # get list from server\n            success, _, message, data = self._operator_server.get_replicas()\n            if success:\n                cur_learners = data[\"learners\"]\n                # self._logger.info(\"current list:\", cur_learners)\n                self._update_connection_learner(cur_learners)\n                self._init_conn_flag = self._init_conn_flag | True\n            else:", "metadata": {"task_id": "opendilab_ACE/196", "ground_truth": "                self._logger.error(\"Failed to sync with server, message: {}\".format(message))", "fpath_tuple": ["opendilab_ACE", "ding", "worker", "adapter", "learner_aggregator.py"], "context_start_lineno": 163, "line_no": 311, "query_window": {"context": "                    dns_name = replica_conn.split(\":\")[0]\n                    pod_name_list = dns_name.split(\".\")[:-1]\n                    pod_name = \".\".join(pod_name_list)\n                    if pod_name not in learner_conn:\n                        learner_conn.append(pod_name)\n                success, _, message, _ = self._operator_server.post_replicas_failed(learners=list(learner_conn))\n                if success:\n                    # do not update learner instantly, update at /GET replicas\n                    self._failed_learner_conn.clear()\n                else:\n                    self._logger.error(\"Failed to send failed list to server, message: {}\".format(message))\n\n            # get list from server\n            success, _, message, data = self._operator_server.get_replicas()\n            if success:\n                cur_learners = data[\"learners\"]\n                # self._logger.info(\"current list:\", cur_learners)\n                self._update_connection_learner(cur_learners)\n                self._init_conn_flag = self._init_conn_flag | True\n            else:", "metadata": {"fpath_tuple": ["opendilab_ACE", "ding", "worker", "adapter", "learner_aggregator.py"], "line_no": 311, "task_id": "opendilab_ACE/196", "start_line_no": 291, "end_line_no": 311, "window_size": 20, "context_start_lineno": 163, "repo": "opendilab_ACE"}}, "top_k_context": [{"context": "                    dns_name = replica_conn.split(\":\")[0]\n                    pod_name_list = dns_name.split(\".\")[:-1]\n                    pod_name = \".\".join(pod_name_list)\n                    learner_conn.append(pod_name)\n\n                success, _, message, _ = self._operator_server.post_replicas_failed(\n                    learners=list(learner_conn), collectors=list(collector_conn)\n                )\n                if success:\n                    # do not update collector or learner instantly, update at /GET replicas\n                    self._failed_collector_conn.clear()\n                    self._failed_learner_conn.clear()\n                else:\n                    self._logger.error(\"Failed to send failed list to server, message: {}\".format(message))\n\n            # get list from server\n            success, _, message, data = self._operator_server.get_replicas()\n            if success:\n                cur_collectors = data[\"collectors\"]\n                cur_learners = data[\"learners\"]", "metadata": [{"fpath_tuple": ["opendilab_ACE", "ding", "worker", "coordinator", "comm_coordinator.py"], "line_no": 494, "start_line_no": 484, "end_line_no": 504, "window_size": 20, "repo": "opendilab_ACE", "slice_size": 10}], "sim_score": 0.8037383177570093}, {"context": "                learner_conn = []\n                for replica_conn in self._failed_learner_conn:\n                    dns_name = replica_conn.split(\":\")[0]\n                    pod_name_list = dns_name.split(\".\")[:-1]\n                    pod_name = \".\".join(pod_name_list)\n                    learner_conn.append(pod_name)\n\n                success, _, message, _ = self._operator_server.post_replicas_failed(\n                    learners=list(learner_conn), collectors=list(collector_conn)\n                )\n                if success:\n                    # do not update collector or learner instantly, update at /GET replicas\n                    self._failed_collector_conn.clear()\n                    self._failed_learner_conn.clear()\n                else:\n                    self._logger.error(\"Failed to send failed list to server, message: {}\".format(message))\n\n            # get list from server\n            success, _, message, data = self._operator_server.get_replicas()\n            if success:", "metadata": [{"fpath_tuple": ["opendilab_ACE", "ding", "worker", "coordinator", "comm_coordinator.py"], "line_no": 492, "start_line_no": 482, "end_line_no": 502, "window_size": 20, "repo": "opendilab_ACE", "slice_size": 10}], "sim_score": 0.7592592592592593}, {"context": "                    pod_name = \".\".join(pod_name_list)\n                    learner_conn.append(pod_name)\n\n                success, _, message, _ = self._operator_server.post_replicas_failed(\n                    learners=list(learner_conn), collectors=list(collector_conn)\n                )\n                if success:\n                    # do not update collector or learner instantly, update at /GET replicas\n                    self._failed_collector_conn.clear()\n                    self._failed_learner_conn.clear()\n                else:\n                    self._logger.error(\"Failed to send failed list to server, message: {}\".format(message))\n\n            # get list from server\n            success, _, message, data = self._operator_server.get_replicas()\n            if success:\n                cur_collectors = data[\"collectors\"]\n                cur_learners = data[\"learners\"]\n                # self._logger.info(\"current list:\", cur_collectors, cur_learners)\n                self._update_connection_collector(cur_collectors)", "metadata": [{"fpath_tuple": ["opendilab_ACE", "ding", "worker", "coordinator", "comm_coordinator.py"], "line_no": 496, "start_line_no": 486, "end_line_no": 506, "window_size": 20, "repo": "opendilab_ACE", "slice_size": 10}], "sim_score": 0.7476635514018691}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# test/test_libs.py\n# --------------------------------------------------\n#             and from_pixels\n#             and (not torch.has_cuda or not torch.cuda.device_count())\n#         ):\n#             raise pytest.skip(\"no cuda device\")\n# \n#         env = GymEnv(\n#             env_name,\n#             frame_skip=frame_skip,\n#             from_pixels=from_pixels,\n#             pixels_only=pixels_only,\n#         )\n#         check_env_specs(env)\n# \n# \n# @implement_for(\"gym\", None, \"0.26\")\n# def _make_gym_environment(env_name):  # noqa: F811\n#     return gym.make(env_name)\n# \n# \n# @implement_for(\"gym\", \"0.26\", None)\n# --------------------------------------------------\n# the below code fragment can be found in:\n# torchrl/envs/libs/dm_control.py\n# --------------------------------------------------\n#         self.from_pixels = from_pixels\n#         self.pixels_only = pixels_only\n# \n#         if from_pixels:\n#             self._set_egl_device(self.device)\n#             self.render_kwargs = {\"camera_id\": camera_id}\n#             if render_kwargs is not None:\n#                 self.render_kwargs.update(render_kwargs)\n#             env = pixels.Wrapper(\n#                 env,\n#                 pixels_only=self.pixels_only,\n#                 render_kwargs=self.render_kwargs,\n#             )\n#         return env\n# \n#     def _make_specs(self, env: \"gym.Env\") -> None:  # noqa: F821\n#         # specs are defined when first called\n#         return\n# \n#     def _check_kwargs(self, kwargs: Dict):\n# --------------------------------------------------\n# the below code fragment can be found in:\n# test/test_libs.py\n# --------------------------------------------------\n#             env_name,\n#             frame_skip=frame_skip,\n#             from_pixels=from_pixels,\n#             pixels_only=pixels_only,\n#         )\n#         check_env_specs(env)\n# \n# \n# @implement_for(\"gym\", None, \"0.26\")\n# def _make_gym_environment(env_name):  # noqa: F811\n#     return gym.make(env_name)\n# \n# \n# @implement_for(\"gym\", \"0.26\", None)\n# def _make_gym_environment(env_name):  # noqa: F811\n#     return gym.make(env_name, render_mode=\"rgb_array\")\n# \n# \n# @pytest.mark.skipif(not _has_dmc, reason=\"no dm_control library found\")\n# @pytest.mark.parametrize(\"env_name,task\", [[\"cheetah\", \"run\"]])\n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\naction_encoding\n            else torch.long\n        )\n        return (\n            MultiDiscreteTensorSpec(spec.nvec, device=device, dtype=dtype)\n            if categorical_action_encoding\n            else MultiOneHotDiscreteTensorSpec(spec.nvec, device=device, dtype=dtype)\n        )\n    elif isinstance(spec, gym.spaces.Box):\n        shape = spec.shape\n        if not len(shape):\n            shape = torch.Size([1])\n        if dtype is None:\n            dtype = numpy_to_torch_dtype_dict[spec.dtype]\n        low = torch.tensor(spec.low, device=device, dtype=dtype)\n        high = torch.tensor(spec.high, device=device, dtype=dtype)\n        is_unbounded = low.isinf().all() and high.isinf().all()\n        return (\n            UnboundedContinuousTensorSpec(shape, device=device, dtype=dtype)\n            if is_unbounded\n            else BoundedTensorSpec(\n                low,\n                high,\n                shape,\n                dtype=dtype,\n                device=device,\n            )\n        )\n    elif isinstance(spec, (Dict,)):\n        spec_out = {}\n        for k in spec.keys():\n            spec_out[k] = _gym_to_torchrl_spec_transform(\n                spec[k],\n                device=device,\n                categorical_action_encoding=categorical_action_encoding,\n            )\n        return CompositeSpec(**spec_out)\n    elif isinstance(spec, gym.spaces.dict.Dict):\n        return _gym_to_torchrl_spec_transform(\n            spec.spaces,\n            device=device,\n            categorical_action_encoding=categorical_action_encoding,\n        )\n    else:\n        raise NotImplementedError(\n            f\"spec of type {type(spec).__name__} is currently unaccounted for\"\n        )\n\n\ndef _get_envs(to_dict=False) -> List:\n    envs = _get_gym_envs()\n    envs = list(envs)\n    envs = sorted(envs)\n    return envs\n\n\n@implement_for(\"gym\", None, \"0.26.0\")\ndef _get_gym_envs():  # noqa: F811\n    return gym.envs.registration.registry.env_specs.keys()\n\n\n@implement_for(\"gym\", \"0.26.0\", None)\ndef _get_gym_envs():  # noqa: F811\n    return gym.envs.registration.registry.keys()\n\n\ndef _get_gym():\n    if _has_gym:\n        return gym\n    else:\n        return None\n\n\ndef _is_from_pixels(env):\n    observation_spec = env.observation_space\n    if isinstance(observation_spec, (Dict,)):\n        if \"pixels\" in set(observation_spec.keys()):\n            return True\n    if isinstance(observation_spec, (gym.spaces.dict.Dict,)):\n        if \"pixels\" in set(observation_spec.spaces.keys()):\n            return True\n    elif (\n        isinstance(observation_spec, gym.spaces.Box)\n        and (observation_spec.low == 0).all()\n        and (observation_spec.high == 255).all()\n        and observation_spec.low.shape[-1] == 3\n        and observation_spec.low.ndim == 3\n    ):\n        return True\n    elif isinstance(env, PixelObservationWrapper):\n        return True\n    return False\n\n\nclass GymWrapper(GymLikeEnv):\n    \"\"\"OpenAI Gym environment wrapper.\n\n    Examples:\n        >>> env = gym.make(\"Pendulum-v0\")\n        >>> env = GymWrapper(env)\n        >>> td = env.rand_step()\n        >>> print(td)\n        >>> print(env.available_envs)\n\n    \"\"\"\n\n    git_url = \"https://github.com/openai/gym\"\n    libname = \"gym\"\n\n    def __init__(self, env=None, categorical_action_encoding=False, **kwargs):\n        if env is not None:\n            kwargs[\"env\"] = env\n        self._seed_calls_reset = None\n        self._categorical_action_encoding = categorical_action_encoding\n        super().__init__(**kwargs)\n\n    def _check_kwargs(self, kwargs: Dict):\n        if \"env\" not in kwargs:\n            raise TypeError(\"Could not find environment key 'env' in kwargs.\")\n        env = kwargs[\"env\"]\n        if not (hasattr(env, \"action_space\") and hasattr(env, \"observation_space\")):\n            raise TypeError(\"env is not of type 'gym.Env'.\")\n\n    def _build_env(\n        self,\n        env,\n        from_pixels: bool = False,\n        pixels_only: bool = False,\n    ) -> \"gym.core.Env\":\n        env_from_pixels = _is_from_pixels(env)\n        from_pixels = from_pixels or env_from_pixels\n        self.from_pixels = from_pixels\n        self.pixels_only = pixels_only\n        if from_pixels and not env_from_pixels:\n            if isinstance(env, PixelObservationWrapper):\n                raise TypeError(\n                    \"PixelObservationWrapper cannot be used to wrap an environment\"\n                    \"that is already a PixelObservationWrapper instance.\"\n                )\n            env = self._build_gym_env(env, pixels_only)\n        return env\n\n    @implement_for(\"gym\", None, \"0.26.0\")\n    def _build_gym_env(self, env, pixels_only):  # noqa: F811\n        return PixelObservationWrapper(env, pixels_only=pixels_only)\n\n    @implement_for(\"gym\", \"0.26.0\", None)\n    def _build_gym_env(self, env, pixels_only):  # noqa: F811\n        from gym.wrappers.compatibility import EnvCompatibility\n\n        if env.render_mode:\n            return PixelObservationWrapper(env, pixels_only=pixels_only)\n\n        warnings.warn(\n            \"Environments provided to GymWrapper that need to be wrapped in PixelObservationWrapper \"\n            \"should be created with `gym.make(env_name, render_mode=mode)` where possible,\"\n            'where mode is either \"rgb_array\" or any other supported mode.'\n        )\n        # resetting as 0.26 comes with a very 'nice' OrderEnforcing wrapper\n        env = EnvCompatibility(env)\n        env.reset()\n        return LegacyPixelObservationWrapper(env, pixels_only=pixels_only)\n\n    @_classproperty\n    def available_envs(cls) -> List[str]:\n        return _get_envs()\n\n    @property\n    def lib(self) -> ModuleType:\n        return gym\n\n    def _set_seed(self, seed: int) -> int:  # noqa: F811\n        if self._seed_calls_reset is None:\n            # Determine basing on gym version whether `reset` is called when setting seed.\n            self._set_seed_initial(seed)\n        elif self._seed_calls_reset:\n            self.reset(seed=seed)\n        else:\n            self._env.seed(seed=seed)\n\n        return seed\n\n    @implement_for(\"gym\", None, \"0.19.0\")\n    def _set_seed_initial(self, seed: int) -> None:  # noqa: F811\n        self._seed_calls_reset = False\n        self._env.seed(seed=seed)\n\n    @implement_for(\"gym\", \"0.19.0\", None)", "metadata": {"task_id": "pytorch_rl/131", "ground_truth": "    def _set_seed_initial(self, seed: int) -> None:  # noqa: F811", "fpath_tuple": ["pytorch_rl", "torchrl", "envs", "libs", "gym.py"], "context_start_lineno": 80, "line_no": 268, "query_window": {"context": "    def lib(self) -> ModuleType:\n        return gym\n\n    def _set_seed(self, seed: int) -> int:  # noqa: F811\n        if self._seed_calls_reset is None:\n            # Determine basing on gym version whether `reset` is called when setting seed.\n            self._set_seed_initial(seed)\n        elif self._seed_calls_reset:\n            self.reset(seed=seed)\n        else:\n            self._env.seed(seed=seed)\n\n        return seed\n\n    @implement_for(\"gym\", None, \"0.19.0\")\n    def _set_seed_initial(self, seed: int) -> None:  # noqa: F811\n        self._seed_calls_reset = False\n        self._env.seed(seed=seed)\n\n    @implement_for(\"gym\", \"0.19.0\", None)", "metadata": {"fpath_tuple": ["pytorch_rl", "torchrl", "envs", "libs", "gym.py"], "line_no": 268, "task_id": "pytorch_rl/131", "start_line_no": 248, "end_line_no": 268, "window_size": 20, "context_start_lineno": 80, "repo": "pytorch_rl"}}, "top_k_context": [{"context": "\n        env = GymEnv(\n            env_name,\n            frame_skip=frame_skip,\n            from_pixels=from_pixels,\n            pixels_only=pixels_only,\n        )\n        check_env_specs(env)\n\n\n@implement_for(\"gym\", None, \"0.26\")\ndef _make_gym_environment(env_name):  # noqa: F811\n    return gym.make(env_name)\n\n\n@implement_for(\"gym\", \"0.26\", None)\ndef _make_gym_environment(env_name):  # noqa: F811\n    return gym.make(env_name, render_mode=\"rgb_array\")\n\n", "metadata": [{"fpath_tuple": ["pytorch_rl", "test", "test_libs.py"], "line_no": 146, "start_line_no": 136, "end_line_no": 156, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}], "sim_score": 0.3465346534653465}, {"context": "        **kwargs,\n    ):\n        self.from_pixels = from_pixels\n        self.pixels_only = pixels_only\n\n        if from_pixels:\n            self._set_egl_device(self.device)\n            self.render_kwargs = {\"camera_id\": camera_id}\n            if render_kwargs is not None:\n                self.render_kwargs.update(render_kwargs)\n            env = pixels.Wrapper(\n                env,\n                pixels_only=self.pixels_only,\n                render_kwargs=self.render_kwargs,\n            )\n        return env\n\n    def _make_specs(self, env: \"gym.Env\") -> None:  # noqa: F821\n        # specs are defined when first called\n        return", "metadata": [{"fpath_tuple": ["pytorch_rl", "torchrl", "envs", "libs", "dm_control.py"], "line_no": 152, "start_line_no": 142, "end_line_no": 162, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}], "sim_score": 0.3302752293577982}, {"context": "        elif (\n            env_name != PONG_VERSIONED\n            and from_pixels\n            and (not torch.has_cuda or not torch.cuda.device_count())\n        ):\n            raise pytest.skip(\"no cuda device\")\n\n        env = GymEnv(\n            env_name,\n            frame_skip=frame_skip,\n            from_pixels=from_pixels,\n            pixels_only=pixels_only,\n        )\n        check_env_specs(env)\n\n\n@implement_for(\"gym\", None, \"0.26\")\ndef _make_gym_environment(env_name):  # noqa: F811\n    return gym.make(env_name)\n", "metadata": [{"fpath_tuple": ["pytorch_rl", "test", "test_libs.py"], "line_no": 140, "start_line_no": 130, "end_line_no": 150, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}], "sim_score": 0.3220338983050847}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# fortuna/training/mixin.py\n# --------------------------------------------------\n#         prefix: str = \"checkpoint_\",\n#     ) -> None:\n#         if save_checkpoint_dir:\n#             checkpoints.save_checkpoint(\n#                 ckpt_dir=str(save_checkpoint_dir),\n#                 target=state,\n#                 step=state.step,\n#                 prefix=prefix,\n#                 keep=keep,\n#                 overwrite=force_save,\n#             )\n# \n#     def restore_checkpoint(\n#         self,\n#         restore_checkpoint_path: Path,\n#         optimizer: Optional[OptaxOptimizer] = None,\n#         prefix: str = \"checkpoint_\",\n#         name_to_train_state: NameToTrainState = NameToTrainState,\n#         **kwargs,\n#     ) -> TrainState:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# fortuna/prob_model/posterior/deep_ensemble/deep_ensemble_repositories.py\n# --------------------------------------------------\n#         def _put(_i):\n#             return self.state[_i].put(\n#                 state=state, checkpoint_path=checkpoint_path, keep=keep, prefix=prefix\n#             )\n# \n#         if i is not None:\n#             _put(i)\n#         else:\n#             for i in range(self.ensemble_size):\n#                 state.append(_put(i))\n# \n#     def pull(\n#         self,\n#         i: int = None,\n#         checkpoint_path: Path = None,\n#         optimizer: Optional[OptaxOptimizer] = None,\n#         prefix: str = \"checkpoint_\",\n#         **kwargs\n#     ) -> Union[DeepEnsembleState, PosteriorState]:\n#         def _pull(_i):\n# --------------------------------------------------\n# the below code fragment can be found in:\n# fortuna/training/mixin.py\n# --------------------------------------------------\n#         if save_checkpoint_dir:\n#             checkpoints.save_checkpoint(\n#                 ckpt_dir=str(save_checkpoint_dir),\n#                 target=state,\n#                 step=state.step,\n#                 prefix=prefix,\n#                 keep=keep,\n#                 overwrite=force_save,\n#             )\n# \n#     def restore_checkpoint(\n#         self,\n#         restore_checkpoint_path: Path,\n#         optimizer: Optional[OptaxOptimizer] = None,\n#         prefix: str = \"checkpoint_\",\n#         name_to_train_state: NameToTrainState = NameToTrainState,\n#         **kwargs,\n#     ) -> TrainState:\n#         if not os.path.isdir(restore_checkpoint_path) and not os.path.isfile(\n#             restore_checkpoint_path\n# --------------------------------------------------\n# the below code fragment can be found in:\n# fortuna/training/mixin.py\n# --------------------------------------------------\n#         keep: int = 1,\n#         force_save: bool = False,\n#         prefix: str = \"checkpoint_\",\n#     ) -> None:\n#         if save_checkpoint_dir:\n#             checkpoints.save_checkpoint(\n#                 ckpt_dir=str(save_checkpoint_dir),\n#                 target=state,\n#                 step=state.step,\n#                 prefix=prefix,\n#                 keep=keep,\n#                 overwrite=force_save,\n#             )\n# \n#     def restore_checkpoint(\n#         self,\n#         restore_checkpoint_path: Path,\n#         optimizer: Optional[OptaxOptimizer] = None,\n#         prefix: str = \"checkpoint_\",\n#         name_to_train_state: NameToTrainState = NameToTrainState,\n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\nimport os\nfrom copy import deepcopy\nfrom typing import Dict, List, Optional, Union\n\nfrom fortuna.training.mixin import WithCheckpointingMixin\nfrom fortuna.training.train_state import TrainState\nfrom fortuna.typing import OptaxOptimizer, Path\n\n\nclass TrainStateRepository(WithCheckpointingMixin):\n    def __init__(self, checkpoint_dir: Optional[Path] = None):\n        super().__init__()\n        self.checkpoint_dir = checkpoint_dir\n        self.__state = None\n\n    def get(\n        self,\n        checkpoint_path: Optional[Path] = None,\n        optimizer: Optional[OptaxOptimizer] = None,\n        prefix: str = \"checkpoint_\",\n        **kwargs\n    ) -> Union[Dict, TrainState]:\n        if not checkpoint_path and not self.checkpoint_dir and not self.__state:\n            raise ValueError(\"No state available.\")\n        return (\n            self.restore_checkpoint(\n                restore_checkpoint_path=checkpoint_path or self.checkpoint_dir,\n                optimizer=optimizer,\n                prefix=prefix,\n                **kwargs\n            )\n            if checkpoint_path or self.checkpoint_dir\n            else deepcopy(self.__state)\n        )\n\n    def put(\n        self,\n        state: TrainState,\n        checkpoint_path: Optional[Path] = None,\n        keep: int = 1,\n        prefix: str = \"checkpoint_\",\n    ) -> None:\n        if checkpoint_path or self.checkpoint_dir:\n            self.save_checkpoint(\n                state=state,\n                save_checkpoint_dir=checkpoint_path or self.checkpoint_dir,\n                keep=keep,\n                force_save=True,\n                prefix=prefix,\n            )\n        else:\n            self.__state = state\n\n    def pull(\n        self,\n        checkpoint_path: Path = None,\n        optimizer: Optional[OptaxOptimizer] = None,\n        prefix: str = \"checkpoint_\",\n        **kwargs\n    ) -> TrainState:\n        state = self.get(\n            checkpoint_path=checkpoint_path,", "metadata": {"task_id": "awslabs_fortuna/83", "ground_truth": "            optimizer=optimizer,", "fpath_tuple": ["awslabs_fortuna", "fortuna", "training", "train_state_repository.py"], "context_start_lineno": 0, "line_no": 62, "query_window": {"context": "        if checkpoint_path or self.checkpoint_dir:\n            self.save_checkpoint(\n                state=state,\n                save_checkpoint_dir=checkpoint_path or self.checkpoint_dir,\n                keep=keep,\n                force_save=True,\n                prefix=prefix,\n            )\n        else:\n            self.__state = state\n\n    def pull(\n        self,\n        checkpoint_path: Path = None,\n        optimizer: Optional[OptaxOptimizer] = None,\n        prefix: str = \"checkpoint_\",\n        **kwargs\n    ) -> TrainState:\n        state = self.get(\n            checkpoint_path=checkpoint_path,", "metadata": {"fpath_tuple": ["awslabs_fortuna", "fortuna", "training", "train_state_repository.py"], "line_no": 62, "task_id": "awslabs_fortuna/83", "start_line_no": 42, "end_line_no": 62, "window_size": 20, "context_start_lineno": 0, "repo": "awslabs_fortuna"}}, "top_k_context": [{"context": "        state: TrainState,\n        save_checkpoint_dir: Path,\n        keep: int = 1,\n        force_save: bool = False,\n        prefix: str = \"checkpoint_\",\n    ) -> None:\n        if save_checkpoint_dir:\n            checkpoints.save_checkpoint(\n                ckpt_dir=str(save_checkpoint_dir),\n                target=state,\n                step=state.step,\n                prefix=prefix,\n                keep=keep,\n                overwrite=force_save,\n            )\n\n    def restore_checkpoint(\n        self,\n        restore_checkpoint_path: Path,\n        optimizer: Optional[OptaxOptimizer] = None,", "metadata": [{"fpath_tuple": ["awslabs_fortuna", "fortuna", "training", "mixin.py"], "line_no": 36, "start_line_no": 26, "end_line_no": 46, "window_size": 20, "repo": "awslabs_fortuna", "slice_size": 10}], "sim_score": 0.6388888888888888}, {"context": "        prefix: str = \"checkpoint_\",\n    ) -> None:\n        if save_checkpoint_dir:\n            checkpoints.save_checkpoint(\n                ckpt_dir=str(save_checkpoint_dir),\n                target=state,\n                step=state.step,\n                prefix=prefix,\n                keep=keep,\n                overwrite=force_save,\n            )\n\n    def restore_checkpoint(\n        self,\n        restore_checkpoint_path: Path,\n        optimizer: Optional[OptaxOptimizer] = None,\n        prefix: str = \"checkpoint_\",\n        name_to_train_state: NameToTrainState = NameToTrainState,\n        **kwargs,\n    ) -> TrainState:", "metadata": [{"fpath_tuple": ["awslabs_fortuna", "fortuna", "training", "mixin.py"], "line_no": 40, "start_line_no": 30, "end_line_no": 50, "window_size": 20, "repo": "awslabs_fortuna", "slice_size": 10}], "sim_score": 0.6351351351351351}, {"context": "        prefix: str = \"checkpoint_\",\n    ) -> None:\n        def _put(_i):\n            return self.state[_i].put(\n                state=state, checkpoint_path=checkpoint_path, keep=keep, prefix=prefix\n            )\n\n        if i is not None:\n            _put(i)\n        else:\n            for i in range(self.ensemble_size):\n                state.append(_put(i))\n\n    def pull(\n        self,\n        i: int = None,\n        checkpoint_path: Path = None,\n        optimizer: Optional[OptaxOptimizer] = None,\n        prefix: str = \"checkpoint_\",\n        **kwargs", "metadata": [{"fpath_tuple": ["awslabs_fortuna", "fortuna", "prob_model", "posterior", "deep_ensemble", "deep_ensemble_repositories.py"], "line_no": 62, "start_line_no": 52, "end_line_no": 72, "window_size": 20, "repo": "awslabs_fortuna", "slice_size": 10}], "sim_score": 0.5897435897435898}, {"context": "        keep: int = 1,\n        force_save: bool = False,\n        prefix: str = \"checkpoint_\",\n    ) -> None:\n        if save_checkpoint_dir:\n            checkpoints.save_checkpoint(\n                ckpt_dir=str(save_checkpoint_dir),\n                target=state,\n                step=state.step,\n                prefix=prefix,\n                keep=keep,\n                overwrite=force_save,\n            )\n\n    def restore_checkpoint(\n        self,\n        restore_checkpoint_path: Path,\n        optimizer: Optional[OptaxOptimizer] = None,\n        prefix: str = \"checkpoint_\",\n        name_to_train_state: NameToTrainState = NameToTrainState,", "metadata": [{"fpath_tuple": ["awslabs_fortuna", "fortuna", "training", "mixin.py"], "line_no": 38, "start_line_no": 28, "end_line_no": 48, "window_size": 20, "repo": "awslabs_fortuna", "slice_size": 10}], "sim_score": 0.5641025641025641}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# metrics/precision/precision.py\n# --------------------------------------------------\n# import evaluate\n# \n# \n# _DESCRIPTION = \"\"\"\n# Precision is the fraction of correctly labeled positive examples out of all of the examples that were labeled as positive. It is computed via the equation:\n# Precision = TP / (TP + FP)\n# where TP is the True positives (i.e. the examples correctly labeled as positive) and FP is the False positive examples (i.e. the examples incorrectly labeled as positive).\n# \"\"\"\n# \n# \n# _KWARGS_DESCRIPTION = \"\"\"\n# Args:\n#     predictions (`list` of `int`): Predicted class labels.\n#     references (`list` of `int`): Actual class labels.\n#     labels (`list` of `int`): The set of labels to include when `average` is not set to `'binary'`. If `average` is `None`, it should be the label order. Labels present in the data can be excluded, for example to calculate a multiclass average ignoring a majority negative class. Labels not present in the data will result in 0 components in a macro average. For multilabel targets, labels are column indices. By default, all labels in `predictions` and `references` are used in sorted order. Defaults to None.\n#     pos_label (`int`): The class to be considered the positive class, in the case where `average` is set to `binary`. Defaults to 1.\n#     average (`string`): This parameter is required for multiclass/multilabel targets. If set to `None`, the scores for each class are returned. Otherwise, this determines the type of averaging performed on the data. Defaults to `'binary'`.\n# \n#         - 'binary': Only report results for the class specified by `pos_label`. This is applicable only if the classes found in `predictions` and `references` are binary.\n#         - 'micro': Calculate metrics globally by counting the total true positives, false negatives and false positives.\n# --------------------------------------------------\n# the below code fragment can be found in:\n# metrics/f1/f1.py\n# --------------------------------------------------\n# import evaluate\n# \n# \n# _DESCRIPTION = \"\"\"\n# The F1 score is the harmonic mean of the precision and recall. It can be computed with the equation:\n# F1 = 2 * (precision * recall) / (precision + recall)\n# \"\"\"\n# \n# \n# _KWARGS_DESCRIPTION = \"\"\"\n# Args:\n#     predictions (`list` of `int`): Predicted labels.\n#     references (`list` of `int`): Ground truth labels.\n#     labels (`list` of `int`): The set of labels to include when `average` is not set to `'binary'`, and the order of the labels if `average` is `None`. Labels present in the data can be excluded, for example to calculate a multiclass average ignoring a majority negative class. Labels not present in the data will result in 0 components in a macro average. For multilabel targets, labels are column indices. By default, all labels in `predictions` and `references` are used in sorted order. Defaults to None.\n#     pos_label (`int`): The class to be considered the positive class, in the case where `average` is set to `binary`. Defaults to 1.\n#     average (`string`): This parameter is required for multiclass/multilabel targets. If set to `None`, the scores for each class are returned. Otherwise, this determines the type of averaging performed on the data. Defaults to `'binary'`.\n# \n#         - 'binary': Only report results for the class specified by `pos_label`. This is applicable only if the classes found in `predictions` and `references` are binary.\n#         - 'micro': Calculate metrics globally by counting the total true positives, false negatives and false positives.\n#         - 'macro': Calculate metrics for each label, and find their unweighted mean. This does not take label imbalance into account.\n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\n# Copyright 2020 The HuggingFace Datasets Authors and the current dataset script contributor.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Recall metric.\"\"\"\n\nimport datasets\nfrom sklearn.metrics import recall_score\n\nimport evaluate\n\n\n_DESCRIPTION = \"\"\"\nRecall is the fraction of the positive examples that were correctly labeled by the model as positive. It can be computed with the equation:\nRecall = TP / (TP + FN)\nWhere TP is the true positives and FN is the false negatives.\n\"\"\"\n\n\n_KWARGS_DESCRIPTION = \"\"\"\nArgs:\n- **predictions** (`list` of `int`): The predicted labels.\n- **references** (`list` of `int`): The ground truth labels.\n- **labels** (`list` of `int`): The set of labels to include when `average` is not set to `binary`, and their order when average is `None`. Labels present in the data can be excluded in this input, for example to calculate a multiclass average ignoring a majority negative class, while labels not present in the data will result in 0 components in a macro average. For multilabel targets, labels are column indices. By default, all labels in y_true and y_pred are used in sorted order. Defaults to None.\n- **pos_label** (`int`): The class label to use as the 'positive class' when calculating the recall. Defaults to `1`.\n- **average** (`string`): This parameter is required for multiclass/multilabel targets. If None, the scores for each class are returned. Otherwise, this determines the type of averaging performed on the data. Defaults to `'binary'`.\n    - `'binary'`: Only report results for the class specified by `pos_label`. This is applicable only if the target labels and predictions are binary.", "metadata": {"task_id": "huggingface_evaluate/55", "ground_truth": "    - `'micro'`: Calculate metrics globally by counting the total true positives, false negatives, and false positives.", "fpath_tuple": ["huggingface_evaluate", "metrics", "recall", "recall.py"], "context_start_lineno": 0, "line_no": 36, "query_window": {"context": "from sklearn.metrics import recall_score\n\nimport evaluate\n\n\n_DESCRIPTION = \"\"\"\nRecall is the fraction of the positive examples that were correctly labeled by the model as positive. It can be computed with the equation:\nRecall = TP / (TP + FN)\nWhere TP is the true positives and FN is the false negatives.\n\"\"\"\n\n\n_KWARGS_DESCRIPTION = \"\"\"\nArgs:\n- **predictions** (`list` of `int`): The predicted labels.\n- **references** (`list` of `int`): The ground truth labels.\n- **labels** (`list` of `int`): The set of labels to include when `average` is not set to `binary`, and their order when average is `None`. Labels present in the data can be excluded in this input, for example to calculate a multiclass average ignoring a majority negative class, while labels not present in the data will result in 0 components in a macro average. For multilabel targets, labels are column indices. By default, all labels in y_true and y_pred are used in sorted order. Defaults to None.\n- **pos_label** (`int`): The class label to use as the 'positive class' when calculating the recall. Defaults to `1`.\n- **average** (`string`): This parameter is required for multiclass/multilabel targets. If None, the scores for each class are returned. Otherwise, this determines the type of averaging performed on the data. Defaults to `'binary'`.\n    - `'binary'`: Only report results for the class specified by `pos_label`. This is applicable only if the target labels and predictions are binary.", "metadata": {"fpath_tuple": ["huggingface_evaluate", "metrics", "recall", "recall.py"], "line_no": 36, "task_id": "huggingface_evaluate/55", "start_line_no": 16, "end_line_no": 36, "window_size": 20, "context_start_lineno": 0, "repo": "huggingface_evaluate"}}, "top_k_context": [{"context": "from sklearn.metrics import f1_score\n\nimport evaluate\n\n\n_DESCRIPTION = \"\"\"\nThe F1 score is the harmonic mean of the precision and recall. It can be computed with the equation:\nF1 = 2 * (precision * recall) / (precision + recall)\n\"\"\"\n\n\n_KWARGS_DESCRIPTION = \"\"\"\nArgs:\n    predictions (`list` of `int`): Predicted labels.\n    references (`list` of `int`): Ground truth labels.\n    labels (`list` of `int`): The set of labels to include when `average` is not set to `'binary'`, and the order of the labels if `average` is `None`. Labels present in the data can be excluded, for example to calculate a multiclass average ignoring a majority negative class. Labels not present in the data will result in 0 components in a macro average. For multilabel targets, labels are column indices. By default, all labels in `predictions` and `references` are used in sorted order. Defaults to None.\n    pos_label (`int`): The class to be considered the positive class, in the case where `average` is set to `binary`. Defaults to 1.\n    average (`string`): This parameter is required for multiclass/multilabel targets. If set to `None`, the scores for each class are returned. Otherwise, this determines the type of averaging performed on the data. Defaults to `'binary'`.\n\n        - 'binary': Only report results for the class specified by `pos_label`. This is applicable only if the classes found in `predictions` and `references` are binary.", "metadata": [{"fpath_tuple": ["huggingface_evaluate", "metrics", "f1", "f1.py"], "line_no": 26, "start_line_no": 16, "end_line_no": 36, "window_size": 20, "repo": "huggingface_evaluate", "slice_size": 10}], "sim_score": 0.6958762886597938}, {"context": "from sklearn.metrics import precision_score\n\nimport evaluate\n\n\n_DESCRIPTION = \"\"\"\nPrecision is the fraction of correctly labeled positive examples out of all of the examples that were labeled as positive. It is computed via the equation:\nPrecision = TP / (TP + FP)\nwhere TP is the True positives (i.e. the examples correctly labeled as positive) and FP is the False positive examples (i.e. the examples incorrectly labeled as positive).\n\"\"\"\n\n\n_KWARGS_DESCRIPTION = \"\"\"\nArgs:\n    predictions (`list` of `int`): Predicted class labels.\n    references (`list` of `int`): Actual class labels.\n    labels (`list` of `int`): The set of labels to include when `average` is not set to `'binary'`. If `average` is `None`, it should be the label order. Labels present in the data can be excluded, for example to calculate a multiclass average ignoring a majority negative class. Labels not present in the data will result in 0 components in a macro average. For multilabel targets, labels are column indices. By default, all labels in `predictions` and `references` are used in sorted order. Defaults to None.\n    pos_label (`int`): The class to be considered the positive class, in the case where `average` is set to `binary`. Defaults to 1.\n    average (`string`): This parameter is required for multiclass/multilabel targets. If set to `None`, the scores for each class are returned. Otherwise, this determines the type of averaging performed on the data. Defaults to `'binary'`.\n", "metadata": [{"fpath_tuple": ["huggingface_evaluate", "metrics", "precision", "precision.py"], "line_no": 26, "start_line_no": 16, "end_line_no": 36, "window_size": 20, "repo": "huggingface_evaluate", "slice_size": 10}], "sim_score": 0.6735751295336787}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# vizier/_src/algorithms/designers/random.py\n# --------------------------------------------------\n#       seed: Any valid seed for np.random.RandomState.\n#     \"\"\"\n#     if search_space.is_conditional:\n#       # TODO: Add conditional sampling case.\n#       raise ValueError(\n#           f'This designer {self} does not support conditional search.')\n# \n#     def create_input_converter(pc):\n#       return converters.DefaultModelInputConverter(\n#           pc, scale=True, max_discrete_indices=0, float_dtype=dtype)\n# \n#     self._converter = converters.DefaultTrialConverter(\n#         [create_input_converter(pc) for pc in search_space.parameters])\n# \n#     self._rng = np.random.RandomState(seed)\n# \n#   def update(self, _) -> None:\n#     pass\n# \n#   def suggest(self,\n# --------------------------------------------------\n# the below code fragment can be found in:\n# vizier/_src/algorithms/designers/random.py\n# --------------------------------------------------\n#       search_space: Must be a flat search space.\n#       dtype:\n#       seed: Any valid seed for np.random.RandomState.\n#     \"\"\"\n#     if search_space.is_conditional:\n#       # TODO: Add conditional sampling case.\n#       raise ValueError(\n#           f'This designer {self} does not support conditional search.')\n# \n#     def create_input_converter(pc):\n#       return converters.DefaultModelInputConverter(\n#           pc, scale=True, max_discrete_indices=0, float_dtype=dtype)\n# \n#     self._converter = converters.DefaultTrialConverter(\n#         [create_input_converter(pc) for pc in search_space.parameters])\n# \n#     self._rng = np.random.RandomState(seed)\n# \n#   def update(self, _) -> None:\n#     pass\n# --------------------------------------------------\n# the below code fragment can be found in:\n# vizier/_src/algorithms/designers/random.py\n# --------------------------------------------------\n#     if search_space.is_conditional:\n#       # TODO: Add conditional sampling case.\n#       raise ValueError(\n#           f'This designer {self} does not support conditional search.')\n# \n#     def create_input_converter(pc):\n#       return converters.DefaultModelInputConverter(\n#           pc, scale=True, max_discrete_indices=0, float_dtype=dtype)\n# \n#     self._converter = converters.DefaultTrialConverter(\n#         [create_input_converter(pc) for pc in search_space.parameters])\n# \n#     self._rng = np.random.RandomState(seed)\n# \n#   def update(self, _) -> None:\n#     pass\n# \n#   def suggest(self,\n#               count: Optional[int] = None) -> Sequence[vz.TrialSuggestion]:\n#     \"\"\"Make new suggestions.\n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\n inner <= half_m1:\n      sieve[outer + inner + (2 * outer * inner)] = 1\n      inner += 1\n  return [2 * i + 1 for i in range(1, half_m1 + 1) if sieve[i] == 0]\n\n\ndef _init_primes(num_dimensions: int) -> List[int]:\n  \"\"\"Get list of primes for all parameters in the search space.\"\"\"\n  primes = []\n  prime_attempts = 1\n  while len(primes) < num_dimensions + 1:\n    primes = _generate_primes(1000 * prime_attempts)\n    prime_attempts += 1\n  primes = primes[-num_dimensions - 1:-1]\n  return primes\n\n\n@attr.define(init=False, kw_only=True)\nclass _HaltonSequence(serializable.PartiallySerializable):\n  \"\"\"Encapsulates the generation of a scrambled halton sequence.\n\n  Specifically, this class is a fork inspired by the implementation of\n  scrambled Halton sequence of quasi-random numbers (by Google):\n\n  https://github.com/google/uncertainty-baselines/blob/main/uncertainty_baselines/halton.py\n\n  \"\"\"\n\n  _num_dimensions: int = attr.field(validator=attr.validators.instance_of(int))\n\n  _skip_points: int = attr.field(validator=attr.validators.instance_of(int))\n\n  _num_points_generated: int = attr.field(\n      validator=attr.validators.instance_of(int))\n\n  _primes: List[int] = attr.field(\n      init=True,\n      converter=list,\n      validator=attr.validators.deep_iterable(\n          member_validator=attr.validators.instance_of(int),\n          iterable_validator=attr.validators.instance_of(Iterable)))\n\n  _scramble: bool = attr.field(\n      default=False, validator=attr.validators.instance_of(bool), kw_only=True)\n\n  def __init__(self,\n               num_dimensions: int,\n               *,\n               skip_points: int,\n               num_points_generated: int = 0,\n               primes_override: Optional[List[int]] = None,\n               scramble: bool = False):\n    \"\"\"Create a Halton sequence generator.\n\n    Args:\n      num_dimensions: Number of dimensions for each point in the seqeunce. This\n        corresponds to the number of parameters in the Vizier search space.\n      skip_points: The number of initial points that should be skipped before\n        the first point is returned.\n      num_points_generated: Number of points that have already been generated.\n      primes_override: If supplied, use these primes to seed each dimension of\n        the Halton sequence. This is useful for testing. NOTE: These values are\n        not validated, so it is the responsibility of the user to supply\n        legitimate primes.\n      scramble: If True, will scramble the resulting Halton sequence. This is\n        intended to be used for testing.\n\n    Returns:\n      A HaltonSequence object.\n    \"\"\"\n    if skip_points < 0:\n      raise ValueError('skip_points must be non-negative: %s' % skip_points)\n\n    if primes_override:\n      if len(primes_override) != num_dimensions:\n        raise ValueError(\n            'Expected len(primes_overrides) and num_dimensions to '\n            f'be the same size. len(primes_overrides): {len(primes_override)},'\n            f'num_dimensions: {num_dimensions}')\n      primes = primes_override\n    else:\n      primes = _init_primes(num_dimensions)\n\n    self.__attrs_init__(\n        num_dimensions=num_dimensions,\n        skip_points=skip_points,\n        num_points_generated=num_points_generated,\n        primes=primes,\n        scramble=scramble)\n\n  def load(self, metadata: vz.Metadata) -> None:\n    self._num_points_generated = int(\n        metadata.ns('halton')['num_points_generated'])\n\n  def dump(self) -> vz.Metadata:\n    metadata = vz.Metadata()\n    metadata.ns('halton')['num_points_generated'] = str(\n        self._num_points_generated)\n    return metadata\n\n  def _get_scrambled_halton_value(self, index: int, base: int) -> float:\n    \"\"\"Get a scrambled Halton value for a given `index`, seeded by `base`.\"\"\"\n    if not _is_prime(base):\n      raise ValueError('base is not prime: %s' % base)\n\n    result = 0.0\n    base_rec = 1.0 / base\n    f = base_rec\n    i = index + 1  # For index 0 we want 1/base returned, not 0.\n\n    # Use a fixed seed to generate the permutation in a deterministic way.\n    if self._scramble:\n      local_random = random.Random(base)\n      permutation = list(range(1, base))\n      local_random.shuffle(permutation)\n      permutation = [0] + permutation\n    while i > 0:\n      i, mod = divmod(i, base)\n      if self._scramble:\n        result += f * permutation[mod]\n      else:\n        result += f * mod\n      f *= base_rec\n\n    if (0.0 > result) or (result > 1.0):\n      raise ValueError(\n          'Something wrong has happened; halton_value should be within [0, 1]: %f'\n          % result)\n    return result\n\n  def get_next_list(self) -> List[float]:\n    \"\"\"Get the next list in a sequence seeded by `primes`.\n\n    This implementation and its associated unit tests are inspired by another\n    implementation from Google.\n\n    https://github.com/mlcommons/algorithmic-efficiency/blob/main/algorithmic_efficiency/halton.py\n\n    Returns:\n      An sublist of the Halton sequence. Every value in the list should be\n      within [0,1].\n    \"\"\"\n    index = self._num_points_generated + self._skip_points\n    halton_list = [\n        self._get_scrambled_halton_value(index, prime) for prime in self._primes\n    ]\n    self._num_points_generated += 1\n    return halton_list\n\n\nclass QuasiRandomDesigner(vza.PartiallySerializableDesigner):\n  \"\"\"Sample points using quasi-random search from the scaled search space.\n\n  This implementation uses a scrambled Halton sequence.\n  \"\"\"\n\n  def __init__(self, search_space: vz.SearchSpace, *, skip_points: int = 100):\n    \"\"\"Init.\n\n    Args:\n      search_space: Must be a flat search space.\n      skip_points: If positive, then these first points in the sequence are\n        discarded in order to avoid unwanted correlations.\n    \"\"\"\n    if search_space.is_conditional:\n      raise ValueError(\n          f'This designer {self} does not support conditional search.')\n\n    def create_input_converter(pc):\n      return converters.DefaultModelInputConverter(\n          pc,\n          scale=True,\n          max_discrete_indices=sys.maxsize,\n          float_dtype=np.float64)\n\n    self._converter = converters.DefaultTrialConverter(\n        [create_input_converter(pc) for pc in search_space.parameters])\n\n    for spec in self._converter.output_specs.values():\n      if spec.type not in [\n          NumpyArraySpecType.CONTINUOUS, NumpyArraySpecType.DISCRETE\n      ]:\n        raise ValueError(f'Unsupported type: {spec.type} in {spec}')\n      if spec.num_dimensions != 1:\n        raise ValueError('Multi-dimensional discrete types are unsuppored. '\n                         'Received spec: %s' % spec)\n\n    self._halton_generator = _HaltonSequence(", "metadata": {"task_id": "google_vizier/36", "ground_truth": "        len(search_space.parameters),", "fpath_tuple": ["google_vizier", "vizier", "_src", "algorithms", "designers", "quasi_random.py"], "context_start_lineno": 52, "line_no": 240, "query_window": {"context": "    def create_input_converter(pc):\n      return converters.DefaultModelInputConverter(\n          pc,\n          scale=True,\n          max_discrete_indices=sys.maxsize,\n          float_dtype=np.float64)\n\n    self._converter = converters.DefaultTrialConverter(\n        [create_input_converter(pc) for pc in search_space.parameters])\n\n    for spec in self._converter.output_specs.values():\n      if spec.type not in [\n          NumpyArraySpecType.CONTINUOUS, NumpyArraySpecType.DISCRETE\n      ]:\n        raise ValueError(f'Unsupported type: {spec.type} in {spec}')\n      if spec.num_dimensions != 1:\n        raise ValueError('Multi-dimensional discrete types are unsuppored. '\n                         'Received spec: %s' % spec)\n\n    self._halton_generator = _HaltonSequence(", "metadata": {"fpath_tuple": ["google_vizier", "vizier", "_src", "algorithms", "designers", "quasi_random.py"], "line_no": 240, "task_id": "google_vizier/36", "start_line_no": 220, "end_line_no": 240, "window_size": 20, "context_start_lineno": 52, "repo": "google_vizier"}}, "top_k_context": [{"context": "      seed: Any valid seed for np.random.RandomState.\n    \"\"\"\n    if search_space.is_conditional:\n      # TODO: Add conditional sampling case.\n      raise ValueError(\n          f'This designer {self} does not support conditional search.')\n\n    def create_input_converter(pc):\n      return converters.DefaultModelInputConverter(\n          pc, scale=True, max_discrete_indices=0, float_dtype=dtype)\n\n    self._converter = converters.DefaultTrialConverter(\n        [create_input_converter(pc) for pc in search_space.parameters])\n\n    self._rng = np.random.RandomState(seed)\n\n  def update(self, _) -> None:\n    pass\n\n  def suggest(self,", "metadata": [{"fpath_tuple": ["google_vizier", "vizier", "_src", "algorithms", "designers", "random.py"], "line_no": 52, "start_line_no": 42, "end_line_no": 62, "window_size": 20, "repo": "google_vizier", "slice_size": 10}], "sim_score": 0.3974358974358974}, {"context": "\n    Args:\n      search_space: Must be a flat search space.\n      dtype:\n      seed: Any valid seed for np.random.RandomState.\n    \"\"\"\n    if search_space.is_conditional:\n      # TODO: Add conditional sampling case.\n      raise ValueError(\n          f'This designer {self} does not support conditional search.')\n\n    def create_input_converter(pc):\n      return converters.DefaultModelInputConverter(\n          pc, scale=True, max_discrete_indices=0, float_dtype=dtype)\n\n    self._converter = converters.DefaultTrialConverter(\n        [create_input_converter(pc) for pc in search_space.parameters])\n\n    self._rng = np.random.RandomState(seed)\n", "metadata": [{"fpath_tuple": ["google_vizier", "vizier", "_src", "algorithms", "designers", "random.py"], "line_no": 48, "start_line_no": 38, "end_line_no": 58, "window_size": 20, "repo": "google_vizier", "slice_size": 10}], "sim_score": 0.3860759493670886}, {"context": "      search_space: Must be a flat search space.\n      dtype:\n      seed: Any valid seed for np.random.RandomState.\n    \"\"\"\n    if search_space.is_conditional:\n      # TODO: Add conditional sampling case.\n      raise ValueError(\n          f'This designer {self} does not support conditional search.')\n\n    def create_input_converter(pc):\n      return converters.DefaultModelInputConverter(\n          pc, scale=True, max_discrete_indices=0, float_dtype=dtype)\n\n    self._converter = converters.DefaultTrialConverter(\n        [create_input_converter(pc) for pc in search_space.parameters])\n\n    self._rng = np.random.RandomState(seed)\n\n  def update(self, _) -> None:\n    pass", "metadata": [{"fpath_tuple": ["google_vizier", "vizier", "_src", "algorithms", "designers", "random.py"], "line_no": 50, "start_line_no": 40, "end_line_no": 60, "window_size": 20, "repo": "google_vizier", "slice_size": 10}], "sim_score": 0.38509316770186336}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# fortuna/prob_model/posterior/normalizing_flow/normalizing_flow_trainer.py\n# --------------------------------------------------\n# from __future__ import annotations\n# \n# import abc\n# from typing import Any, Callable, Dict, Optional, Tuple, Union\n# \n# import jax.numpy as jnp\n# import numpy as np\n# from flax.core import FrozenDict\n# from jax import random, vmap\n# from jax._src.prng import PRNGKeyArray\n# from jax.tree_util import tree_map\n# from optax._src.base import PyTree\n# \n# from fortuna.distribution.base import Distribution\n# --------------------------------------------------\n# the below code fragment can be found in:\n# tests/fortuna/test_trainer.py\n# --------------------------------------------------\n# import unittest\n# import unittest.mock as mock\n# from typing import Any, Callable, Dict, Optional, Tuple, Union\n# \n# import jax.random\n# import numpy as np\n# from flax.core import FrozenDict\n# from jax import numpy as jnp\n# from jax._src.prng import PRNGKeyArray\n# from optax._src.base import GradientTransformation, PyTree\n# \n# from fortuna.prob_model.joint.state import JointState\n# from fortuna.training.train_state import TrainState\n# from fortuna.training.trainer import TrainerABC\n# --------------------------------------------------\n# the below code fragment can be found in:\n# fortuna/output_calibrator/output_calib_manager/base.py\n# --------------------------------------------------\n# from typing import Optional, Tuple, Union\n# \n# import flax.linen as nn\n# import jax.numpy as jnp\n# from flax.core import FrozenDict\n# from flax.training.checkpoints import PyTree\n# from jax import random\n# from jax._src.prng import PRNGKeyArray\n# \n# from fortuna.typing import Array, CalibMutable, CalibParams\n# from fortuna.utils.random import WithRNG\n# \n# \n# class OutputCalibManager(WithRNG):\n# --------------------------------------------------\n# the below code fragment can be found in:\n# fortuna/output_calibrator/output_calib_manager/base.py\n# --------------------------------------------------\n# from typing import Optional, Tuple, Union\n# \n# import flax.linen as nn\n# import jax.numpy as jnp\n# from flax.core import FrozenDict\n# from flax.training.checkpoints import PyTree\n# from jax import random\n# from jax._src.prng import PRNGKeyArray\n# \n# from fortuna.typing import Array, CalibMutable, CalibParams\n# from fortuna.utils.random import WithRNG\n# \n# --------------------------------------------------\n# the below code fragment can be found in:\n# tests/fortuna/test_trainer.py\n# --------------------------------------------------\n# import unittest\n# import unittest.mock as mock\n# from typing import Any, Callable, Dict, Optional, Tuple, Union\n# \n# import jax.random\n# import numpy as np\n# from flax.core import FrozenDict\n# from jax import numpy as jnp\n# from jax._src.prng import PRNGKeyArray\n# from optax._src.base import GradientTransformation, PyTree\n# \n# from fortuna.prob_model.joint.state import JointState\n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\nimport pathlib\nfrom typing import Dict, Tuple, Union\n\nimport jax.numpy as jnp\nimport numpy as np\nfrom flax.core import FrozenDict\nfrom optax._src.base import GradientTransformation, PyTree\n\nParams = FrozenDict[str, FrozenDict[str, PyTree]]\nMutable = FrozenDict[str, FrozenDict[str, PyTree]]\nCalibParams = FrozenDict[str, PyTree]\nCalibMutable = FrozenDict[str, PyTree]", "metadata": {"task_id": "awslabs_fortuna/161", "ground_truth": "OptaxOptimizer = GradientTransformation", "fpath_tuple": ["awslabs_fortuna", "fortuna", "typing.py"], "context_start_lineno": 0, "line_no": 12, "query_window": {"context": "import pathlib\nfrom typing import Dict, Tuple, Union\n\nimport jax.numpy as jnp\nimport numpy as np\nfrom flax.core import FrozenDict\nfrom optax._src.base import GradientTransformation, PyTree\n\nParams = FrozenDict[str, FrozenDict[str, PyTree]]\nMutable = FrozenDict[str, FrozenDict[str, PyTree]]\nCalibParams = FrozenDict[str, PyTree]\nCalibMutable = FrozenDict[str, PyTree]", "metadata": {"fpath_tuple": ["awslabs_fortuna", "fortuna", "typing.py"], "line_no": 12, "task_id": "awslabs_fortuna/161", "start_line_no": 0, "end_line_no": 12, "window_size": 20, "context_start_lineno": 0, "repo": "awslabs_fortuna"}}, "top_k_context": [{"context": "import unittest\nimport unittest.mock as mock\nfrom typing import Any, Callable, Dict, Optional, Tuple, Union\n\nimport jax.random\nimport numpy as np\nfrom flax.core import FrozenDict\nfrom jax import numpy as jnp\nfrom jax._src.prng import PRNGKeyArray\nfrom optax._src.base import GradientTransformation, PyTree", "metadata": [{"fpath_tuple": ["awslabs_fortuna", "tests", "fortuna", "test_trainer.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 10, "window_size": 20, "repo": "awslabs_fortuna", "slice_size": 10}], "sim_score": 0.515625}, {"context": "from typing import Optional, Tuple, Union\n\nimport flax.linen as nn\nimport jax.numpy as jnp\nfrom flax.core import FrozenDict\nfrom flax.training.checkpoints import PyTree\nfrom jax import random\nfrom jax._src.prng import PRNGKeyArray\n\nfrom fortuna.typing import Array, CalibMutable, CalibParams", "metadata": [{"fpath_tuple": ["awslabs_fortuna", "fortuna", "output_calibrator", "output_calib_manager", "base.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 10, "window_size": 20, "repo": "awslabs_fortuna", "slice_size": 10}], "sim_score": 0.4696969696969697}, {"context": "from typing import Optional, Tuple, Union\n\nimport flax.linen as nn\nimport jax.numpy as jnp\nfrom flax.core import FrozenDict\nfrom flax.training.checkpoints import PyTree\nfrom jax import random\nfrom jax._src.prng import PRNGKeyArray\n\nfrom fortuna.typing import Array, CalibMutable, CalibParams\nfrom fortuna.utils.random import WithRNG\n", "metadata": [{"fpath_tuple": ["awslabs_fortuna", "fortuna", "output_calibrator", "output_calib_manager", "base.py"], "line_no": 2, "start_line_no": 0, "end_line_no": 12, "window_size": 20, "repo": "awslabs_fortuna", "slice_size": 10}], "sim_score": 0.44285714285714284}, {"context": "import unittest\nimport unittest.mock as mock\nfrom typing import Any, Callable, Dict, Optional, Tuple, Union\n\nimport jax.random\nimport numpy as np\nfrom flax.core import FrozenDict\nfrom jax import numpy as jnp\nfrom jax._src.prng import PRNGKeyArray\nfrom optax._src.base import GradientTransformation, PyTree\n\nfrom fortuna.prob_model.joint.state import JointState", "metadata": [{"fpath_tuple": ["awslabs_fortuna", "tests", "fortuna", "test_trainer.py"], "line_no": 2, "start_line_no": 0, "end_line_no": 12, "window_size": 20, "repo": "awslabs_fortuna", "slice_size": 10}], "sim_score": 0.44}, {"context": "from __future__ import annotations\n\nimport abc\nfrom typing import Any, Callable, Dict, Optional, Tuple, Union\n\nimport jax.numpy as jnp\nimport numpy as np\nfrom flax.core import FrozenDict\nfrom jax import random, vmap\nfrom jax._src.prng import PRNGKeyArray\nfrom jax.tree_util import tree_map\nfrom optax._src.base import PyTree", "metadata": [{"fpath_tuple": ["awslabs_fortuna", "fortuna", "prob_model", "posterior", "normalizing_flow", "normalizing_flow_trainer.py"], "line_no": 2, "start_line_no": 0, "end_line_no": 12, "window_size": 20, "repo": "awslabs_fortuna", "slice_size": 10}], "sim_score": 0.42857142857142855}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# torchrl/envs/libs/gym.py\n# --------------------------------------------------\n#         from_pixels = from_pixels or env_from_pixels\n#         self.from_pixels = from_pixels\n#         self.pixels_only = pixels_only\n#         if from_pixels and not env_from_pixels:\n#             if isinstance(env, PixelObservationWrapper):\n#                 raise TypeError(\n#                     \"PixelObservationWrapper cannot be used to wrap an environment\"\n#                     \"that is already a PixelObservationWrapper instance.\"\n#                 )\n#             env = self._build_gym_env(env, pixels_only)\n#         return env\n# \n#     @implement_for(\"gym\", None, \"0.26.0\")\n#     def _build_gym_env(self, env, pixels_only):  # noqa: F811\n#         return PixelObservationWrapper(env, pixels_only=pixels_only)\n# \n#     @implement_for(\"gym\", \"0.26.0\", None)\n#     def _build_gym_env(self, env, pixels_only):  # noqa: F811\n#         from gym.wrappers.compatibility import EnvCompatibility\n# \n# --------------------------------------------------\n# the below code fragment can be found in:\n# torchrl/envs/libs/gym.py\n# --------------------------------------------------\n#     return gym.envs.registration.registry.env_specs.keys()\n# \n# \n# @implement_for(\"gym\", \"0.26.0\", None)\n# def _get_gym_envs():  # noqa: F811\n#     return gym.envs.registration.registry.keys()\n# \n# \n# def _get_gym():\n#     if _has_gym:\n#         return gym\n#     else:\n#         return None\n# \n# \n# def _is_from_pixels(env):\n#     observation_spec = env.observation_space\n#     if isinstance(observation_spec, (Dict,)):\n#         if \"pixels\" in set(observation_spec.keys()):\n#             return True\n# --------------------------------------------------\n# the below code fragment can be found in:\n# torchrl/envs/libs/gym.py\n# --------------------------------------------------\n# @implement_for(\"gym\", None, \"0.26.0\")\n# def _get_gym_envs():  # noqa: F811\n#     return gym.envs.registration.registry.env_specs.keys()\n# \n# \n# @implement_for(\"gym\", \"0.26.0\", None)\n# def _get_gym_envs():  # noqa: F811\n#     return gym.envs.registration.registry.keys()\n# \n# \n# def _get_gym():\n#     if _has_gym:\n#         return gym\n#     else:\n#         return None\n# \n# \n# def _is_from_pixels(env):\n#     observation_spec = env.observation_space\n#     if isinstance(observation_spec, (Dict,)):\n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\n# Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\nimport argparse\nfrom sys import platform\n\nimport numpy as np\nimport pytest\nimport torch\nfrom _utils_internal import (\n    get_available_devices,\n    HALFCHEETAH_VERSIONED,\n    PENDULUM_VERSIONED,\n    PONG_VERSIONED,\n)\nfrom packaging import version\nfrom tensordict.tensordict import assert_allclose_td, TensorDict\nfrom torchrl._utils import implement_for\nfrom torchrl.collectors import MultiaSyncDataCollector\nfrom torchrl.collectors.collectors import RandomPolicy\nfrom torchrl.envs import EnvCreator, ParallelEnv\nfrom torchrl.envs.libs.brax import _has_brax, BraxEnv\nfrom torchrl.envs.libs.dm_control import _has_dmc, DMControlEnv, DMControlWrapper\nfrom torchrl.envs.libs.gym import _has_gym, _is_from_pixels, GymEnv, GymWrapper\nfrom torchrl.envs.libs.habitat import _has_habitat, HabitatEnv\nfrom torchrl.envs.libs.jumanji import _has_jumanji, JumanjiEnv\nfrom torchrl.envs.libs.vmas import _has_vmas, VmasEnv, VmasWrapper\nfrom torchrl.envs.utils import check_env_specs\n\nif _has_gym:\n    import gym\n\n    gym_version = version.parse(gym.__version__)\n    if gym_version > version.parse(\"0.19\"):\n        from gym.wrappers.pixel_observation import PixelObservationWrapper\n    else:\n        from torchrl.envs.libs.utils import (\n            GymPixelObservationWrapper as PixelObservationWrapper,\n        )\n\nif _has_dmc:\n    from dm_control import suite\n    from dm_control.suite.wrappers import pixels\n\nif _has_vmas:\n    import vmas\n\nIS_OSX = platform == \"darwin\"\n\n\n@pytest.mark.skipif(not _has_gym, reason=\"no gym library found\")\n@pytest.mark.parametrize(\n    \"env_name\",\n    [\n        PONG_VERSIONED,\n        PENDULUM_VERSIONED,\n    ],\n)\n@pytest.mark.parametrize(\"frame_skip\", [1, 3])\n@pytest.mark.parametrize(\n    \"from_pixels,pixels_only\",\n    [\n        [False, False],\n        [True, True],\n        [True, False],\n    ],\n)\nclass TestGym:\n    def test_gym(self, env_name, frame_skip, from_pixels, pixels_only):\n        if env_name == PONG_VERSIONED and not from_pixels:\n            raise pytest.skip(\"already pixel\")\n        elif (\n            env_name != PONG_VERSIONED\n            and from_pixels\n            and (not torch.has_cuda or not torch.cuda.device_count())\n        ):\n            raise pytest.skip(\"no cuda device\")\n\n        tdreset = []\n        tdrollout = []\n        final_seed = []\n        for _ in range(2):\n            env0 = GymEnv(\n                env_name,\n                frame_skip=frame_skip,\n                from_pixels=from_pixels,\n                pixels_only=pixels_only,\n            )\n            torch.manual_seed(0)\n            np.random.seed(0)\n            final_seed.append(env0.set_seed(0))\n            tdreset.append(env0.reset())\n            tdrollout.append(env0.rollout(max_steps=50))\n            assert env0.from_pixels is from_pixels\n            env0.close()\n            env_type = type(env0._env)\n            del env0\n\n        assert_allclose_td(*tdreset)\n        assert_allclose_td(*tdrollout)\n        final_seed0, final_seed1 = final_seed\n        assert final_seed0 == final_seed1\n\n        if env_name == PONG_VERSIONED:\n            base_env = gym.make(env_name, frameskip=frame_skip)\n            frame_skip = 1\n        else:\n            base_env = _make_gym_environment(env_name)\n\n        if from_pixels and not _is_from_pixels(base_env):\n            base_env = PixelObservationWrapper(base_env, pixels_only=pixels_only)\n        assert type(base_env) is env_type\n        env1 = GymWrapper(base_env, frame_skip=frame_skip)\n        torch.manual_seed(0)\n        np.random.seed(0)\n        final_seed2 = env1.set_seed(0)\n        tdreset2 = env1.reset()\n        rollout2 = env1.rollout(max_steps=50)\n        assert env1.from_pixels is from_pixels\n        env1.close()\n        del env1, base_env\n\n        assert_allclose_td(tdreset[0], tdreset2, rtol=1e-4, atol=1e-4)\n        assert final_seed0 == final_seed2\n        assert_allclose_td(tdrollout[0], rollout2, rtol=1e-4, atol=1e-4)\n\n    def test_gym_fake_td(self, env_name, frame_skip, from_pixels, pixels_only):\n        if env_name == PONG_VERSIONED and not from_pixels:\n            raise pytest.skip(\"already pixel\")\n        elif (\n            env_name != PONG_VERSIONED\n            and from_pixels\n            and (not torch.has_cuda or not torch.cuda.device_count())\n        ):\n            raise pytest.skip(\"no cuda device\")\n\n        env = GymEnv(\n            env_name,\n            frame_skip=frame_skip,\n            from_pixels=from_pixels,\n            pixels_only=pixels_only,\n        )\n        check_env_specs(env)\n\n\n@implement_for(\"gym\", None, \"0.26\")\ndef _make_gym_environment(env_name):  # noqa: F811\n    return gym.make(env_name)\n\n\n@implement_for(\"gym\", \"0.26\", None)\ndef _make_gym_environment(env_name):  # noqa: F811", "metadata": {"task_id": "pytorch_rl/152", "ground_truth": "    return gym.make(env_name, render_mode=\"rgb_array\")", "fpath_tuple": ["pytorch_rl", "test", "test_libs.py"], "context_start_lineno": 0, "line_no": 153, "query_window": {"context": "            and (not torch.has_cuda or not torch.cuda.device_count())\n        ):\n            raise pytest.skip(\"no cuda device\")\n\n        env = GymEnv(\n            env_name,\n            frame_skip=frame_skip,\n            from_pixels=from_pixels,\n            pixels_only=pixels_only,\n        )\n        check_env_specs(env)\n\n\n@implement_for(\"gym\", None, \"0.26\")\ndef _make_gym_environment(env_name):  # noqa: F811\n    return gym.make(env_name)\n\n\n@implement_for(\"gym\", \"0.26\", None)\ndef _make_gym_environment(env_name):  # noqa: F811", "metadata": {"fpath_tuple": ["pytorch_rl", "test", "test_libs.py"], "line_no": 153, "task_id": "pytorch_rl/152", "start_line_no": 133, "end_line_no": 153, "window_size": 20, "context_start_lineno": 0, "repo": "pytorch_rl"}}, "top_k_context": [{"context": "\n\n@implement_for(\"gym\", None, \"0.26.0\")\ndef _get_gym_envs():  # noqa: F811\n    return gym.envs.registration.registry.env_specs.keys()\n\n\n@implement_for(\"gym\", \"0.26.0\", None)\ndef _get_gym_envs():  # noqa: F811\n    return gym.envs.registration.registry.keys()\n\n\ndef _get_gym():\n    if _has_gym:\n        return gym\n    else:\n        return None\n\n\ndef _is_from_pixels(env):", "metadata": [{"fpath_tuple": ["pytorch_rl", "torchrl", "envs", "libs", "gym.py"], "line_no": 144, "start_line_no": 134, "end_line_no": 154, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}], "sim_score": 0.4666666666666667}, {"context": "@implement_for(\"gym\", None, \"0.26.0\")\ndef _get_gym_envs():  # noqa: F811\n    return gym.envs.registration.registry.env_specs.keys()\n\n\n@implement_for(\"gym\", \"0.26.0\", None)\ndef _get_gym_envs():  # noqa: F811\n    return gym.envs.registration.registry.keys()\n\n\ndef _get_gym():\n    if _has_gym:\n        return gym\n    else:\n        return None\n\n\ndef _is_from_pixels(env):\n    observation_spec = env.observation_space\n    if isinstance(observation_spec, (Dict,)):", "metadata": [{"fpath_tuple": ["pytorch_rl", "torchrl", "envs", "libs", "gym.py"], "line_no": 146, "start_line_no": 136, "end_line_no": 156, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}], "sim_score": 0.44554455445544555}, {"context": "    ) -> \"gym.core.Env\":\n        env_from_pixels = _is_from_pixels(env)\n        from_pixels = from_pixels or env_from_pixels\n        self.from_pixels = from_pixels\n        self.pixels_only = pixels_only\n        if from_pixels and not env_from_pixels:\n            if isinstance(env, PixelObservationWrapper):\n                raise TypeError(\n                    \"PixelObservationWrapper cannot be used to wrap an environment\"\n                    \"that is already a PixelObservationWrapper instance.\"\n                )\n            env = self._build_gym_env(env, pixels_only)\n        return env\n\n    @implement_for(\"gym\", None, \"0.26.0\")\n    def _build_gym_env(self, env, pixels_only):  # noqa: F811\n        return PixelObservationWrapper(env, pixels_only=pixels_only)\n\n    @implement_for(\"gym\", \"0.26.0\", None)\n    def _build_gym_env(self, env, pixels_only):  # noqa: F811", "metadata": [{"fpath_tuple": ["pytorch_rl", "torchrl", "envs", "libs", "gym.py"], "line_no": 218, "start_line_no": 208, "end_line_no": 228, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}], "sim_score": 0.43859649122807015}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# fortuna/calib_model/predictive/classification.py\n# --------------------------------------------------\n# from typing import Optional\n# \n# import jax.numpy as jnp\n# \n# from fortuna.calib_model.predictive.base import Predictive\n# from fortuna.output_calibrator.output_calib_manager.base import \\\n#     OutputCalibManager\n# from fortuna.prob_output_layer.classification import \\\n#     ClassificationProbOutputLayer\n# from fortuna.typing import Array\n# \n# \n# class ClassificationPredictive(Predictive):\n#     def __init__(\n#         self,\n#         output_calib_manager: OutputCalibManager,\n# --------------------------------------------------\n# the below code fragment can be found in:\n# tests/fortuna/test_temp_scaling.py\n# --------------------------------------------------\n# import unittest\n# \n# import jax.numpy as jnp\n# import optax\n# from jax import random\n# \n# from fortuna.data.loader import DataLoader\n# from fortuna.model.mlp import MLP\n# from fortuna.output_calibrator.classification import \\\n#     ClassificationTemperatureScaler\n# from fortuna.prob_model.classification import ProbClassifier\n# from fortuna.prob_model.posterior.map.map_approximator import \\\n#     MAPPosteriorApproximator\n# from tests.make_data import make_array_random_data\n# \n# \n# class TestCalibrators(unittest.TestCase):\n#     def __init__(self, *args, **kwargs):\n#         super().__init__(*args, **kwargs)\n#         self.rng = random.PRNGKey(0)\n# --------------------------------------------------\n# the below code fragment can be found in:\n# tests/fortuna/test_prob_output_layer.py\n# --------------------------------------------------\n# import unittest\n# \n# import jax.numpy as jnp\n# import jax.scipy as jsp\n# from jax import random\n# \n# from fortuna.prob_output_layer.classification import \\\n#     ClassificationProbOutputLayer\n# from fortuna.prob_output_layer.regression import RegressionProbOutputLayer\n# from fortuna.utils.random import RandomNumberGenerator\n# \n# \n# class TestProbOutputLayers(unittest.TestCase):\n#     def __init__(self, *args, **kwargs):\n#         super().__init__(*args, **kwargs)\n#         self.dim_outputs = 2\n# --------------------------------------------------\n# the below code fragment can be found in:\n# tests/fortuna/prob_model/test_joint.py\n# --------------------------------------------------\n# import jax.numpy as jnp\n# from flax.core import FrozenDict\n# from jax import random\n# \n# from fortuna.data.loader import DataLoader\n# from fortuna.model.mlp import MLP\n# from fortuna.model.model_manager.regression import RegressionModelManager\n# from fortuna.output_calibrator.output_calib_manager.base import \\\n#     OutputCalibManager\n# from fortuna.prob_model.joint.base import Joint\n# from fortuna.prob_model.likelihood.regression import RegressionLikelihood\n# from fortuna.prob_model.prior import IsotropicGaussianPrior\n# from fortuna.prob_output_layer.regression import RegressionProbOutputLayer\n# from tests.make_data import make_array_random_data\n# \n# \n# class TestJoints(unittest.TestCase):\n#     def __init__(self, *args, **kwargs):\n#         super().__init__(*args, **kwargs)\n#         self.shape_inputs = (3,)\n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\nimport unittest\n\nimport jax.numpy as jnp\n\nfrom fortuna.calibration.state import CalibState\nfrom fortuna.output_calibrator.output_calib_manager.state import \\\n    OutputCalibManagerState\nfrom fortuna.prob_model.joint.state import JointState\n\n\nclass TestStates(unittest.TestCase):\n    def __init__(self, *args, **kwargs):", "metadata": {"task_id": "awslabs_fortuna/45", "ground_truth": "        super().__init__(*args, **kwargs)", "fpath_tuple": ["awslabs_fortuna", "tests", "fortuna", "test_state.py"], "context_start_lineno": 0, "line_no": 12, "query_window": {"context": "import unittest\n\nimport jax.numpy as jnp\n\nfrom fortuna.calibration.state import CalibState\nfrom fortuna.output_calibrator.output_calib_manager.state import \\\n    OutputCalibManagerState\nfrom fortuna.prob_model.joint.state import JointState\n\n\nclass TestStates(unittest.TestCase):\n    def __init__(self, *args, **kwargs):", "metadata": {"fpath_tuple": ["awslabs_fortuna", "tests", "fortuna", "test_state.py"], "line_no": 12, "task_id": "awslabs_fortuna/45", "start_line_no": 0, "end_line_no": 12, "window_size": 20, "context_start_lineno": 0, "repo": "awslabs_fortuna"}}, "top_k_context": [{"context": "import unittest\n\nimport jax.numpy as jnp\nfrom flax.core import FrozenDict\nfrom jax import random\n\nfrom fortuna.data.loader import DataLoader\nfrom fortuna.model.mlp import MLP\nfrom fortuna.model.model_manager.regression import RegressionModelManager\nfrom fortuna.output_calibrator.output_calib_manager.base import \\\n    OutputCalibManager\nfrom fortuna.prob_model.joint.base import Joint\nfrom fortuna.prob_model.likelihood.regression import RegressionLikelihood\nfrom fortuna.prob_model.prior import IsotropicGaussianPrior\nfrom fortuna.prob_output_layer.regression import RegressionProbOutputLayer\nfrom tests.make_data import make_array_random_data\n\n\nclass TestJoints(unittest.TestCase):\n    def __init__(self, *args, **kwargs):", "metadata": [{"fpath_tuple": ["awslabs_fortuna", "tests", "fortuna", "prob_model", "test_joint.py"], "line_no": 10, "start_line_no": 0, "end_line_no": 20, "window_size": 20, "repo": "awslabs_fortuna", "slice_size": 10}], "sim_score": 0.5360824742268041}, {"context": "import unittest\n\nimport jax.numpy as jnp\nimport jax.scipy as jsp\nfrom jax import random\n\nfrom fortuna.prob_output_layer.classification import \\\n    ClassificationProbOutputLayer\nfrom fortuna.prob_output_layer.regression import RegressionProbOutputLayer\nfrom fortuna.utils.random import RandomNumberGenerator\n\n\nclass TestProbOutputLayers(unittest.TestCase):\n    def __init__(self, *args, **kwargs):", "metadata": [{"fpath_tuple": ["awslabs_fortuna", "tests", "fortuna", "test_prob_output_layer.py"], "line_no": 4, "start_line_no": 0, "end_line_no": 14, "window_size": 20, "repo": "awslabs_fortuna", "slice_size": 10}], "sim_score": 0.5256410256410257}, {"context": "import unittest\n\nimport jax.numpy as jnp\nimport optax\nfrom jax import random\n\nfrom fortuna.data.loader import DataLoader\nfrom fortuna.model.mlp import MLP\nfrom fortuna.output_calibrator.classification import \\\n    ClassificationTemperatureScaler\nfrom fortuna.prob_model.classification import ProbClassifier\nfrom fortuna.prob_model.posterior.map.map_approximator import \\\n    MAPPosteriorApproximator\nfrom tests.make_data import make_array_random_data\n\n\nclass TestCalibrators(unittest.TestCase):\n    def __init__(self, *args, **kwargs):", "metadata": [{"fpath_tuple": ["awslabs_fortuna", "tests", "fortuna", "test_temp_scaling.py"], "line_no": 8, "start_line_no": 0, "end_line_no": 18, "window_size": 20, "repo": "awslabs_fortuna", "slice_size": 10}], "sim_score": 0.5}, {"context": "from typing import Optional\n\nimport jax.numpy as jnp\n\nfrom fortuna.calib_model.predictive.base import Predictive\nfrom fortuna.output_calibrator.output_calib_manager.base import \\\n    OutputCalibManager\nfrom fortuna.prob_output_layer.classification import \\\n    ClassificationProbOutputLayer\nfrom fortuna.typing import Array\n\n\nclass ClassificationPredictive(Predictive):\n    def __init__(", "metadata": [{"fpath_tuple": ["awslabs_fortuna", "fortuna", "calib_model", "predictive", "classification.py"], "line_no": 4, "start_line_no": 0, "end_line_no": 14, "window_size": 20, "repo": "awslabs_fortuna", "slice_size": 10}], "sim_score": 0.48}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# federatedscope/core/trainers/trainer_pFedMe.py\n# --------------------------------------------------\n#     \"\"\"\n#     # update local weight after finding approximate theta\n#     for client_param, local_para_tmp in zip(\n#             ctx.model.parameters(), ctx.pFedMe_local_model_tmp.parameters()):\n#         local_para_tmp.data = local_para_tmp.data - \\\n#                               ctx.optimizer.regular_weight * \\\n#                               ctx.pFedMe_outer_lr * (local_para_tmp.data -\n#                                                      client_param.data)\n# \n#     # set the compared model data, then the optimizer will find approximate\n#     # model using trainer.cfg.personalization.lr\n#     compared_global_model_para = [{\n#         \"params\": list(ctx.pFedMe_local_model_tmp.parameters())\n#     }]\n#     ctx.optimizer.set_compared_para_group(compared_global_model_para)\n# \n# \n# def _hook_on_fit_end_update_local(ctx):\n#     \"\"\"\n#     Note:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# federatedscope/core/trainers/trainer_pFedMe.py\n# --------------------------------------------------\n#         ``ctx.model``                       Update parameters by \\\n#         ``ctx.pFedMe_local_model_tmp``\n#         ``ctx.optimizer``                   Set compared parameter group\n#         ==================================  ===========================\n#     \"\"\"\n#     # update local weight after finding approximate theta\n#     for client_param, local_para_tmp in zip(\n#             ctx.model.parameters(), ctx.pFedMe_local_model_tmp.parameters()):\n#         local_para_tmp.data = local_para_tmp.data - \\\n#                               ctx.optimizer.regular_weight * \\\n#                               ctx.pFedMe_outer_lr * (local_para_tmp.data -\n#                                                      client_param.data)\n# \n#     # set the compared model data, then the optimizer will find approximate\n#     # model using trainer.cfg.personalization.lr\n#     compared_global_model_para = [{\n#         \"params\": list(ctx.pFedMe_local_model_tmp.parameters())\n#     }]\n#     ctx.optimizer.set_compared_para_group(compared_global_model_para)\n# \n# --------------------------------------------------\n# the below code fragment can be found in:\n# federatedscope/core/trainers/trainer_pFedMe.py\n# --------------------------------------------------\n#         ``ctx.optimizer``                   Set compared parameter group\n#         ==================================  ===========================\n#     \"\"\"\n#     # update local weight after finding approximate theta\n#     for client_param, local_para_tmp in zip(\n#             ctx.model.parameters(), ctx.pFedMe_local_model_tmp.parameters()):\n#         local_para_tmp.data = local_para_tmp.data - \\\n#                               ctx.optimizer.regular_weight * \\\n#                               ctx.pFedMe_outer_lr * (local_para_tmp.data -\n#                                                      client_param.data)\n# \n#     # set the compared model data, then the optimizer will find approximate\n#     # model using trainer.cfg.personalization.lr\n#     compared_global_model_para = [{\n#         \"params\": list(ctx.pFedMe_local_model_tmp.parameters())\n#     }]\n#     ctx.optimizer.set_compared_para_group(compared_global_model_para)\n# \n# \n# def _hook_on_fit_end_update_local(ctx):\n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\nimport copy\nimport logging\n\nimport torch\n\nfrom federatedscope.core.auxiliaries.optimizer_builder import get_optimizer\nfrom federatedscope.core.trainers.torch_trainer import GeneralTorchTrainer\nfrom federatedscope.core.optimizer import wrap_regularized_optimizer\nfrom federatedscope.core.trainers.utils import calculate_batch_epoch_num\nfrom typing import Type\n\nlogger = logging.getLogger(__name__)\n\nDEBUG_DITTO = False\n\n\ndef wrap_DittoTrainer(\n        base_trainer: Type[GeneralTorchTrainer]) -> Type[GeneralTorchTrainer]:\n    \"\"\"\n    Build a `DittoTrainer` with a plug-in manner, by registering new\n    functions into specific `BaseTrainer`\n\n    The Ditto implementation, \"Ditto: Fair and Robust Federated Learning\n    Through Personalization. (ICML2021)\"\n    based on the Algorithm 2 in their paper and official codes:\n    https://github.com/litian96/ditto\n    \"\"\"\n\n    # ---------------- attribute-level plug-in -----------------------\n    init_Ditto_ctx(base_trainer)\n\n    # ---------------- action-level plug-in -----------------------\n    base_trainer.register_hook_in_train(new_hook=_hook_on_fit_start_clean,\n                                        trigger='on_fit_start',\n                                        insert_pos=-1)\n    base_trainer.register_hook_in_train(\n        new_hook=_hook_on_fit_start_set_regularized_para,\n        trigger=\"on_fit_start\",\n        insert_pos=0)\n    base_trainer.register_hook_in_train(\n        new_hook=_hook_on_batch_start_switch_model,\n        trigger=\"on_batch_start\",\n        insert_pos=0)\n    base_trainer.register_hook_in_train(\n        new_hook=_hook_on_batch_forward_cnt_num,\n        trigger=\"on_batch_forward\",\n        insert_pos=-1)\n    base_trainer.register_hook_in_train(new_hook=_hook_on_batch_end_flop_count,\n                                        trigger=\"on_batch_end\",\n                                        insert_pos=-1)\n    base_trainer.register_hook_in_train(new_hook=_hook_on_fit_end_calibrate,\n                                        trigger='on_fit_end',\n                                        insert_pos=-1)\n    # evaluation is based on the local personalized model\n    base_trainer.register_hook_in_eval(\n        new_hook=_hook_on_fit_start_switch_local_model,\n        trigger=\"on_fit_start\",\n        insert_pos=0)\n    base_trainer.register_hook_in_eval(\n        new_hook=_hook_on_fit_end_switch_global_model,\n        trigger=\"on_fit_end\",\n        insert_pos=-1)\n\n    base_trainer.register_hook_in_train(new_hook=_hook_on_fit_end_free_cuda,\n                                        trigger=\"on_fit_end\",\n                                        insert_pos=-1)\n    base_trainer.register_hook_in_eval(new_hook=_hook_on_fit_end_free_cuda,\n                                       trigger=\"on_fit_end\",\n                                       insert_pos=-1)\n\n    return base_trainer\n\n\ndef init_Ditto_ctx(base_trainer):\n    \"\"\"\n    init necessary attributes used in Ditto,\n    `global_model` acts as the shared global model in FedAvg;\n    `local_model` acts as personalized model will be optimized with\n    regularization based on weights of `global_model`\n\n    \"\"\"\n    ctx = base_trainer.ctx\n    cfg = base_trainer.cfg\n\n    ctx.global_model = copy.deepcopy(ctx.model)\n    ctx.local_model = copy.deepcopy(ctx.model)  # the personalized model\n    ctx.models = [ctx.local_model, ctx.global_model]\n\n    ctx.model = ctx.global_model\n    ctx.use_local_model_current = False\n\n    ctx.num_samples_local_model_train = 0\n\n    # track the batch_num, epoch_num, for local & global model respectively\n    cfg_p_local_update_steps = cfg.personalization.local_update_steps\n    ctx.num_train_batch_for_local_model, \\\n        ctx.num_train_batch_last_epoch_for_local_model, \\\n        ctx.num_train_epoch_for_local_model, \\\n        ctx.num_total_train_batch = \\\n        calculate_batch_epoch_num(cfg_p_local_update_steps,\n                                  cfg.train.batch_or_epoch,\n                                  ctx.num_train_data,\n                                  cfg.dataloader.batch_size,\n                                  cfg.dataloader.drop_last)\n\n    # In the first\n    # 1. `num_train_batch` and `num_train_batch_last_epoch`\n    # (batch_or_epoch == 'batch' case) or\n    # 2. `num_train_epoch`,\n    # (batch_or_epoch == 'epoch' case)\n    # we will manipulate local models, and manipulate global model in the\n    # remaining steps\n    if cfg.train.batch_or_epoch == 'batch':\n        ctx.num_train_batch += ctx.num_train_batch_for_local_model\n        ctx.num_train_batch_last_epoch += \\\n            ctx.num_train_batch_last_epoch_for_local_model\n    else:\n        ctx.num_train_epoch += ctx.num_train_epoch_for_local_model\n\n\ndef _hook_on_fit_start_set_regularized_para(ctx):\n    \"\"\"\n    Note:\n      The modified attributes and according operations are shown below:\n        ==================================  ===========================\n        Attribute                           Operation\n        ==================================  ===========================\n        ``ctx.global_model``                Move to ``ctx.device`` and set \\\n        to ``train`` mode\n        ``ctx.local_model``                 Move to ``ctx.device`` and set \\\n        to ``train`` mode\n        ``ctx.optimizer_for_global_model``  Initialize by ``ctx.cfg`` and \\\n        wrapped by ``wrap_regularized_optimizer``\n        ``ctx.optimizer_for_local_model``   Initialize by ``ctx.cfg`` and \\\n        set compared parameter group\n        ==================================  ===========================\n    \"\"\"\n    # set the compared model data for local personalized model\n    ctx.global_model.to(ctx.device)\n    ctx.local_model.to(ctx.device)\n    ctx.global_model.train()\n    ctx.local_model.train()\n    compared_global_model_para = [{\n        \"params\": list(ctx.global_model.parameters())\n    }]\n\n    ctx.optimizer_for_global_model = get_optimizer(ctx.global_model,\n                                                   **ctx.cfg.train.optimizer)\n    ctx.optimizer_for_local_model = get_optimizer(ctx.local_model,", "metadata": {"task_id": "alibaba_FederatedScope/178", "ground_truth": "                                                  **ctx.cfg.train.optimizer)", "fpath_tuple": ["alibaba_FederatedScope", "federatedscope", "core", "trainers", "trainer_Ditto.py"], "context_start_lineno": 0, "line_no": 149, "query_window": {"context": "        ``ctx.local_model``                 Move to ``ctx.device`` and set \\\n        to ``train`` mode\n        ``ctx.optimizer_for_global_model``  Initialize by ``ctx.cfg`` and \\\n        wrapped by ``wrap_regularized_optimizer``\n        ``ctx.optimizer_for_local_model``   Initialize by ``ctx.cfg`` and \\\n        set compared parameter group\n        ==================================  ===========================\n    \"\"\"\n    # set the compared model data for local personalized model\n    ctx.global_model.to(ctx.device)\n    ctx.local_model.to(ctx.device)\n    ctx.global_model.train()\n    ctx.local_model.train()\n    compared_global_model_para = [{\n        \"params\": list(ctx.global_model.parameters())\n    }]\n\n    ctx.optimizer_for_global_model = get_optimizer(ctx.global_model,\n                                                   **ctx.cfg.train.optimizer)\n    ctx.optimizer_for_local_model = get_optimizer(ctx.local_model,", "metadata": {"fpath_tuple": ["alibaba_FederatedScope", "federatedscope", "core", "trainers", "trainer_Ditto.py"], "line_no": 149, "task_id": "alibaba_FederatedScope/178", "start_line_no": 129, "end_line_no": 149, "window_size": 20, "context_start_lineno": 0, "repo": "alibaba_FederatedScope"}}, "top_k_context": [{"context": "        ``ctx.model``                       Update parameters by \\\n        ``ctx.pFedMe_local_model_tmp``\n        ``ctx.optimizer``                   Set compared parameter group\n        ==================================  ===========================\n    \"\"\"\n    # update local weight after finding approximate theta\n    for client_param, local_para_tmp in zip(\n            ctx.model.parameters(), ctx.pFedMe_local_model_tmp.parameters()):\n        local_para_tmp.data = local_para_tmp.data - \\\n                              ctx.optimizer.regular_weight * \\\n                              ctx.pFedMe_outer_lr * (local_para_tmp.data -\n                                                     client_param.data)\n\n    # set the compared model data, then the optimizer will find approximate\n    # model using trainer.cfg.personalization.lr\n    compared_global_model_para = [{\n        \"params\": list(ctx.pFedMe_local_model_tmp.parameters())\n    }]\n    ctx.optimizer.set_compared_para_group(compared_global_model_para)\n", "metadata": [{"fpath_tuple": ["alibaba_FederatedScope", "federatedscope", "core", "trainers", "trainer_pFedMe.py"], "line_no": 192, "start_line_no": 182, "end_line_no": 202, "window_size": 20, "repo": "alibaba_FederatedScope", "slice_size": 10}], "sim_score": 0.4956521739130435}, {"context": "        Attribute                           Operation\n        ==================================  ===========================\n        ``ctx.model``                       Update parameters by \\\n        ``ctx.pFedMe_local_model_tmp``\n        ``ctx.optimizer``                   Set compared parameter group\n        ==================================  ===========================\n    \"\"\"\n    # update local weight after finding approximate theta\n    for client_param, local_para_tmp in zip(\n            ctx.model.parameters(), ctx.pFedMe_local_model_tmp.parameters()):\n        local_para_tmp.data = local_para_tmp.data - \\\n                              ctx.optimizer.regular_weight * \\\n                              ctx.pFedMe_outer_lr * (local_para_tmp.data -\n                                                     client_param.data)\n\n    # set the compared model data, then the optimizer will find approximate\n    # model using trainer.cfg.personalization.lr\n    compared_global_model_para = [{\n        \"params\": list(ctx.pFedMe_local_model_tmp.parameters())\n    }]", "metadata": [{"fpath_tuple": ["alibaba_FederatedScope", "federatedscope", "core", "trainers", "trainer_pFedMe.py"], "line_no": 190, "start_line_no": 180, "end_line_no": 200, "window_size": 20, "repo": "alibaba_FederatedScope", "slice_size": 10}], "sim_score": 0.4956521739130435}, {"context": "        ``ctx.optimizer``                   Set compared parameter group\n        ==================================  ===========================\n    \"\"\"\n    # update local weight after finding approximate theta\n    for client_param, local_para_tmp in zip(\n            ctx.model.parameters(), ctx.pFedMe_local_model_tmp.parameters()):\n        local_para_tmp.data = local_para_tmp.data - \\\n                              ctx.optimizer.regular_weight * \\\n                              ctx.pFedMe_outer_lr * (local_para_tmp.data -\n                                                     client_param.data)\n\n    # set the compared model data, then the optimizer will find approximate\n    # model using trainer.cfg.personalization.lr\n    compared_global_model_para = [{\n        \"params\": list(ctx.pFedMe_local_model_tmp.parameters())\n    }]\n    ctx.optimizer.set_compared_para_group(compared_global_model_para)\n\n\ndef _hook_on_fit_end_update_local(ctx):", "metadata": [{"fpath_tuple": ["alibaba_FederatedScope", "federatedscope", "core", "trainers", "trainer_pFedMe.py"], "line_no": 194, "start_line_no": 184, "end_line_no": 204, "window_size": 20, "repo": "alibaba_FederatedScope", "slice_size": 10}], "sim_score": 0.47058823529411764}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# ding/policy/qmix.py\n# --------------------------------------------------\n# \n#                 The _init_learn method takes the argument from the self._cfg.learn in the config file\n# \n#             - learning_rate (:obj:`float`): The learning rate fo the optimizer\n#             - gamma (:obj:`float`): The discount factor\n#             - agent_num (:obj:`int`): Since this is a multi-agent algorithm, we need to input the agent num.\n#             - batch_size (:obj:`int`): Need batch size info to init hidden_state plugins\n#         \"\"\"\n#         self._priority = self._cfg.priority\n#         self._priority_IS_weight = self._cfg.priority_IS_weight\n#         assert not self._priority and not self._priority_IS_weight, \"Priority is not implemented in QMIX\"\n#         self._optimizer = RMSprop(\n#             params=self._model.parameters(), lr=self._cfg.learn.learning_rate, alpha=0.99, eps=0.00001\n#         )\n#         self._gamma = self._cfg.learn.discount_factor\n# \n#         self._target_model = copy.deepcopy(self._model)\n#         self._target_model = model_wrap(\n#             self._target_model,\n#             wrapper_name='target',\n# --------------------------------------------------\n# the below code fragment can be found in:\n# ding/policy/qmix.py\n# --------------------------------------------------\n# \n#             - learning_rate (:obj:`float`): The learning rate fo the optimizer\n#             - gamma (:obj:`float`): The discount factor\n#             - agent_num (:obj:`int`): Since this is a multi-agent algorithm, we need to input the agent num.\n#             - batch_size (:obj:`int`): Need batch size info to init hidden_state plugins\n#         \"\"\"\n#         self._priority = self._cfg.priority\n#         self._priority_IS_weight = self._cfg.priority_IS_weight\n#         assert not self._priority and not self._priority_IS_weight, \"Priority is not implemented in QMIX\"\n#         self._optimizer = RMSprop(\n#             params=self._model.parameters(), lr=self._cfg.learn.learning_rate, alpha=0.99, eps=0.00001\n#         )\n#         self._gamma = self._cfg.learn.discount_factor\n# \n#         self._target_model = copy.deepcopy(self._model)\n#         self._target_model = model_wrap(\n#             self._target_model,\n#             wrapper_name='target',\n#             update_type='momentum',\n#             update_kwargs={'theta': self._cfg.learn.target_update_theta}\n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\nfrom typing import List, Dict, Any, Tuple, Union, Optional\nfrom collections import namedtuple\nimport torch\nimport copy\nimport math\n\nfrom ding.torch_utils import Adam, RMSprop, to_device\nfrom ding.rl_utils import q_nstep_td_data, q_nstep_td_error, get_nstep_return_data, get_train_sample, l2_balance\nfrom ding.model import model_wrap\nfrom ding.utils import POLICY_REGISTRY\nfrom ding.utils.data import timestep_collate, default_collate, default_decollate\nfrom .base_policy import Policy\n\n@POLICY_REGISTRY.register('smac_ace_dqn')\nclass SMACACEDQNPolicy(Policy):\n    \"\"\"\n    Overview:\n        Policy class of ACE algorithm. ACE is a multi agent reinforcement learning algorithm, \\\n            you can view the paper in the following link https://arxiv.org/abs/2211.16068\n    Interface:\n        _init_learn, _data_preprocess_learn, _forward_learn, _reset_learn, _state_dict_learn, _load_state_dict_learn \\\n            _init_collect, _forward_collect, _reset_collect, _process_transition, _init_eval, _forward_eval \\\n            _reset_eval, _get_train_sample, default_model\n    Config:\n        == ==================== ======== ============== ======================================== =======================\n        ID Symbol               Type     Default Value  Description                              Other(Shape)\n        == ==================== ======== ============== ======================================== =======================\n        1  ``type``             str      qmix           | RL policy register name, refer to      | this arg is optional,\n                                                        | registry ``POLICY_REGISTRY``           | a placeholder\n        2  ``cuda``             bool     True           | Whether to use cuda for network        | this arg can be diff-\n                                                                                                 | erent from modes\n        3  ``on_policy``        bool     False          | Whether the RL algorithm is on-policy\n                                                        | or off-policy\n        4. ``priority``         bool     False          | Whether use priority(PER)              | priority sample,\n                                                                                                 | update priority\n        5  | ``priority_``      bool     False          | Whether use Importance Sampling        | IS weight\n           | ``IS_weight``                              | Weight to correct biased update.\n        6  | ``learn.update_``  int      20             | How many updates(iterations) to train  | this args can be vary\n           | ``per_collect``                            | after collector's one collection. Only | from envs. Bigger val\n                                                        | valid in serial training               | means more off-policy\n        7  | ``learn.target_``   float    0.001         | Target network update momentum         | between[0,1]\n           | ``update_theta``                           | parameter.\n        8  | ``learn.discount`` float    0.99           | Reward's future discount factor, aka.  | may be 1 when sparse\n           | ``_factor``                                | gamma                                  | reward env\n        == ==================== ======== ============== ======================================== =======================\n    \"\"\"\n    config = dict(\n        # (str) RL policy register name (refer to function \"POLICY_REGISTRY\").\n        type='smac_ace_dqn',\n        # (bool) Whether to use cuda for network.\n        cuda=True,\n        # (bool) Whether the RL algorithm is on-policy or off-policy.\n        on_policy=False,\n        # (bool) Whether use priority(priority sample, IS weight, update priority)\n        priority=False,\n        # (bool) Whether use Importance Sampling Weight to correct biased update. If True, priority must be True.\n        priority_IS_weight=False,\n        learn=dict(\n            # (bool) Whether to use multi gpu\n            multi_gpu=False,\n            update_per_collect=20,\n            batch_size=32,\n            learning_rate=0.0005,\n            clip_value=100,\n            # ==============================================================\n            # The following configs is algorithm-specific\n            # ==============================================================\n            # (float) Target network update momentum parameter.\n            # in [0, 1].\n            target_update_theta=0.008,\n            # (float) The discount factor for future rewards,\n            # in [0, 1].\n            discount_factor=0.99,\n            nstep=1,\n            shuffle=False,\n            aux_loss_weight=0.0,\n            learning_rate_type='constant',\n            weight_decay=1e-5,\n            optimizer_type='rmsprop',\n        ),\n        collect=dict(\n            # (int) Only one of [n_sample, n_episode] shoule be set\n            # n_episode=32,\n            # (int) Cut trajectories into pieces with length \"unroll_len\", the length of timesteps\n            # in each forward when training. In qmix, it is greater than 1 because there is RNN.\n            unroll_len=1,\n        ),\n        eval=dict(),\n        other=dict(\n            eps=dict(\n                # (str) Type of epsilon decay\n                type='exp',\n                # (float) Start value for epsilon decay, in [0, 1].\n                # 0 means not use epsilon decay.\n                start=1,\n                # (float) Start value for epsilon decay, in [0, 1].\n                end=0.05,\n                # (int) Decay length(env step)\n                decay=50000,\n            ),\n            replay_buffer=dict(\n                replay_buffer_size=10000,\n                # (int) The maximum reuse times of each data\n                max_reuse=1e+9,\n                max_staleness=1e+9,\n            ),\n        ),\n    )\n\n    def _init_learn(self) -> None:\n        \"\"\"\n        Overview:\n            Learn mode init method. Called by ``self.__init__``.\n            Init the learner model\n        Arguments:\n            .. note::\n\n                The _init_learn method takes the argument from the self._cfg.learn in the config file\n\n            - learning_rate (:obj:`float`): The learning rate fo the optimizer\n            - gamma (:obj:`float`): The discount factor\n            - agent_num (:obj:`int`): Since this is a multi-agent algorithm, we need to input the agent num.\n            - batch_size (:obj:`int`): Need batch size info to init hidden_state plugins\n        \"\"\"\n        self._priority = self._cfg.priority\n        self._priority_IS_weight = self._cfg.priority_IS_weight\n        if self._cfg.learn.optimizer_type == 'adam':\n            self._optimizer = Adam(\n                self._model.parameters(),\n                lr=self._cfg.learn.learning_rate,\n                weight_decay=self._cfg.learn.weight_decay,\n            )\n        else:\n            self._optimizer = RMSprop(\n                params=self._model.parameters(),\n                lr=self._cfg.learn.learning_rate,", "metadata": {"task_id": "opendilab_ACE/77", "ground_truth": "                alpha=0.99,", "fpath_tuple": ["opendilab_ACE", "ding", "policy", "ace_dqn.py"], "context_start_lineno": 0, "line_no": 136, "query_window": {"context": "\n                The _init_learn method takes the argument from the self._cfg.learn in the config file\n\n            - learning_rate (:obj:`float`): The learning rate fo the optimizer\n            - gamma (:obj:`float`): The discount factor\n            - agent_num (:obj:`int`): Since this is a multi-agent algorithm, we need to input the agent num.\n            - batch_size (:obj:`int`): Need batch size info to init hidden_state plugins\n        \"\"\"\n        self._priority = self._cfg.priority\n        self._priority_IS_weight = self._cfg.priority_IS_weight\n        if self._cfg.learn.optimizer_type == 'adam':\n            self._optimizer = Adam(\n                self._model.parameters(),\n                lr=self._cfg.learn.learning_rate,\n                weight_decay=self._cfg.learn.weight_decay,\n            )\n        else:\n            self._optimizer = RMSprop(\n                params=self._model.parameters(),\n                lr=self._cfg.learn.learning_rate,", "metadata": {"fpath_tuple": ["opendilab_ACE", "ding", "policy", "ace_dqn.py"], "line_no": 136, "task_id": "opendilab_ACE/77", "start_line_no": 116, "end_line_no": 136, "window_size": 20, "context_start_lineno": 0, "repo": "opendilab_ACE"}}, "top_k_context": [{"context": "\n                The _init_learn method takes the argument from the self._cfg.learn in the config file\n\n            - learning_rate (:obj:`float`): The learning rate fo the optimizer\n            - gamma (:obj:`float`): The discount factor\n            - agent_num (:obj:`int`): Since this is a multi-agent algorithm, we need to input the agent num.\n            - batch_size (:obj:`int`): Need batch size info to init hidden_state plugins\n        \"\"\"\n        self._priority = self._cfg.priority\n        self._priority_IS_weight = self._cfg.priority_IS_weight\n        assert not self._priority and not self._priority_IS_weight, \"Priority is not implemented in QMIX\"\n        self._optimizer = RMSprop(\n            params=self._model.parameters(), lr=self._cfg.learn.learning_rate, alpha=0.99, eps=0.00001\n        )\n        self._gamma = self._cfg.learn.discount_factor\n\n        self._target_model = copy.deepcopy(self._model)\n        self._target_model = model_wrap(\n            self._target_model,\n            wrapper_name='target',", "metadata": [{"fpath_tuple": ["opendilab_ACE", "ding", "policy", "qmix.py"], "line_no": 122, "start_line_no": 112, "end_line_no": 132, "window_size": 20, "repo": "opendilab_ACE", "slice_size": 10}], "sim_score": 0.648854961832061}, {"context": "        Arguments:\n            .. note::\n\n                The _init_learn method takes the argument from the self._cfg.learn in the config file\n\n            - learning_rate (:obj:`float`): The learning rate fo the optimizer\n            - gamma (:obj:`float`): The discount factor\n            - agent_num (:obj:`int`): Since this is a multi-agent algorithm, we need to input the agent num.\n            - batch_size (:obj:`int`): Need batch size info to init hidden_state plugins\n        \"\"\"\n        self._priority = self._cfg.priority\n        self._priority_IS_weight = self._cfg.priority_IS_weight\n        assert not self._priority and not self._priority_IS_weight, \"Priority is not implemented in QMIX\"\n        self._optimizer = RMSprop(\n            params=self._model.parameters(), lr=self._cfg.learn.learning_rate, alpha=0.99, eps=0.00001\n        )\n        self._gamma = self._cfg.learn.discount_factor\n\n        self._target_model = copy.deepcopy(self._model)\n        self._target_model = model_wrap(", "metadata": [{"fpath_tuple": ["opendilab_ACE", "ding", "policy", "qmix.py"], "line_no": 120, "start_line_no": 110, "end_line_no": 130, "window_size": 20, "repo": "opendilab_ACE", "slice_size": 10}], "sim_score": 0.6439393939393939}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# metrics/recall/app.py\n# metrics/recall/app.py\n# metrics/recall/app.py\n# --------------------------------------------------\n# import evaluate\n# from evaluate.utils import launch_gradio_widget\n# \n# \n# module = evaluate.load(\"recall\")\n# launch_gradio_widget(module)\n# --------------------------------------------------\n# the below code fragment can be found in:\n# metrics/roc_auc/app.py\n# metrics/roc_auc/app.py\n# metrics/roc_auc/app.py\n# --------------------------------------------------\n# import evaluate\n# from evaluate.utils import launch_gradio_widget\n# \n# \n# module = evaluate.load(\"roc_auc\")\n# launch_gradio_widget(module)\n# --------------------------------------------------\n# the below code fragment can be found in:\n# metrics/rouge/app.py\n# metrics/rouge/app.py\n# metrics/rouge/app.py\n# --------------------------------------------------\n# import evaluate\n# from evaluate.utils import launch_gradio_widget\n# \n# \n# module = evaluate.load(\"rouge\")\n# launch_gradio_widget(module)\n# --------------------------------------------------\n# the below code fragment can be found in:\n# metrics/smape/app.py\n# metrics/smape/app.py\n# metrics/smape/app.py\n# --------------------------------------------------\n# import evaluate\n# from evaluate.utils import launch_gradio_widget\n# \n# \n# module = evaluate.load(\"smape\")\n# launch_gradio_widget(module)\n# --------------------------------------------------\n# the below code fragment can be found in:\n# metrics/squad/app.py\n# metrics/squad/app.py\n# metrics/squad/app.py\n# --------------------------------------------------\n# import evaluate\n# from evaluate.utils import launch_gradio_widget\n# \n# \n# module = evaluate.load(\"squad\")\n# launch_gradio_widget(module)\n# --------------------------------------------------\n# the below code fragment can be found in:\n# metrics/wiki_split/app.py\n# metrics/wiki_split/app.py\n# metrics/wiki_split/app.py\n# --------------------------------------------------\n# import evaluate\n# from evaluate.utils import launch_gradio_widget\n# \n# \n# module = evaluate.load(\"wiki_split\")\n# launch_gradio_widget(module)\n# --------------------------------------------------\n# the below code fragment can be found in:\n# metrics/character/app.py\n# metrics/character/app.py\n# metrics/character/app.py\n# --------------------------------------------------\n# import evaluate\n# from evaluate.utils import launch_gradio_widget\n# \n# \n# module = evaluate.load(\"character\")\n# launch_gradio_widget(module)\n# --------------------------------------------------\n# the below code fragment can be found in:\n# metrics/cer/app.py\n# metrics/cer/app.py\n# metrics/cer/app.py\n# --------------------------------------------------\n# import evaluate\n# from evaluate.utils import launch_gradio_widget\n# \n# \n# module = evaluate.load(\"cer\")\n# launch_gradio_widget(module)\n# --------------------------------------------------\n# the below code fragment can be found in:\n# metrics/ter/app.py\n# metrics/ter/app.py\n# metrics/ter/app.py\n# --------------------------------------------------\n# import evaluate\n# from evaluate.utils import launch_gradio_widget\n# \n# \n# module = evaluate.load(\"ter\")\n# launch_gradio_widget(module)\n# --------------------------------------------------\n# the below code fragment can be found in:\n# metrics/wer/app.py\n# metrics/wer/app.py\n# metrics/wer/app.py\n# --------------------------------------------------\n# import evaluate\n# from evaluate.utils import launch_gradio_widget\n# \n# \n# module = evaluate.load(\"wer\")\n# launch_gradio_widget(module)\n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\nimport evaluate\nfrom evaluate.utils import launch_gradio_widget\n\n\nmodule = evaluate.load(\"sari\")", "metadata": {"task_id": "huggingface_evaluate/53", "ground_truth": "launch_gradio_widget(module)", "fpath_tuple": ["huggingface_evaluate", "metrics", "sari", "app.py"], "context_start_lineno": 0, "line_no": 5, "query_window": {"context": "import evaluate\nfrom evaluate.utils import launch_gradio_widget\n\n\nmodule = evaluate.load(\"sari\")", "metadata": {"fpath_tuple": ["huggingface_evaluate", "metrics", "sari", "app.py"], "line_no": 5, "task_id": "huggingface_evaluate/53", "start_line_no": 0, "end_line_no": 5, "window_size": 20, "context_start_lineno": 0, "repo": "huggingface_evaluate"}}, "top_k_context": [{"context": "import evaluate\nfrom evaluate.utils import launch_gradio_widget\n\n\nmodule = evaluate.load(\"wer\")\nlaunch_gradio_widget(module)", "metadata": [{"fpath_tuple": ["huggingface_evaluate", "metrics", "wer", "app.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 6, "window_size": 20, "repo": "huggingface_evaluate", "slice_size": 10}, {"fpath_tuple": ["huggingface_evaluate", "metrics", "wer", "app.py"], "line_no": 2, "start_line_no": 0, "end_line_no": 6, "window_size": 20, "repo": "huggingface_evaluate", "slice_size": 10}, {"fpath_tuple": ["huggingface_evaluate", "metrics", "wer", "app.py"], "line_no": 4, "start_line_no": 0, "end_line_no": 6, "window_size": 20, "repo": "huggingface_evaluate", "slice_size": 10}], "sim_score": 0.75}, {"context": "import evaluate\nfrom evaluate.utils import launch_gradio_widget\n\n\nmodule = evaluate.load(\"ter\")\nlaunch_gradio_widget(module)", "metadata": [{"fpath_tuple": ["huggingface_evaluate", "metrics", "ter", "app.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 6, "window_size": 20, "repo": "huggingface_evaluate", "slice_size": 10}, {"fpath_tuple": ["huggingface_evaluate", "metrics", "ter", "app.py"], "line_no": 2, "start_line_no": 0, "end_line_no": 6, "window_size": 20, "repo": "huggingface_evaluate", "slice_size": 10}, {"fpath_tuple": ["huggingface_evaluate", "metrics", "ter", "app.py"], "line_no": 4, "start_line_no": 0, "end_line_no": 6, "window_size": 20, "repo": "huggingface_evaluate", "slice_size": 10}], "sim_score": 0.75}, {"context": "import evaluate\nfrom evaluate.utils import launch_gradio_widget\n\n\nmodule = evaluate.load(\"cer\")\nlaunch_gradio_widget(module)", "metadata": [{"fpath_tuple": ["huggingface_evaluate", "metrics", "cer", "app.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 6, "window_size": 20, "repo": "huggingface_evaluate", "slice_size": 10}, {"fpath_tuple": ["huggingface_evaluate", "metrics", "cer", "app.py"], "line_no": 2, "start_line_no": 0, "end_line_no": 6, "window_size": 20, "repo": "huggingface_evaluate", "slice_size": 10}, {"fpath_tuple": ["huggingface_evaluate", "metrics", "cer", "app.py"], "line_no": 4, "start_line_no": 0, "end_line_no": 6, "window_size": 20, "repo": "huggingface_evaluate", "slice_size": 10}], "sim_score": 0.75}, {"context": "import evaluate\nfrom evaluate.utils import launch_gradio_widget\n\n\nmodule = evaluate.load(\"character\")\nlaunch_gradio_widget(module)", "metadata": [{"fpath_tuple": ["huggingface_evaluate", "metrics", "character", "app.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 6, "window_size": 20, "repo": "huggingface_evaluate", "slice_size": 10}, {"fpath_tuple": ["huggingface_evaluate", "metrics", "character", "app.py"], "line_no": 2, "start_line_no": 0, "end_line_no": 6, "window_size": 20, "repo": "huggingface_evaluate", "slice_size": 10}, {"fpath_tuple": ["huggingface_evaluate", "metrics", "character", "app.py"], "line_no": 4, "start_line_no": 0, "end_line_no": 6, "window_size": 20, "repo": "huggingface_evaluate", "slice_size": 10}], "sim_score": 0.75}, {"context": "import evaluate\nfrom evaluate.utils import launch_gradio_widget\n\n\nmodule = evaluate.load(\"wiki_split\")\nlaunch_gradio_widget(module)", "metadata": [{"fpath_tuple": ["huggingface_evaluate", "metrics", "wiki_split", "app.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 6, "window_size": 20, "repo": "huggingface_evaluate", "slice_size": 10}, {"fpath_tuple": ["huggingface_evaluate", "metrics", "wiki_split", "app.py"], "line_no": 2, "start_line_no": 0, "end_line_no": 6, "window_size": 20, "repo": "huggingface_evaluate", "slice_size": 10}, {"fpath_tuple": ["huggingface_evaluate", "metrics", "wiki_split", "app.py"], "line_no": 4, "start_line_no": 0, "end_line_no": 6, "window_size": 20, "repo": "huggingface_evaluate", "slice_size": 10}], "sim_score": 0.72}, {"context": "import evaluate\nfrom evaluate.utils import launch_gradio_widget\n\n\nmodule = evaluate.load(\"squad\")\nlaunch_gradio_widget(module)", "metadata": [{"fpath_tuple": ["huggingface_evaluate", "metrics", "squad", "app.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 6, "window_size": 20, "repo": "huggingface_evaluate", "slice_size": 10}, {"fpath_tuple": ["huggingface_evaluate", "metrics", "squad", "app.py"], "line_no": 2, "start_line_no": 0, "end_line_no": 6, "window_size": 20, "repo": "huggingface_evaluate", "slice_size": 10}, {"fpath_tuple": ["huggingface_evaluate", "metrics", "squad", "app.py"], "line_no": 4, "start_line_no": 0, "end_line_no": 6, "window_size": 20, "repo": "huggingface_evaluate", "slice_size": 10}], "sim_score": 0.72}, {"context": "import evaluate\nfrom evaluate.utils import launch_gradio_widget\n\n\nmodule = evaluate.load(\"smape\")\nlaunch_gradio_widget(module)", "metadata": [{"fpath_tuple": ["huggingface_evaluate", "metrics", "smape", "app.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 6, "window_size": 20, "repo": "huggingface_evaluate", "slice_size": 10}, {"fpath_tuple": ["huggingface_evaluate", "metrics", "smape", "app.py"], "line_no": 2, "start_line_no": 0, "end_line_no": 6, "window_size": 20, "repo": "huggingface_evaluate", "slice_size": 10}, {"fpath_tuple": ["huggingface_evaluate", "metrics", "smape", "app.py"], "line_no": 4, "start_line_no": 0, "end_line_no": 6, "window_size": 20, "repo": "huggingface_evaluate", "slice_size": 10}], "sim_score": 0.72}, {"context": "import evaluate\nfrom evaluate.utils import launch_gradio_widget\n\n\nmodule = evaluate.load(\"rouge\")\nlaunch_gradio_widget(module)", "metadata": [{"fpath_tuple": ["huggingface_evaluate", "metrics", "rouge", "app.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 6, "window_size": 20, "repo": "huggingface_evaluate", "slice_size": 10}, {"fpath_tuple": ["huggingface_evaluate", "metrics", "rouge", "app.py"], "line_no": 2, "start_line_no": 0, "end_line_no": 6, "window_size": 20, "repo": "huggingface_evaluate", "slice_size": 10}, {"fpath_tuple": ["huggingface_evaluate", "metrics", "rouge", "app.py"], "line_no": 4, "start_line_no": 0, "end_line_no": 6, "window_size": 20, "repo": "huggingface_evaluate", "slice_size": 10}], "sim_score": 0.72}, {"context": "import evaluate\nfrom evaluate.utils import launch_gradio_widget\n\n\nmodule = evaluate.load(\"roc_auc\")\nlaunch_gradio_widget(module)", "metadata": [{"fpath_tuple": ["huggingface_evaluate", "metrics", "roc_auc", "app.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 6, "window_size": 20, "repo": "huggingface_evaluate", "slice_size": 10}, {"fpath_tuple": ["huggingface_evaluate", "metrics", "roc_auc", "app.py"], "line_no": 2, "start_line_no": 0, "end_line_no": 6, "window_size": 20, "repo": "huggingface_evaluate", "slice_size": 10}, {"fpath_tuple": ["huggingface_evaluate", "metrics", "roc_auc", "app.py"], "line_no": 4, "start_line_no": 0, "end_line_no": 6, "window_size": 20, "repo": "huggingface_evaluate", "slice_size": 10}], "sim_score": 0.72}, {"context": "import evaluate\nfrom evaluate.utils import launch_gradio_widget\n\n\nmodule = evaluate.load(\"recall\")\nlaunch_gradio_widget(module)", "metadata": [{"fpath_tuple": ["huggingface_evaluate", "metrics", "recall", "app.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 6, "window_size": 20, "repo": "huggingface_evaluate", "slice_size": 10}, {"fpath_tuple": ["huggingface_evaluate", "metrics", "recall", "app.py"], "line_no": 2, "start_line_no": 0, "end_line_no": 6, "window_size": 20, "repo": "huggingface_evaluate", "slice_size": 10}, {"fpath_tuple": ["huggingface_evaluate", "metrics", "recall", "app.py"], "line_no": 4, "start_line_no": 0, "end_line_no": 6, "window_size": 20, "repo": "huggingface_evaluate", "slice_size": 10}], "sim_score": 0.72}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# tests/fortuna/test_trainer.py\n# --------------------------------------------------\n#         with unittest.mock.patch.object(trainer, \"save_checkpoint\") as msc:\n#             with self.assertRaises(KeyError):\n#                 trainer.training_step_end(1, state, {}, batch, (), {})\n#         msc.assert_called_once_with(state, \"tmp_dir\", keep=3)\n# \n#         with unittest.mock.patch.object(trainer, \"save_checkpoint\") as msc:\n#             with self.assertRaises(KeyError):\n#                 trainer.training_step_end(1, state, {\"loss\": 1}, batch, (), {})\n#         msc.assert_called_once_with(state, \"tmp_dir\", keep=3)\n# \n#         with unittest.mock.patch.object(trainer, \"save_checkpoint\") as msc:\n#             with self.assertRaises(KeyError):\n#                 trainer.training_step_end(\n#                     1, state, {\"loss\": 1, \"logging_kwargs\": None}, batch, (), {}\n#                 )\n#         msc.assert_called_once_with(state, \"tmp_dir\", keep=3)\n# \n#     def test_training_step_end_ok_no_training_metrics_computation(self):\n#         trainer = FakeTrainer(\n#             predict_fn=lambda x: x,\n# --------------------------------------------------\n# the below code fragment can be found in:\n# tests/fortuna/test_trainer.py\n# --------------------------------------------------\n#                 trainer.training_step_end(1, state, {}, batch, (), {})\n#         msc.assert_called_once_with(state, \"tmp_dir\", keep=3)\n# \n#         with unittest.mock.patch.object(trainer, \"save_checkpoint\") as msc:\n#             with self.assertRaises(KeyError):\n#                 trainer.training_step_end(1, state, {\"loss\": 1}, batch, (), {})\n#         msc.assert_called_once_with(state, \"tmp_dir\", keep=3)\n# \n#         with unittest.mock.patch.object(trainer, \"save_checkpoint\") as msc:\n#             with self.assertRaises(KeyError):\n#                 trainer.training_step_end(\n#                     1, state, {\"loss\": 1, \"logging_kwargs\": None}, batch, (), {}\n#                 )\n#         msc.assert_called_once_with(state, \"tmp_dir\", keep=3)\n# \n#     def test_training_step_end_ok_no_training_metrics_computation(self):\n#         trainer = FakeTrainer(\n#             predict_fn=lambda x: x,\n#             disable_training_metrics_computation=True,\n#             save_checkpoint_dir=\"tmp_dir\",\n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\n=True,\n            save_checkpoint_dir=\"tmp_dir\",\n            save_every_n_steps=1,\n            keep_top_n_checkpoints=3,\n        )\n        state = FakeTrainState()\n        batch = [[1, 2, 3], [0, 0, 1]]\n\n        with unittest.mock.patch.object(trainer, \"save_checkpoint\") as msc:\n            training_losses_and_metrics = trainer.training_step_end(\n                1, state, {\"loss\": 1, \"logging_kwargs\": None}, batch, (), {}\n            )\n        msc.assert_called_once_with(state, \"tmp_dir\", keep=3)\n        self.assertEqual(training_losses_and_metrics, {\"loss\": 1})\n\n        with unittest.mock.patch.object(trainer, \"save_checkpoint\") as msc:\n            training_losses_and_metrics = trainer.training_step_end(\n                1,\n                state,\n                {\"loss\": 1, \"logging_kwargs\": {\"metric1:\": 0.1, \"metrics2\": 0.2}},\n                batch,\n                (),\n                {},\n            )\n        msc.assert_called_once_with(state, \"tmp_dir\", keep=3)\n        self.assertEqual(\n            training_losses_and_metrics, {\"loss\": 1, \"metric1:\": 0.1, \"metrics2\": 0.2}\n        )\n\n        trainer = FakeTrainer(\n            predict_fn=lambda x: x,\n            disable_training_metrics_computation=False,\n            save_checkpoint_dir=\"tmp_dir\",\n            save_every_n_steps=1,\n            keep_top_n_checkpoints=3,\n        )\n\n        with unittest.mock.patch.object(trainer, \"save_checkpoint\") as msc:\n            training_losses_and_metrics = trainer.training_step_end(\n                1, state, {\"loss\": 1, \"logging_kwargs\": None}, batch, None, {}\n            )\n        msc.assert_called_once_with(state, \"tmp_dir\", keep=3)\n        self.assertEqual(training_losses_and_metrics, {\"loss\": 1})\n\n        with unittest.mock.patch.object(trainer, \"save_checkpoint\") as msc:\n            training_losses_and_metrics = trainer.training_step_end(\n                1,\n                state,\n                {\"loss\": 1, \"logging_kwargs\": {\"metric1:\": 0.1, \"metrics2\": 0.2}},\n                batch,\n                None,\n                {},\n            )\n        msc.assert_called_once_with(state, \"tmp_dir\", keep=3)\n        self.assertEqual(\n            training_losses_and_metrics, {\"loss\": 1, \"metric1:\": 0.1, \"metrics2\": 0.2}\n        )\n\n    def test_training_step_end_ok(self):\n        trainer = FakeTrainer(\n            predict_fn=lambda x: x,\n            disable_training_metrics_computation=False,\n            save_checkpoint_dir=\"tmp_dir\",\n            save_every_n_steps=1,\n            keep_top_n_checkpoints=3,\n        )\n        state = FakeTrainState()\n        batch = [[1, 2, 3], [0, 0, 1]]\n\n        def train_m1(a, b):\n            return 12.0\n\n        with unittest.mock.patch.object(trainer, \"save_checkpoint\") as msc:\n            training_losses_and_metrics = trainer.training_step_end(\n                1,\n                state,\n                {\"loss\": 1, \"logging_kwargs\": None, \"outputs\": [10, 20, 30]},\n                batch,\n                (train_m1,),\n                {},\n            )\n        msc.assert_called_once_with(state, \"tmp_dir\", keep=3)\n        self.assertEqual(training_losses_and_metrics, {\"loss\": 1, \"train_m1\": 12.0})\n\n    def test__get_mean_losses_and_metrics_ok(self):\n        trainer = FakeTrainer(\n            predict_fn=lambda x: x, disable_training_metrics_computation=False\n        )\n        losses_and_metrics = [\n            {\n                \"train_loss\": jnp.array(0.1),\n                \"val_loss\": jnp.array(0.2),\n                \"val_accuracy\": jnp.array(0.1),\n            },\n            {\n                \"train_loss\": jnp.array(0.05),\n                \"val_loss\": jnp.array(0.21),\n                \"val_accuracy\": jnp.array(0.1),\n            },\n            {\n                \"train_loss\": jnp.array(0.0),\n                \"val_loss\": jnp.array(0.22),\n                \"val_accuracy\": jnp.array(0.1),\n            },\n        ]\n        observed_losses_and_metrics = trainer._get_mean_losses_and_metrics(\n            losses_and_metrics\n        )\n        expected_losses_and_metrics = {\n            \"train_loss\": jnp.array(0.05),\n            \"val_accuracy\": jnp.array(0.1),\n            \"val_loss\": jnp.array(0.21),\n        }\n        self.assertDictEqual(observed_losses_and_metrics, expected_losses_and_metrics)\n\n    def test__get_mean_losses_and_metrics_ko(self):\n        trainer = FakeTrainer(\n            predict_fn=lambda x: x, disable_training_metrics_computation=False\n        )\n        losses_and_metrics = [\n            {\n                \"train_loss\": jnp.array(0.1),\n                \"val_loss\": jnp.array(0.2),\n                \"val_accuracy\": jnp.array(0.1),\n            },\n            {\"train_loss\": jnp.array(0.05), \"val_accuracy\": jnp.array(0.1)},\n            {\n                \"train_loss\": jnp.array(0.0),\n                \"val_loss\": jnp.array(0.22),\n                \"val_accuracy\": jnp.array(0.1),\n            },\n        ]\n        with self.assertRaises(ValueError):\n            _ = trainer._get_mean_losses_and_metrics(losses_and_metrics)\n\n    def test_training_epoch_end(self):\n        trainer = FakeTrainer(\n            predict_fn=lambda x: x, disable_training_metrics_computation=False\n        )\n\n        losses_and_metrics = [\n            {\n                \"train_loss\": jnp.array(0.1),\n                \"val_loss\": jnp.array(0.2),\n                \"val_accuracy\": jnp.array(0.1),\n            },\n            {\n                \"train_loss\": jnp.array(0.0),\n                \"val_loss\": jnp.array(0.22),\n                \"val_accuracy\": jnp.array(0.1),\n            },\n        ]\n        fake_out = {\n            \"train_loss\": jnp.array(0.0),\n            \"val_loss\": jnp.array(0.22),\n            \"val_accuracy\": jnp.array(0.1),\n        }\n\n        with unittest.mock.patch.object(\n            trainer, \"_get_mean_losses_and_metrics\", return_value=fake_out\n        ) as m:\n            observed = trainer.training_epoch_end(losses_and_metrics)\n        m.assert_called_once_with(losses_and_metrics)\n        self.assertDictEqual(observed, fake_out)\n\n    def test_validation_epoch_end(self):\n        trainer = FakeTrainer(", "metadata": {"task_id": "awslabs_fortuna/48", "ground_truth": "            predict_fn=lambda x: x,", "fpath_tuple": ["awslabs_fortuna", "tests", "fortuna", "test_trainer.py"], "context_start_lineno": 112, "line_no": 279, "query_window": {"context": "                \"train_loss\": jnp.array(0.0),\n                \"val_loss\": jnp.array(0.22),\n                \"val_accuracy\": jnp.array(0.1),\n            },\n        ]\n        fake_out = {\n            \"train_loss\": jnp.array(0.0),\n            \"val_loss\": jnp.array(0.22),\n            \"val_accuracy\": jnp.array(0.1),\n        }\n\n        with unittest.mock.patch.object(\n            trainer, \"_get_mean_losses_and_metrics\", return_value=fake_out\n        ) as m:\n            observed = trainer.training_epoch_end(losses_and_metrics)\n        m.assert_called_once_with(losses_and_metrics)\n        self.assertDictEqual(observed, fake_out)\n\n    def test_validation_epoch_end(self):\n        trainer = FakeTrainer(", "metadata": {"fpath_tuple": ["awslabs_fortuna", "tests", "fortuna", "test_trainer.py"], "line_no": 279, "task_id": "awslabs_fortuna/48", "start_line_no": 259, "end_line_no": 279, "window_size": 20, "context_start_lineno": 112, "repo": "awslabs_fortuna"}}, "top_k_context": [{"context": "        with unittest.mock.patch.object(trainer, \"save_checkpoint\") as msc:\n            with self.assertRaises(KeyError):\n                trainer.training_step_end(1, state, {}, batch, (), {})\n        msc.assert_called_once_with(state, \"tmp_dir\", keep=3)\n\n        with unittest.mock.patch.object(trainer, \"save_checkpoint\") as msc:\n            with self.assertRaises(KeyError):\n                trainer.training_step_end(1, state, {\"loss\": 1}, batch, (), {})\n        msc.assert_called_once_with(state, \"tmp_dir\", keep=3)\n\n        with unittest.mock.patch.object(trainer, \"save_checkpoint\") as msc:\n            with self.assertRaises(KeyError):\n                trainer.training_step_end(\n                    1, state, {\"loss\": 1, \"logging_kwargs\": None}, batch, (), {}\n                )\n        msc.assert_called_once_with(state, \"tmp_dir\", keep=3)\n\n    def test_training_step_end_ok_no_training_metrics_computation(self):\n        trainer = FakeTrainer(\n            predict_fn=lambda x: x,", "metadata": [{"fpath_tuple": ["awslabs_fortuna", "tests", "fortuna", "test_trainer.py"], "line_no": 102, "start_line_no": 92, "end_line_no": 112, "window_size": 20, "repo": "awslabs_fortuna", "slice_size": 10}], "sim_score": 0.4117647058823529}, {"context": "        state = FakeTrainState()\n        batch = [[1, 2, 3], [0, 0, 1]]\n        with unittest.mock.patch.object(trainer, \"save_checkpoint\") as msc:\n            with self.assertRaises(KeyError):\n                trainer.training_step_end(1, state, {}, batch, (), {})\n        msc.assert_called_once_with(state, \"tmp_dir\", keep=3)\n\n        with unittest.mock.patch.object(trainer, \"save_checkpoint\") as msc:\n            with self.assertRaises(KeyError):\n                trainer.training_step_end(1, state, {\"loss\": 1}, batch, (), {})\n        msc.assert_called_once_with(state, \"tmp_dir\", keep=3)\n\n        with unittest.mock.patch.object(trainer, \"save_checkpoint\") as msc:\n            with self.assertRaises(KeyError):\n                trainer.training_step_end(\n                    1, state, {\"loss\": 1, \"logging_kwargs\": None}, batch, (), {}\n                )\n        msc.assert_called_once_with(state, \"tmp_dir\", keep=3)\n\n    def test_training_step_end_ok_no_training_metrics_computation(self):", "metadata": [{"fpath_tuple": ["awslabs_fortuna", "tests", "fortuna", "test_trainer.py"], "line_no": 100, "start_line_no": 90, "end_line_no": 110, "window_size": 20, "repo": "awslabs_fortuna", "slice_size": 10}], "sim_score": 0.392}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# test/test_modules.py\n# --------------------------------------------------\n#             SafeModule(\n#                 rssm_prior,\n#                 in_keys=[\"state\", \"belief\", \"action\"],\n#                 out_keys=[\n#                     (\"next\", \"prior_mean\"),\n#                     (\"next\", \"prior_std\"),\n#                     \"_\",\n#                     (\"next\", \"belief\"),\n#                 ],\n#             ),\n#             SafeModule(\n#                 rssm_posterior,\n#                 in_keys=[(\"next\", \"belief\"), (\"next\", \"encoded_latents\")],\n#                 out_keys=[\n#                     (\"next\", \"posterior_mean\"),\n#                     (\"next\", \"posterior_std\"),\n#                     (\"next\", \"state\"),\n#                 ],\n#             ),\n#         )\n# --------------------------------------------------\n# the below code fragment can be found in:\n# test/test_cost.py\n# test/test_modules.py\n# --------------------------------------------------\n#                 in_keys=[\"state\", \"belief\", \"action\"],\n#                 out_keys=[\n#                     (\"next\", \"prior_mean\"),\n#                     (\"next\", \"prior_std\"),\n#                     \"_\",\n#                     (\"next\", \"belief\"),\n#                 ],\n#             ),\n#             SafeModule(\n#                 rssm_posterior,\n#                 in_keys=[(\"next\", \"belief\"), (\"next\", \"encoded_latents\")],\n#                 out_keys=[\n#                     (\"next\", \"posterior_mean\"),\n#                     (\"next\", \"posterior_std\"),\n#                     (\"next\", \"state\"),\n#                 ],\n#             ),\n#         )\n#         reward_module = MLP(\n#             out_features=1, depth=2, num_cells=mlp_num_units, activation_class=nn.ELU\n# --------------------------------------------------\n# the below code fragment can be found in:\n# test/test_cost.py\n# --------------------------------------------------\n#             SafeModule(\n#                 rssm_prior,\n#                 in_keys=[\"state\", \"belief\", \"action\"],\n#                 out_keys=[\n#                     (\"next\", \"prior_mean\"),\n#                     (\"next\", \"prior_std\"),\n#                     \"_\",\n#                     (\"next\", \"belief\"),\n#                 ],\n#             ),\n#             SafeModule(\n#                 rssm_posterior,\n#                 in_keys=[(\"next\", \"belief\"), (\"next\", \"encoded_latents\")],\n#                 out_keys=[\n#                     (\"next\", \"posterior_mean\"),\n#                     (\"next\", \"posterior_std\"),\n#                     (\"next\", \"state\"),\n#                 ],\n#             ),\n#         )\n# --------------------------------------------------\n# the below code fragment can be found in:\n# test/test_cost.py\n# --------------------------------------------------\n#         world_modeler = SafeSequential(\n#             SafeModule(\n#                 obs_encoder,\n#                 in_keys=[(\"next\", \"pixels\")],\n#                 out_keys=[(\"next\", \"encoded_latents\")],\n#             ),\n#             rssm_rollout,\n#             SafeModule(\n#                 obs_decoder,\n#                 in_keys=[(\"next\", \"state\"), (\"next\", \"belief\")],\n#                 out_keys=[(\"next\", \"reco_pixels\")],\n#             ),\n#         )\n#         reward_module = SafeModule(\n#             reward_module,\n#             in_keys=[(\"next\", \"state\"), (\"next\", \"belief\")],\n#             out_keys=[\"reward\"],\n#         )\n#         world_model = WorldModelWrapper(world_modeler, reward_module)\n# \n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\n\n        nn.TensorDictModel: Dreamer Model based environnement.\n        nn.TensorDictModel: Dreamer Actor the world model space.\n        nn.TensorDictModel: Dreamer Value model.\n        nn.TensorDictModel: Dreamer Actor for the real world space.\n\n    \"\"\"\n    proof_env_is_none = proof_environment is None\n    if proof_env_is_none:\n        proof_environment = transformed_env_constructor(\n            cfg=cfg, use_env_creator=False, obs_norm_state_dict=obs_norm_state_dict\n        )()\n\n    # Modules\n    obs_encoder = ObsEncoder()\n    obs_decoder = ObsDecoder()\n\n    rssm_prior = RSSMPrior(\n        hidden_dim=cfg.rssm_hidden_dim,\n        rnn_hidden_dim=cfg.rssm_hidden_dim,\n        state_dim=cfg.state_dim,\n        action_spec=proof_environment.action_spec,\n    )\n    rssm_posterior = RSSMPosterior(\n        hidden_dim=cfg.rssm_hidden_dim, state_dim=cfg.state_dim\n    )\n    reward_module = MLP(\n        out_features=1, depth=2, num_cells=cfg.mlp_num_units, activation_class=nn.ELU\n    )\n\n    world_model = _dreamer_make_world_model(\n        obs_encoder, obs_decoder, rssm_prior, rssm_posterior, reward_module\n    ).to(device)\n    with torch.no_grad(), set_exploration_mode(\"random\"):\n        tensordict = proof_environment.rollout(4)\n        tensordict = tensordict.to_tensordict().to(device)\n        tensordict = tensordict.to(device)\n        world_model(tensordict)\n\n    model_based_env = _dreamer_make_mbenv(\n        reward_module,\n        rssm_prior,\n        obs_decoder,\n        proof_environment,\n        use_decoder_in_env,\n        cfg.state_dim,\n        cfg.rssm_hidden_dim,\n    )\n    model_based_env = model_based_env.to(device)\n\n    actor_simulator, actor_realworld = _dreamer_make_actors(\n        obs_encoder,\n        rssm_prior,\n        rssm_posterior,\n        cfg.mlp_num_units,\n        action_key,\n        proof_environment,\n    )\n    actor_simulator = actor_simulator.to(device)\n\n    value_model = _dreamer_make_value_model(cfg.mlp_num_units, value_key)\n    value_model = value_model.to(device)\n    with torch.no_grad(), set_exploration_mode(\"random\"):\n        tensordict = model_based_env.rollout(4)\n        tensordict = tensordict.to(device)\n        tensordict = actor_simulator(tensordict)\n        value_model(tensordict)\n\n    actor_realworld = actor_realworld.to(device)\n    if proof_env_is_none:\n        proof_environment.close()\n        torch.cuda.empty_cache()\n        del proof_environment\n\n    del tensordict\n    return world_model, model_based_env, actor_simulator, value_model, actor_realworld\n\n\ndef _dreamer_make_world_model(\n    obs_encoder, obs_decoder, rssm_prior, rssm_posterior, reward_module\n):\n    # World Model and reward model\n    rssm_rollout = RSSMRollout(\n        SafeModule(\n            rssm_prior,\n            in_keys=[\"state\", \"belief\", \"action\"],\n            out_keys=[\n                (\"next\", \"prior_mean\"),\n                (\"next\", \"prior_std\"),\n                \"_\",\n                (\"next\", \"belief\"),\n            ],\n        ),\n        SafeModule(\n            rssm_posterior,\n            in_keys=[(\"next\", \"belief\"), (\"next\", \"encoded_latents\")],\n            out_keys=[\n                (\"next\", \"posterior_mean\"),\n                (\"next\", \"posterior_std\"),\n                (\"next\", \"state\"),\n            ],\n        ),\n    )\n\n    transition_model = SafeSequential(\n        SafeModule(\n            obs_encoder,\n            in_keys=[(\"next\", \"pixels\")],\n            out_keys=[(\"next\", \"encoded_latents\")],\n        ),\n        rssm_rollout,\n        SafeModule(\n            obs_decoder,\n            in_keys=[(\"next\", \"state\"), (\"next\", \"belief\")],\n            out_keys=[(\"next\", \"reco_pixels\")],\n        ),\n    )\n    reward_model = SafeModule(\n        reward_module,\n        in_keys=[(\"next\", \"state\"), (\"next\", \"belief\")],\n        out_keys=[\"reward\"],\n    )\n    world_model = WorldModelWrapper(\n        transition_model,\n        reward_model,\n    )\n    return world_model\n\n\ndef _dreamer_make_actors(\n    obs_encoder,\n    rssm_prior,\n    rssm_posterior,\n    mlp_num_units,\n    action_key,\n    proof_environment,\n):\n    actor_module = DreamerActor(\n        out_features=proof_environment.action_spec.shape[0],\n        depth=3,\n        num_cells=mlp_num_units,\n        activation_class=nn.ELU,\n    )\n    actor_simulator = _dreamer_make_actor_sim(\n        action_key, proof_environment, actor_module\n    )\n    actor_realworld = _dreamer_make_actor_real(\n        obs_encoder,\n        rssm_prior,\n        rssm_posterior,\n        actor_module,\n        action_key,\n        proof_environment,\n    )\n    return actor_simulator, actor_realworld\n\n\ndef _dreamer_make_actor_sim(action_key, proof_environment, actor_module):\n    actor_simulator = SafeProbabilisticSequential(\n        SafeModule(\n            actor_module,\n            in_keys=[\"state\", \"belief\"],\n            out_keys=[\"loc\", \"scale\"],\n            spec=CompositeSpec(\n                **{\n                    \"loc\": UnboundedContinuousTensorSpec(\n                        proof_environment.action_spec.shape,\n                        device=proof_environment.action_spec.device,\n                    ),\n                    \"scale\": UnboundedContinuousTensorSpec(\n                        proof_environment.action_spec.shape,\n                        device=proof_environment.action_spec.device,\n                    ),\n                }\n            ),\n        ),\n        SafeProbabilisticModule(\n            in_keys=[\"loc\", \"scale\"],\n            out_keys=[action_key],\n            default_interaction_mode=\"random\",\n            distribution_class=TanhNormal,\n            spec=CompositeSpec(**{action_key: proof_environment.action_spec}),\n        ),\n    )\n    return actor_simulator\n\n\ndef _dreamer_make_actor_real(\n    obs_encoder, rssm_prior, rssm_posterior, actor_module, action_key, proof_environment\n):\n    # actor for real world: interacts with states ~ posterior\n    # Out actor differs from the original paper where first they compute prior and posterior and then act on it\n    # but we found that this approach worked better.\n    actor_realworld = SafeSequential(\n        SafeModule(\n            obs_encoder,\n            in_keys=[\"pixels\"],\n            out_keys=[\"encoded_latents\"],\n        ),\n        SafeModule(\n            rssm_posterior,\n            in_keys=[\"belief\", \"encoded_latents\"],\n            out_keys=[\n                \"_\",\n                \"_\",\n                \"state\",\n            ],\n        ),", "metadata": {"task_id": "pytorch_rl/159", "ground_truth": "        SafeProbabilisticSequential(", "fpath_tuple": ["pytorch_rl", "torchrl", "trainers", "helpers", "models.py"], "context_start_lineno": 1488, "line_no": 1696, "query_window": {"context": "    obs_encoder, rssm_prior, rssm_posterior, actor_module, action_key, proof_environment\n):\n    # actor for real world: interacts with states ~ posterior\n    # Out actor differs from the original paper where first they compute prior and posterior and then act on it\n    # but we found that this approach worked better.\n    actor_realworld = SafeSequential(\n        SafeModule(\n            obs_encoder,\n            in_keys=[\"pixels\"],\n            out_keys=[\"encoded_latents\"],\n        ),\n        SafeModule(\n            rssm_posterior,\n            in_keys=[\"belief\", \"encoded_latents\"],\n            out_keys=[\n                \"_\",\n                \"_\",\n                \"state\",\n            ],\n        ),", "metadata": {"fpath_tuple": ["pytorch_rl", "torchrl", "trainers", "helpers", "models.py"], "line_no": 1696, "task_id": "pytorch_rl/159", "start_line_no": 1676, "end_line_no": 1696, "window_size": 20, "context_start_lineno": 1488, "repo": "pytorch_rl"}}, "top_k_context": [{"context": "        )\n        # World Model and reward model\n        world_modeler = SafeSequential(\n            SafeModule(\n                obs_encoder,\n                in_keys=[(\"next\", \"pixels\")],\n                out_keys=[(\"next\", \"encoded_latents\")],\n            ),\n            rssm_rollout,\n            SafeModule(\n                obs_decoder,\n                in_keys=[(\"next\", \"state\"), (\"next\", \"belief\")],\n                out_keys=[(\"next\", \"reco_pixels\")],\n            ),\n        )\n        reward_module = SafeModule(\n            reward_module,\n            in_keys=[(\"next\", \"state\"), (\"next\", \"belief\")],\n            out_keys=[\"reward\"],\n        )", "metadata": [{"fpath_tuple": ["pytorch_rl", "test", "test_cost.py"], "line_no": 2504, "start_line_no": 2494, "end_line_no": 2514, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}], "sim_score": 0.37735849056603776}, {"context": "        # World Model and reward model\n        rssm_rollout = RSSMRollout(\n            SafeModule(\n                rssm_prior,\n                in_keys=[\"state\", \"belief\", \"action\"],\n                out_keys=[\n                    (\"next\", \"prior_mean\"),\n                    (\"next\", \"prior_std\"),\n                    \"_\",\n                    (\"next\", \"belief\"),\n                ],\n            ),\n            SafeModule(\n                rssm_posterior,\n                in_keys=[(\"next\", \"belief\"), (\"next\", \"encoded_latents\")],\n                out_keys=[\n                    (\"next\", \"posterior_mean\"),\n                    (\"next\", \"posterior_std\"),\n                    (\"next\", \"state\"),\n                ],", "metadata": [{"fpath_tuple": ["pytorch_rl", "test", "test_cost.py"], "line_no": 2480, "start_line_no": 2470, "end_line_no": 2490, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}], "sim_score": 0.36792452830188677}, {"context": "            SafeModule(\n                rssm_prior,\n                in_keys=[\"state\", \"belief\", \"action\"],\n                out_keys=[\n                    (\"next\", \"prior_mean\"),\n                    (\"next\", \"prior_std\"),\n                    \"_\",\n                    (\"next\", \"belief\"),\n                ],\n            ),\n            SafeModule(\n                rssm_posterior,\n                in_keys=[(\"next\", \"belief\"), (\"next\", \"encoded_latents\")],\n                out_keys=[\n                    (\"next\", \"posterior_mean\"),\n                    (\"next\", \"posterior_std\"),\n                    (\"next\", \"state\"),\n                ],\n            ),\n        )", "metadata": [{"fpath_tuple": ["pytorch_rl", "test", "test_cost.py"], "line_no": 2482, "start_line_no": 2472, "end_line_no": 2492, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}, {"fpath_tuple": ["pytorch_rl", "test", "test_modules.py"], "line_no": 630, "start_line_no": 620, "end_line_no": 640, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}], "sim_score": 0.3673469387755102}, {"context": "\n        rssm_rollout = RSSMRollout(\n            SafeModule(\n                rssm_prior,\n                in_keys=[\"state\", \"belief\", \"action\"],\n                out_keys=[\n                    (\"next\", \"prior_mean\"),\n                    (\"next\", \"prior_std\"),\n                    \"_\",\n                    (\"next\", \"belief\"),\n                ],\n            ),\n            SafeModule(\n                rssm_posterior,\n                in_keys=[(\"next\", \"belief\"), (\"next\", \"encoded_latents\")],\n                out_keys=[\n                    (\"next\", \"posterior_mean\"),\n                    (\"next\", \"posterior_std\"),\n                    (\"next\", \"state\"),\n                ],", "metadata": [{"fpath_tuple": ["pytorch_rl", "test", "test_modules.py"], "line_no": 628, "start_line_no": 618, "end_line_no": 638, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}], "sim_score": 0.3627450980392157}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# federatedscope/nlp/hetero_tasks/trainer/trainer.py\n# --------------------------------------------------\n# \n#         if self._in_contrast_prepare:\n#             ctx.batch_size = CtxVar(0, LIFECYCLE.BATCH)\n#             dec_out, dec_hidden, example_indices = \\\n#                 outputs.logits, outputs.hidden_states, outputs.example_indices\n#             if len(example_indices) > 0:\n#                 for ex, out in zip(example_indices, dec_out.detach().cpu()):\n#                     ctx.contrast_monitor.update_dec_out(out, k=ex.item())\n#                 for ex, hids in zip(example_indices,\n#                                     dec_hidden.detach().cpu()):\n#                     ctx.contrast_monitor.update_dec_hidden(hids, k=ex.item())\n#         else:\n#             ctx.loss_agg.update(ctx.loss_batch.detach().item(), ctx.batch_size)\n#             if self.use_contrastive_loss:\n#                 if ctx.regular_loss_batch is not None and \\\n#                         ctx.contrastive_loss_batch is not None:\n#                     ctx.regular_loss_agg.update(\n#                         ctx.regular_loss_batch.detach().item(), ctx.batch_size)\n#                     ctx.contrastive_loss_agg.update(\n#                         ctx.contrastive_loss_batch.detach().item(),\n# --------------------------------------------------\n# the below code fragment can be found in:\n# federatedscope/cl/trainer/trainer.py\n# --------------------------------------------------\n#             label = label.unsqueeze(0)\n# \n#         ctx.y_true = CtxVar(label, LIFECYCLE.BATCH)\n#         ctx.y_prob = CtxVar((z1, z2), LIFECYCLE.BATCH)\n#         ctx.loss_batch = CtxVar(ctx.criterion(z1, z2), LIFECYCLE.BATCH)\n#         ctx.batch_size = CtxVar(len(label), LIFECYCLE.BATCH)\n# \n#     def _hook_on_batch_backward(self, ctx):\n#         ctx.optimizer.zero_grad()\n#         ctx.loss_task = ctx.loss_task * self.local_loss_ratio\n#         ctx.loss_task.backward()\n#         if ctx.grad_clip > 0:\n#             torch.nn.utils.clip_grad_norm_(ctx.model.parameters(),\n#                                            ctx.grad_clip)\n# \n#         ctx.optimizer.step()\n#         if ctx.scheduler is not None:\n#             ctx.scheduler.step()\n# \n#     def _hook_on_batch_end(self, ctx):\n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\nimport torch\nfrom copy import deepcopy\n\nfrom federatedscope.core.trainers.enums import LIFECYCLE\nfrom federatedscope.core.trainers.context import CtxVar\nfrom federatedscope.gfl.loss.vat import VATLoss\nfrom federatedscope.core.trainers import GeneralTorchTrainer\n\n\nclass FLITTrainer(GeneralTorchTrainer):\n    def register_default_hooks_train(self):\n        super(FLITTrainer, self).register_default_hooks_train()\n        self.register_hook_in_train(new_hook=record_initialization_local,\n                                    trigger='on_fit_start',\n                                    insert_pos=-1)\n        self.register_hook_in_train(new_hook=del_initialization_local,\n                                    trigger='on_fit_end',\n                                    insert_pos=-1)\n        self.register_hook_in_train(new_hook=record_initialization_global,\n                                    trigger='on_fit_start',\n                                    insert_pos=-1)\n        self.register_hook_in_train(new_hook=del_initialization_global,\n                                    trigger='on_fit_end',\n                                    insert_pos=-1)\n\n    def register_default_hooks_eval(self):\n        super(FLITTrainer, self).register_default_hooks_eval()\n        self.register_hook_in_eval(new_hook=record_initialization_local,\n                                   trigger='on_fit_start',\n                                   insert_pos=-1)\n        self.register_hook_in_eval(new_hook=del_initialization_local,\n                                   trigger='on_fit_end',\n                                   insert_pos=-1)\n        self.register_hook_in_eval(new_hook=record_initialization_global,\n                                   trigger='on_fit_start',\n                                   insert_pos=-1)\n        self.register_hook_in_eval(new_hook=del_initialization_global,\n                                   trigger='on_fit_end',\n                                   insert_pos=-1)\n\n    def _hook_on_batch_forward(self, ctx):\n        batch = ctx.data_batch.to(ctx.device)\n        pred = ctx.model(batch)\n        ctx.global_model.to(ctx.device)\n        predG = ctx.global_model(batch)\n        if ctx.criterion._get_name() == 'CrossEntropyLoss':\n            label = batch.y.squeeze(-1).long()\n        elif ctx.criterion._get_name() == 'MSELoss':\n            label = batch.y.float()\n        else:\n            raise ValueError(\n                f'FLIT trainer not support {ctx.criterion._get_name()}.')\n        if len(label.size()) == 0:\n            label = label.unsqueeze(0)\n\n        lossGlobalLabel = ctx.criterion(predG, label)\n        lossLocalLabel = ctx.criterion(pred, label)\n\n        weightloss = lossLocalLabel + torch.relu(lossLocalLabel -\n                                                 lossGlobalLabel.detach())\n        if ctx.weight_denomaitor is None:\n            ctx.weight_denomaitor = weightloss.mean(dim=0,\n                                                    keepdim=True).detach()\n        else:\n            ctx.weight_denomaitor = self.cfg.flitplus.factor_ema * \\\n                                    ctx.weight_denomaitor + (\n                                            -self.cfg.flitplus.factor_ema +\n                                            1) * weightloss.mean(\n                                            keepdim=True, dim=0).detach()\n        loss = (1 - torch.exp(-weightloss / (ctx.weight_denomaitor + 1e-7)) +\n                1e-7)**self.cfg.flitplus.tmpFed * (lossLocalLabel)\n        ctx.loss_batch = loss.mean()\n\n        ctx.batch_size = len(label)\n        ctx.y_true = CtxVar(label, LIFECYCLE.BATCH)", "metadata": {"task_id": "alibaba_FederatedScope/40", "ground_truth": "        ctx.y_prob = CtxVar(pred, LIFECYCLE.BATCH)", "fpath_tuple": ["alibaba_FederatedScope", "federatedscope", "gfl", "flitplus", "trainer.py"], "context_start_lineno": 0, "line_no": 75, "query_window": {"context": "        lossGlobalLabel = ctx.criterion(predG, label)\n        lossLocalLabel = ctx.criterion(pred, label)\n\n        weightloss = lossLocalLabel + torch.relu(lossLocalLabel -\n                                                 lossGlobalLabel.detach())\n        if ctx.weight_denomaitor is None:\n            ctx.weight_denomaitor = weightloss.mean(dim=0,\n                                                    keepdim=True).detach()\n        else:\n            ctx.weight_denomaitor = self.cfg.flitplus.factor_ema * \\\n                                    ctx.weight_denomaitor + (\n                                            -self.cfg.flitplus.factor_ema +\n                                            1) * weightloss.mean(\n                                            keepdim=True, dim=0).detach()\n        loss = (1 - torch.exp(-weightloss / (ctx.weight_denomaitor + 1e-7)) +\n                1e-7)**self.cfg.flitplus.tmpFed * (lossLocalLabel)\n        ctx.loss_batch = loss.mean()\n\n        ctx.batch_size = len(label)\n        ctx.y_true = CtxVar(label, LIFECYCLE.BATCH)", "metadata": {"fpath_tuple": ["alibaba_FederatedScope", "federatedscope", "gfl", "flitplus", "trainer.py"], "line_no": 75, "task_id": "alibaba_FederatedScope/40", "start_line_no": 55, "end_line_no": 75, "window_size": 20, "context_start_lineno": 0, "repo": "alibaba_FederatedScope"}}, "top_k_context": [{"context": "        z1, z2 = ctx.model(x1, x2)\n        if len(label.size()) == 0:\n            label = label.unsqueeze(0)\n\n        ctx.y_true = CtxVar(label, LIFECYCLE.BATCH)\n        ctx.y_prob = CtxVar((z1, z2), LIFECYCLE.BATCH)\n        ctx.loss_batch = CtxVar(ctx.criterion(z1, z2), LIFECYCLE.BATCH)\n        ctx.batch_size = CtxVar(len(label), LIFECYCLE.BATCH)\n\n    def _hook_on_batch_backward(self, ctx):\n        ctx.optimizer.zero_grad()\n        ctx.loss_task = ctx.loss_task * self.local_loss_ratio\n        ctx.loss_task.backward()\n        if ctx.grad_clip > 0:\n            torch.nn.utils.clip_grad_norm_(ctx.model.parameters(),\n                                           ctx.grad_clip)\n\n        ctx.optimizer.step()\n        if ctx.scheduler is not None:\n            ctx.scheduler.step()", "metadata": [{"fpath_tuple": ["alibaba_FederatedScope", "federatedscope", "cl", "trainer", "trainer.py"], "line_no": 52, "start_line_no": 42, "end_line_no": 62, "window_size": 20, "repo": "alibaba_FederatedScope", "slice_size": 10}], "sim_score": 0.3237410071942446}, {"context": "                    ctx.y_true = CtxVar(labels[:, 1:], LIFECYCLE.BATCH)\n                    return\n\n        if self._in_contrast_prepare:\n            ctx.batch_size = CtxVar(0, LIFECYCLE.BATCH)\n            dec_out, dec_hidden, example_indices = \\\n                outputs.logits, outputs.hidden_states, outputs.example_indices\n            if len(example_indices) > 0:\n                for ex, out in zip(example_indices, dec_out.detach().cpu()):\n                    ctx.contrast_monitor.update_dec_out(out, k=ex.item())\n                for ex, hids in zip(example_indices,\n                                    dec_hidden.detach().cpu()):\n                    ctx.contrast_monitor.update_dec_hidden(hids, k=ex.item())\n        else:\n            ctx.loss_agg.update(ctx.loss_batch.detach().item(), ctx.batch_size)\n            if self.use_contrastive_loss:\n                if ctx.regular_loss_batch is not None and \\\n                        ctx.contrastive_loss_batch is not None:\n                    ctx.regular_loss_agg.update(\n                        ctx.regular_loss_batch.detach().item(), ctx.batch_size)", "metadata": [{"fpath_tuple": ["alibaba_FederatedScope", "federatedscope", "nlp", "hetero_tasks", "trainer", "trainer.py"], "line_no": 504, "start_line_no": 494, "end_line_no": 514, "window_size": 20, "repo": "alibaba_FederatedScope", "slice_size": 10}], "sim_score": 0.3191489361702128}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# vizier/_src/pyvizier/shared/trial.py\n# --------------------------------------------------\n#   status: Optional[FrozenSet[TrialStatus]] = attr.field(\n#       default=None,\n#       converter=lambda x: frozenset(x) if x is not None else None,\n#       validator=attr.validators.optional(\n#           attr.validators.deep_iterable(\n#               attr.validators.instance_of(TrialStatus),\n#               attr.validators.instance_of(frozenset))))\n# \n#   # TODO: Add \"search_space\" argument\n# \n#   def __call__(self, trial: Trial) -> bool:\n#     if self.ids is not None:\n#       if trial.id not in self.ids:\n#         return False\n#     if self.min_id is not None:\n#       if trial.id < self.min_id:\n#         return False\n#     if self.max_id is not None:\n#       if trial.id > self.max_id:\n#         return False\n# --------------------------------------------------\n# the below code fragment can be found in:\n# vizier/_src/pyvizier/shared/parameter_config.py\n# --------------------------------------------------\n#   # SearchSpace.\n#   _selected: tuple['SearchSpace'] = attr.field(init=True)\n# \n#   def __len__(self) -> int:\n#     return len(self._selected)\n# \n#   def __init__(self, selected: SearchSpaceOrSpaces):\n#     if isinstance(selected, Collection):\n#       self.__attrs_init__(tuple(selected))\n#     else:\n#       self.__attrs_init__(tuple([selected]))\n# \n#   def add_float_param(self,\n#                       name: str,\n#                       min_value: float,\n#                       max_value: float,\n#                       *,\n#                       default_value: Optional[float] = None,\n#                       scale_type: Optional[ScaleType] = ScaleType.LINEAR,\n#                       index: Optional[int] = None) -> 'ParameterConfigSelector':\n# --------------------------------------------------\n# the below code fragment can be found in:\n# vizier/_src/pyvizier/shared/parameter_config.py\n# --------------------------------------------------\n# \n#     new_params = []\n#     for param_name in param_names:\n#       new_pc = ParameterConfig.factory(\n#           name=param_name,\n#           bounds=bounds,\n#           scale_type=scale_type,\n#           default_value=default_value)\n#       new_params.append(new_pc)\n#     return self._add_parameters(new_params)\n# \n#   def add_int_param(\n#       self,\n#       name: str,\n#       min_value: int,\n#       max_value: int,\n#       *,\n#       default_value: Optional[int] = None,\n#       scale_type: Optional[ScaleType] = None,\n#       index: Optional[int] = None,\n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\n# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import annotations\n\n\"\"\"HPO-B dataset.\n\nNote that we denote (X,Y) as a batched set of trials (i.e. suggestions X and\nobjectives Y) and (x,y) as a single trial. This is slightly different from (X,y)\nnotation used in the handler to denote batched trials.\n\"\"\"\n# TODO: Replace internal HPOB experimenter with this.\n# pylint:disable=invalid-name\nimport copy\nimport enum\nimport json\nfrom typing import Any, Dict, Iterable, List, Optional, Sequence, Tuple\n\nimport attr\nimport attrs\nimport numpy as np\n\nfrom vizier import pyvizier as vz\nfrom vizier._src.benchmarks.experimenters import experimenter\nfrom vizier._src.benchmarks.experimenters.hpob import handler as handler_lib\n\nimport xgboost as xgb\n\nOpen = open\n\nMETRIC_NAME = 'objective_value'\n# Offset applied to parameter values before the log transformation.\n_TF_OFFSET = 1e-4\n\n\n@attrs.define(auto_attribs=True)\nclass _Dataset:\n  \"\"\"Raw data from HPO-B.\n\n  X and Y are guaranteed to have compatible shapes. A dataset can be sliced like\n  regular numpy arrays but but cannot be indexed at a single point, i.e.\n    `dataset[0]` is not allowed\n    `dataset[0:1]` is allowed\n    `dataset[dataset.Y > 0]` is allowed.\n\n  If the log-transformation is applied to a feature, it's offset by a constant\n    x_log = np.log(x+0.0001)\n\n  Attributes:\n    X: 2-D array of shape (number of observations) * (number of input features).\n      The features may be scaled and log-transformed. _SearchspaceDescriptor\n      holds the necessary information to recover the original values.\n    Y: 2-D array of objective values, of shape (number of observations, 1). The\n      values are not pre-processed.\n  \"\"\"\n\n  X: np.ndarray = attrs.field(converter=np.asarray)\n  Y: np.ndarray = attrs.field(converter=np.asarray)\n\n  def __attrs_post_init__(self) -> None:\n    \"\"\"Performs validation.\"\"\"\n    if len(self.X.shape) != 2:\n      raise ValueError(f'X must be 2-D. Given: {self.X.shape}')\n    if len(self.Y.shape) != 2:\n      raise ValueError(f'Y must be 2-D. Given: {self.Y.shape}')\n\n    if self.X.shape[0] != self.Y.shape[0]:\n      raise ValueError(f'X and y must have same number of rows. '\n                       f'X.shape={self.X.shape}, y.shape={self.Y.shape}')\n\n  def __getitem__(self, idx: slice) -> '_Dataset':\n    return _Dataset(self.X[idx], self.Y[idx])\n\n  def __len__(self) -> int:\n    return self.Y.shape[0]\n\n\n@attr.define(init=True, kw_only=False)\nclass _VariableDescriptor:\n  \"\"\"Variable descriptor.\"\"\"\n  name: str = attrs.field()\n  min_value: Optional[float] = attrs.field(default=None)\n  max_value: Optional[float] = attrs.field(default=None)\n  min_value_before_tf: Optional[float] = attrs.field(default=None)\n  max_value_before_tf: Optional[float] = attrs.field(default=None)\n  apply_log: bool = attrs.field(default=True)\n  optional: bool = attrs.field(default=False)\n  categories: List[str] = attrs.field(factory=list)\n\n  def scale(self, x: np.ndarray) -> np.ndarray:\n    # Raw numbers to HPOB scale. Input and output are 1-D\n    if self.is_categorical:\n      return x\n    if self.apply_log:\n      x = np.log(x)\n    x = (x - self.min_value) / (self.max_value - self.min_value)\n    return x\n\n  def unscale(self, x: np.ndarray) -> np.ndarray:\n    # HPOB scale to raw numbers. Input and output are 1-D.\n    if self.is_categorical:\n      return x\n    x = x * (self.max_value - self.min_value) + self.min_value", "metadata": {"task_id": "google_vizier/25", "ground_truth": "    if self.apply_log:", "fpath_tuple": ["google_vizier", "vizier", "_src", "benchmarks", "experimenters", "hpob_experimenter.py"], "context_start_lineno": 0, "line_no": 114, "query_window": {"context": "  min_value_before_tf: Optional[float] = attrs.field(default=None)\n  max_value_before_tf: Optional[float] = attrs.field(default=None)\n  apply_log: bool = attrs.field(default=True)\n  optional: bool = attrs.field(default=False)\n  categories: List[str] = attrs.field(factory=list)\n\n  def scale(self, x: np.ndarray) -> np.ndarray:\n    # Raw numbers to HPOB scale. Input and output are 1-D\n    if self.is_categorical:\n      return x\n    if self.apply_log:\n      x = np.log(x)\n    x = (x - self.min_value) / (self.max_value - self.min_value)\n    return x\n\n  def unscale(self, x: np.ndarray) -> np.ndarray:\n    # HPOB scale to raw numbers. Input and output are 1-D.\n    if self.is_categorical:\n      return x\n    x = x * (self.max_value - self.min_value) + self.min_value", "metadata": {"fpath_tuple": ["google_vizier", "vizier", "_src", "benchmarks", "experimenters", "hpob_experimenter.py"], "line_no": 114, "task_id": "google_vizier/25", "start_line_no": 94, "end_line_no": 114, "window_size": 20, "context_start_lineno": 0, "repo": "google_vizier"}}, "top_k_context": [{"context": "    bounds = (float(min_value), float(max_value))\n    param_names = self._get_parameter_names_to_create(name=name, index=index)\n\n    new_params = []\n    for param_name in param_names:\n      new_pc = ParameterConfig.factory(\n          name=param_name,\n          bounds=bounds,\n          scale_type=scale_type,\n          default_value=default_value)\n      new_params.append(new_pc)\n    return self._add_parameters(new_params)\n\n  def add_int_param(\n      self,\n      name: str,\n      min_value: int,\n      max_value: int,\n      *,\n      default_value: Optional[int] = None,", "metadata": [{"fpath_tuple": ["google_vizier", "vizier", "_src", "pyvizier", "shared", "parameter_config.py"], "line_no": 730, "start_line_no": 720, "end_line_no": 740, "window_size": 20, "repo": "google_vizier", "slice_size": 10}], "sim_score": 0.27350427350427353}, {"context": "  # Selected (sub)-spaces.\n  # TODO: Consider switching the order of SearchSpaceSelector and\n  # SearchSpace.\n  _selected: tuple['SearchSpace'] = attr.field(init=True)\n\n  def __len__(self) -> int:\n    return len(self._selected)\n\n  def __init__(self, selected: SearchSpaceOrSpaces):\n    if isinstance(selected, Collection):\n      self.__attrs_init__(tuple(selected))\n    else:\n      self.__attrs_init__(tuple([selected]))\n\n  def add_float_param(self,\n                      name: str,\n                      min_value: float,\n                      max_value: float,\n                      *,\n                      default_value: Optional[float] = None,", "metadata": [{"fpath_tuple": ["google_vizier", "vizier", "_src", "pyvizier", "shared", "parameter_config.py"], "line_no": 688, "start_line_no": 678, "end_line_no": 698, "window_size": 20, "repo": "google_vizier", "slice_size": 10}], "sim_score": 0.26153846153846155}, {"context": "  min_id: Optional[int] = attr.field(default=None)\n  max_id: Optional[int] = attr.field(default=None)\n  status: Optional[FrozenSet[TrialStatus]] = attr.field(\n      default=None,\n      converter=lambda x: frozenset(x) if x is not None else None,\n      validator=attr.validators.optional(\n          attr.validators.deep_iterable(\n              attr.validators.instance_of(TrialStatus),\n              attr.validators.instance_of(frozenset))))\n\n  # TODO: Add \"search_space\" argument\n\n  def __call__(self, trial: Trial) -> bool:\n    if self.ids is not None:\n      if trial.id not in self.ids:\n        return False\n    if self.min_id is not None:\n      if trial.id < self.min_id:\n        return False\n    if self.max_id is not None:", "metadata": [{"fpath_tuple": ["google_vizier", "vizier", "_src", "pyvizier", "shared", "trial.py"], "line_no": 618, "start_line_no": 608, "end_line_no": 628, "window_size": 20, "repo": "google_vizier", "slice_size": 10}], "sim_score": 0.26119402985074625}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# torchrl/envs/transforms/transforms.py\n# --------------------------------------------------\n#         if not isinstance(loc, torch.Tensor):\n#             loc = torch.tensor(loc)\n#         if not isinstance(scale, torch.Tensor):\n#             scale = torch.tensor(scale)\n# \n#         self.register_buffer(\"loc\", loc)\n#         self.register_buffer(\"scale\", scale.clamp_min(1e-6))\n# \n#     def _apply_transform(self, reward: torch.Tensor) -> torch.Tensor:\n#         if self.standard_normal:\n#             loc = self.loc\n#             scale = self.scale\n#             reward = (reward - loc) / scale\n#             return reward\n#         else:\n#             scale = self.scale\n#             loc = self.loc\n#             reward = reward * scale + loc\n#             return reward\n# \n# --------------------------------------------------\n# the below code fragment can be found in:\n# torchrl/envs/transforms/transforms.py\n# --------------------------------------------------\n#         if not isinstance(scale, torch.Tensor):\n#             scale = torch.tensor(scale)\n# \n#         self.register_buffer(\"loc\", loc)\n#         self.register_buffer(\"scale\", scale.clamp_min(1e-6))\n# \n#     def _apply_transform(self, reward: torch.Tensor) -> torch.Tensor:\n#         if self.standard_normal:\n#             loc = self.loc\n#             scale = self.scale\n#             reward = (reward - loc) / scale\n#             return reward\n#         else:\n#             scale = self.scale\n#             loc = self.loc\n#             reward = reward * scale + loc\n#             return reward\n# \n#     def transform_reward_spec(self, reward_spec: TensorSpec) -> TensorSpec:\n#         if isinstance(reward_spec, UnboundedContinuousTensorSpec):\n# --------------------------------------------------\n# the below code fragment can be found in:\n# torchrl/data/tensor_specs.py\n# --------------------------------------------------\n#         if dtype is None:\n#             dtype = torch.get_default_dtype()\n#         if device is None:\n#             device = torch._get_default_device()\n# \n#         if not isinstance(minimum, torch.Tensor):\n#             minimum = torch.tensor(minimum, dtype=dtype, device=device)\n#         if not isinstance(maximum, torch.Tensor):\n#             maximum = torch.tensor(maximum, dtype=dtype, device=device)\n#         if maximum.device != device:\n#             maximum = maximum.to(device)\n#         if minimum.device != device:\n#             minimum = minimum.to(device)\n#         if dtype is not None and minimum.dtype is not dtype:\n#             minimum = minimum.to(dtype)\n#         if dtype is not None and maximum.dtype is not dtype:\n#             maximum = maximum.to(dtype)\n#         err_msg = (\n#             \"BoundedTensorSpec requires the shape to be explicitely (via \"\n#             \"the shape argument) or implicitely defined (via either the \"\n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\n sigma parameter (squared root of variance)\n        upscale (torch.Tensor or number, optional): 'a' scaling factor in the formula:\n\n            .. math::\n                loc = tanh(loc / upscale) * upscale.\n\n            Default is 5.0\n\n        tanh_loc (bool, optional): if True, the above formula is used for the location scaling, otherwise the raw value\n            is kept.\n            Default is :obj:`True`;\n    \"\"\"\n\n    num_params: int = 2\n\n    def __init__(\n        self,\n        loc: torch.Tensor,\n        scale: torch.Tensor,\n        upscale: float = 5.0,\n        tanh_loc: bool = True,\n        event_dim: int = 1,\n        **kwargs,\n    ):\n        self.tanh_loc = tanh_loc\n        self.upscale = upscale\n        self._event_dim = event_dim\n        self._kwargs = kwargs\n        super().__init__(D.Normal(loc, scale, **kwargs), event_dim)\n\n    def update(self, loc, scale):\n        if self.tanh_loc:\n            loc = self.upscale * (loc / self.upscale).tanh()\n        super().__init__(D.Normal(loc, scale, **self._kwargs), self._event_dim)\n\n    @property\n    def mode(self):\n        return self.base_dist.mean\n\n\nclass SafeTanhTransform(D.TanhTransform):\n    \"\"\"TanhTransform subclass that ensured that the transformation is numerically invertible.\"\"\"\n\n    def _call(self, x: torch.Tensor) -> torch.Tensor:\n        y = safetanh(x)\n        return y\n\n    def _inverse(self, y: torch.Tensor) -> torch.Tensor:\n        if y.dtype.is_floating_point:\n            eps = torch.finfo(y.dtype).eps\n        else:\n            raise NotImplementedError(\"No inverse tanh for integer inputs.\")\n        y = y.clamp(-1 + eps, 1 - eps)\n        x = super()._inverse(y)\n        return x\n\n\nclass NormalParamWrapper(nn.Module):\n    \"\"\"A wrapper for normal distribution parameters.\n\n    Args:\n        operator (nn.Module): operator whose output will be transformed_in in location and scale parameters\n        scale_mapping (str, optional): positive mapping function to be used with the std.\n            default = \"biased_softplus_1.0\" (i.e. softplus map with bias such that fn(0.0) = 1.0)\n            choices: \"softplus\", \"exp\", \"relu\", \"biased_softplus_1\";\n        scale_lb (Number, optional): The minimum value that the variance can take. Default is 1e-4.\n\n    Examples:\n        >>> from torch import nn\n        >>> import torch\n        >>> module = nn.Linear(3, 4)\n        >>> module_normal = NormalParamWrapper(module)\n        >>> tensor = torch.randn(3)\n        >>> loc, scale = module_normal(tensor)\n        >>> print(loc.shape, scale.shape)\n        torch.Size([2]) torch.Size([2])\n        >>> assert (scale > 0).all()\n        >>> # with modules that return more than one tensor\n        >>> module = nn.LSTM(3, 4)\n        >>> module_normal = NormalParamWrapper(module)\n        >>> tensor = torch.randn(4, 2, 3)\n        >>> loc, scale, others = module_normal(tensor)\n        >>> print(loc.shape, scale.shape)\n        torch.Size([4, 2, 2]) torch.Size([4, 2, 2])\n        >>> assert (scale > 0).all()\n\n    \"\"\"\n\n    def __init__(\n        self,\n        operator: nn.Module,\n        scale_mapping: str = \"biased_softplus_1.0\",\n        scale_lb: Number = 1e-4,\n    ) -> None:\n        super().__init__()\n        self.operator = operator\n        self.scale_mapping = scale_mapping\n        self.scale_lb = scale_lb\n\n    def forward(self, *tensors: torch.Tensor) -> Tuple[torch.Tensor]:\n        net_output = self.operator(*tensors)\n        others = ()\n        if not isinstance(net_output, torch.Tensor):\n            net_output, *others = net_output\n        loc, scale = net_output.chunk(2, -1)\n        scale = mappings(self.scale_mapping)(scale).clamp_min(self.scale_lb)\n        return (loc, scale, *others)\n\n\nclass TruncatedNormal(D.Independent):\n    \"\"\"Implements a Truncated Normal distribution with location scaling.\n\n    Location scaling prevents the location to be \"too far\" from 0, which ultimately\n    leads to numerically unstable samples and poor gradient computation (e.g. gradient explosion).\n    In practice, the location is computed according to\n\n        .. math::\n            loc = tanh(loc / upscale) * upscale.\n\n    This behaviour can be disabled by switching off the tanh_loc parameter (see below).\n\n\n    Args:\n        loc (torch.Tensor): normal distribution location parameter\n        scale (torch.Tensor): normal distribution sigma parameter (squared root of variance)\n        upscale (torch.Tensor or number, optional): 'a' scaling factor in the formula:\n\n            .. math::\n                loc = tanh(loc / upscale) * upscale.\n\n            Default is 5.0\n\n        min (torch.Tensor or number, optional): minimum value of the distribution. Default = -1.0;\n        max (torch.Tensor or number, optional): maximum value of the distribution. Default = 1.0;\n        tanh_loc (bool, optional): if True, the above formula is used for the location scaling, otherwise the raw value\n            is kept.\n            Default is :obj:`True`;\n    \"\"\"\n\n    num_params: int = 2\n\n    arg_constraints = {\n        \"loc\": constraints.real,\n        \"scale\": constraints.greater_than(1e-6),\n    }\n\n    def __init__(\n        self,\n        loc: torch.Tensor,\n        scale: torch.Tensor,\n        upscale: Union[torch.Tensor, float] = 5.0,\n        min: Union[torch.Tensor, float] = -1.0,\n        max: Union[torch.Tensor, float] = 1.0,\n        tanh_loc: bool = True,\n    ):\n        err_msg = \"TanhNormal max values must be strictly greater than min values\"\n        if isinstance(max, torch.Tensor) or isinstance(min, torch.Tensor):\n            if not (max > min).all():\n                raise RuntimeError(err_msg)\n        elif isinstance(max, Number) and isinstance(min, Number):\n            if not max > min:\n                raise RuntimeError(err_msg)\n        else:\n            if not all(max > min):\n                raise RuntimeError(err_msg)\n\n        if isinstance(max, torch.Tensor):\n            self.non_trivial_max = (max != 1.0).any()\n        else:\n            self.non_trivial_max = max != 1.0\n\n        if isinstance(min, torch.Tensor):\n            self.non_trivial_min = (min != -1.0).any()\n        else:\n            self.non_trivial_min = min != -1.0\n        self.tanh_loc = tanh_loc\n\n        self.device = loc.device\n        self.upscale = (\n            upscale\n            if not isinstance(upscale, torch.Tensor)\n            else upscale.to(self.device)\n        )\n\n        if isinstance(max, torch.Tensor):\n            max = max.to(self.device)\n        else:\n            max = torch.tensor(max, device=self.device)\n        if isinstance(min, torch.Tensor):\n            min = min.to(self.device)\n        else:\n            min = torch.tensor(min, device=self.device)\n        self.min = min\n        self.max = max", "metadata": {"task_id": "pytorch_rl/133", "ground_truth": "        self.update(loc, scale)", "fpath_tuple": ["pytorch_rl", "torchrl", "modules", "distributions", "continuous.py"], "context_start_lineno": 48, "line_no": 242, "query_window": {"context": "            self.non_trivial_min = min != -1.0\n        self.tanh_loc = tanh_loc\n\n        self.device = loc.device\n        self.upscale = (\n            upscale\n            if not isinstance(upscale, torch.Tensor)\n            else upscale.to(self.device)\n        )\n\n        if isinstance(max, torch.Tensor):\n            max = max.to(self.device)\n        else:\n            max = torch.tensor(max, device=self.device)\n        if isinstance(min, torch.Tensor):\n            min = min.to(self.device)\n        else:\n            min = torch.tensor(min, device=self.device)\n        self.min = min\n        self.max = max", "metadata": {"fpath_tuple": ["pytorch_rl", "torchrl", "modules", "distributions", "continuous.py"], "line_no": 242, "task_id": "pytorch_rl/133", "start_line_no": 222, "end_line_no": 242, "window_size": 20, "context_start_lineno": 48, "repo": "pytorch_rl"}}, "top_k_context": [{"context": "    ):\n        dtype, device = _default_dtype_and_device(dtype, device)\n        if dtype is None:\n            dtype = torch.get_default_dtype()\n        if device is None:\n            device = torch._get_default_device()\n\n        if not isinstance(minimum, torch.Tensor):\n            minimum = torch.tensor(minimum, dtype=dtype, device=device)\n        if not isinstance(maximum, torch.Tensor):\n            maximum = torch.tensor(maximum, dtype=dtype, device=device)\n        if maximum.device != device:\n            maximum = maximum.to(device)\n        if minimum.device != device:\n            minimum = minimum.to(device)\n        if dtype is not None and minimum.dtype is not dtype:\n            minimum = minimum.to(dtype)\n        if dtype is not None and maximum.dtype is not dtype:\n            maximum = maximum.to(dtype)\n        err_msg = (", "metadata": [{"fpath_tuple": ["pytorch_rl", "torchrl", "data", "tensor_specs.py"], "line_no": 642, "start_line_no": 632, "end_line_no": 652, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}], "sim_score": 0.3880597014925373}, {"context": "        if not isinstance(loc, torch.Tensor):\n            loc = torch.tensor(loc)\n        if not isinstance(scale, torch.Tensor):\n            scale = torch.tensor(scale)\n\n        self.register_buffer(\"loc\", loc)\n        self.register_buffer(\"scale\", scale.clamp_min(1e-6))\n\n    def _apply_transform(self, reward: torch.Tensor) -> torch.Tensor:\n        if self.standard_normal:\n            loc = self.loc\n            scale = self.scale\n            reward = (reward - loc) / scale\n            return reward\n        else:\n            scale = self.scale\n            loc = self.loc\n            reward = reward * scale + loc\n            return reward\n", "metadata": [{"fpath_tuple": ["pytorch_rl", "torchrl", "envs", "transforms", "transforms.py"], "line_no": 1660, "start_line_no": 1650, "end_line_no": 1670, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}], "sim_score": 0.38666666666666666}, {"context": "        self.standard_normal = standard_normal\n\n        if not isinstance(loc, torch.Tensor):\n            loc = torch.tensor(loc)\n        if not isinstance(scale, torch.Tensor):\n            scale = torch.tensor(scale)\n\n        self.register_buffer(\"loc\", loc)\n        self.register_buffer(\"scale\", scale.clamp_min(1e-6))\n\n    def _apply_transform(self, reward: torch.Tensor) -> torch.Tensor:\n        if self.standard_normal:\n            loc = self.loc\n            scale = self.scale\n            reward = (reward - loc) / scale\n            return reward\n        else:\n            scale = self.scale\n            loc = self.loc\n            reward = reward * scale + loc", "metadata": [{"fpath_tuple": ["pytorch_rl", "torchrl", "envs", "transforms", "transforms.py"], "line_no": 1658, "start_line_no": 1648, "end_line_no": 1668, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}], "sim_score": 0.3815789473684211}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# vizier/_src/pyvizier/shared/trial.py\n# --------------------------------------------------\n#     return len(self._items)\n# \n#   def __iter__(self):\n#     return iter(self._items)\n# \n#   def get_value(\n#       self,\n#       key: str,\n#       default: Optional[ParameterValueTypes] = None\n#   ) -> Optional[ParameterValueTypes]:\n#     \"\"\"Returns the raw value of the given parameter name.\"\"\"\n#     pv = self.get(key, default)\n#     if isinstance(pv, ParameterValue):\n#       return pv.value\n#     else:\n#       return pv\n# \n# \n# @attr.define(auto_attribs=True, frozen=False, init=True, slots=True)\n# class TrialSuggestion:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# vizier/_src/pyvizier/shared/parameter_config.py\n# --------------------------------------------------\n#     Returns:\n#       ParameterConfigSelector for `ParameterConfig`(s) if the values are not\n#         specified.\n#       SearchSpaceSelector for subspace(s) if parameter_values are specified.\n#     \"\"\"\n#     if parameter_values is None:\n#       selected_configs = []\n#       for space in self._selected:\n#         selected_configs.append(space.get(parameter_name))\n#       return ParameterConfigSelector(selected_configs)\n#     else:\n#       selected_spaces = []\n#       for space in self._selected:\n#         selected_parameter = space.get(parameter_name)\n#         for value in parameter_values:\n#           selected_spaces.append(selected_parameter.subspace(value))\n#       return SearchSpaceSelector(selected_spaces)\n# \n#   @classmethod\n#   def _get_parameter_names_to_create(cls,\n# --------------------------------------------------\n# the below code fragment can be found in:\n# vizier/_src/pyvizier/oss/study_config.py\n# --------------------------------------------------\n#     trial_final_values: Dict[str, ParameterValueSequence] = {}\n#     # multi_dim_params: Dict[str, List[Tuple[int, ParameterValueSequence]]]\n#     multi_dim_params = collections.defaultdict(list)\n#     for name in trial_external_values:\n#       base_index = (\n#           vz.SearchSpaceSelector.parse_multi_dimensional_parameter_name(name)\n#       )\n#       if base_index is None:\n#         trial_final_values[name] = trial_external_values[name]\n#       else:\n#         base_name, index = base_index\n#         multi_dim_params[base_name].append((index, trial_external_values[name]))\n#     for name in multi_dim_params:\n#       multi_dim_params[name].sort(key=lambda x: x[0])\n#       trial_final_values[name] = [x[1] for x in multi_dim_params[name]]\n# \n#     return trial_final_values\n# \n#   def trial_metrics(self,\n#                     proto: study_pb2.Trial,\n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\n# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import annotations\n\n\"\"\"The file has utilities to regress on trial intermediate measurements.\n\nThis contains utilities to fit regression models that predict the objective\nat a particular future step of ACTIVE trials. We notably support LightGBM\nmodels and necessary datastructures.\n\"\"\"\n\nimport copy\nfrom typing import (Any, Callable, Dict, Optional, Tuple, Union)\n\nfrom absl import logging\nimport attrs\nimport lightgbm.sklearn as lightgbm\nimport numpy as np\nfrom scipy.interpolate.fitpack2 import InterpolatedUnivariateSpline\nimport six\nfrom six.moves import range\nfrom sklearn.model_selection import GridSearchCV\nfrom vizier import algorithms as vza\nfrom vizier import pyvizier\nfrom vizier.pyvizier import converters\n\n\n@attrs.define\nclass TrialData:\n  \"\"\"Light weight trial data class to be used for training regression models.\"\"\"\n  id: int\n  learning_rate: float\n  final_objective: float\n  steps: list[int]\n  objective_values: list[float]\n\n  @classmethod\n  def from_trial(cls, trial: pyvizier.Trial, learning_rate_param_name: str,\n                 metric_name: str, converter: converters.TimedLabelsExtractor):\n    \"\"\"Preprocess the pyvizier trial into an instance of the class.\n\n    Args:\n      trial: pyvizier.Trial containing trial to process.\n      learning_rate_param_name: name of learning rate param\n      metric_name: name of optimization metric\n      converter: vizier tool to convert trials to times sequences\n\n    Returns:\n      returned_trial: the trial in TrialData format\n    \"\"\"\n\n    learning_rate = trial.parameters.get(learning_rate_param_name,\n                                         pyvizier.ParameterValue(0.0)).value\n\n    timedlabels = converter.convert([trial])[0]\n    steps, values = np.asarray(timedlabels.times, np.int32).reshape(\n        -1).tolist(), timedlabels.labels[metric_name].reshape(-1).tolist()\n\n    final_value = values[-1] if values else 0.0\n\n    if trial.final_measurement and (metric_name\n                                    in trial.final_measurement.metrics):\n      final_value = converter.metric_converters[0].convert(\n          [trial.final_measurement])[0]\n    else:\n      final_value = values[-1] if values else 0.0\n\n    return cls(\n        id=trial.id,\n        learning_rate=learning_rate,\n        final_objective=final_value,\n        steps=steps,\n        objective_values=values)\n\n  def extrapolate_trial_objective_value(self, max_num_steps: int):\n    \"\"\"Extend the measurements of self to max_num_steps.\n\n    Args:\n      max_num_steps: target steps to extend the measurement.\n    \"\"\"\n    last_step = self.steps[-1]\n    if last_step >= max_num_steps:\n      return\n\n    last_objective = self.objective_values[-1]\n    self.steps.append(max_num_steps)\n    self.objective_values.append(last_objective)\n\n\ndef _generate_interpolation_fn_from_trial(\n    steps: list[int], values: list[float]) -> Callable[[int], float]:\n  \"\"\"Generates an interpolation function from a trial's measurement data.\n\n  Since different trials have evaluations at different step numbers,\n  we need to be able to interpolate the objective value between steps\n  in order to compare trials and regress against trial data. This function\n  converts a trial into a function suitable for this use.\n\n  Args:\n    steps:  list of integers indicating the x-axis of the input data points.\n    values: list of floats indicating the y-axis of the input data points. steps\n      and values list contains the same number of elements.\n\n  Returns:\n    interpolation function that takes input a number t and returns\n    interpolated value of objective function for this trial at t steps.\n  \"\"\"\n  return InterpolatedUnivariateSpline(steps, values, k=1)\n\n\ndef _sort_dedupe_measurements(\n    steps: list[Union[int, float]],\n    values: list[float]) -> (Tuple[list[Union[int, float]], list[float]]):\n  \"\"\"Sort and remove duplicates in the trial's measurements.\n\n  Args:\n    steps: a list of integer measurement steps for a given trial.\n    values: a list of objective values corresponding to the steps for a given\n      trial.\n\n  Returns:\n    steps: a list of integer measurement steps after dedupe.\n    values: a list of objective values corresponding to the steps after dedupe.\n  \"\"\"\n  if isinstance(steps[0], float):\n    # Dedupe is skipped when steps are not integers.\n    return steps, values\n  step_obj_dict = {}\n  updated_steps = []\n  updated_values = []\n  for index in range(len(steps)):\n    step_obj_dict[steps[index]] = values[index]\n  last_step = None\n  for step, value in sorted(six.iteritems(step_obj_dict)):\n    if last_step is None or step > last_step:\n      updated_steps.append(step)\n      updated_values.append(value)", "metadata": {"task_id": "google_vizier/154", "ground_truth": "      last_step = step", "fpath_tuple": ["google_vizier", "vizier", "_src", "algorithms", "regression", "trial_regression_utils.py"], "context_start_lineno": 0, "line_no": 149, "query_window": {"context": "    values: a list of objective values corresponding to the steps for a given\n      trial.\n\n  Returns:\n    steps: a list of integer measurement steps after dedupe.\n    values: a list of objective values corresponding to the steps after dedupe.\n  \"\"\"\n  if isinstance(steps[0], float):\n    # Dedupe is skipped when steps are not integers.\n    return steps, values\n  step_obj_dict = {}\n  updated_steps = []\n  updated_values = []\n  for index in range(len(steps)):\n    step_obj_dict[steps[index]] = values[index]\n  last_step = None\n  for step, value in sorted(six.iteritems(step_obj_dict)):\n    if last_step is None or step > last_step:\n      updated_steps.append(step)\n      updated_values.append(value)", "metadata": {"fpath_tuple": ["google_vizier", "vizier", "_src", "algorithms", "regression", "trial_regression_utils.py"], "line_no": 149, "task_id": "google_vizier/154", "start_line_no": 129, "end_line_no": 149, "window_size": 20, "context_start_lineno": 0, "repo": "google_vizier"}}, "top_k_context": [{"context": "\n    # Combine multi-dimensional parameter values to a list of values.\n    trial_final_values: Dict[str, ParameterValueSequence] = {}\n    # multi_dim_params: Dict[str, List[Tuple[int, ParameterValueSequence]]]\n    multi_dim_params = collections.defaultdict(list)\n    for name in trial_external_values:\n      base_index = (\n          vz.SearchSpaceSelector.parse_multi_dimensional_parameter_name(name)\n      )\n      if base_index is None:\n        trial_final_values[name] = trial_external_values[name]\n      else:\n        base_name, index = base_index\n        multi_dim_params[base_name].append((index, trial_external_values[name]))\n    for name in multi_dim_params:\n      multi_dim_params[name].sort(key=lambda x: x[0])\n      trial_final_values[name] = [x[1] for x in multi_dim_params[name]]\n\n    return trial_final_values\n", "metadata": [{"fpath_tuple": ["google_vizier", "vizier", "_src", "pyvizier", "oss", "study_config.py"], "line_no": 372, "start_line_no": 362, "end_line_no": 382, "window_size": 20, "repo": "google_vizier", "slice_size": 10}], "sim_score": 0.2713178294573643}, {"context": "        be used to add child parameters, or traverse a conditional tree.\n\n    Returns:\n      ParameterConfigSelector for `ParameterConfig`(s) if the values are not\n        specified.\n      SearchSpaceSelector for subspace(s) if parameter_values are specified.\n    \"\"\"\n    if parameter_values is None:\n      selected_configs = []\n      for space in self._selected:\n        selected_configs.append(space.get(parameter_name))\n      return ParameterConfigSelector(selected_configs)\n    else:\n      selected_spaces = []\n      for space in self._selected:\n        selected_parameter = space.get(parameter_name)\n        for value in parameter_values:\n          selected_spaces.append(selected_parameter.subspace(value))\n      return SearchSpaceSelector(selected_spaces)\n", "metadata": [{"fpath_tuple": ["google_vizier", "vizier", "_src", "pyvizier", "shared", "parameter_config.py"], "line_no": 1002, "start_line_no": 992, "end_line_no": 1012, "window_size": 20, "repo": "google_vizier", "slice_size": 10}], "sim_score": 0.26956521739130435}, {"context": "\n  def __len__(self) -> int:\n    return len(self._items)\n\n  def __iter__(self):\n    return iter(self._items)\n\n  def get_value(\n      self,\n      key: str,\n      default: Optional[ParameterValueTypes] = None\n  ) -> Optional[ParameterValueTypes]:\n    \"\"\"Returns the raw value of the given parameter name.\"\"\"\n    pv = self.get(key, default)\n    if isinstance(pv, ParameterValue):\n      return pv.value\n    else:\n      return pv\n\n", "metadata": [{"fpath_tuple": ["google_vizier", "vizier", "_src", "pyvizier", "shared", "trial.py"], "line_no": 360, "start_line_no": 350, "end_line_no": 370, "window_size": 20, "repo": "google_vizier", "slice_size": 10}], "sim_score": 0.2636363636363636}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# tests/models/test_models_unet_2d_condition.py\n# --------------------------------------------------\n# class UNet2DConditionModelIntegrationTests(unittest.TestCase):\n#     def get_file_format(self, seed, shape):\n#         return f\"gaussian_noise_s={seed}_shape={'_'.join([str(s) for s in shape])}.npy\"\n# \n#     def tearDown(self):\n#         # clean up the VRAM after each test\n#         super().tearDown()\n#         gc.collect()\n#         torch.cuda.empty_cache()\n# \n#     def get_latents(self, seed=0, shape=(4, 4, 64, 64), fp16=False):\n#         dtype = torch.float16 if fp16 else torch.float32\n#         image = torch.from_numpy(load_hf_numpy(self.get_file_format(seed, shape))).to(torch_device).to(dtype)\n#         return image\n# \n#     def get_unet_model(self, fp16=False, model_id=\"CompVis/stable-diffusion-v1-4\"):\n#         revision = \"fp16\" if fp16 else None\n#         torch_dtype = torch.float16 if fp16 else torch.float32\n# \n#         model = UNet2DConditionModel.from_pretrained(\n# --------------------------------------------------\n# the below code fragment can be found in:\n# tests/models/test_models_unet_2d_condition.py\n# --------------------------------------------------\n#         return f\"gaussian_noise_s={seed}_shape={'_'.join([str(s) for s in shape])}.npy\"\n# \n#     def tearDown(self):\n#         # clean up the VRAM after each test\n#         super().tearDown()\n#         gc.collect()\n#         torch.cuda.empty_cache()\n# \n#     def get_latents(self, seed=0, shape=(4, 4, 64, 64), fp16=False):\n#         dtype = torch.float16 if fp16 else torch.float32\n#         image = torch.from_numpy(load_hf_numpy(self.get_file_format(seed, shape))).to(torch_device).to(dtype)\n#         return image\n# \n#     def get_unet_model(self, fp16=False, model_id=\"CompVis/stable-diffusion-v1-4\"):\n#         revision = \"fp16\" if fp16 else None\n#         torch_dtype = torch.float16 if fp16 else torch.float32\n# \n#         model = UNet2DConditionModel.from_pretrained(\n#             model_id, subfolder=\"unet\", torch_dtype=torch_dtype, revision=revision\n#         )\n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\nimport gc\nimport unittest\n\nfrom diffusers import FlaxUNet2DConditionModel\nfrom diffusers.utils import is_flax_available\nfrom diffusers.utils.testing_utils import load_hf_numpy, require_flax, slow\nfrom parameterized import parameterized\n\n\nif is_flax_available():\n    import jax\n    import jax.numpy as jnp\n\n\n@slow\n@require_flax\nclass FlaxUNet2DConditionModelIntegrationTests(unittest.TestCase):\n    def get_file_format(self, seed, shape):\n        return f\"gaussian_noise_s={seed}_shape={'_'.join([str(s) for s in shape])}.npy\"\n\n    def tearDown(self):\n        # clean up the VRAM after each test\n        super().tearDown()\n        gc.collect()\n\n    def get_latents(self, seed=0, shape=(4, 4, 64, 64), fp16=False):\n        dtype = jnp.bfloat16 if fp16 else jnp.float32\n        image = jnp.array(load_hf_numpy(self.get_file_format(seed, shape)), dtype=dtype)\n        return image\n\n    def get_unet_model(self, fp16=False, model_id=\"CompVis/stable-diffusion-v1-4\"):\n        dtype = jnp.bfloat16 if fp16 else jnp.float32\n        revision = \"bf16\" if fp16 else None\n\n        model, params = FlaxUNet2DConditionModel.from_pretrained(", "metadata": {"task_id": "huggingface_diffusers/104", "ground_truth": "            model_id, subfolder=\"unet\", dtype=dtype, revision=revision", "fpath_tuple": ["huggingface_diffusers", "tests", "models", "test_models_unet_2d_flax.py"], "context_start_lineno": 0, "line_no": 35, "query_window": {"context": "@require_flax\nclass FlaxUNet2DConditionModelIntegrationTests(unittest.TestCase):\n    def get_file_format(self, seed, shape):\n        return f\"gaussian_noise_s={seed}_shape={'_'.join([str(s) for s in shape])}.npy\"\n\n    def tearDown(self):\n        # clean up the VRAM after each test\n        super().tearDown()\n        gc.collect()\n\n    def get_latents(self, seed=0, shape=(4, 4, 64, 64), fp16=False):\n        dtype = jnp.bfloat16 if fp16 else jnp.float32\n        image = jnp.array(load_hf_numpy(self.get_file_format(seed, shape)), dtype=dtype)\n        return image\n\n    def get_unet_model(self, fp16=False, model_id=\"CompVis/stable-diffusion-v1-4\"):\n        dtype = jnp.bfloat16 if fp16 else jnp.float32\n        revision = \"bf16\" if fp16 else None\n\n        model, params = FlaxUNet2DConditionModel.from_pretrained(", "metadata": {"fpath_tuple": ["huggingface_diffusers", "tests", "models", "test_models_unet_2d_flax.py"], "line_no": 35, "task_id": "huggingface_diffusers/104", "start_line_no": 15, "end_line_no": 35, "window_size": 20, "context_start_lineno": 0, "repo": "huggingface_diffusers"}}, "top_k_context": [{"context": "class UNet2DConditionModelIntegrationTests(unittest.TestCase):\n    def get_file_format(self, seed, shape):\n        return f\"gaussian_noise_s={seed}_shape={'_'.join([str(s) for s in shape])}.npy\"\n\n    def tearDown(self):\n        # clean up the VRAM after each test\n        super().tearDown()\n        gc.collect()\n        torch.cuda.empty_cache()\n\n    def get_latents(self, seed=0, shape=(4, 4, 64, 64), fp16=False):\n        dtype = torch.float16 if fp16 else torch.float32\n        image = torch.from_numpy(load_hf_numpy(self.get_file_format(seed, shape))).to(torch_device).to(dtype)\n        return image\n\n    def get_unet_model(self, fp16=False, model_id=\"CompVis/stable-diffusion-v1-4\"):\n        revision = \"fp16\" if fp16 else None\n        torch_dtype = torch.float16 if fp16 else torch.float32\n\n        model = UNet2DConditionModel.from_pretrained(", "metadata": [{"fpath_tuple": ["huggingface_diffusers", "tests", "models", "test_models_unet_2d_condition.py"], "line_no": 426, "start_line_no": 416, "end_line_no": 436, "window_size": 20, "repo": "huggingface_diffusers", "slice_size": 10}], "sim_score": 0.8287671232876712}, {"context": "\n@slow\nclass UNet2DConditionModelIntegrationTests(unittest.TestCase):\n    def get_file_format(self, seed, shape):\n        return f\"gaussian_noise_s={seed}_shape={'_'.join([str(s) for s in shape])}.npy\"\n\n    def tearDown(self):\n        # clean up the VRAM after each test\n        super().tearDown()\n        gc.collect()\n        torch.cuda.empty_cache()\n\n    def get_latents(self, seed=0, shape=(4, 4, 64, 64), fp16=False):\n        dtype = torch.float16 if fp16 else torch.float32\n        image = torch.from_numpy(load_hf_numpy(self.get_file_format(seed, shape))).to(torch_device).to(dtype)\n        return image\n\n    def get_unet_model(self, fp16=False, model_id=\"CompVis/stable-diffusion-v1-4\"):\n        revision = \"fp16\" if fp16 else None\n        torch_dtype = torch.float16 if fp16 else torch.float32", "metadata": [{"fpath_tuple": ["huggingface_diffusers", "tests", "models", "test_models_unet_2d_condition.py"], "line_no": 424, "start_line_no": 414, "end_line_no": 434, "window_size": 20, "repo": "huggingface_diffusers", "slice_size": 10}], "sim_score": 0.8163265306122449}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# fortuna/calib_model/calib_model_calibrator.py\n# --------------------------------------------------\n#         self,\n#         state: CalibState,\n#         targets: Array,\n#         outputs: Array,\n#         fun: Callable,\n#         rng: PRNGKeyArray,\n#         metrics: Optional[\n#             Tuple[Callable[[jnp.ndarray, jnp.ndarray, Array], Array], ...]\n#         ] = None,\n#     ) -> Dict[str, jnp.ndarray]:\n#         val_loss, aux = self.val_loss_step(state, targets, outputs, fun, rng)\n#         val_metrics = self.val_metrics_step(aux, targets, metrics)\n#         return {\"val_loss\": val_loss, **val_metrics}\n# \n#     def val_loss_step(\n#         self,\n#         state: CalibState,\n#         targets: Array,\n#         outputs: Array,\n#         fun: Callable,\n# --------------------------------------------------\n# the below code fragment can be found in:\n# fortuna/calib_model/calib_model_calibrator.py\n# --------------------------------------------------\n#         targets: Array,\n#         outputs: Array,\n#         fun: Callable,\n#         rng: PRNGKeyArray,\n#         metrics: Optional[\n#             Tuple[Callable[[jnp.ndarray, jnp.ndarray, Array], Array], ...]\n#         ] = None,\n#     ) -> Dict[str, jnp.ndarray]:\n#         val_loss, aux = self.val_loss_step(state, targets, outputs, fun, rng)\n#         val_metrics = self.val_metrics_step(aux, targets, metrics)\n#         return {\"val_loss\": val_loss, **val_metrics}\n# \n#     def val_loss_step(\n#         self,\n#         state: CalibState,\n#         targets: Array,\n#         outputs: Array,\n#         fun: Callable,\n#         rng: PRNGKeyArray,\n#     ) -> Tuple[jnp.ndarray, Dict[str, jnp.ndarray]]:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# fortuna/calib_model/calib_model_calibrator.py\n# --------------------------------------------------\n#         fun: Callable,\n#         rng: PRNGKeyArray,\n#         metrics: Optional[\n#             Tuple[Callable[[jnp.ndarray, jnp.ndarray, Array], Array], ...]\n#         ] = None,\n#     ) -> Dict[str, jnp.ndarray]:\n#         val_loss, aux = self.val_loss_step(state, targets, outputs, fun, rng)\n#         val_metrics = self.val_metrics_step(aux, targets, metrics)\n#         return {\"val_loss\": val_loss, **val_metrics}\n# \n#     def val_loss_step(\n#         self,\n#         state: CalibState,\n#         targets: Array,\n#         outputs: Array,\n#         fun: Callable,\n#         rng: PRNGKeyArray,\n#     ) -> Tuple[jnp.ndarray, Dict[str, jnp.ndarray]]:\n#         log_joint_probs, aux = fun(\n#             params=state.params,\n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\n and metrics for the current batch\n            training_losses_and_metrics_current_batch = self.training_step_end(\n                current_epoch=current_epoch,\n                state=state,\n                aux=aux,\n                batch=batch,\n                metrics=metrics,\n            )\n            # keep track of training losses and metrics [granularity=batch]\n            training_losses_and_metrics_epoch_all_steps.append(\n                training_losses_and_metrics_current_batch\n            )\n            # logging\n            if verbose:\n                training_batch_metrics_str = \" | \".join(\n                    [\n                        f\"{m}: {round(float(v), 5)}\"\n                        for m, v in training_losses_and_metrics_current_batch.items()\n                    ]\n                )\n                progress_bar.set_description(\n                    f\"Epoch: {current_epoch + 1} | \" + training_batch_metrics_str,\n                    refresh=True,\n                )\n\n        # compute training losses and metrics avg for the current epoch + other ops (if needed)\n        training_losses_and_metrics_current_epoch = self.training_epoch_end(\n            training_losses_and_metrics_epoch_all_steps\n        )\n\n        return (\n            state,\n            training_losses_and_metrics_current_epoch,\n            training_batch_metrics_str,\n        )\n\n    def training_step(\n        self,\n        state: CalibState,\n        batch: Batch,\n        outputs: Array,\n        fun: Callable,\n        rng: PRNGKeyArray,\n        n_data: int,\n    ) -> Tuple[CalibState, Dict[str, Any]]:\n        # ensure to use a different key at each step\n        model_key = random.fold_in(rng, state.step)\n\n        grad_fn = value_and_grad(\n            lambda params: self.training_loss_step(\n                fun, params, batch, outputs, state.mutable, model_key, n_data\n            ),\n            has_aux=True,\n        )\n        (loss, aux), grad = grad_fn(state.params)\n        grad, loss = self.sync_gradients_and_loss(grad, loss)\n\n        state = state.apply_gradients(grads=grad, mutable=aux[\"mutable\"])\n        return (\n            state,\n            {\n                \"loss\": loss,\n                \"outputs\": aux[\"outputs\"],\n                \"logging_kwargs\": aux[\"logging_kwargs\"],\n            },\n        )\n\n    @abc.abstractmethod\n    def training_loss_step(\n        self,\n        fun: Callable[[Any], Union[float, Tuple[float, dict]]],\n        params: CalibParams,\n        batch: Batch,\n        outputs: Array,\n        mutable: CalibMutable,\n        rng: PRNGKeyArray,\n        n_data: int,\n    ) -> Tuple[jnp.ndarray, Dict[str, Any]]:\n        pass\n\n    def training_step_end(\n        self,\n        current_epoch: int,\n        state: CalibState,\n        aux: Dict[str, Any],\n        batch: Batch,\n        metrics: Optional[\n            Tuple[Callable[[jnp.ndarray, jnp.ndarray, Array], Array], ...]\n        ],\n    ) -> Dict[str, jnp.ndarray]:\n        if (\n            self.save_checkpoint_dir\n            and self.save_every_n_steps\n            and current_epoch % self.save_every_n_steps == 0\n        ):\n            self.save_checkpoint(\n                state, self.save_checkpoint_dir, keep=self.keep_top_n_checkpoints\n            )\n        training_losses_and_metrics = {\"loss\": aux[\"loss\"]}\n\n        if aux[\"logging_kwargs\"] is not None:\n            for k, v in aux[\"logging_kwargs\"].items():\n                training_losses_and_metrics[k] = v\n\n        if not self.disable_training_metrics_computation and metrics is not None:\n            preds = self.predict_fn(aux[\"outputs\"])\n            uncertainties = self.uncertainty_fn(aux[\"outputs\"])\n            if self.multi_device:\n                training_batch_metrics = self.compute_metrics(\n                    preds.reshape((preds.shape[0] * preds.shape[1],) + preds.shape[2:]),\n                    uncertainties.reshape(\n                        (uncertainties.shape[0] * uncertainties.shape[1],)\n                        + uncertainties.shape[2:]\n                    ),\n                    batch[1].reshape(\n                        (batch[1].shape[0] * batch[1].shape[1],) + batch[1].shape[2:]\n                    ),\n                    metrics,\n                )\n            else:\n                training_batch_metrics = self.compute_metrics(\n                    preds, uncertainties, batch[1], metrics\n                )\n            for k, v in training_batch_metrics.items():\n                training_losses_and_metrics[k] = v\n        return training_losses_and_metrics\n\n    def _val_loop(\n        self,\n        fun: Callable,\n        metrics: Optional[\n            Tuple[Callable[[jnp.ndarray, jnp.ndarray, Array], Array], ...]\n        ],\n        rng: PRNGKeyArray,\n        state: CalibState,\n        val_data_loader: DataLoader,\n        val_outputs_loader: TargetsLoader,\n        val_dataset_size: int,\n        verbose: bool = True,\n    ) -> Tuple[Dict[str, float], str]:\n        val_losses_and_metrics_epoch_all_steps = []\n        val_epoch_metrics_str = \"\"\n        for batch, outputs in zip(val_data_loader, val_outputs_loader):\n            val_losses_and_metrics_current_batch = self.val_step(\n                state, batch, outputs, fun, rng, val_dataset_size, metrics,\n            )\n            val_losses_and_metrics_epoch_all_steps.append(\n                val_losses_and_metrics_current_batch\n            )\n        # compute validation losses and metrics for the current epoch\n        val_losses_and_metrics_current_epoch = self.val_epoch_end(\n            val_losses_and_metrics_epoch_all_steps, state\n        )\n        # logging\n        if verbose:\n            val_epoch_metrics_str = \" | \".join(\n                [\n                    f\"{m}: {round(float(v), 5)}\"\n                    for m, v in val_losses_and_metrics_current_epoch.items()\n                ]\n            )\n        return val_losses_and_metrics_current_epoch, val_epoch_metrics_str\n\n    def val_step(\n        self,\n        state: CalibState,\n        batch: Batch,\n        outputs: Array,\n        fun: Callable,\n        rng: PRNGKeyArray,\n        n_data: int,\n        metrics: Optional[\n            Tuple[Callable[[jnp.ndarray, jnp.ndarray, Array], Array], ...]\n        ] = None,\n    ) -> Dict[str, jnp.ndarray]:\n        val_loss, aux = self.val_loss_step(state, batch, outputs, fun, rng, n_data)\n        val_metrics = self.val_metrics_step(aux, batch, metrics)\n        return {\"val_loss\": val_loss, **val_metrics}\n\n    @abc.abstractmethod\n    def val_loss_step(\n        self,\n        state: CalibState,\n        batch: Batch,\n        outputs: Array,\n        fun: Callable,\n        rng: PRNGKeyArray,\n        n_data: int,\n    ) -> Tuple[jnp.ndarray, Dict[str, jnp.ndarray]]:", "metadata": {"task_id": "awslabs_fortuna/155", "ground_truth": "        pass", "fpath_tuple": ["awslabs_fortuna", "fortuna", "calibration", "calibrator.py"], "context_start_lineno": 174, "line_no": 363, "query_window": {"context": "        rng: PRNGKeyArray,\n        n_data: int,\n        metrics: Optional[\n            Tuple[Callable[[jnp.ndarray, jnp.ndarray, Array], Array], ...]\n        ] = None,\n    ) -> Dict[str, jnp.ndarray]:\n        val_loss, aux = self.val_loss_step(state, batch, outputs, fun, rng, n_data)\n        val_metrics = self.val_metrics_step(aux, batch, metrics)\n        return {\"val_loss\": val_loss, **val_metrics}\n\n    @abc.abstractmethod\n    def val_loss_step(\n        self,\n        state: CalibState,\n        batch: Batch,\n        outputs: Array,\n        fun: Callable,\n        rng: PRNGKeyArray,\n        n_data: int,\n    ) -> Tuple[jnp.ndarray, Dict[str, jnp.ndarray]]:", "metadata": {"fpath_tuple": ["awslabs_fortuna", "fortuna", "calibration", "calibrator.py"], "line_no": 363, "task_id": "awslabs_fortuna/155", "start_line_no": 343, "end_line_no": 363, "window_size": 20, "context_start_lineno": 174, "repo": "awslabs_fortuna"}}, "top_k_context": [{"context": "        targets: Array,\n        outputs: Array,\n        fun: Callable,\n        rng: PRNGKeyArray,\n        metrics: Optional[\n            Tuple[Callable[[jnp.ndarray, jnp.ndarray, Array], Array], ...]\n        ] = None,\n    ) -> Dict[str, jnp.ndarray]:\n        val_loss, aux = self.val_loss_step(state, targets, outputs, fun, rng)\n        val_metrics = self.val_metrics_step(aux, targets, metrics)\n        return {\"val_loss\": val_loss, **val_metrics}\n\n    def val_loss_step(\n        self,\n        state: CalibState,\n        targets: Array,\n        outputs: Array,\n        fun: Callable,\n        rng: PRNGKeyArray,\n    ) -> Tuple[jnp.ndarray, Dict[str, jnp.ndarray]]:", "metadata": [{"fpath_tuple": ["awslabs_fortuna", "fortuna", "calib_model", "calib_model_calibrator.py"], "line_no": 356, "start_line_no": 346, "end_line_no": 366, "window_size": 20, "repo": "awslabs_fortuna", "slice_size": 10}], "sim_score": 0.8461538461538461}, {"context": "        self,\n        state: CalibState,\n        targets: Array,\n        outputs: Array,\n        fun: Callable,\n        rng: PRNGKeyArray,\n        metrics: Optional[\n            Tuple[Callable[[jnp.ndarray, jnp.ndarray, Array], Array], ...]\n        ] = None,\n    ) -> Dict[str, jnp.ndarray]:\n        val_loss, aux = self.val_loss_step(state, targets, outputs, fun, rng)\n        val_metrics = self.val_metrics_step(aux, targets, metrics)\n        return {\"val_loss\": val_loss, **val_metrics}\n\n    def val_loss_step(\n        self,\n        state: CalibState,\n        targets: Array,\n        outputs: Array,\n        fun: Callable,", "metadata": [{"fpath_tuple": ["awslabs_fortuna", "fortuna", "calib_model", "calib_model_calibrator.py"], "line_no": 354, "start_line_no": 344, "end_line_no": 364, "window_size": 20, "repo": "awslabs_fortuna", "slice_size": 10}], "sim_score": 0.8461538461538461}, {"context": "\n    def val_step(\n        self,\n        state: CalibState,\n        targets: Array,\n        outputs: Array,\n        fun: Callable,\n        rng: PRNGKeyArray,\n        metrics: Optional[\n            Tuple[Callable[[jnp.ndarray, jnp.ndarray, Array], Array], ...]\n        ] = None,\n    ) -> Dict[str, jnp.ndarray]:\n        val_loss, aux = self.val_loss_step(state, targets, outputs, fun, rng)\n        val_metrics = self.val_metrics_step(aux, targets, metrics)\n        return {\"val_loss\": val_loss, **val_metrics}\n\n    def val_loss_step(\n        self,\n        state: CalibState,\n        targets: Array,", "metadata": [{"fpath_tuple": ["awslabs_fortuna", "fortuna", "calib_model", "calib_model_calibrator.py"], "line_no": 352, "start_line_no": 342, "end_line_no": 362, "window_size": 20, "repo": "awslabs_fortuna", "slice_size": 10}], "sim_score": 0.8461538461538461}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# tests/fortuna/test_trainer.py\n# --------------------------------------------------\n#         ]\n#         observed_losses_and_metrics = trainer._get_mean_losses_and_metrics(\n#             losses_and_metrics\n#         )\n#         expected_losses_and_metrics = {\n#             \"train_loss\": jnp.array(0.05),\n#             \"val_accuracy\": jnp.array(0.1),\n#             \"val_loss\": jnp.array(0.21),\n#         }\n#         self.assertDictEqual(observed_losses_and_metrics, expected_losses_and_metrics)\n# \n#     def test__get_mean_losses_and_metrics_ko(self):\n#         trainer = FakeTrainer(\n#             predict_fn=lambda x: x, disable_training_metrics_computation=False\n#         )\n#         losses_and_metrics = [\n#             {\n#                 \"train_loss\": jnp.array(0.1),\n#                 \"val_loss\": jnp.array(0.2),\n#                 \"val_accuracy\": jnp.array(0.1),\n# --------------------------------------------------\n# the below code fragment can be found in:\n# tests/fortuna/test_trainer.py\n# --------------------------------------------------\n#             losses_and_metrics\n#         )\n#         expected_losses_and_metrics = {\n#             \"train_loss\": jnp.array(0.05),\n#             \"val_accuracy\": jnp.array(0.1),\n#             \"val_loss\": jnp.array(0.21),\n#         }\n#         self.assertDictEqual(observed_losses_and_metrics, expected_losses_and_metrics)\n# \n#     def test__get_mean_losses_and_metrics_ko(self):\n#         trainer = FakeTrainer(\n#             predict_fn=lambda x: x, disable_training_metrics_computation=False\n#         )\n#         losses_and_metrics = [\n#             {\n#                 \"train_loss\": jnp.array(0.1),\n#                 \"val_loss\": jnp.array(0.2),\n#                 \"val_accuracy\": jnp.array(0.1),\n#             },\n#             {\"train_loss\": jnp.array(0.05), \"val_accuracy\": jnp.array(0.1)},\n# --------------------------------------------------\n# the below code fragment can be found in:\n# tests/fortuna/test_trainer.py\n# --------------------------------------------------\n#                 \"val_loss\": jnp.array(0.22),\n#                 \"val_accuracy\": jnp.array(0.1),\n#             },\n#         ]\n#         with self.assertRaises(ValueError):\n#             _ = trainer._get_mean_losses_and_metrics(losses_and_metrics)\n# \n#     def test_training_epoch_end(self):\n#         trainer = FakeTrainer(\n#             predict_fn=lambda x: x, disable_training_metrics_computation=False\n#         )\n# \n#         losses_and_metrics = [\n#             {\n#                 \"train_loss\": jnp.array(0.1),\n#                 \"val_loss\": jnp.array(0.2),\n#                 \"val_accuracy\": jnp.array(0.1),\n#             },\n#             {\n#                 \"train_loss\": jnp.array(0.0),\n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\n            {\n                \"train_loss\": jnp.array(0.0),\n                \"val_loss\": jnp.array(0.22),\n                \"val_accuracy\": jnp.array(0.1),\n            },\n        ]\n        fake_out = {\n            \"train_loss\": jnp.array(0.0),\n            \"val_loss\": jnp.array(0.22),\n            \"val_accuracy\": jnp.array(0.1),\n        }\n\n        with unittest.mock.patch.object(\n            trainer, \"_get_mean_losses_and_metrics\", return_value=fake_out\n        ) as m:\n            observed = trainer.training_epoch_end(losses_and_metrics)\n        m.assert_called_once_with(losses_and_metrics)\n        self.assertDictEqual(observed, fake_out)\n\n    def test_validation_epoch_end(self):\n        trainer = FakeTrainer(\n            predict_fn=lambda x: x,\n            disable_training_metrics_computation=False,\n            save_checkpoint_dir=\"tmp_dir\",\n            save_every_n_steps=1,\n        )\n\n        losses_and_metrics = [\n            {\n                \"train_loss\": jnp.array(0.1),\n                \"val_loss\": jnp.array(0.2),\n                \"val_accuracy\": jnp.array(0.1),\n            },\n            {\n                \"train_loss\": jnp.array(0.0),\n                \"val_loss\": jnp.array(0.22),\n                \"val_accuracy\": jnp.array(0.1),\n            },\n        ]\n        fake_out = {\n            \"train_loss\": jnp.array(0.0),\n            \"val_loss\": jnp.array(0.22),\n            \"val_accuracy\": jnp.array(0.1),\n        }\n\n        with unittest.mock.patch.multiple(\n            trainer,\n            _get_mean_losses_and_metrics=mock.DEFAULT,\n            early_stopping_update=mock.DEFAULT,\n            save_checkpoint=mock.DEFAULT,\n        ) as m:\n            m[\"_get_mean_losses_and_metrics\"].return_value = fake_out\n            m[\"early_stopping_update\"].return_value = False\n            m[\"save_checkpoint\"].return_value = fake_out\n\n            observed = trainer.validation_epoch_end(losses_and_metrics, None)\n\n        m[\"_get_mean_losses_and_metrics\"].assert_called_once_with(losses_and_metrics)\n        m[\"early_stopping_update\"].assert_called_once_with(fake_out)\n        m[\"save_checkpoint\"].assert_not_called()\n        self.assertDictEqual(observed, fake_out)\n\n        with unittest.mock.patch.multiple(\n            trainer,\n            _get_mean_losses_and_metrics=mock.DEFAULT,\n            early_stopping_update=mock.DEFAULT,\n            save_checkpoint=mock.DEFAULT,\n        ) as m:\n            m[\"_get_mean_losses_and_metrics\"].return_value = fake_out\n            m[\"early_stopping_update\"].return_value = True\n            m[\"save_checkpoint\"].return_value = fake_out\n\n            observed = trainer.validation_epoch_end(losses_and_metrics, None)\n\n        m[\"_get_mean_losses_and_metrics\"].assert_called_once_with(losses_and_metrics)\n        m[\"early_stopping_update\"].assert_called_once_with(fake_out)\n        m[\"save_checkpoint\"].assert_called_once_with(None, \"tmp_dir\", force_save=True)\n        self.assertDictEqual(observed, fake_out)\n\n    def test_should_perform_validation(self):\n        trainer = FakeTrainer(\n            predict_fn=lambda x: x, disable_training_metrics_computation=False\n        )\n        self.assertFalse(trainer.should_perform_validation(None, 1))\n\n        trainer.eval_every_n_epochs = 10\n        self.assertFalse(trainer.should_perform_validation({}, 9))\n        self.assertTrue(trainer.should_perform_validation({}, 10))\n\n    def test__validation_loop(self):\n        validation_dataloader = [\n            [jnp.array([[0, 0.0, 0.0], [0, 0.0, 0]]), jnp.array([0.0, 0.0])],\n            [jnp.array([[0.1, 0.0, 10], [0, 0.0, 0]]), jnp.array([1.0, 0.0])],\n        ]\n        trainer = FakeTrainer(\n            predict_fn=lambda x: x, disable_training_metrics_computation=False\n        )\n        (\n            observed_validation_losses_and_metrics_current_epoch,\n            observed_validation_epoch_metrics_str,\n        ) = trainer._validation_loop(\n            state=None,\n            validation_dataloader=validation_dataloader,\n            validation_dataset_size=2,\n            fun=lambda x: x,\n            rng=jax.random.PRNGKey(0),\n            metrics=(\"accuracy\",),\n            training_kwargs=FrozenDict({}),\n            unravel=None,\n            verbose=False,\n        )\n        self.assertEqual(observed_validation_epoch_metrics_str, \"\")\n        self.assertDictEqual(\n            observed_validation_losses_and_metrics_current_epoch,\n            {\"val_loss\": jnp.array(0.1), \"val_accuracy\": jnp.array(0.5)},\n        )\n\n        (\n            observed_validation_losses_and_metrics_current_epoch,\n            observed_validation_epoch_metrics_str,\n        ) = trainer._validation_loop(\n            state=None,\n            validation_dataloader=validation_dataloader,\n            validation_dataset_size=2,\n            fun=lambda x: x,\n            rng=jax.random.PRNGKey(0),\n            metrics=(\"accuracy\",),\n            training_kwargs=FrozenDict({}),\n            unravel=None,\n            verbose=True,\n        )\n        self.assertEqual(\n            observed_validation_epoch_metrics_str, \"val_accuracy: 0.5 | val_loss: 0.1\"\n        )\n        self.assertDictEqual(\n            observed_validation_losses_and_metrics_current_epoch,\n            {\"val_loss\": jnp.array(0.1), \"val_accuracy\": jnp.array(0.5)},\n        )\n\n    def test__training_loop(self):\n        training_dataloader = [\n            [jnp.array([[0, 0.0, 0.0], [0, 0.0, 0]]), jnp.array([0.0, 0.0])],\n            [jnp.array([[0.1, 0.0, 10], [0, 0.0, 0]]), jnp.array([1.0, 0.0])],\n        ]\n        trainer = FakeTrainer(\n            predict_fn=lambda x: x, disable_training_metrics_computation=True\n        )\n        (\n            observed_state,\n            observed_train_losses_and_metrics_current_epoch,\n            observed_train_epoch_metrics_str,\n        ) = trainer._training_loop(\n            current_epoch=1,\n            fun=lambda x: x,\n            metrics=(\"accuracy\",),\n            rng=jax.random.PRNGKey(0),\n            state=FakeTrainState(),", "metadata": {"task_id": "awslabs_fortuna/118", "ground_truth": "            training_dataloader=training_dataloader,", "fpath_tuple": ["awslabs_fortuna", "tests", "fortuna", "test_trainer.py"], "context_start_lineno": 258, "line_no": 415, "query_window": {"context": "        )\n\n    def test__training_loop(self):\n        training_dataloader = [\n            [jnp.array([[0, 0.0, 0.0], [0, 0.0, 0]]), jnp.array([0.0, 0.0])],\n            [jnp.array([[0.1, 0.0, 10], [0, 0.0, 0]]), jnp.array([1.0, 0.0])],\n        ]\n        trainer = FakeTrainer(\n            predict_fn=lambda x: x, disable_training_metrics_computation=True\n        )\n        (\n            observed_state,\n            observed_train_losses_and_metrics_current_epoch,\n            observed_train_epoch_metrics_str,\n        ) = trainer._training_loop(\n            current_epoch=1,\n            fun=lambda x: x,\n            metrics=(\"accuracy\",),\n            rng=jax.random.PRNGKey(0),\n            state=FakeTrainState(),", "metadata": {"fpath_tuple": ["awslabs_fortuna", "tests", "fortuna", "test_trainer.py"], "line_no": 415, "task_id": "awslabs_fortuna/118", "start_line_no": 395, "end_line_no": 415, "window_size": 20, "context_start_lineno": 258, "repo": "awslabs_fortuna"}}, "top_k_context": [{"context": "            {\n                \"train_loss\": jnp.array(0.0),\n                \"val_loss\": jnp.array(0.22),\n                \"val_accuracy\": jnp.array(0.1),\n            },\n        ]\n        with self.assertRaises(ValueError):\n            _ = trainer._get_mean_losses_and_metrics(losses_and_metrics)\n\n    def test_training_epoch_end(self):\n        trainer = FakeTrainer(\n            predict_fn=lambda x: x, disable_training_metrics_computation=False\n        )\n\n        losses_and_metrics = [\n            {\n                \"train_loss\": jnp.array(0.1),\n                \"val_loss\": jnp.array(0.2),\n                \"val_accuracy\": jnp.array(0.1),\n            },", "metadata": [{"fpath_tuple": ["awslabs_fortuna", "tests", "fortuna", "test_trainer.py"], "line_no": 248, "start_line_no": 238, "end_line_no": 258, "window_size": 20, "repo": "awslabs_fortuna", "slice_size": 10}], "sim_score": 0.44954128440366975}, {"context": "        ]\n        observed_losses_and_metrics = trainer._get_mean_losses_and_metrics(\n            losses_and_metrics\n        )\n        expected_losses_and_metrics = {\n            \"train_loss\": jnp.array(0.05),\n            \"val_accuracy\": jnp.array(0.1),\n            \"val_loss\": jnp.array(0.21),\n        }\n        self.assertDictEqual(observed_losses_and_metrics, expected_losses_and_metrics)\n\n    def test__get_mean_losses_and_metrics_ko(self):\n        trainer = FakeTrainer(\n            predict_fn=lambda x: x, disable_training_metrics_computation=False\n        )\n        losses_and_metrics = [\n            {\n                \"train_loss\": jnp.array(0.1),\n                \"val_loss\": jnp.array(0.2),\n                \"val_accuracy\": jnp.array(0.1),", "metadata": [{"fpath_tuple": ["awslabs_fortuna", "tests", "fortuna", "test_trainer.py"], "line_no": 226, "start_line_no": 216, "end_line_no": 236, "window_size": 20, "repo": "awslabs_fortuna", "slice_size": 10}], "sim_score": 0.44144144144144143}, {"context": "                \"val_accuracy\": jnp.array(0.1),\n            },\n        ]\n        observed_losses_and_metrics = trainer._get_mean_losses_and_metrics(\n            losses_and_metrics\n        )\n        expected_losses_and_metrics = {\n            \"train_loss\": jnp.array(0.05),\n            \"val_accuracy\": jnp.array(0.1),\n            \"val_loss\": jnp.array(0.21),\n        }\n        self.assertDictEqual(observed_losses_and_metrics, expected_losses_and_metrics)\n\n    def test__get_mean_losses_and_metrics_ko(self):\n        trainer = FakeTrainer(\n            predict_fn=lambda x: x, disable_training_metrics_computation=False\n        )\n        losses_and_metrics = [\n            {\n                \"train_loss\": jnp.array(0.1),", "metadata": [{"fpath_tuple": ["awslabs_fortuna", "tests", "fortuna", "test_trainer.py"], "line_no": 224, "start_line_no": 214, "end_line_no": 234, "window_size": 20, "repo": "awslabs_fortuna", "slice_size": 10}], "sim_score": 0.44144144144144143}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# ding/worker/coordinator/tests/test_coordinator.py\n# --------------------------------------------------\n#     for _, (name, host, port) in cfg.items():\n#         collector[name] = NaiveCollector(host, port, prefix=DATA_PREFIX)\n#         collector[name].start()\n#     yield collector\n#     for a in collector.values():\n#         a.close()\n# \n# \n# @pytest.fixture(scope='function')\n# def setup_learner(setup_config):\n#     cfg = setup_config.system.coordinator.learner\n#     learner = {}\n#     for _, (name, host, port) in cfg.items():\n#         learner[name] = NaiveLearner(host, port, prefix=DATA_PREFIX)\n#         learner[name].start()\n#     yield learner\n#     for l in learner.values():\n#         l.close()\n# \n# \n# --------------------------------------------------\n# the below code fragment can be found in:\n# ding/worker/coordinator/tests/test_coordinator.py\n# --------------------------------------------------\n# @pytest.fixture(scope='function')\n# def setup_collector(setup_config):\n#     cfg = setup_config.system.coordinator.collector\n#     collector = {}\n#     for _, (name, host, port) in cfg.items():\n#         collector[name] = NaiveCollector(host, port, prefix=DATA_PREFIX)\n#         collector[name].start()\n#     yield collector\n#     for a in collector.values():\n#         a.close()\n# \n# \n# @pytest.fixture(scope='function')\n# def setup_learner(setup_config):\n#     cfg = setup_config.system.coordinator.learner\n#     learner = {}\n#     for _, (name, host, port) in cfg.items():\n#         learner[name] = NaiveLearner(host, port, prefix=DATA_PREFIX)\n#         learner[name].start()\n#     yield learner\n# --------------------------------------------------\n# the below code fragment can be found in:\n# ding/worker/coordinator/tests/test_coordinator.py\n# --------------------------------------------------\n#     cfg = setup_config.system.coordinator.collector\n#     collector = {}\n#     for _, (name, host, port) in cfg.items():\n#         collector[name] = NaiveCollector(host, port, prefix=DATA_PREFIX)\n#         collector[name].start()\n#     yield collector\n#     for a in collector.values():\n#         a.close()\n# \n# \n# @pytest.fixture(scope='function')\n# def setup_learner(setup_config):\n#     cfg = setup_config.system.coordinator.learner\n#     learner = {}\n#     for _, (name, host, port) in cfg.items():\n#         learner[name] = NaiveLearner(host, port, prefix=DATA_PREFIX)\n#         learner[name].start()\n#     yield learner\n#     for l in learner.values():\n#         l.close()\n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\nimport pytest\nimport os\nimport copy\nimport time\nfrom threading import Thread\nimport json\nfrom queue import Queue\nfrom flask import Flask, request\n\nfrom ding.worker import Coordinator\nfrom ding.worker.learner.comm import NaiveLearner\nfrom ding.worker.collector.comm import NaiveCollector\nfrom ding.utils import find_free_port\nfrom ding.config import compile_config_parallel\nfrom ding.config.utils import parallel_test_main_config, parallel_test_create_config, parallel_test_system_config\n\nDATA_PREFIX = 'SLAVE_COLLECTOR_DATA_FAKE_OPERATOR_TEST'\ninit_replicas_request = {\n    \"collectors\": {\n        \"cpu\": \"0.5\",\n        \"memory\": \"200Mi\",\n        \"replicas\": 2,\n    },\n    \"learners\": {\n        \"cpu\": \"0.5\",\n        \"memory\": \"200Mi\",\n        \"gpu\": \"0\",\n        \"replicas\": 1,\n    },\n}\napi_version = 'v1alpha1'\nsystem_addr = 'https://0.0.0.0:14502'\n\n\ndef create_app(creator):\n    app = Flask(__name__)\n\n    @app.route('/{}/replicas'.format(api_version), methods=['POST'])\n    def post_replicas():\n        data = json.loads(request.data.decode())\n        collectors = data['collectors'][\"replicas\"]\n        learners = data['learners'][\"replicas\"]\n        creator.set_target_source(learners, collectors)\n        return {'success': True, 'code': 0, 'message': '', 'data': ''}\n\n    @app.route('/{}/replicas'.format(api_version), methods=['GET'])\n    def get_replicas():\n        data = json.loads(request.data.decode())\n        return {'success': True, 'code': 0, 'message': '', 'data': creator.current_resource}\n\n    return app\n\n\n@pytest.fixture(scope='function')\ndef setup_config():\n    cfg = compile_config_parallel(\n        parallel_test_main_config, create_cfg=parallel_test_create_config, system_cfg=parallel_test_system_config\n    )\n    cfg.system.coordinator.operator_server = dict(\n        system_addr=system_addr,\n        api_version=api_version,\n        init_replicas_request=init_replicas_request,\n        collector_target_num=len(cfg.system.coordinator.collector),\n        learner_target_num=len(cfg.system.coordinator.learner),\n    )\n    return cfg\n\n\nclass Creator:\n\n    def __init__(self, learner_addr, collector_addr):\n        self.learner_addr = learner_addr\n        self.collector_addr = collector_addr\n        self.collector_demand = Queue()\n        self.learner_demand = Queue()\n        self.learners = {}\n        self.collectors = {}\n        self.end_flag = False\n\n    def set_target_source(self, learner_target, collector_target):\n        print('set_target_source', learner_target, collector_target)\n        time.sleep(3)  # simulate\n        self.collector_demand.put(collector_target)\n        self.learner_demand.put(learner_target)\n\n    def start(self):\n        while not self.end_flag:\n            if self.learner_demand.empty() and self.collector_demand.empty():\n                time.sleep(0.1)\n                continue\n            else:\n                learner_demand, collector_demand = None, None\n                if not self.learner_demand.empty():\n                    learner_demand = self.learner_demand.get()\n                if not self.collector_demand.empty():\n                    collector_demand = self.collector_demand.get()\n\n                for i in range(collector_demand):\n                    name, host, port = self.collector_addr[i]\n                    self.collectors[name] = NaiveCollector(host, port, prefix=DATA_PREFIX)\n                    self.collectors[name].start()\n                for i in range(learner_demand):\n                    name, host, port = self.learner_addr[i]\n                    self.learners[name] = NaiveLearner(host, port, prefix=DATA_PREFIX)\n                    self.learners[name].start()\n\n    def close(self):\n        self.end_flag = True\n        time.sleep(1)\n        for t in self.learners.values():\n            t.close()\n        for t in self.collectors.values():", "metadata": {"task_id": "opendilab_ACE/132", "ground_truth": "            t.close()", "fpath_tuple": ["opendilab_ACE", "ding", "worker", "coordinator", "tests", "test_fake_operator_server.py"], "context_start_lineno": 0, "line_no": 112, "query_window": {"context": "                if not self.learner_demand.empty():\n                    learner_demand = self.learner_demand.get()\n                if not self.collector_demand.empty():\n                    collector_demand = self.collector_demand.get()\n\n                for i in range(collector_demand):\n                    name, host, port = self.collector_addr[i]\n                    self.collectors[name] = NaiveCollector(host, port, prefix=DATA_PREFIX)\n                    self.collectors[name].start()\n                for i in range(learner_demand):\n                    name, host, port = self.learner_addr[i]\n                    self.learners[name] = NaiveLearner(host, port, prefix=DATA_PREFIX)\n                    self.learners[name].start()\n\n    def close(self):\n        self.end_flag = True\n        time.sleep(1)\n        for t in self.learners.values():\n            t.close()\n        for t in self.collectors.values():", "metadata": {"fpath_tuple": ["opendilab_ACE", "ding", "worker", "coordinator", "tests", "test_fake_operator_server.py"], "line_no": 112, "task_id": "opendilab_ACE/132", "start_line_no": 92, "end_line_no": 112, "window_size": 20, "context_start_lineno": 0, "repo": "opendilab_ACE"}}, "top_k_context": [{"context": "@pytest.fixture(scope='function')\ndef setup_collector(setup_config):\n    cfg = setup_config.system.coordinator.collector\n    collector = {}\n    for _, (name, host, port) in cfg.items():\n        collector[name] = NaiveCollector(host, port, prefix=DATA_PREFIX)\n        collector[name].start()\n    yield collector\n    for a in collector.values():\n        a.close()\n\n\n@pytest.fixture(scope='function')\ndef setup_learner(setup_config):\n    cfg = setup_config.system.coordinator.learner\n    learner = {}\n    for _, (name, host, port) in cfg.items():\n        learner[name] = NaiveLearner(host, port, prefix=DATA_PREFIX)\n        learner[name].start()\n    yield learner", "metadata": [{"fpath_tuple": ["opendilab_ACE", "ding", "worker", "coordinator", "tests", "test_coordinator.py"], "line_no": 30, "start_line_no": 20, "end_line_no": 40, "window_size": 20, "repo": "opendilab_ACE", "slice_size": 10}], "sim_score": 0.44565217391304346}, {"context": "\n\n@pytest.fixture(scope='function')\ndef setup_collector(setup_config):\n    cfg = setup_config.system.coordinator.collector\n    collector = {}\n    for _, (name, host, port) in cfg.items():\n        collector[name] = NaiveCollector(host, port, prefix=DATA_PREFIX)\n        collector[name].start()\n    yield collector\n    for a in collector.values():\n        a.close()\n\n\n@pytest.fixture(scope='function')\ndef setup_learner(setup_config):\n    cfg = setup_config.system.coordinator.learner\n    learner = {}\n    for _, (name, host, port) in cfg.items():\n        learner[name] = NaiveLearner(host, port, prefix=DATA_PREFIX)", "metadata": [{"fpath_tuple": ["opendilab_ACE", "ding", "worker", "coordinator", "tests", "test_coordinator.py"], "line_no": 28, "start_line_no": 18, "end_line_no": 38, "window_size": 20, "repo": "opendilab_ACE", "slice_size": 10}], "sim_score": 0.44565217391304346}, {"context": "    cfg = setup_config.system.coordinator.collector\n    collector = {}\n    for _, (name, host, port) in cfg.items():\n        collector[name] = NaiveCollector(host, port, prefix=DATA_PREFIX)\n        collector[name].start()\n    yield collector\n    for a in collector.values():\n        a.close()\n\n\n@pytest.fixture(scope='function')\ndef setup_learner(setup_config):\n    cfg = setup_config.system.coordinator.learner\n    learner = {}\n    for _, (name, host, port) in cfg.items():\n        learner[name] = NaiveLearner(host, port, prefix=DATA_PREFIX)\n        learner[name].start()\n    yield learner\n    for l in learner.values():\n        l.close()", "metadata": [{"fpath_tuple": ["opendilab_ACE", "ding", "worker", "coordinator", "tests", "test_coordinator.py"], "line_no": 32, "start_line_no": 22, "end_line_no": 42, "window_size": 20, "repo": "opendilab_ACE", "slice_size": 10}], "sim_score": 0.44086021505376344}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# benchmark/pFL-Bench/res_analysis_plot/wandb_to_latex_res.py\n# --------------------------------------------------\n#         rank_order=rank_order,\n#         filter_out=[\"Global-Train\", \"Isolated\"],\n#         convergence_case=True)\n#     for res_to_print in res_to_print_matrix:\n#         print(\"&\".join(res_to_print) + \"\\\\\\\\\")\n# \n# \n# def print_res_non_rec():\n#     print_table_datasets_list(filters_each_line_main_table, order_acc,\n#                               all_res_structed, column_names_generalization,\n#                               column_names_efficiency, column_names_fair,\n#                               expected_keys, sorted_method_name_to_print)\n#     print_table_datasets_list(filters_each_line_femnist_all_s, order_acc,\n#                               all_res_structed, column_names_generalization,\n#                               column_names_efficiency, column_names_fair,\n#                               expected_keys, sorted_method_name_to_print)\n#     print_table_datasets_list(filters_each_line_all_cifar10, order_acc,\n#                               all_res_structed, column_names_generalization,\n#                               column_names_efficiency, column_names_fair,\n#                               expected_keys, sorted_method_name_to_print)\n# --------------------------------------------------\n# the below code fragment can be found in:\n# benchmark/pFL-Bench/res_analysis_plot/wandb_to_latex_res.py\n# --------------------------------------------------\n#                               column_names_efficiency, column_names_fair,\n#                               expected_keys, sorted_method_name_to_print)\n#     print_table_datasets_list(filters_each_line_all_cifar10, order_acc,\n#                               all_res_structed, column_names_generalization,\n#                               column_names_efficiency, column_names_fair,\n#                               expected_keys, sorted_method_name_to_print)\n#     print_table_datasets_list(filters_each_line_all_nlp, order_acc,\n#                               all_res_structed, column_names_generalization,\n#                               column_names_efficiency, column_names_fair,\n#                               expected_keys, sorted_method_name_to_print)\n#     print_table_datasets_list(filters_each_line_all_graph, order_acc,\n#                               all_res_structed, column_names_generalization,\n#                               column_names_efficiency, column_names_fair,\n#                               expected_keys, sorted_method_name_to_print)\n#     for expname_tag in expected_expname_tag:\n#         for metric in column_names_generalization + column_names_efficiency + column_names_fair:\n#             if all_res_structed[expname_tag][metric] == \"-\":\n#                 print(f\"Missing {expname_tag} for metric {metric}\")\n# \n#     with open('best_res_all_metric.json', 'w') as fp:\n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\nextend(\n                [str(res_of_each_line_efficiency[key][i * 4])] + \\\n                [str(res_of_each_line_efficiency[key][i * 4 + 1])] + \\\n                [\"{:.2f}\".format(v * 100) if v != \"-\" else v for v in\n                 res_of_each_line_generalization[key][i * 3:i * 3 + 1]]\n            )\n\n        res_to_print = [str(v) for v in res_of_each_line_commu_acc_trade[key]]\n        res_to_print = [key] + res_to_print\n        res_to_print_matrix.append(res_to_print)\n        # print(\"&\".join(res_to_print)+ \"\\\\\\\\\")\n\n    colum_order_per_data = [\"-\", \"-\", \"+\"]\n    # \"+\" indicates the larger, the better\n    rank_order = colum_order_per_data * len(filters_each_line_table)\n    res_to_print_matrix = highlight_tex_res_in_table(\n        res_to_print_matrix,\n        rank_order=rank_order,\n        need_scale=True,\n        filter_out=[\"Global-Train\", \"Isolated\"])\n    for res_to_print in res_to_print_matrix:\n        print(\"&\".join(res_to_print) + \"\\\\\\\\\")\n\n    print(\n        \"\\n=============res_of_each_line [converge_round, acc]===============\"\n        + \",\".join(list(filters_each_line_table.keys())))\n    res_to_print_matrix = []\n    for key in expected_method_names:\n        res_of_each_line_conver_acc_trade[key] = []\n        dataset_num = 2 if \"cola\" in list(\n            filters_each_line_table.keys()) else 3\n        for i in range(dataset_num):\n            res_of_each_line_conver_acc_trade[key].extend(\n                [str(res_of_each_line_efficiency[key][i * 4 + 3])] + \\\n                # [str(res_of_each_line_efficiency[key][i * 4 + 4])] + \\\n                [\"{:.2f}\".format(v * 100) if v != \"-\" else v for v in res_of_each_line_fair[key][i * 3:i * 3 + 1]]\n            )\n\n        res_to_print = [str(v) for v in res_of_each_line_conver_acc_trade[key]]\n        res_to_print = [key] + res_to_print\n        res_to_print_matrix.append(res_to_print)\n        # print(\"&\".join(res_to_print) + \"\\\\\\\\\")\n\n    colum_order_per_data = [\"-\", \"+\"]\n    # \"+\" indicates the larger, the better\n    rank_order = colum_order_per_data * len(filters_each_line_table)\n    res_to_print_matrix = highlight_tex_res_in_table(\n        res_to_print_matrix,\n        rank_order=rank_order,\n        filter_out=[\"Global-Train\", \"Isolated\"],\n        convergence_case=True)\n    for res_to_print in res_to_print_matrix:\n        print(\"&\".join(res_to_print) + \"\\\\\\\\\")\n\n\nimport json\n\nwith open('best_res_all_metric.json', 'r') as fp:\n    all_res_structed_load = json.load(fp)\n    for expname_tag in expected_expname_tag:\n        if \"repeat\" in expname_tag:\n            continue\n        for metric in column_names_generalization + column_names_efficiency + column_names_fair:\n            all_res_structed[expname_tag][metric] = all_res_structed_load[\n                expname_tag][metric]\n\n# add all res to a df\nimport pandas as pd\n\n\ndef load_data_to_pd(use_repeat_res=False):\n    all_res_for_pd = []\n    for expname_tag in expected_expname_tag:\n        if not use_repeat_res:\n            if \"repeat\" in expname_tag:\n                continue\n        else:\n            if not \"repeat\" in expname_tag:\n                continue\n        res = expname_tag.split(\"_\")  # method, data, seed\n        for metric in column_names_generalization + column_names_fair + column_names_efficiency:\n            res.append(all_res_structed[expname_tag][metric])\n        s = \"-\"\n        alpha = \"-\"\n        if \"FEMNIST-s0\" in res[1]:\n            s = float(res[1].replace(\"FEMNIST-s0\", \"0.\"))\n        if \"cifar10-alpha0\" in res[1]:\n            alpha = float(res[1].replace(\"cifar10-alpha0\", \"0.\"))\n        elif \"cifar10-alpha\" in res[1]:\n            alpha = float(res[1].replace(\"cifar10-alpha\", \"\"))\n        res.append(s)\n        res.append(alpha)\n        total_com_bytes = unit_size_to_bytes(res[-5]) + unit_size_to_bytes(\n            res[-4])\n        total_flops = unit_size_to_bytes(res[-6])\n        res.append(total_com_bytes)\n        res.append(total_flops)\n        all_res_for_pd.append(res)\n\n    all_res_pd = pd.DataFrame().from_records(\n        all_res_for_pd,\n        columns=[\"method\", \"data\", \"seed\"] + column_names_generalization +\n        column_names_fair + column_names_efficiency +\n        [\"s\", \"alpha\", \"communication_bytes\", \"total_flops\"])\n    return all_res_pd\n\n\ndef plot_generalization_lines(all_res_pd, data_cate, data_cate_name):\n    import seaborn as sns\n    from matplotlib import pyplot as plt\n    import matplotlib.pylab as pylab\n\n    plt.clf()\n    sns.set()\n    fig, axes = plt.subplots(1, 3, figsize=(6, 4))\n    print(all_res_pd.columns.tolist())\n\n    plot_data = all_res_pd.loc[all_res_pd[\"data\"].isin(data_cate)]\n\n    plot_data = plot_data.loc[plot_data[\"method\"] != \"Global-Train\"]\n    plot_data = plot_data.loc[plot_data[\"method\"] != \"Isolated\"]\n    plot_data = plot_data.loc[plot_data[\"method\"] != \"FedOpt\"]\n    plot_data = plot_data.loc[plot_data[\"method\"] != \"FedOpt-FT\"]\n    filter_out_methods = [\"Global-Train\", \"Isolated\", \"FedOpt\", \"FedOpt-FT\"]\n    for i, metric in enumerate(column_names_generalization):\n        plt.clf()\n        sns.set()\n        fig, axes = plt.subplots(1, 1, figsize=(2, 3))\n        x = \"data\"\n        if data_cate_name == \"femnist_all\":\n            x = \"s\"\n        if data_cate_name == \"cifar10_all\":\n            x = \"alpha\"\n\n        ax = sns.lineplot(\n            ax=axes,\n            data=plot_data,\n            x=x,\n            y=metric,\n            hue=\"method\",\n            style=\"method\",\n            markers=True,\n            dashes=True,\n            hue_order=[\n                m for m in expected_method_names if m not in filter_out_methods\n            ],\n            sort=True,\n        )\n        ax.set(ylabel=column_names_generalization_for_plot[i])", "metadata": {"task_id": "alibaba_FederatedScope/121", "ground_truth": "        plt.gca().invert_xaxis()", "fpath_tuple": ["alibaba_FederatedScope", "benchmark", "pFL-Bench", "res_analysis_plot", "render_paper_res.py"], "context_start_lineno": 574, "line_no": 723, "query_window": {"context": "        if data_cate_name == \"femnist_all\":\n            x = \"s\"\n        if data_cate_name == \"cifar10_all\":\n            x = \"alpha\"\n\n        ax = sns.lineplot(\n            ax=axes,\n            data=plot_data,\n            x=x,\n            y=metric,\n            hue=\"method\",\n            style=\"method\",\n            markers=True,\n            dashes=True,\n            hue_order=[\n                m for m in expected_method_names if m not in filter_out_methods\n            ],\n            sort=True,\n        )\n        ax.set(ylabel=column_names_generalization_for_plot[i])", "metadata": {"fpath_tuple": ["alibaba_FederatedScope", "benchmark", "pFL-Bench", "res_analysis_plot", "render_paper_res.py"], "line_no": 723, "task_id": "alibaba_FederatedScope/121", "start_line_no": 703, "end_line_no": 723, "window_size": 20, "context_start_lineno": 574, "repo": "alibaba_FederatedScope"}}, "top_k_context": [{"context": "    print_table_datasets_list(filters_each_line_femnist_all_s, order_acc,\n                              all_res_structed, column_names_generalization,\n                              column_names_efficiency, column_names_fair,\n                              expected_keys, sorted_method_name_to_print)\n    print_table_datasets_list(filters_each_line_all_cifar10, order_acc,\n                              all_res_structed, column_names_generalization,\n                              column_names_efficiency, column_names_fair,\n                              expected_keys, sorted_method_name_to_print)\n    print_table_datasets_list(filters_each_line_all_nlp, order_acc,\n                              all_res_structed, column_names_generalization,\n                              column_names_efficiency, column_names_fair,\n                              expected_keys, sorted_method_name_to_print)\n    print_table_datasets_list(filters_each_line_all_graph, order_acc,\n                              all_res_structed, column_names_generalization,\n                              column_names_efficiency, column_names_fair,\n                              expected_keys, sorted_method_name_to_print)\n    for expname_tag in expected_expname_tag:\n        for metric in column_names_generalization + column_names_efficiency + column_names_fair:\n            if all_res_structed[expname_tag][metric] == \"-\":\n                print(f\"Missing {expname_tag} for metric {metric}\")", "metadata": [{"fpath_tuple": ["alibaba_FederatedScope", "benchmark", "pFL-Bench", "res_analysis_plot", "wandb_to_latex_res.py"], "line_no": 744, "start_line_no": 734, "end_line_no": 754, "window_size": 20, "repo": "alibaba_FederatedScope", "slice_size": 10}], "sim_score": 0.2807017543859649}, {"context": "    res_to_print_matrix = highlight_tex_res_in_table(\n        res_to_print_matrix,\n        rank_order=rank_order,\n        filter_out=[\"Global-Train\", \"Isolated\"],\n        convergence_case=True)\n    for res_to_print in res_to_print_matrix:\n        print(\"&\".join(res_to_print) + \"\\\\\\\\\")\n\n\ndef print_res_non_rec():\n    print_table_datasets_list(filters_each_line_main_table, order_acc,\n                              all_res_structed, column_names_generalization,\n                              column_names_efficiency, column_names_fair,\n                              expected_keys, sorted_method_name_to_print)\n    print_table_datasets_list(filters_each_line_femnist_all_s, order_acc,\n                              all_res_structed, column_names_generalization,\n                              column_names_efficiency, column_names_fair,\n                              expected_keys, sorted_method_name_to_print)\n    print_table_datasets_list(filters_each_line_all_cifar10, order_acc,\n                              all_res_structed, column_names_generalization,", "metadata": [{"fpath_tuple": ["alibaba_FederatedScope", "benchmark", "pFL-Bench", "res_analysis_plot", "wandb_to_latex_res.py"], "line_no": 730, "start_line_no": 720, "end_line_no": 740, "window_size": 20, "repo": "alibaba_FederatedScope", "slice_size": 10}], "sim_score": 0.25}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# fortuna/make_data.py\n# tests/make_data.py\n# --------------------------------------------------\n#             yield np.random.normal(size=(batch_size,) + shape_inputs)\n# \n#     return inner\n# \n# \n# def make_generator_random_inputs(\n#     batch_size: int, n_batches: int, shape_inputs: Tuple[int, ...]\n# ) -> Generator[np.ndarray, None, None]:\n#     for i in range(n_batches):\n#         yield np.random.normal(size=(batch_size,) + shape_inputs)\n# \n# \n# def make_array_random_targets(n_inputs: int, output_dim: int, output_type: str):\n#     if output_type not in [\"discrete\", \"continuous\"]:\n#         raise Exception(\n#             \"`output_type={}` not recognized. Please choose among the following list: {}.\".format(\n#                 \"discrete\", \"continuous\"\n#             )\n#         )\n#     return (\n# --------------------------------------------------\n# the below code fragment can be found in:\n# fortuna/make_data.py\n# tests/make_data.py\n# --------------------------------------------------\n#     return inner\n# \n# \n# def make_generator_random_inputs(\n#     batch_size: int, n_batches: int, shape_inputs: Tuple[int, ...]\n# ) -> Generator[np.ndarray, None, None]:\n#     for i in range(n_batches):\n#         yield np.random.normal(size=(batch_size,) + shape_inputs)\n# \n# \n# def make_array_random_targets(n_inputs: int, output_dim: int, output_type: str):\n#     if output_type not in [\"discrete\", \"continuous\"]:\n#         raise Exception(\n#             \"`output_type={}` not recognized. Please choose among the following list: {}.\".format(\n#                 \"discrete\", \"continuous\"\n#             )\n#         )\n#     return (\n#         np.random.normal(size=(n_inputs, output_dim))\n#         if output_type == \"continuous\"\n# --------------------------------------------------\n# the below code fragment can be found in:\n# fortuna/make_data.py\n# tests/make_data.py\n# --------------------------------------------------\n# \n# def make_generator_random_inputs(\n#     batch_size: int, n_batches: int, shape_inputs: Tuple[int, ...]\n# ) -> Generator[np.ndarray, None, None]:\n#     for i in range(n_batches):\n#         yield np.random.normal(size=(batch_size,) + shape_inputs)\n# \n# \n# def make_array_random_targets(n_inputs: int, output_dim: int, output_type: str):\n#     if output_type not in [\"discrete\", \"continuous\"]:\n#         raise Exception(\n#             \"`output_type={}` not recognized. Please choose among the following list: {}.\".format(\n#                 \"discrete\", \"continuous\"\n#             )\n#         )\n#     return (\n#         np.random.normal(size=(n_inputs, output_dim))\n#         if output_type == \"continuous\"\n#         else np.random.choice(output_dim, size=n_inputs)\n#     )\n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\nfrom typing import Callable, Generator, Tuple\n\nimport numpy as np\n\n\ndef make_array_random_inputs(\n    n_inputs: int, shape_inputs: Tuple[int, ...]\n) -> np.ndarray:\n    return np.random.normal(size=(n_inputs,) + shape_inputs)\n\n\ndef make_generator_fun_random_inputs(\n    batch_size: int, n_batches: int, shape_inputs: Tuple[int, ...]\n) -> Callable[[], Generator[np.ndarray, None, None]]:\n    def inner():\n        for i in range(n_batches):\n            yield np.random.normal(size=(batch_size,) + shape_inputs)\n\n    return inner\n\n\ndef make_generator_random_inputs(\n    batch_size: int, n_batches: int, shape_inputs: Tuple[int, ...]\n) -> Generator[np.ndarray, None, None]:\n    for i in range(n_batches):\n        yield np.random.normal(size=(batch_size,) + shape_inputs)\n\n\ndef make_array_random_targets(n_inputs: int, output_dim: int, output_type: str):\n    if output_type not in [\"discrete\", \"continuous\"]:\n        raise Exception(\n            \"`output_type={}` not recognized. Please choose among the following list: {}.\".format(\n                \"discrete\", \"continuous\"\n            )\n        )\n    return (\n        np.random.normal(size=(n_inputs, output_dim))", "metadata": {"task_id": "awslabs_fortuna/151", "ground_truth": "        if output_type == \"continuous\"", "fpath_tuple": ["awslabs_fortuna", "tests", "make_data.py"], "context_start_lineno": 0, "line_no": 37, "query_window": {"context": "\n    return inner\n\n\ndef make_generator_random_inputs(\n    batch_size: int, n_batches: int, shape_inputs: Tuple[int, ...]\n) -> Generator[np.ndarray, None, None]:\n    for i in range(n_batches):\n        yield np.random.normal(size=(batch_size,) + shape_inputs)\n\n\ndef make_array_random_targets(n_inputs: int, output_dim: int, output_type: str):\n    if output_type not in [\"discrete\", \"continuous\"]:\n        raise Exception(\n            \"`output_type={}` not recognized. Please choose among the following list: {}.\".format(\n                \"discrete\", \"continuous\"\n            )\n        )\n    return (\n        np.random.normal(size=(n_inputs, output_dim))", "metadata": {"fpath_tuple": ["awslabs_fortuna", "tests", "make_data.py"], "line_no": 37, "task_id": "awslabs_fortuna/151", "start_line_no": 17, "end_line_no": 37, "window_size": 20, "context_start_lineno": 0, "repo": "awslabs_fortuna"}}, "top_k_context": [{"context": "    return inner\n\n\ndef make_generator_random_inputs(\n    batch_size: int, n_batches: int, shape_inputs: Tuple[int, ...]\n) -> Generator[np.ndarray, None, None]:\n    for i in range(n_batches):\n        yield np.random.normal(size=(batch_size,) + shape_inputs)\n\n\ndef make_array_random_targets(n_inputs: int, output_dim: int, output_type: str):\n    if output_type not in [\"discrete\", \"continuous\"]:\n        raise Exception(\n            \"`output_type={}` not recognized. Please choose among the following list: {}.\".format(\n                \"discrete\", \"continuous\"\n            )\n        )\n    return (\n        np.random.normal(size=(n_inputs, output_dim))\n        if output_type == \"continuous\"", "metadata": [{"fpath_tuple": ["awslabs_fortuna", "fortuna", "make_data.py"], "line_no": 28, "start_line_no": 18, "end_line_no": 38, "window_size": 20, "repo": "awslabs_fortuna", "slice_size": 10}, {"fpath_tuple": ["awslabs_fortuna", "tests", "make_data.py"], "line_no": 28, "start_line_no": 18, "end_line_no": 38, "window_size": 20, "repo": "awslabs_fortuna", "slice_size": 10}], "sim_score": 0.989010989010989}, {"context": "            yield np.random.normal(size=(batch_size,) + shape_inputs)\n\n    return inner\n\n\ndef make_generator_random_inputs(\n    batch_size: int, n_batches: int, shape_inputs: Tuple[int, ...]\n) -> Generator[np.ndarray, None, None]:\n    for i in range(n_batches):\n        yield np.random.normal(size=(batch_size,) + shape_inputs)\n\n\ndef make_array_random_targets(n_inputs: int, output_dim: int, output_type: str):\n    if output_type not in [\"discrete\", \"continuous\"]:\n        raise Exception(\n            \"`output_type={}` not recognized. Please choose among the following list: {}.\".format(\n                \"discrete\", \"continuous\"\n            )\n        )\n    return (", "metadata": [{"fpath_tuple": ["awslabs_fortuna", "fortuna", "make_data.py"], "line_no": 26, "start_line_no": 16, "end_line_no": 36, "window_size": 20, "repo": "awslabs_fortuna", "slice_size": 10}, {"fpath_tuple": ["awslabs_fortuna", "tests", "make_data.py"], "line_no": 26, "start_line_no": 16, "end_line_no": 36, "window_size": 20, "repo": "awslabs_fortuna", "slice_size": 10}], "sim_score": 0.9888888888888889}, {"context": "    def inner():\n        for i in range(n_batches):\n            yield np.random.normal(size=(batch_size,) + shape_inputs)\n\n    return inner\n\n\ndef make_generator_random_inputs(\n    batch_size: int, n_batches: int, shape_inputs: Tuple[int, ...]\n) -> Generator[np.ndarray, None, None]:\n    for i in range(n_batches):\n        yield np.random.normal(size=(batch_size,) + shape_inputs)\n\n\ndef make_array_random_targets(n_inputs: int, output_dim: int, output_type: str):\n    if output_type not in [\"discrete\", \"continuous\"]:\n        raise Exception(\n            \"`output_type={}` not recognized. Please choose among the following list: {}.\".format(\n                \"discrete\", \"continuous\"\n            )", "metadata": [{"fpath_tuple": ["awslabs_fortuna", "fortuna", "make_data.py"], "line_no": 24, "start_line_no": 14, "end_line_no": 34, "window_size": 20, "repo": "awslabs_fortuna", "slice_size": 10}, {"fpath_tuple": ["awslabs_fortuna", "tests", "make_data.py"], "line_no": 24, "start_line_no": 14, "end_line_no": 34, "window_size": 20, "repo": "awslabs_fortuna", "slice_size": 10}], "sim_score": 0.9565217391304348}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# ding/policy/cql.py\n# ding/policy/sac.py\n# --------------------------------------------------\n#             data = to_device(data, self._device)\n#         self._collect_model.eval()\n#         with torch.no_grad():\n#             (mu, sigma) = self._collect_model.forward(data, mode='compute_actor')['logit']\n#             dist = Independent(Normal(mu, sigma), 1)\n#             action = torch.tanh(dist.rsample())\n#             output = {'logit': (mu, sigma), 'action': action}\n#         if self._cuda:\n#             output = to_device(output, 'cpu')\n#         output = default_decollate(output)\n#         return {i: d for i, d in zip(data_id, output)}\n# \n#     def _process_transition(self, obs: Any, model_output: dict, timestep: namedtuple) -> dict:\n#         r\"\"\"\n#         Overview:\n#             Generate dict type transition data from inputs.\n#         Arguments:\n#             - obs (:obj:`Any`): Env observation\n#             - model_output (:obj:`dict`): Output of collect model, including at least ['action']\n#             - timestep (:obj:`namedtuple`): Output after env step, including at least ['obs', 'reward', 'done'] \\\n# --------------------------------------------------\n# the below code fragment can be found in:\n# ding/policy/cql.py\n# ding/policy/sac.py\n# --------------------------------------------------\n#         with torch.no_grad():\n#             (mu, sigma) = self._collect_model.forward(data, mode='compute_actor')['logit']\n#             dist = Independent(Normal(mu, sigma), 1)\n#             action = torch.tanh(dist.rsample())\n#             output = {'logit': (mu, sigma), 'action': action}\n#         if self._cuda:\n#             output = to_device(output, 'cpu')\n#         output = default_decollate(output)\n#         return {i: d for i, d in zip(data_id, output)}\n# \n#     def _process_transition(self, obs: Any, model_output: dict, timestep: namedtuple) -> dict:\n#         r\"\"\"\n#         Overview:\n#             Generate dict type transition data from inputs.\n#         Arguments:\n#             - obs (:obj:`Any`): Env observation\n#             - model_output (:obj:`dict`): Output of collect model, including at least ['action']\n#             - timestep (:obj:`namedtuple`): Output after env step, including at least ['obs', 'reward', 'done'] \\\n#                 (here 'obs' indicates obs after env step, i.e. next_obs).\n#         Return:\n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\n=True)\n            self.alpha_prime_optimizer.step()\n\n        loss_dict['critic_loss'] += min_qf1_loss\n        if self._twin_critic:\n            loss_dict['twin_critic_loss'] += min_qf2_loss\n\n        # update q network\n        self._optimizer_q.zero_grad()\n        loss_dict['critic_loss'].backward(retain_graph=True)\n        if self._twin_critic:\n            loss_dict['twin_critic_loss'].backward()\n        self._optimizer_q.step()\n\n        # evaluate to get action distribution\n        (mu, sigma) = self._learn_model.forward(data['obs'], mode='compute_actor')['logit']\n        dist = Independent(Normal(mu, sigma), 1)\n        pred = dist.rsample()\n        action = torch.tanh(pred)\n        y = 1 - action.pow(2) + 1e-6\n        log_prob = dist.log_prob(pred).unsqueeze(-1)\n        log_prob = log_prob - torch.log(y).sum(-1, keepdim=True)\n\n        eval_data = {'obs': obs, 'action': action}\n        new_q_value = self._learn_model.forward(eval_data, mode='compute_critic')['q_value']\n        if self._twin_critic:\n            new_q_value = torch.min(new_q_value[0], new_q_value[1])\n\n        # =================\n        # value network\n        # =================\n        # compute value loss\n        if self._value_network:\n            # new_q_value: (bs, ), log_prob: (bs, act_shape) -> target_v_value: (bs, )\n            target_v_value = (new_q_value.unsqueeze(-1) - self._alpha * log_prob).mean(dim=-1)\n            loss_dict['value_loss'] = F.mse_loss(v_value, target_v_value.detach())\n\n            # update value network\n            self._optimizer_value.zero_grad()\n            loss_dict['value_loss'].backward()\n            self._optimizer_value.step()\n\n        # =================\n        # policy network\n        # =================\n        # compute policy loss\n        policy_loss = (self._alpha * log_prob - new_q_value.unsqueeze(-1)).mean()\n\n        loss_dict['policy_loss'] = policy_loss\n\n        # update policy network\n        self._optimizer_policy.zero_grad()\n        loss_dict['policy_loss'].backward()\n        self._optimizer_policy.step()\n\n        # compute alpha loss\n        if self._auto_alpha:\n            if self._log_space:\n                log_prob = log_prob + self._target_entropy\n                loss_dict['alpha_loss'] = -(self._log_alpha * log_prob.detach()).mean()\n\n                self._alpha_optim.zero_grad()\n                loss_dict['alpha_loss'].backward()\n                self._alpha_optim.step()\n                self._alpha = self._log_alpha.detach().exp()\n            else:\n                log_prob = log_prob + self._target_entropy\n                loss_dict['alpha_loss'] = -(self._alpha * log_prob.detach()).mean()\n\n                self._alpha_optim.zero_grad()\n                loss_dict['alpha_loss'].backward()\n                self._alpha_optim.step()\n                self._alpha = max(0, self._alpha)\n\n        loss_dict['total_loss'] = sum(loss_dict.values())\n\n        info_dict = {}\n        if self._value_network:\n            info_dict['cur_lr_v'] = self._optimizer_value.defaults['lr']\n\n        # =============\n        # after update\n        # =============\n        self._forward_learn_cnt += 1\n        # target update\n        self._target_model.update(self._learn_model.state_dict())\n        return {\n            'cur_lr_q': self._optimizer_q.defaults['lr'],\n            'cur_lr_p': self._optimizer_policy.defaults['lr'],\n            'priority': td_error_per_sample.abs().tolist(),\n            'td_error': td_error_per_sample.detach().mean().item(),\n            'alpha': self._alpha.item(),\n            'target_value': target_value.detach().mean().item(),\n            **info_dict,\n            **loss_dict\n        }\n\n    def _state_dict_learn(self) -> Dict[str, Any]:\n        ret = {\n            'model': self._learn_model.state_dict(),\n            'target_model': self._target_model.state_dict(),\n            'optimizer_q': self._optimizer_q.state_dict(),\n            'optimizer_policy': self._optimizer_policy.state_dict(),\n        }\n        if self._value_network:\n            ret.update({'optimizer_value': self._optimizer_value.state_dict()})\n        if self._auto_alpha:\n            ret.update({'optimizer_alpha': self._alpha_optim.state_dict()})\n        return ret\n\n    def _load_state_dict_learn(self, state_dict: Dict[str, Any]) -> None:\n        self._learn_model.load_state_dict(state_dict['model'])\n        self._target_model.load_state_dict(state_dict['target_model'])\n        self._optimizer_q.load_state_dict(state_dict['optimizer_q'])\n        if self._value_network:\n            self._optimizer_value.load_state_dict(state_dict['optimizer_value'])\n        self._optimizer_policy.load_state_dict(state_dict['optimizer_policy'])\n        if self._auto_alpha:\n            self._alpha_optim.load_state_dict(state_dict['optimizer_alpha'])\n\n    def _init_collect(self) -> None:\n        r\"\"\"\n        Overview:\n            Collect mode init method. Called by ``self.__init__``.\n            Init traj and unroll length, collect model.\n            Use action noise for exploration.\n        \"\"\"\n        self._unroll_len = self._cfg.collect.unroll_len\n        self._collect_model = model_wrap(self._model, wrapper_name='base')\n        self._collect_model.reset()\n\n    def _forward_collect(self, data: dict) -> dict:\n        r\"\"\"\n        Overview:\n            Forward function of collect mode.\n        Arguments:\n            - data (:obj:`dict`): Dict type data, including at least ['obs'].\n        Returns:\n            - output (:obj:`dict`): Dict type data, including at least inferred action according to input obs.\n        \"\"\"\n        data_id = list(data.keys())\n        data = default_collate(list(data.values()))\n        if self._cuda:\n            data = to_device(data, self._device)\n        self._collect_model.eval()\n        with torch.no_grad():\n            (mu, sigma) = self._collect_model.forward(data, mode='compute_actor')['logit']\n            dist = Independent(Normal(mu, sigma), 1)\n            action = torch.tanh(dist.rsample())\n            output = {'logit': (mu, sigma), 'action': action}\n        if self._cuda:\n            output = to_device(output, 'cpu')\n        output = default_decollate(output)\n        return {i: d for i, d in zip(data_id, output)}\n\n    def _process_transition(self, obs: Any, model_output: dict, timestep: namedtuple) -> dict:\n        r\"\"\"\n        Overview:\n            Generate dict type transition data from inputs.\n        Arguments:\n            - obs (:obj:`Any`): Env observation\n            - model_output (:obj:`dict`): Output of collect model, including at least ['action']", "metadata": {"task_id": "opendilab_ACE/136", "ground_truth": "            - timestep (:obj:`namedtuple`): Output after env step, including at least ['obs', 'reward', 'done'] \\", "fpath_tuple": ["opendilab_ACE", "ding", "policy", "cql.py"], "context_start_lineno": 417, "line_no": 579, "query_window": {"context": "        if self._cuda:\n            data = to_device(data, self._device)\n        self._collect_model.eval()\n        with torch.no_grad():\n            (mu, sigma) = self._collect_model.forward(data, mode='compute_actor')['logit']\n            dist = Independent(Normal(mu, sigma), 1)\n            action = torch.tanh(dist.rsample())\n            output = {'logit': (mu, sigma), 'action': action}\n        if self._cuda:\n            output = to_device(output, 'cpu')\n        output = default_decollate(output)\n        return {i: d for i, d in zip(data_id, output)}\n\n    def _process_transition(self, obs: Any, model_output: dict, timestep: namedtuple) -> dict:\n        r\"\"\"\n        Overview:\n            Generate dict type transition data from inputs.\n        Arguments:\n            - obs (:obj:`Any`): Env observation\n            - model_output (:obj:`dict`): Output of collect model, including at least ['action']", "metadata": {"fpath_tuple": ["opendilab_ACE", "ding", "policy", "cql.py"], "line_no": 579, "task_id": "opendilab_ACE/136", "start_line_no": 559, "end_line_no": 579, "window_size": 20, "context_start_lineno": 417, "repo": "opendilab_ACE"}}, "top_k_context": [{"context": "            data = to_device(data, self._device)\n        self._collect_model.eval()\n        with torch.no_grad():\n            (mu, sigma) = self._collect_model.forward(data, mode='compute_actor')['logit']\n            dist = Independent(Normal(mu, sigma), 1)\n            action = torch.tanh(dist.rsample())\n            output = {'logit': (mu, sigma), 'action': action}\n        if self._cuda:\n            output = to_device(output, 'cpu')\n        output = default_decollate(output)\n        return {i: d for i, d in zip(data_id, output)}\n\n    def _process_transition(self, obs: Any, model_output: dict, timestep: namedtuple) -> dict:\n        r\"\"\"\n        Overview:\n            Generate dict type transition data from inputs.\n        Arguments:\n            - obs (:obj:`Any`): Env observation\n            - model_output (:obj:`dict`): Output of collect model, including at least ['action']\n            - timestep (:obj:`namedtuple`): Output after env step, including at least ['obs', 'reward', 'done'] \\", "metadata": [{"fpath_tuple": ["opendilab_ACE", "ding", "policy", "cql.py"], "line_no": 570, "start_line_no": 560, "end_line_no": 580, "window_size": 20, "repo": "opendilab_ACE", "slice_size": 10}, {"fpath_tuple": ["opendilab_ACE", "ding", "policy", "sac.py"], "line_no": 482, "start_line_no": 472, "end_line_no": 492, "window_size": 20, "repo": "opendilab_ACE", "slice_size": 10}], "sim_score": 0.9242424242424242}, {"context": "        data = default_collate(list(data.values()))\n        if self._cuda:\n            data = to_device(data, self._device)\n        self._collect_model.eval()\n        with torch.no_grad():\n            (mu, sigma) = self._collect_model.forward(data, mode='compute_actor')['logit']\n            dist = Independent(Normal(mu, sigma), 1)\n            action = torch.tanh(dist.rsample())\n            output = {'logit': (mu, sigma), 'action': action}\n        if self._cuda:\n            output = to_device(output, 'cpu')\n        output = default_decollate(output)\n        return {i: d for i, d in zip(data_id, output)}\n\n    def _process_transition(self, obs: Any, model_output: dict, timestep: namedtuple) -> dict:\n        r\"\"\"\n        Overview:\n            Generate dict type transition data from inputs.\n        Arguments:\n            - obs (:obj:`Any`): Env observation", "metadata": [{"fpath_tuple": ["opendilab_ACE", "ding", "policy", "cql.py"], "line_no": 568, "start_line_no": 558, "end_line_no": 578, "window_size": 20, "repo": "opendilab_ACE", "slice_size": 10}, {"fpath_tuple": ["opendilab_ACE", "ding", "policy", "sac.py"], "line_no": 480, "start_line_no": 470, "end_line_no": 490, "window_size": 20, "repo": "opendilab_ACE", "slice_size": 10}], "sim_score": 0.9047619047619048}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# federatedscope/vertical_fl/xgb_base/worker/evaluation_wrapper.py\n# --------------------------------------------------\n#             Message(msg_type='split_result',\n#                     sender=self.ID,\n#                     state=self.state,\n#                     receiver=[sender],\n#                     content=(tree_num, node_num, left_child, right_child)))\n# \n#     def callback_func_for_split_result(self, message: Message):\n#         tree_num, node_num, left_child, right_child = message.content\n#         self.model[tree_num][2 * node_num + 1].indicator = self.model[\n#             tree_num][node_num].indicator * left_child\n#         self.model[tree_num][2 * node_num + 2].indicator = self.model[\n#             tree_num][node_num].indicator * right_child\n#         self._test_for_node(tree_num, node_num + 1)\n# \n#     def callback_func_for_feature_importance(self, message: Message):\n#         state = message.state\n#         self.comm_manager.send(\n#             Message(msg_type='feature_importance',\n#                     sender=self.ID,\n#                     state=state,\n# --------------------------------------------------\n# the below code fragment can be found in:\n# federatedscope/vertical_fl/xgb_base/worker/evaluation_wrapper.py\n# --------------------------------------------------\n#         feature_value = self.model[tree_num][node_num].feature_value\n#         left_child, right_child = self.model[tree_num].split_childern(\n#             self.test_x[:, feature_idx], feature_value)\n#         self.comm_manager.send(\n#             Message(msg_type='split_result',\n#                     sender=self.ID,\n#                     state=self.state,\n#                     receiver=[sender],\n#                     content=(tree_num, node_num, left_child, right_child)))\n# \n#     def callback_func_for_split_result(self, message: Message):\n#         tree_num, node_num, left_child, right_child = message.content\n#         self.model[tree_num][2 * node_num + 1].indicator = self.model[\n#             tree_num][node_num].indicator * left_child\n#         self.model[tree_num][2 * node_num + 2].indicator = self.model[\n#             tree_num][node_num].indicator * right_child\n#         self._test_for_node(tree_num, node_num + 1)\n# \n#     def callback_func_for_feature_importance(self, message: Message):\n#         state = message.state\n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\nimport types\nimport logging\n\nfrom federatedscope.core.message import Message\n\nlogger = logging.getLogger(__name__)\n\n\ndef wrap_client_for_train(client):\n    def train(self, tree_num, node_num=None):\n        if node_num is None:\n            logger.info(f'----------- Building a new tree (Tree '\n                        f'#{tree_num}) -------------')\n            self.state = tree_num\n            finish_flag, results = self.trainer.train(\n                feature_order=self.merged_feature_order, tree_num=tree_num)\n        else:\n            assert node_num is not None\n            finish_flag, results = self.trainer.train(tree_num=tree_num,\n                                                      node_num=node_num)\n\n        if not finish_flag:\n            split_ref, tree_num, node_num = results\n            self._find_and_send_split(split_ref, tree_num, node_num)\n        else:\n            # Report evaluation results\n            self.eval_finish_flag = False\n            self.eval(tree_num)\n            self._check_eval_finish(tree_num)\n\n    def _check_eval_finish(self, tree_num):\n        if self.eval_finish_flag:\n            if tree_num + 1 < self._cfg.model.num_of_trees:\n                self.train(tree_num + 1)\n\n    def _find_and_send_split(self, split_ref, tree_num, node_num):\n        for index, dim in enumerate(self._cfg.vertical.dims):\n            if split_ref['feature_idx'] < dim:\n                prefix = self._cfg.vertical.dims[index -\n                                                 1] if index != 0 else 0\n                client_id = index + 1\n                self.model[tree_num][node_num].member = client_id\n                self.model[tree_num][\n                    node_num].feature_idx = split_ref['feature_idx'] - prefix\n\n                self.comm_manager.send(\n                    Message(msg_type='split',\n                            sender=self.ID,\n                            state=self.state,\n                            receiver=[client_id],\n                            content=(tree_num, node_num,\n                                     split_ref['feature_idx'] - prefix,\n                                     split_ref['value_idx'])))\n                break\n\n    def callback_func_for_split(self, message: Message):\n        tree_num, node_num, feature_idx, value_idx = message.content\n        sender = message.sender\n        self.state = message.state\n        self.feature_importance[feature_idx] += 1\n\n        instance_idx = self.feature_order[feature_idx][value_idx]\n        feature_value = self.trainer.batch_x[instance_idx, feature_idx]\n\n        self.model[tree_num][node_num].feature_idx = feature_idx\n        self.model[tree_num][node_num].feature_value = feature_value\n\n        self.comm_manager.send(\n            Message(msg_type='continue_training',\n                    sender=self.ID,\n                    state=self.state,\n                    receiver=[sender],\n                    content=(tree_num, node_num)))\n\n    def callback_funcs_for_continue_training(self, message: Message):\n        tree_num, node_num = message.content\n        self.train(tree_num=tree_num, node_num=node_num + 1)\n\n    # Bind method to instance\n    client.train = types.MethodType(train, client)\n    client.callback_func_for_split = types.MethodType(callback_func_for_split,\n                                                      client)\n    client.callback_funcs_for_continue_training = types.MethodType(\n        callback_funcs_for_continue_training, client)", "metadata": {"task_id": "alibaba_FederatedScope/18", "ground_truth": "    client._find_and_send_split = types.MethodType(_find_and_send_split,", "fpath_tuple": ["alibaba_FederatedScope", "federatedscope", "vertical_fl", "xgb_base", "worker", "train_wrapper.py"], "context_start_lineno": 0, "line_no": 84, "query_window": {"context": "        self.model[tree_num][node_num].feature_idx = feature_idx\n        self.model[tree_num][node_num].feature_value = feature_value\n\n        self.comm_manager.send(\n            Message(msg_type='continue_training',\n                    sender=self.ID,\n                    state=self.state,\n                    receiver=[sender],\n                    content=(tree_num, node_num)))\n\n    def callback_funcs_for_continue_training(self, message: Message):\n        tree_num, node_num = message.content\n        self.train(tree_num=tree_num, node_num=node_num + 1)\n\n    # Bind method to instance\n    client.train = types.MethodType(train, client)\n    client.callback_func_for_split = types.MethodType(callback_func_for_split,\n                                                      client)\n    client.callback_funcs_for_continue_training = types.MethodType(\n        callback_funcs_for_continue_training, client)", "metadata": {"fpath_tuple": ["alibaba_FederatedScope", "federatedscope", "vertical_fl", "xgb_base", "worker", "train_wrapper.py"], "line_no": 84, "task_id": "alibaba_FederatedScope/18", "start_line_no": 64, "end_line_no": 84, "window_size": 20, "context_start_lineno": 0, "repo": "alibaba_FederatedScope"}}, "top_k_context": [{"context": "        sender = message.sender\n        feature_idx = self.model[tree_num][node_num].feature_idx\n        feature_value = self.model[tree_num][node_num].feature_value\n        left_child, right_child = self.model[tree_num].split_childern(\n            self.test_x[:, feature_idx], feature_value)\n        self.comm_manager.send(\n            Message(msg_type='split_result',\n                    sender=self.ID,\n                    state=self.state,\n                    receiver=[sender],\n                    content=(tree_num, node_num, left_child, right_child)))\n\n    def callback_func_for_split_result(self, message: Message):\n        tree_num, node_num, left_child, right_child = message.content\n        self.model[tree_num][2 * node_num + 1].indicator = self.model[\n            tree_num][node_num].indicator * left_child\n        self.model[tree_num][2 * node_num + 2].indicator = self.model[\n            tree_num][node_num].indicator * right_child\n        self._test_for_node(tree_num, node_num + 1)\n", "metadata": [{"fpath_tuple": ["alibaba_FederatedScope", "federatedscope", "vertical_fl", "xgb_base", "worker", "evaluation_wrapper.py"], "line_no": 104, "start_line_no": 94, "end_line_no": 114, "window_size": 20, "repo": "alibaba_FederatedScope", "slice_size": 10}], "sim_score": 0.6666666666666666}, {"context": "            self.test_x[:, feature_idx], feature_value)\n        self.comm_manager.send(\n            Message(msg_type='split_result',\n                    sender=self.ID,\n                    state=self.state,\n                    receiver=[sender],\n                    content=(tree_num, node_num, left_child, right_child)))\n\n    def callback_func_for_split_result(self, message: Message):\n        tree_num, node_num, left_child, right_child = message.content\n        self.model[tree_num][2 * node_num + 1].indicator = self.model[\n            tree_num][node_num].indicator * left_child\n        self.model[tree_num][2 * node_num + 2].indicator = self.model[\n            tree_num][node_num].indicator * right_child\n        self._test_for_node(tree_num, node_num + 1)\n\n    def callback_func_for_feature_importance(self, message: Message):\n        state = message.state\n        self.comm_manager.send(\n            Message(msg_type='feature_importance',", "metadata": [{"fpath_tuple": ["alibaba_FederatedScope", "federatedscope", "vertical_fl", "xgb_base", "worker", "evaluation_wrapper.py"], "line_no": 108, "start_line_no": 98, "end_line_no": 118, "window_size": 20, "repo": "alibaba_FederatedScope", "slice_size": 10}], "sim_score": 0.6593406593406593}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# vizier/_src/algorithms/optimizers/eagle_strategy.py\n# --------------------------------------------------\n#       self._trim_pool()\n#     self._increment_batch()\n#     self._iterations += 1\n# \n#   def _update_best_reward(self, batch_rewards: np.ndarray) -> None:\n#     \"\"\"Store the best result seen thus far to be used in pool trimming.\"\"\"\n#     self._best_reward = np.max([self._best_reward, np.max(batch_rewards)])\n# \n#   def _update_pool_features_and_rewards(\n#       self,\n#       batch_rewards: np.ndarray,\n#   ):\n#     \"\"\"Update the features and rewards for flies with improved rewards.\n# \n#     Arguments:\n#       batch_rewards: (batch_size, )\n#     \"\"\"\n#     sliced_features = self._features[self._batch_slice]\n#     sliced_rewards = self._rewards[self._batch_slice]\n#     sliced_perturbations = self._perturbations[self._batch_slice]\n# --------------------------------------------------\n# the below code fragment can be found in:\n# vizier/_src/algorithms/optimizers/eagle_strategy.py\n# --------------------------------------------------\n# \n#     Arguments:\n#       batch_rewards: (batch_size, )\n#     \"\"\"\n#     self._update_best_reward(batch_rewards)\n#     if self._iterations < self.pool_size // self.batch_size:\n#       # The strategy is still initializing. Assign rewards.\n#       self._features[self._batch_slice] = self._last_suggested_features\n#       self._rewards[self._batch_slice] = batch_rewards\n#     else:\n#       # Pass the new batch rewards and the associated last suggested features.\n#       self._update_pool_features_and_rewards(batch_rewards)\n#       self._trim_pool()\n#     self._increment_batch()\n#     self._iterations += 1\n# \n#   def _update_best_reward(self, batch_rewards: np.ndarray) -> None:\n#     \"\"\"Store the best result seen thus far to be used in pool trimming.\"\"\"\n#     self._best_reward = np.max([self._best_reward, np.max(batch_rewards)])\n# \n# --------------------------------------------------\n# the below code fragment can be found in:\n# vizier/_src/algorithms/optimizers/eagle_strategy.py\n# --------------------------------------------------\n#   def _update_best_reward(self, batch_rewards: np.ndarray) -> None:\n#     \"\"\"Store the best result seen thus far to be used in pool trimming.\"\"\"\n#     self._best_reward = np.max([self._best_reward, np.max(batch_rewards)])\n# \n#   def _update_pool_features_and_rewards(\n#       self,\n#       batch_rewards: np.ndarray,\n#   ):\n#     \"\"\"Update the features and rewards for flies with improved rewards.\n# \n#     Arguments:\n#       batch_rewards: (batch_size, )\n#     \"\"\"\n#     sliced_features = self._features[self._batch_slice]\n#     sliced_rewards = self._rewards[self._batch_slice]\n#     sliced_perturbations = self._perturbations[self._batch_slice]\n#     # Find indices of flies that their generated features made an improvement.\n#     improve_indx = batch_rewards > sliced_rewards\n#     # Update successful flies' with the associated last features and rewards.\n#     sliced_features[improve_indx] = self._last_suggested_features[improve_indx]\n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\n\n  \"\"\"Container for a vectorized strategy result.\"\"\"\n\n  features: Array\n  reward: float\n\n  def __lt__(self, other):\n    return self.reward < other.reward\n\n\nclass VectorizedStrategy(abc.ABC):\n  \"\"\"Interface class to implement a pure vectorized strategy.\n\n  The strategy is responsible for generating suggestions that will maximize the\n  reward. The order of calls is important. It's expected to be used in\n  'suggest','update', 'suggest', 'update', etc.\n  \"\"\"\n\n  @abc.abstractmethod\n  def suggest(self) -> Array:\n    \"\"\"Generate new suggestions.\n\n    Returns:\n      suggested features: (batch_size, features_count)\n    \"\"\"\n\n  @property\n  @abc.abstractmethod\n  def suggestion_batch_size(self) -> int:\n    \"\"\"The number of suggestions returned at every suggest call.\"\"\"\n\n  @abc.abstractmethod\n  def update(self, rewards: Array) -> None:\n    \"\"\"Update the strategy state with the results of the last suggestions.\n\n    Arguments:\n      rewards: (batch_size, )\n    \"\"\"\n\n\nclass VectorizedStrategyFactory(Protocol):\n  \"\"\"Factory class to generate vectorized strategy.\n\n  It's used in VectorizedOptimizer to create a new strategy every 'optimize'\n  call.\n  \"\"\"\n\n  def __call__(\n      self,\n      converter: converters.TrialToArrayConverter,\n      *,\n      suggestion_batch_size: int,\n      seed: Optional[int] = None,\n      prior_features: Optional[np.ndarray] = None,\n      prior_rewards: Optional[np.ndarray] = None,\n  ) -> VectorizedStrategy:\n    \"\"\"Create a new vectorized strategy.\n\n    Arguments:\n      converter: The trial to array converter.\n      suggestion_batch_size: The number of trials to be evaluated at once.\n      seed: The random seed.\n      prior_features: Prior trial features for knowledge transfer with a shape\n        of (n_trial_featurs, n_features)\n      prior_rewards: Prior trial rewards for knowledge transfer with a shape of\n        (n_trial_featurs,)\n    \"\"\"\n    ...\n\n\nclass BatchArrayScoreFunction(Protocol):\n  \"\"\"Protocol for scoring array of batched trials.\"\"\"\n\n  def __call__(self, batched_array_trials: Array) -> Array:\n    \"\"\"Evaluates the array of batched trials.\n\n    Arguments:\n      batched_array_trials: 2D Array of shape (batch_size, n_features).\n\n    Returns:\n      1D Array of shape (batch_size,).\n    \"\"\"\n\n\n@attr.define(kw_only=True)\nclass VectorizedOptimizer:\n  \"\"\"Vectorized strategy optimizer.\n\n  The optimizer is stateless and will create a new vectorized strategy at the\n  beginning of every 'optimize' call.\n\n  The optimizer is responsible for running the iterative optimization process\n  using the vectorized strategy, which consists of:\n  1. Ask the strategy for suggestions.\n  2. Evaluate the suggestions to get rewards.\n  3. Tell the strategy about the rewards of its suggestion, so the strategy can\n  update its internal state.\n\n  The optimization process will terminate when the time limit or the total\n  objective function evaluations limit has exceeded.\n\n  Attributes:\n    strategy_factory: A factory for generating new strategy.\n    max_evaluations: The maximum number of objective function evaluations.\n    max_duration: The maximum duration of the optimization process.\n    suggestion_batch_size: The batch size of the suggestion vector received at\n      each 'suggest' call.\n  \"\"\"\n\n  strategy_factory: VectorizedStrategyFactory\n  suggestion_batch_size: int = 25\n  max_evaluations: int = 75_000\n  max_duration: Optional[datetime.timedelta] = None\n\n  def optimize(\n      self,\n      converter: converters.TrialToArrayConverter,\n      score_fn: BatchArrayScoreFunction,\n      *,\n      count: int = 1,\n      prior_trials: Optional[Sequence[vz.Trial]] = None,\n      seed: Optional[int] = None,\n  ) -> list[vz.Trial]:\n    \"\"\"Optimize the objective function.\n\n    The ask-evaluate-tell optimization procedure that runs until the allocated\n    time or evaluations count exceeds.\n\n    The number of suggestions is determined by the strategy, which is the\n    `suggestion_count` property.\n\n    The converter should be the same one used to convert trials to arrays in the\n    format that the objective function expects to receive. It's used to convert\n    backward the strategy's best array candidates to trial candidates.\n    In addition, the converter is passed to the strategy so it could be aware\n    of the type associated with each array index, and which indices are part of\n    the same CATEGORICAL parameter one-hot encoding.\n\n    The optimization stops when either of 'max_evaluations' or 'max_duration' is\n    reached.\n\n    Arguments:\n      converter: The converter used to convert Trials to arrays.\n      score_fn: A callback that expects 2D Array with dimensions (batch_size,\n        features_count) and returns a 1D Array (batch_size,).\n      count: The number of suggestions to generate.\n      prior_trials: Completed trials to be used for knowledge transfer. When the\n        optimizer is used to optimize a designer's acquisition function, the\n        prior trials are the previous designer suggestions provided in the\n        ordered they were suggested.\n      seed: The seed to use in the random generator.\n\n    Returns:\n      The best trials found in the optimization.\n    \"\"\"\n    if prior_trials:\n      # Sort the trials by the order they were created.\n      prior_trials = sorted(prior_trials, key=lambda x: x.creation_time)\n      prior_features = converter.to_features(prior_trials)\n      prior_rewards = np.array(score_fn(prior_features)).reshape(-1)\n    else:\n      prior_features, prior_rewards = None, None\n\n    strategy = self.strategy_factory(\n        converter=converter,\n        suggestion_batch_size=self.suggestion_batch_size,\n        seed=seed,\n        prior_features=prior_features,\n        prior_rewards=prior_rewards,\n    )\n\n    start_time = datetime.datetime.now()\n    evaluated_count = 0\n    best_results = []\n\n    while not self._should_stop(start_time, evaluated_count):\n      new_features = strategy.suggest()\n      new_rewards = np.asarray(score_fn(new_features))\n      strategy.update(new_rewards)\n      self._update_best_results(best_results, count, new_features, new_rewards)\n      evaluated_count += len(new_rewards)\n    logging.info(\n        (\n            'Optimization completed. Duration: %s. Evaluations: %s. Best'\n            ' Results: %s'\n        ),\n        datetime.datetime.now() - start_time,\n        evaluated_count,\n        best_results,\n    )\n\n    return self._best_candidates(best_results, converter)\n\n  def _update_best_results(\n      self,\n      best_results: list[VectorizedStrategyResult],\n      count: int,\n      batch_features: np.ndarray,\n      batch_rewards: np.ndarray,\n  ) -> None:\n    \"\"\"Update the best results the optimizer seen thus far.\n\n    The best results are kept in a heap to efficiently maintain the top 'count'\n    results throughout the optimizer run.\n\n    Arguments:\n      best_results: A heap storing the best results seen thus far. Implemented\n        as a list of maximum size of 'count'.\n      count: The number of best results to store.\n      batch_features: The current suggested features batch array with a\n        dimension of (batch_size, feature_dim).\n      batch_rewards: The current reward batch array with a dimension of\n        (batch_size,).\n    \"\"\"\n    sorted_indices = sorted(\n        range(len(batch_rewards)), key=lambda x: batch_rewards[x], reverse=True\n    )[:count]\n    if count == 1:\n      # In this case we can simply track the single best result.\n      ind = sorted_indices[0]", "metadata": {"task_id": "google_vizier/91", "ground_truth": "      if not best_results:", "fpath_tuple": ["google_vizier", "vizier", "_src", "algorithms", "optimizers", "vectorized_base.py"], "context_start_lineno": 35, "line_no": 255, "query_window": {"context": "    \"\"\"Update the best results the optimizer seen thus far.\n\n    The best results are kept in a heap to efficiently maintain the top 'count'\n    results throughout the optimizer run.\n\n    Arguments:\n      best_results: A heap storing the best results seen thus far. Implemented\n        as a list of maximum size of 'count'.\n      count: The number of best results to store.\n      batch_features: The current suggested features batch array with a\n        dimension of (batch_size, feature_dim).\n      batch_rewards: The current reward batch array with a dimension of\n        (batch_size,).\n    \"\"\"\n    sorted_indices = sorted(\n        range(len(batch_rewards)), key=lambda x: batch_rewards[x], reverse=True\n    )[:count]\n    if count == 1:\n      # In this case we can simply track the single best result.\n      ind = sorted_indices[0]", "metadata": {"fpath_tuple": ["google_vizier", "vizier", "_src", "algorithms", "optimizers", "vectorized_base.py"], "line_no": 255, "task_id": "google_vizier/91", "start_line_no": 235, "end_line_no": 255, "window_size": 20, "context_start_lineno": 35, "repo": "google_vizier"}}, "top_k_context": [{"context": "    self._iterations += 1\n\n  def _update_best_reward(self, batch_rewards: np.ndarray) -> None:\n    \"\"\"Store the best result seen thus far to be used in pool trimming.\"\"\"\n    self._best_reward = np.max([self._best_reward, np.max(batch_rewards)])\n\n  def _update_pool_features_and_rewards(\n      self,\n      batch_rewards: np.ndarray,\n  ):\n    \"\"\"Update the features and rewards for flies with improved rewards.\n\n    Arguments:\n      batch_rewards: (batch_size, )\n    \"\"\"\n    sliced_features = self._features[self._batch_slice]\n    sliced_rewards = self._rewards[self._batch_slice]\n    sliced_perturbations = self._perturbations[self._batch_slice]\n    # Find indices of flies that their generated features made an improvement.\n    improve_indx = batch_rewards > sliced_rewards", "metadata": [{"fpath_tuple": ["google_vizier", "vizier", "_src", "algorithms", "optimizers", "eagle_strategy.py"], "line_no": 566, "start_line_no": 556, "end_line_no": 576, "window_size": 20, "repo": "google_vizier", "slice_size": 10}], "sim_score": 0.2549019607843137}, {"context": "  def update(self, batch_rewards: np.ndarray) -> None:\n    \"\"\"Update the firefly pool based on the new batch of results.\n\n    Arguments:\n      batch_rewards: (batch_size, )\n    \"\"\"\n    self._update_best_reward(batch_rewards)\n    if self._iterations < self.pool_size // self.batch_size:\n      # The strategy is still initializing. Assign rewards.\n      self._features[self._batch_slice] = self._last_suggested_features\n      self._rewards[self._batch_slice] = batch_rewards\n    else:\n      # Pass the new batch rewards and the associated last suggested features.\n      self._update_pool_features_and_rewards(batch_rewards)\n      self._trim_pool()\n    self._increment_batch()\n    self._iterations += 1\n\n  def _update_best_reward(self, batch_rewards: np.ndarray) -> None:\n    \"\"\"Store the best result seen thus far to be used in pool trimming.\"\"\"", "metadata": [{"fpath_tuple": ["google_vizier", "vizier", "_src", "algorithms", "optimizers", "eagle_strategy.py"], "line_no": 550, "start_line_no": 540, "end_line_no": 560, "window_size": 20, "repo": "google_vizier", "slice_size": 10}], "sim_score": 0.25}, {"context": "      # Pass the new batch rewards and the associated last suggested features.\n      self._update_pool_features_and_rewards(batch_rewards)\n      self._trim_pool()\n    self._increment_batch()\n    self._iterations += 1\n\n  def _update_best_reward(self, batch_rewards: np.ndarray) -> None:\n    \"\"\"Store the best result seen thus far to be used in pool trimming.\"\"\"\n    self._best_reward = np.max([self._best_reward, np.max(batch_rewards)])\n\n  def _update_pool_features_and_rewards(\n      self,\n      batch_rewards: np.ndarray,\n  ):\n    \"\"\"Update the features and rewards for flies with improved rewards.\n\n    Arguments:\n      batch_rewards: (batch_size, )\n    \"\"\"\n    sliced_features = self._features[self._batch_slice]", "metadata": [{"fpath_tuple": ["google_vizier", "vizier", "_src", "algorithms", "optimizers", "eagle_strategy.py"], "line_no": 562, "start_line_no": 552, "end_line_no": 572, "window_size": 20, "repo": "google_vizier", "slice_size": 10}], "sim_score": 0.24666666666666667}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# federatedscope/vertical_fl/dataloader/utils.py\n# --------------------------------------------------\n#     Arguments:\n#         data(dict): data\n#         batch_size (int): the batch size\n#         shuffled (bool): whether to shuffle the data at the start of each epoch\n#     :returns: sample index, batch of x, batch_of y\n#     :rtype: int, ndarray, ndarry\n#     \"\"\"\n# \n#     assert 'x' in data and 'y' in data\n#     data_x = data['x']\n#     data_y = data['y']\n#     data_size = len(data_y)\n#     num_batches_per_epoch = math.ceil(data_size / batch_size)\n# \n#     while True:\n#         shuffled_index = np.random.permutation(\n#             np.arange(data_size)) if shuffled else np.arange(data_size)\n#         for batch in range(num_batches_per_epoch):\n#             start_index = batch * batch_size\n#             end_index = min(data_size, (batch + 1) * batch_size)\n# --------------------------------------------------\n# the below code fragment can be found in:\n# federatedscope/vertical_fl/dataloader/utils.py\n# --------------------------------------------------\n#         batch_size (int): the batch size\n#         shuffled (bool): whether to shuffle the data at the start of each epoch\n#     :returns: sample index, batch of x, batch_of y\n#     :rtype: int, ndarray, ndarry\n#     \"\"\"\n# \n#     assert 'x' in data and 'y' in data\n#     data_x = data['x']\n#     data_y = data['y']\n#     data_size = len(data_y)\n#     num_batches_per_epoch = math.ceil(data_size / batch_size)\n# \n#     while True:\n#         shuffled_index = np.random.permutation(\n#             np.arange(data_size)) if shuffled else np.arange(data_size)\n#         for batch in range(num_batches_per_epoch):\n#             start_index = batch * batch_size\n#             end_index = min(data_size, (batch + 1) * batch_size)\n#             sample_index = shuffled_index[start_index:end_index]\n#             yield sample_index, data_x[sample_index], data_y[sample_index]\n# --------------------------------------------------\n# the below code fragment can be found in:\n# federatedscope/vertical_fl/dataloader/utils.py\n# --------------------------------------------------\n#         batch_size (int): the batch size\n#         shuffled (bool): whether to shuffle the data at the start of each epoch\n#     :returns: sample index, batch of x, batch_of y\n#     :rtype: int, ndarray, ndarry\n#     \"\"\"\n# \n#     assert 'x' in data and 'y' in data\n#     data_x = data['x']\n#     data_y = data['y']\n#     data_size = len(data_y)\n#     num_batches_per_epoch = math.ceil(data_size / batch_size)\n# \n#     while True:\n#         shuffled_index = np.random.permutation(\n#             np.arange(data_size)) if shuffled else np.arange(data_size)\n#         for batch in range(num_batches_per_epoch):\n#             start_index = batch * batch_size\n#             end_index = min(data_size, (batch + 1) * batch_size)\n#             sample_index = shuffled_index[start_index:end_index]\n#             yield sample_index, data_x[sample_index], data_y[sample_index]\n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\nimport logging\nimport math\nimport os\nimport random\nimport signal\nimport pickle\n\nimport numpy as np\n\ntry:\n    import torch\nexcept ImportError:\n    torch = None\n\ntry:\n    import tensorflow as tf\nexcept ImportError:\n    tf = None\n\nlogger = logging.getLogger(__name__)\n\n\n# ****** Worker-related utils ******\nclass Timeout(object):\n    def __init__(self, seconds, max_failure=5):\n        self.seconds = seconds\n        self.max_failure = max_failure\n\n    def __enter__(self):\n        def signal_handler(signum, frame):\n            raise TimeoutError()\n\n        if self.seconds > 0:\n            signal.signal(signal.SIGALRM, signal_handler)\n            signal.alarm(self.seconds)\n        return self\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        signal.alarm(0)\n\n    def reset(self):\n        signal.alarm(self.seconds)\n\n    def block(self):\n        signal.alarm(0)\n\n    def exceed_max_failure(self, num_failure):\n        return num_failure > self.max_failure\n\n\ndef batch_iter(data, batch_size=64, shuffled=True):\n    assert 'x' in data and 'y' in data\n    data_x = data['x']\n    data_y = data['y']\n    data_size = len(data_y)\n    num_batches_per_epoch = math.ceil(data_size / batch_size)\n\n    while True:\n        shuffled_index = np.random.permutation(\n            np.arange(data_size)) if shuffled else np.arange(data_size)\n        for batch in range(num_batches_per_epoch):\n            start_index = batch * batch_size", "metadata": {"task_id": "alibaba_FederatedScope/11", "ground_truth": "            end_index = min(data_size, (batch + 1) * batch_size)", "fpath_tuple": ["alibaba_FederatedScope", "federatedscope", "core", "auxiliaries", "utils.py"], "context_start_lineno": 0, "line_no": 62, "query_window": {"context": "\n    def block(self):\n        signal.alarm(0)\n\n    def exceed_max_failure(self, num_failure):\n        return num_failure > self.max_failure\n\n\ndef batch_iter(data, batch_size=64, shuffled=True):\n    assert 'x' in data and 'y' in data\n    data_x = data['x']\n    data_y = data['y']\n    data_size = len(data_y)\n    num_batches_per_epoch = math.ceil(data_size / batch_size)\n\n    while True:\n        shuffled_index = np.random.permutation(\n            np.arange(data_size)) if shuffled else np.arange(data_size)\n        for batch in range(num_batches_per_epoch):\n            start_index = batch * batch_size", "metadata": {"fpath_tuple": ["alibaba_FederatedScope", "federatedscope", "core", "auxiliaries", "utils.py"], "line_no": 62, "task_id": "alibaba_FederatedScope/11", "start_line_no": 42, "end_line_no": 62, "window_size": 20, "context_start_lineno": 0, "repo": "alibaba_FederatedScope"}}, "top_k_context": [{"context": "    assert 'x' in data and 'y' in data\n    data_x = data['x']\n    data_y = data['y']\n    data_size = len(data_y)\n    num_batches_per_epoch = math.ceil(data_size / batch_size)\n\n    while True:\n        shuffled_index = np.random.permutation(\n            np.arange(data_size)) if shuffled else np.arange(data_size)\n        for batch in range(num_batches_per_epoch):\n            start_index = batch * batch_size\n            end_index = min(data_size, (batch + 1) * batch_size)\n            sample_index = shuffled_index[start_index:end_index]\n            yield sample_index, data_x[sample_index], data_y[sample_index]", "metadata": [{"fpath_tuple": ["alibaba_FederatedScope", "federatedscope", "vertical_fl", "dataloader", "utils.py"], "line_no": 26, "start_line_no": 16, "end_line_no": 30, "window_size": 20, "repo": "alibaba_FederatedScope", "slice_size": 10}], "sim_score": 0.6292134831460674}, {"context": "    \"\"\"\n\n    assert 'x' in data and 'y' in data\n    data_x = data['x']\n    data_y = data['y']\n    data_size = len(data_y)\n    num_batches_per_epoch = math.ceil(data_size / batch_size)\n\n    while True:\n        shuffled_index = np.random.permutation(\n            np.arange(data_size)) if shuffled else np.arange(data_size)\n        for batch in range(num_batches_per_epoch):\n            start_index = batch * batch_size\n            end_index = min(data_size, (batch + 1) * batch_size)\n            sample_index = shuffled_index[start_index:end_index]\n            yield sample_index, data_x[sample_index], data_y[sample_index]", "metadata": [{"fpath_tuple": ["alibaba_FederatedScope", "federatedscope", "vertical_fl", "dataloader", "utils.py"], "line_no": 24, "start_line_no": 14, "end_line_no": 30, "window_size": 20, "repo": "alibaba_FederatedScope", "slice_size": 10}], "sim_score": 0.6222222222222222}, {"context": "    A batch iteration\n\n    Arguments:\n        data(dict): data\n        batch_size (int): the batch size\n        shuffled (bool): whether to shuffle the data at the start of each epoch\n    :returns: sample index, batch of x, batch_of y\n    :rtype: int, ndarray, ndarry\n    \"\"\"\n\n    assert 'x' in data and 'y' in data\n    data_x = data['x']\n    data_y = data['y']\n    data_size = len(data_y)\n    num_batches_per_epoch = math.ceil(data_size / batch_size)\n\n    while True:\n        shuffled_index = np.random.permutation(\n            np.arange(data_size)) if shuffled else np.arange(data_size)\n        for batch in range(num_batches_per_epoch):", "metadata": [{"fpath_tuple": ["alibaba_FederatedScope", "federatedscope", "vertical_fl", "dataloader", "utils.py"], "line_no": 16, "start_line_no": 6, "end_line_no": 26, "window_size": 20, "repo": "alibaba_FederatedScope", "slice_size": 10}], "sim_score": 0.5092592592592593}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# vizier/_src/pyvizier/multimetric/safety_test.py\n# --------------------------------------------------\n# # Copyright 2023 Google LLC.\n# #\n# # Licensed under the Apache License, Version 2.0 (the \"License\");\n# # you may not use this file except in compliance with the License.\n# # You may obtain a copy of the License at\n# #\n# #     http://www.apache.org/licenses/LICENSE-2.0\n# #\n# # Unless required by applicable law or agreed to in writing, software\n# # distributed under the License is distributed on an \"AS IS\" BASIS,\n# # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# # See the License for the specific language governing permissions and\n# # limitations under the License.\n# \n# from __future__ import annotations\n# \n# from vizier import pyvizier\n# from vizier._src.pyvizier.multimetric import safety\n# from absl.testing import absltest\n# \n# --------------------------------------------------\n# the below code fragment can be found in:\n# vizier/_src/benchmarks/experimenters/noisy_experimenter_test.py\n# --------------------------------------------------\n# # Licensed under the Apache License, Version 2.0 (the \"License\");\n# # you may not use this file except in compliance with the License.\n# # You may obtain a copy of the License at\n# #\n# #     http://www.apache.org/licenses/LICENSE-2.0\n# #\n# # Unless required by applicable law or agreed to in writing, software\n# # distributed under the License is distributed on an \"AS IS\" BASIS,\n# # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# # See the License for the specific language governing permissions and\n# # limitations under the License.\n# \n# from __future__ import annotations\n# \n# \"\"\"Tests for noisy_experimenter.\"\"\"\n# \n# from vizier import pyvizier\n# from vizier._src.benchmarks.experimenters import noisy_experimenter\n# from vizier._src.benchmarks.experimenters import numpy_experimenter\n# from vizier._src.benchmarks.experimenters.synthetic import bbob\n# --------------------------------------------------\n# the below code fragment can be found in:\n# vizier/_src/benchmarks/experimenters/discretizing_experimenter_test.py\n# --------------------------------------------------\n# # Licensed under the Apache License, Version 2.0 (the \"License\");\n# # you may not use this file except in compliance with the License.\n# # You may obtain a copy of the License at\n# #\n# #     http://www.apache.org/licenses/LICENSE-2.0\n# #\n# # Unless required by applicable law or agreed to in writing, software\n# # distributed under the License is distributed on an \"AS IS\" BASIS,\n# # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# # See the License for the specific language governing permissions and\n# # limitations under the License.\n# \n# from __future__ import annotations\n# \n# import numpy as np\n# from vizier import pyvizier\n# from vizier._src.benchmarks.experimenters import discretizing_experimenter\n# from vizier._src.benchmarks.experimenters import numpy_experimenter\n# from vizier._src.benchmarks.experimenters.synthetic import bbob\n# \n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\n# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import annotations\n\nfrom vizier import pyvizier\nfrom vizier._src.benchmarks.experimenters import normalizing_experimenter", "metadata": {"task_id": "google_vizier/32", "ground_truth": "from vizier._src.benchmarks.experimenters import numpy_experimenter", "fpath_tuple": ["google_vizier", "vizier", "_src", "benchmarks", "experimenters", "normalizing_experimenter_test.py"], "context_start_lineno": 0, "line_no": 18, "query_window": {"context": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import annotations\n\nfrom vizier import pyvizier\nfrom vizier._src.benchmarks.experimenters import normalizing_experimenter", "metadata": {"fpath_tuple": ["google_vizier", "vizier", "_src", "benchmarks", "experimenters", "normalizing_experimenter_test.py"], "line_no": 18, "task_id": "google_vizier/32", "start_line_no": 0, "end_line_no": 18, "window_size": 20, "context_start_lineno": 0, "repo": "google_vizier"}}, "top_k_context": [{"context": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import annotations\n\nimport numpy as np\nfrom vizier import pyvizier\nfrom vizier._src.benchmarks.experimenters import discretizing_experimenter\nfrom vizier._src.benchmarks.experimenters import numpy_experimenter", "metadata": [{"fpath_tuple": ["google_vizier", "vizier", "_src", "benchmarks", "experimenters", "discretizing_experimenter_test.py"], "line_no": 10, "start_line_no": 0, "end_line_no": 20, "window_size": 20, "repo": "google_vizier", "slice_size": 10}], "sim_score": 0.9344262295081968}, {"context": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import annotations\n\n\"\"\"Tests for noisy_experimenter.\"\"\"\n\nfrom vizier import pyvizier\nfrom vizier._src.benchmarks.experimenters import noisy_experimenter", "metadata": [{"fpath_tuple": ["google_vizier", "vizier", "_src", "benchmarks", "experimenters", "noisy_experimenter_test.py"], "line_no": 10, "start_line_no": 0, "end_line_no": 20, "window_size": 20, "repo": "google_vizier", "slice_size": 10}], "sim_score": 0.9338842975206612}, {"context": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import annotations\n\nfrom vizier import pyvizier\nfrom vizier._src.pyvizier.multimetric import safety", "metadata": [{"fpath_tuple": ["google_vizier", "vizier", "_src", "pyvizier", "multimetric", "safety_test.py"], "line_no": 8, "start_line_no": 0, "end_line_no": 18, "window_size": 20, "repo": "google_vizier", "slice_size": 10}], "sim_score": 0.8833333333333333}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# tests/test_evaluator.py\n# --------------------------------------------------\n# \n#     def test_class_init(self):\n#         evaluator = Text2TextGenerationEvaluator()\n#         self.assertEqual(evaluator.task, \"text2text-generation\")\n#         self.assertIsNone(evaluator.default_metric_name)\n# \n#         results = evaluator.compute(\n#             model_or_pipeline=self.pipe,\n#             data=self.data,\n#             metric=\"bleu\",\n#         )\n#         self.assertEqual(results[\"bleu\"], 0)\n# \n#     @slow\n#     def test_default_pipe_init(self):\n#         results = self.evaluator.compute(data=self.data)\n#         self.assertEqual(results[\"bleu\"], 0)\n# \n#     def test_overwrite_default_metric(self):\n#         rouge = load(\"rouge\")\n# --------------------------------------------------\n# the below code fragment can be found in:\n# tests/test_evaluator.py\n# --------------------------------------------------\n#     def test_pipe_init(self):\n#         results = self.evaluator.compute(\n#             model_or_pipeline=self.pipe,\n#             data=self.data,\n#         )\n#         self.assertEqual(results[\"bleu\"], 0)\n# \n#     def test_class_init(self):\n#         evaluator = Text2TextGenerationEvaluator()\n#         self.assertEqual(evaluator.task, \"text2text-generation\")\n#         self.assertIsNone(evaluator.default_metric_name)\n# \n#         results = evaluator.compute(\n#             model_or_pipeline=self.pipe,\n#             data=self.data,\n#             metric=\"bleu\",\n#         )\n#         self.assertEqual(results[\"bleu\"], 0)\n# \n#     @slow\n# --------------------------------------------------\n# the below code fragment can be found in:\n# tests/test_evaluator.py\n# --------------------------------------------------\n#         evaluator = Text2TextGenerationEvaluator()\n#         self.assertEqual(evaluator.task, \"text2text-generation\")\n#         self.assertIsNone(evaluator.default_metric_name)\n# \n#         results = evaluator.compute(\n#             model_or_pipeline=self.pipe,\n#             data=self.data,\n#             metric=\"bleu\",\n#         )\n#         self.assertEqual(results[\"bleu\"], 0)\n# \n#     @slow\n#     def test_default_pipe_init(self):\n#         results = self.evaluator.compute(data=self.data)\n#         self.assertEqual(results[\"bleu\"], 0)\n# \n#     def test_overwrite_default_metric(self):\n#         rouge = load(\"rouge\")\n#         results = self.evaluator.compute(\n#             model_or_pipeline=self.pipe,\n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\n# Copyright 2022 The HuggingFace Datasets Authors and the TensorFlow Datasets Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\ntry:\n    from transformers.pipelines import SUPPORTED_TASKS as SUPPORTED_PIPELINE_TASKS\n    from transformers.pipelines import TASK_ALIASES\n    from transformers.pipelines import check_task as check_pipeline_task\n\n    TRANSFORMERS_AVAILABLE = True\nexcept ImportError:\n    TRANSFORMERS_AVAILABLE = False\n\nfrom typing import Dict, List\n\nfrom .automatic_speech_recognition import AutomaticSpeechRecognitionEvaluator\nfrom .base import Evaluator\nfrom .image_classification import ImageClassificationEvaluator\nfrom .question_answering import QuestionAnsweringEvaluator\nfrom .text2text_generation import SummarizationEvaluator, Text2TextGenerationEvaluator, TranslationEvaluator\nfrom .text_classification import TextClassificationEvaluator\nfrom .text_generation import TextGenerationEvaluator\nfrom .token_classification import TokenClassificationEvaluator\n\n\nSUPPORTED_EVALUATOR_TASKS = {\n    \"text-classification\": {\n        \"implementation\": TextClassificationEvaluator,\n        \"default_metric_name\": \"accuracy\",\n    },\n    \"image-classification\": {\n        \"implementation\": ImageClassificationEvaluator,\n        \"default_metric_name\": \"accuracy\",\n    },\n    \"question-answering\": {\n        \"implementation\": QuestionAnsweringEvaluator,\n        \"default_metric_name\": \"squad\",\n    },\n    \"token-classification\": {\n        \"implementation\": TokenClassificationEvaluator,\n        \"default_metric_name\": \"seqeval\",\n    },\n    \"text-generation\": {\n        \"implementation\": TextGenerationEvaluator,\n        \"default_metric_name\": \"word_count\",\n    },\n    \"text2text-generation\": {\n        \"implementation\": Text2TextGenerationEvaluator,\n        \"default_metric_name\": \"bleu\",\n    },\n    \"summarization\": {\n        \"implementation\": SummarizationEvaluator,\n        \"default_metric_name\": \"rouge\",\n    },\n    \"translation\": {\n        \"implementation\": TranslationEvaluator,\n        \"default_metric_name\": \"bleu\",", "metadata": {"task_id": "huggingface_evaluate/151", "ground_truth": "    },", "fpath_tuple": ["huggingface_evaluate", "src", "evaluate", "evaluator", "__init__.py"], "context_start_lineno": 0, "line_no": 68, "query_window": {"context": "    },\n    \"token-classification\": {\n        \"implementation\": TokenClassificationEvaluator,\n        \"default_metric_name\": \"seqeval\",\n    },\n    \"text-generation\": {\n        \"implementation\": TextGenerationEvaluator,\n        \"default_metric_name\": \"word_count\",\n    },\n    \"text2text-generation\": {\n        \"implementation\": Text2TextGenerationEvaluator,\n        \"default_metric_name\": \"bleu\",\n    },\n    \"summarization\": {\n        \"implementation\": SummarizationEvaluator,\n        \"default_metric_name\": \"rouge\",\n    },\n    \"translation\": {\n        \"implementation\": TranslationEvaluator,\n        \"default_metric_name\": \"bleu\",", "metadata": {"fpath_tuple": ["huggingface_evaluate", "src", "evaluate", "evaluator", "__init__.py"], "line_no": 68, "task_id": "huggingface_evaluate/151", "start_line_no": 48, "end_line_no": 68, "window_size": 20, "context_start_lineno": 0, "repo": "huggingface_evaluate"}}, "top_k_context": [{"context": "\n    def test_class_init(self):\n        evaluator = Text2TextGenerationEvaluator()\n        self.assertEqual(evaluator.task, \"text2text-generation\")\n        self.assertIsNone(evaluator.default_metric_name)\n\n        results = evaluator.compute(\n            model_or_pipeline=self.pipe,\n            data=self.data,\n            metric=\"bleu\",\n        )\n        self.assertEqual(results[\"bleu\"], 0)\n\n    @slow\n    def test_default_pipe_init(self):\n        results = self.evaluator.compute(data=self.data)\n        self.assertEqual(results[\"bleu\"], 0)\n\n    def test_overwrite_default_metric(self):\n        rouge = load(\"rouge\")", "metadata": [{"fpath_tuple": ["huggingface_evaluate", "tests", "test_evaluator.py"], "line_no": 928, "start_line_no": 918, "end_line_no": 938, "window_size": 20, "repo": "huggingface_evaluate", "slice_size": 10}], "sim_score": 0.30434782608695654}, {"context": "        self.evaluator = evaluator(\"text2text-generation\")\n\n    def test_pipe_init(self):\n        results = self.evaluator.compute(\n            model_or_pipeline=self.pipe,\n            data=self.data,\n        )\n        self.assertEqual(results[\"bleu\"], 0)\n\n    def test_class_init(self):\n        evaluator = Text2TextGenerationEvaluator()\n        self.assertEqual(evaluator.task, \"text2text-generation\")\n        self.assertIsNone(evaluator.default_metric_name)\n\n        results = evaluator.compute(\n            model_or_pipeline=self.pipe,\n            data=self.data,\n            metric=\"bleu\",\n        )\n        self.assertEqual(results[\"bleu\"], 0)", "metadata": [{"fpath_tuple": ["huggingface_evaluate", "tests", "test_evaluator.py"], "line_no": 920, "start_line_no": 910, "end_line_no": 930, "window_size": 20, "repo": "huggingface_evaluate", "slice_size": 10}], "sim_score": 0.3023255813953488}, {"context": "        )\n        self.assertEqual(results[\"bleu\"], 0)\n\n    def test_class_init(self):\n        evaluator = Text2TextGenerationEvaluator()\n        self.assertEqual(evaluator.task, \"text2text-generation\")\n        self.assertIsNone(evaluator.default_metric_name)\n\n        results = evaluator.compute(\n            model_or_pipeline=self.pipe,\n            data=self.data,\n            metric=\"bleu\",\n        )\n        self.assertEqual(results[\"bleu\"], 0)\n\n    @slow\n    def test_default_pipe_init(self):\n        results = self.evaluator.compute(data=self.data)\n        self.assertEqual(results[\"bleu\"], 0)\n", "metadata": [{"fpath_tuple": ["huggingface_evaluate", "tests", "test_evaluator.py"], "line_no": 926, "start_line_no": 916, "end_line_no": 936, "window_size": 20, "repo": "huggingface_evaluate", "slice_size": 10}], "sim_score": 0.2988505747126437}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# examples/research_projects/intel_opts/textual_inversion/textual_inversion_bf16.py\n# --------------------------------------------------\n#         default=\"no\",\n#         choices=[\"no\", \"fp16\", \"bf16\"],\n#         help=(\n#             \"Whether to use mixed precision. Choose\"\n#             \"between fp16 and bf16 (bfloat16). Bf16 requires PyTorch >= 1.10.\"\n#             \"and an Nvidia Ampere GPU.\"\n#         ),\n#     )\n#     parser.add_argument(\"--local_rank\", type=int, default=-1, help=\"For distributed training: local_rank\")\n# \n#     args = parser.parse_args()\n#     env_local_rank = int(os.environ.get(\"LOCAL_RANK\", -1))\n#     if env_local_rank != -1 and env_local_rank != args.local_rank:\n#         args.local_rank = env_local_rank\n# \n#     if args.train_data_dir is None:\n#         raise ValueError(\"You must specify a train data directory.\")\n# \n#     return args\n# \n# --------------------------------------------------\n# the below code fragment can be found in:\n# examples/research_projects/intel_opts/textual_inversion/textual_inversion_bf16.py\n# --------------------------------------------------\n#         help=(\n#             \"Whether to use mixed precision. Choose\"\n#             \"between fp16 and bf16 (bfloat16). Bf16 requires PyTorch >= 1.10.\"\n#             \"and an Nvidia Ampere GPU.\"\n#         ),\n#     )\n#     parser.add_argument(\"--local_rank\", type=int, default=-1, help=\"For distributed training: local_rank\")\n# \n#     args = parser.parse_args()\n#     env_local_rank = int(os.environ.get(\"LOCAL_RANK\", -1))\n#     if env_local_rank != -1 and env_local_rank != args.local_rank:\n#         args.local_rank = env_local_rank\n# \n#     if args.train_data_dir is None:\n#         raise ValueError(\"You must specify a train data directory.\")\n# \n#     return args\n# \n# \n# imagenet_templates_small = [\n# --------------------------------------------------\n# the below code fragment can be found in:\n# examples/research_projects/intel_opts/textual_inversion/textual_inversion_bf16.py\n# --------------------------------------------------\n#             \"between fp16 and bf16 (bfloat16). Bf16 requires PyTorch >= 1.10.\"\n#             \"and an Nvidia Ampere GPU.\"\n#         ),\n#     )\n#     parser.add_argument(\"--local_rank\", type=int, default=-1, help=\"For distributed training: local_rank\")\n# \n#     args = parser.parse_args()\n#     env_local_rank = int(os.environ.get(\"LOCAL_RANK\", -1))\n#     if env_local_rank != -1 and env_local_rank != args.local_rank:\n#         args.local_rank = env_local_rank\n# \n#     if args.train_data_dir is None:\n#         raise ValueError(\"You must specify a train data directory.\")\n# \n#     return args\n# \n# \n# imagenet_templates_small = [\n#     \"a photo of a {}\",\n#     \"a rendering of a {}\",\n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\n(\"0.13.0.dev0\")\n\nlogger = logging.getLogger(__name__)\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=\"Simple example of a training script.\")\n    parser.add_argument(\n        \"--pretrained_model_name_or_path\",\n        type=str,\n        default=None,\n        required=True,\n        help=\"Path to pretrained model or model identifier from huggingface.co/models.\",\n    )\n    parser.add_argument(\n        \"--dataset_name\",\n        type=str,\n        default=None,\n        help=(\n            \"The name of the Dataset (from the HuggingFace hub) to train on (could be your own, possibly private,\"\n            \" dataset). It can also be a path pointing to a local copy of a dataset in your filesystem,\"\n            \" or to a folder containing files that \ud83e\udd17 Datasets can understand.\"\n        ),\n    )\n    parser.add_argument(\n        \"--dataset_config_name\",\n        type=str,\n        default=None,\n        help=\"The config of the Dataset, leave as None if there's only one config.\",\n    )\n    parser.add_argument(\n        \"--train_data_dir\",\n        type=str,\n        default=None,\n        help=(\n            \"A folder containing the training data. Folder contents must follow the structure described in\"\n            \" https://huggingface.co/docs/datasets/image_dataset#imagefolder. In particular, a `metadata.jsonl` file\"\n            \" must exist to provide the captions for the images. Ignored if `dataset_name` is specified.\"\n        ),\n    )\n    parser.add_argument(\n        \"--image_column\", type=str, default=\"image\", help=\"The column of the dataset containing an image.\"\n    )\n    parser.add_argument(\n        \"--caption_column\",\n        type=str,\n        default=\"text\",\n        help=\"The column of the dataset containing a caption or a list of captions.\",\n    )\n    parser.add_argument(\n        \"--max_train_samples\",\n        type=int,\n        default=None,\n        help=(\n            \"For debugging purposes or quicker training, truncate the number of training examples to this \"\n            \"value if set.\"\n        ),\n    )\n    parser.add_argument(\n        \"--output_dir\",\n        type=str,\n        default=\"sd-model-finetuned\",\n        help=\"The output directory where the model predictions and checkpoints will be written.\",\n    )\n    parser.add_argument(\n        \"--cache_dir\",\n        type=str,\n        default=None,\n        help=\"The directory where the downloaded models and datasets will be stored.\",\n    )\n    parser.add_argument(\"--seed\", type=int, default=0, help=\"A seed for reproducible training.\")\n    parser.add_argument(\n        \"--resolution\",\n        type=int,\n        default=512,\n        help=(\n            \"The resolution for input images, all the images in the train/validation dataset will be resized to this\"\n            \" resolution\"\n        ),\n    )\n    parser.add_argument(\n        \"--center_crop\",\n        default=False,\n        action=\"store_true\",\n        help=(\n            \"Whether to center crop the input images to the resolution. If not set, the images will be randomly\"\n            \" cropped. The images will be resized to the resolution first before cropping.\"\n        ),\n    )\n    parser.add_argument(\n        \"--random_flip\",\n        action=\"store_true\",\n        help=\"whether to randomly flip images horizontally\",\n    )\n    parser.add_argument(\n        \"--train_batch_size\", type=int, default=16, help=\"Batch size (per device) for the training dataloader.\"\n    )\n    parser.add_argument(\"--num_train_epochs\", type=int, default=100)\n    parser.add_argument(\n        \"--max_train_steps\",\n        type=int,\n        default=None,\n        help=\"Total number of training steps to perform.  If provided, overrides num_train_epochs.\",\n    )\n    parser.add_argument(\n        \"--learning_rate\",\n        type=float,\n        default=1e-4,\n        help=\"Initial learning rate (after the potential warmup period) to use.\",\n    )\n    parser.add_argument(\n        \"--scale_lr\",\n        action=\"store_true\",\n        default=False,\n        help=\"Scale the learning rate by the number of GPUs, gradient accumulation steps, and batch size.\",\n    )\n    parser.add_argument(\n        \"--lr_scheduler\",\n        type=str,\n        default=\"constant\",\n        help=(\n            'The scheduler type to use. Choose between [\"linear\", \"cosine\", \"cosine_with_restarts\", \"polynomial\",'\n            ' \"constant\", \"constant_with_warmup\"]'\n        ),\n    )\n    parser.add_argument(\"--adam_beta1\", type=float, default=0.9, help=\"The beta1 parameter for the Adam optimizer.\")\n    parser.add_argument(\"--adam_beta2\", type=float, default=0.999, help=\"The beta2 parameter for the Adam optimizer.\")\n    parser.add_argument(\"--adam_weight_decay\", type=float, default=1e-2, help=\"Weight decay to use.\")\n    parser.add_argument(\"--adam_epsilon\", type=float, default=1e-08, help=\"Epsilon value for the Adam optimizer\")\n    parser.add_argument(\"--max_grad_norm\", default=1.0, type=float, help=\"Max gradient norm.\")\n    parser.add_argument(\"--push_to_hub\", action=\"store_true\", help=\"Whether or not to push the model to the Hub.\")\n    parser.add_argument(\"--hub_token\", type=str, default=None, help=\"The token to use to push to the Model Hub.\")\n    parser.add_argument(\n        \"--hub_model_id\",\n        type=str,\n        default=None,\n        help=\"The name of the repository to keep in sync with the local `output_dir`.\",\n    )\n    parser.add_argument(\n        \"--logging_dir\",\n        type=str,\n        default=\"logs\",\n        help=(\n            \"[TensorBoard](https://www.tensorflow.org/tensorboard) log directory. Will default to\"\n            \" *output_dir/runs/**CURRENT_DATETIME_HOSTNAME***.\"\n        ),\n    )\n    parser.add_argument(\n        \"--report_to\",\n        type=str,\n        default=\"tensorboard\",\n        help=(\n            'The integration to report the results and logs to. Supported platforms are `\"tensorboard\"`'\n            ' (default), `\"wandb\"` and `\"comet_ml\"`. Use `\"all\"` to report to all integrations.'\n        ),\n    )\n    parser.add_argument(\n        \"--mixed_precision\",\n        type=str,\n        default=\"no\",\n        choices=[\"no\", \"fp16\", \"bf16\"],\n        help=(\n            \"Whether to use mixed precision. Choose\"\n            \"between fp16 and bf16 (bfloat16). Bf16 requires PyTorch >= 1.10.\"\n            \"and an Nvidia Ampere GPU.\"\n        ),\n    )\n    parser.add_argument(\"--local_rank\", type=int, default=-1, help=\"For distributed training: local_rank\")\n\n    args = parser.parse_args()\n    env_local_rank = int(os.environ.get(\"LOCAL_RANK\", -1))\n    if env_local_rank != -1 and env_local_rank != args.local_rank:\n        args.local_rank = env_local_rank\n\n    # Sanity checks\n    if args.dataset_name is None and args.train_data_dir is None:\n        raise ValueError(\"Need either a dataset name or a training folder.\")\n\n    return args\n\n\ndef get_full_repo_name(model_id: str, organization: Optional[str] = None, token: Optional[str] = None):", "metadata": {"task_id": "huggingface_diffusers/95", "ground_truth": "    if token is None:", "fpath_tuple": ["huggingface_diffusers", "examples", "text_to_image", "train_text_to_image_flax.py"], "context_start_lineno": 36, "line_no": 218, "query_window": {"context": "            \"Whether to use mixed precision. Choose\"\n            \"between fp16 and bf16 (bfloat16). Bf16 requires PyTorch >= 1.10.\"\n            \"and an Nvidia Ampere GPU.\"\n        ),\n    )\n    parser.add_argument(\"--local_rank\", type=int, default=-1, help=\"For distributed training: local_rank\")\n\n    args = parser.parse_args()\n    env_local_rank = int(os.environ.get(\"LOCAL_RANK\", -1))\n    if env_local_rank != -1 and env_local_rank != args.local_rank:\n        args.local_rank = env_local_rank\n\n    # Sanity checks\n    if args.dataset_name is None and args.train_data_dir is None:\n        raise ValueError(\"Need either a dataset name or a training folder.\")\n\n    return args\n\n\ndef get_full_repo_name(model_id: str, organization: Optional[str] = None, token: Optional[str] = None):", "metadata": {"fpath_tuple": ["huggingface_diffusers", "examples", "text_to_image", "train_text_to_image_flax.py"], "line_no": 218, "task_id": "huggingface_diffusers/95", "start_line_no": 198, "end_line_no": 218, "window_size": 20, "context_start_lineno": 36, "repo": "huggingface_diffusers"}}, "top_k_context": [{"context": "        help=(\n            \"Whether to use mixed precision. Choose\"\n            \"between fp16 and bf16 (bfloat16). Bf16 requires PyTorch >= 1.10.\"\n            \"and an Nvidia Ampere GPU.\"\n        ),\n    )\n    parser.add_argument(\"--local_rank\", type=int, default=-1, help=\"For distributed training: local_rank\")\n\n    args = parser.parse_args()\n    env_local_rank = int(os.environ.get(\"LOCAL_RANK\", -1))\n    if env_local_rank != -1 and env_local_rank != args.local_rank:\n        args.local_rank = env_local_rank\n\n    if args.train_data_dir is None:\n        raise ValueError(\"You must specify a train data directory.\")\n\n    return args\n\n\nimagenet_templates_small = [", "metadata": [{"fpath_tuple": ["huggingface_diffusers", "examples", "research_projects", "intel_opts", "textual_inversion", "textual_inversion_bf16.py"], "line_no": 210, "start_line_no": 200, "end_line_no": 220, "window_size": 20, "repo": "huggingface_diffusers", "slice_size": 10}], "sim_score": 0.7122302158273381}, {"context": "        default=\"no\",\n        choices=[\"no\", \"fp16\", \"bf16\"],\n        help=(\n            \"Whether to use mixed precision. Choose\"\n            \"between fp16 and bf16 (bfloat16). Bf16 requires PyTorch >= 1.10.\"\n            \"and an Nvidia Ampere GPU.\"\n        ),\n    )\n    parser.add_argument(\"--local_rank\", type=int, default=-1, help=\"For distributed training: local_rank\")\n\n    args = parser.parse_args()\n    env_local_rank = int(os.environ.get(\"LOCAL_RANK\", -1))\n    if env_local_rank != -1 and env_local_rank != args.local_rank:\n        args.local_rank = env_local_rank\n\n    if args.train_data_dir is None:\n        raise ValueError(\"You must specify a train data directory.\")\n\n    return args\n", "metadata": [{"fpath_tuple": ["huggingface_diffusers", "examples", "research_projects", "intel_opts", "textual_inversion", "textual_inversion_bf16.py"], "line_no": 208, "start_line_no": 198, "end_line_no": 218, "window_size": 20, "repo": "huggingface_diffusers", "slice_size": 10}], "sim_score": 0.7050359712230215}, {"context": "        \"--mixed_precision\",\n        type=str,\n        default=\"no\",\n        choices=[\"no\", \"fp16\", \"bf16\"],\n        help=(\n            \"Whether to use mixed precision. Choose\"\n            \"between fp16 and bf16 (bfloat16). Bf16 requires PyTorch >= 1.10.\"\n            \"and an Nvidia Ampere GPU.\"\n        ),\n    )\n    parser.add_argument(\"--local_rank\", type=int, default=-1, help=\"For distributed training: local_rank\")\n\n    args = parser.parse_args()\n    env_local_rank = int(os.environ.get(\"LOCAL_RANK\", -1))\n    if env_local_rank != -1 and env_local_rank != args.local_rank:\n        args.local_rank = env_local_rank\n\n    if args.train_data_dir is None:\n        raise ValueError(\"You must specify a train data directory.\")\n", "metadata": [{"fpath_tuple": ["huggingface_diffusers", "examples", "research_projects", "intel_opts", "textual_inversion", "textual_inversion_bf16.py"], "line_no": 206, "start_line_no": 196, "end_line_no": 216, "window_size": 20, "repo": "huggingface_diffusers", "slice_size": 10}], "sim_score": 0.6853146853146853}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# benchmark/pFL-Bench/res_analysis_plot/render_paper_res.py\n# --------------------------------------------------\n#     # {dataset_name: filter}\n#     [\n#         (\"FEMNIST-s02\", {\n#             \"$and\": [\n#                 {\n#                     \"config.data.type\": \"femnist\"\n#                 },\n#                 {\n#                     \"config.federate.sample_client_rate\": 0.2\n#                 },\n#                 {\n#                     \"state\": \"finished\"\n#                 },\n#             ]\n#         }),\n#         (\"FEMNIST-s01\", {\n#             \"$and\": [\n#                 {\n#                     \"config.data.type\": \"femnist\"\n#                 },\n# --------------------------------------------------\n# the below code fragment can be found in:\n# benchmark/pFL-Bench/res_analysis_plot/repeat_best_exp.py\n# benchmark/pFL-Bench/res_analysis_plot/wandb_to_latex_res.py\n# --------------------------------------------------\n#     [\n#         (\"FEMNIST-s02\", {\n#             \"$and\": [\n#                 {\n#                     \"config.data.type\": \"femnist\"\n#                 },\n#                 {\n#                     \"config.federate.sample_client_rate\": 0.2\n#                 },\n#                 {\n#                     \"state\": \"finished\"\n#                 },\n#             ]\n#         }),\n#         (\"FEMNIST-s01\", {\n#             \"$and\": [\n#                 {\n#                     \"config.data.type\": \"femnist\"\n#                 },\n#                 {\n# --------------------------------------------------\n# the below code fragment can be found in:\n# benchmark/pFL-Bench/res_analysis_plot/repeat_best_exp.py\n# benchmark/pFL-Bench/res_analysis_plot/wandb_to_latex_res.py\n# --------------------------------------------------\n#             \"$and\": [\n#                 {\n#                     \"config.data.type\": \"femnist\"\n#                 },\n#                 {\n#                     \"config.federate.sample_client_rate\": 0.2\n#                 },\n#                 {\n#                     \"state\": \"finished\"\n#                 },\n#             ]\n#         }),\n#         (\"FEMNIST-s01\", {\n#             \"$and\": [\n#                 {\n#                     \"config.data.type\": \"femnist\"\n#                 },\n#                 {\n#                     \"config.federate.sample_client_rate\": 0.1\n#                 },\n# --------------------------------------------------\n# the below code fragment can be found in:\n# benchmark/pFL-Bench/res_analysis_plot/render_paper_res.py\n# --------------------------------------------------\n#         (\"FEMNIST-s02\", {\n#             \"$and\": [\n#                 {\n#                     \"config.data.type\": \"femnist\"\n#                 },\n#                 {\n#                     \"config.federate.sample_client_rate\": 0.2\n#                 },\n#                 {\n#                     \"state\": \"finished\"\n#                 },\n#             ]\n#         }),\n#         (\"FEMNIST-s01\", {\n#             \"$and\": [\n#                 {\n#                     \"config.data.type\": \"femnist\"\n#                 },\n#                 {\n#                     \"config.federate.sample_client_rate\": 0.1\n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\nimport copy\nimport json\nimport os\n\nimport wandb\nfrom collections import OrderedDict\n\nimport yaml\n\napi = wandb.Api()\n\nname_project = \"daoyuan/pFL-bench\"\n\nfilters_each_line_main_table = OrderedDict(\n    # {dataset_name: filter}\n    [\n        # (\"all\",\n        # None,\n        # ),\n        # (\"FEMNIST-all\",\n        #  {\"$and\":\n        #      [\n        #          {\"config.data.type\": \"femnist\"},\n        #      ]\n        #  }\n        #  ),\n        (\"FEMNIST-s02\", {\n            \"$and\": [\n                {\n                    \"config.data.type\": \"femnist\"\n                },\n                {\n                    \"config.federate.sample_client_rate\": 0.2\n                },\n                {\n                    \"state\": \"finished\"\n                },\n            ]\n        }),\n        # (\"cifar10-alpha05\",\n        #  {\"$and\":\n        #      [\n        #          {\"config.data.type\": \"CIFAR10@torchvision\"},\n        #          {\"config.data.splitter_args\": [{\"alpha\": 0.5}]},\n        #      ]\n        #  }\n        #  ),\n        (\"sst2\", {\n            \"$and\": [\n                {\n                    \"config.data.type\": \"sst2@huggingface_datasets\"\n                },\n            ]\n        }),\n        (\"pubmed\", {\n            \"$and\": [\n                {\n                    \"config.data.type\": \"pubmed\"\n                },\n            ]\n        }),\n    ])\n\nfilters_each_line_all_cifar10 = OrderedDict(\n    # {dataset_name: filter}\n    [\n        (\"cifar10-alpha5\", {\n            \"$and\": [\n                {\n                    \"config.data.type\": \"CIFAR10@torchvision\"\n                },\n                {\n                    \"config.data.splitter_args\": [{\n                        \"alpha\": 5\n                    }]\n                },\n            ]\n        }),\n        (\"cifar10-alpha05\", {\n            \"$and\": [\n                {\n                    \"config.data.type\": \"CIFAR10@torchvision\"\n                },\n                {\n                    \"config.data.splitter_args\": [{\n                        \"alpha\": 0.5\n                    }]\n                },\n            ]\n        }),\n        (\"cifar10-alpha01\", {\n            \"$and\": [\n                {\n                    \"config.data.type\": \"CIFAR10@torchvision\"\n                },\n                {\n                    \"config.data.splitter_args\": [{\n                        \"alpha\": 0.1\n                    }]\n                },\n            ]\n        }),\n    ])\n\nfilters_each_line_femnist_all_s = OrderedDict(\n    # {dataset_name: filter}\n    [\n        (\"FEMNIST-s02\", {\n            \"$and\": [\n                {\n                    \"config.data.type\": \"femnist\"\n                },\n                {\n                    \"config.federate.sample_client_rate\": 0.2\n                },\n                {\n                    \"state\": \"finished\"\n                },\n            ]\n        }),\n        (\"FEMNIST-s01\", {\n            \"$and\": [\n                {\n                    \"config.data.type\": \"femnist\"\n                },", "metadata": {"task_id": "alibaba_FederatedScope/61", "ground_truth": "                {", "fpath_tuple": ["alibaba_FederatedScope", "benchmark", "pFL-Bench", "res_analysis_plot", "repeat_best_exp.py"], "context_start_lineno": 0, "line_no": 125, "query_window": {"context": "    # {dataset_name: filter}\n    [\n        (\"FEMNIST-s02\", {\n            \"$and\": [\n                {\n                    \"config.data.type\": \"femnist\"\n                },\n                {\n                    \"config.federate.sample_client_rate\": 0.2\n                },\n                {\n                    \"state\": \"finished\"\n                },\n            ]\n        }),\n        (\"FEMNIST-s01\", {\n            \"$and\": [\n                {\n                    \"config.data.type\": \"femnist\"\n                },", "metadata": {"fpath_tuple": ["alibaba_FederatedScope", "benchmark", "pFL-Bench", "res_analysis_plot", "repeat_best_exp.py"], "line_no": 125, "task_id": "alibaba_FederatedScope/61", "start_line_no": 105, "end_line_no": 125, "window_size": 20, "context_start_lineno": 0, "repo": "alibaba_FederatedScope"}}, "top_k_context": [{"context": "    # {dataset_name: filter}\n    [\n        (\"FEMNIST-s02\", {\n            \"$and\": [\n                {\n                    \"config.data.type\": \"femnist\"\n                },\n                {\n                    \"config.federate.sample_client_rate\": 0.2\n                },\n                {\n                    \"state\": \"finished\"\n                },\n            ]\n        }),\n        (\"FEMNIST-s01\", {\n            \"$and\": [\n                {\n                    \"config.data.type\": \"femnist\"\n                },", "metadata": [{"fpath_tuple": ["alibaba_FederatedScope", "benchmark", "pFL-Bench", "res_analysis_plot", "render_paper_res.py"], "line_no": 116, "start_line_no": 106, "end_line_no": 126, "window_size": 20, "repo": "alibaba_FederatedScope", "slice_size": 10}], "sim_score": 1.0}, {"context": "    [\n        (\"FEMNIST-s02\", {\n            \"$and\": [\n                {\n                    \"config.data.type\": \"femnist\"\n                },\n                {\n                    \"config.federate.sample_client_rate\": 0.2\n                },\n                {\n                    \"state\": \"finished\"\n                },\n            ]\n        }),\n        (\"FEMNIST-s01\", {\n            \"$and\": [\n                {\n                    \"config.data.type\": \"femnist\"\n                },\n                {", "metadata": [{"fpath_tuple": ["alibaba_FederatedScope", "benchmark", "pFL-Bench", "res_analysis_plot", "repeat_best_exp.py"], "line_no": 116, "start_line_no": 106, "end_line_no": 126, "window_size": 20, "repo": "alibaba_FederatedScope", "slice_size": 10}, {"fpath_tuple": ["alibaba_FederatedScope", "benchmark", "pFL-Bench", "res_analysis_plot", "wandb_to_latex_res.py"], "line_no": 118, "start_line_no": 108, "end_line_no": 128, "window_size": 20, "repo": "alibaba_FederatedScope", "slice_size": 10}], "sim_score": 0.8461538461538461}, {"context": "filters_each_line_femnist_all_s = OrderedDict(\n    # {dataset_name: filter}\n    [\n        (\"FEMNIST-s02\", {\n            \"$and\": [\n                {\n                    \"config.data.type\": \"femnist\"\n                },\n                {\n                    \"config.federate.sample_client_rate\": 0.2\n                },\n                {\n                    \"state\": \"finished\"\n                },\n            ]\n        }),\n        (\"FEMNIST-s01\", {\n            \"$and\": [\n                {\n                    \"config.data.type\": \"femnist\"", "metadata": [{"fpath_tuple": ["alibaba_FederatedScope", "benchmark", "pFL-Bench", "res_analysis_plot", "repeat_best_exp.py"], "line_no": 114, "start_line_no": 104, "end_line_no": 124, "window_size": 20, "repo": "alibaba_FederatedScope", "slice_size": 10}, {"fpath_tuple": ["alibaba_FederatedScope", "benchmark", "pFL-Bench", "res_analysis_plot", "wandb_to_latex_res.py"], "line_no": 116, "start_line_no": 106, "end_line_no": 126, "window_size": 20, "repo": "alibaba_FederatedScope", "slice_size": 10}], "sim_score": 0.8253968253968254}, {"context": "\nfilters_each_line_femnist_all_s = OrderedDict(\n    # {dataset_name: filter}\n    [\n        (\"FEMNIST-s02\", {\n            \"$and\": [\n                {\n                    \"config.data.type\": \"femnist\"\n                },\n                {\n                    \"config.federate.sample_client_rate\": 0.2\n                },\n                {\n                    \"state\": \"finished\"\n                },\n            ]\n        }),\n        (\"FEMNIST-s01\", {\n            \"$and\": [\n                {", "metadata": [{"fpath_tuple": ["alibaba_FederatedScope", "benchmark", "pFL-Bench", "res_analysis_plot", "render_paper_res.py"], "line_no": 114, "start_line_no": 104, "end_line_no": 124, "window_size": 20, "repo": "alibaba_FederatedScope", "slice_size": 10}], "sim_score": 0.8253968253968254}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# federatedscope/core/trainers/trainer_Ditto.py\n# --------------------------------------------------\n# import copy\n# import logging\n# \n# import torch\n# \n# from federatedscope.core.auxiliaries.optimizer_builder import get_optimizer\n# from federatedscope.core.trainers.torch_trainer import GeneralTorchTrainer\n# from federatedscope.core.optimizer import wrap_regularized_optimizer\n# from federatedscope.core.trainers.utils import calculate_batch_epoch_num\n# from typing import Type\n# \n# logger = logging.getLogger(__name__)\n# --------------------------------------------------\n# the below code fragment can be found in:\n# federatedscope/attack/trainer/benign_trainer.py\n# --------------------------------------------------\n# import logging\n# from typing import Type\n# import numpy as np\n# \n# from federatedscope.core.trainers import GeneralTorchTrainer\n# \n# logger = logging.getLogger(__name__)\n# \n# \n# def wrap_benignTrainer(\n#         base_trainer: Type[GeneralTorchTrainer]) -> Type[GeneralTorchTrainer]:\n#     '''\n# --------------------------------------------------\n# the below code fragment can be found in:\n# federatedscope/attack/trainer/GAN_trainer.py\n# --------------------------------------------------\n# import logging\n# from typing import Type\n# \n# from federatedscope.core.trainers import GeneralTorchTrainer\n# from federatedscope.attack.privacy_attacks.GAN_based_attack import GANCRA\n# \n# logger = logging.getLogger(__name__)\n# \n# \n# def wrap_GANTrainer(\n#         base_trainer: Type[GeneralTorchTrainer]) -> Type[GeneralTorchTrainer]:\n#     '''\n# --------------------------------------------------\n# the below code fragment can be found in:\n# federatedscope/attack/trainer/MIA_invert_gradient_trainer.py\n# --------------------------------------------------\n# import logging\n# from typing import Type\n# \n# import torch\n# \n# from federatedscope.core.trainers import GeneralTorchTrainer\n# from federatedscope.core.data.wrap_dataset import WrapDataset\n# from federatedscope.attack.auxiliary.MIA_get_target_data import get_target_data\n# \n# logger = logging.getLogger(__name__)\n# \n# \n# --------------------------------------------------\n# the below code fragment can be found in:\n# federatedscope/attack/trainer/MIA_invert_gradient_trainer.py\n# --------------------------------------------------\n# import logging\n# from typing import Type\n# \n# import torch\n# \n# from federatedscope.core.trainers import GeneralTorchTrainer\n# from federatedscope.core.data.wrap_dataset import WrapDataset\n# from federatedscope.attack.auxiliary.MIA_get_target_data import get_target_data\n# \n# logger = logging.getLogger(__name__)\n# \n# \n# def wrap_GradientAscentTrainer(\n#         base_trainer: Type[GeneralTorchTrainer]) -> Type[GeneralTorchTrainer]:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# federatedscope/core/trainers/trainer_nbafl.py\n# --------------------------------------------------\n# from federatedscope.core.trainers.utils import get_random\n# from federatedscope.core.trainers.torch_trainer import GeneralTorchTrainer\n# from typing import Type\n# from copy import deepcopy\n# \n# import numpy as np\n# import torch\n# \n# \n# def wrap_nbafl_trainer(\n#         base_trainer: Type[GeneralTorchTrainer]) -> Type[GeneralTorchTrainer]:\n#     \"\"\"Implementation of NbAFL refer to `Federated Learning with\n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\nfrom typing import Type\n\nfrom federatedscope.core.trainers import GeneralTorchTrainer\nfrom federatedscope.attack.auxiliary.utils import get_data_property\n\n\ndef wrap_ActivePIATrainer(", "metadata": {"task_id": "alibaba_FederatedScope/28", "ground_truth": "        base_trainer: Type[GeneralTorchTrainer]) -> Type[GeneralTorchTrainer]:", "fpath_tuple": ["alibaba_FederatedScope", "federatedscope", "attack", "trainer", "PIA_trainer.py"], "context_start_lineno": 0, "line_no": 7, "query_window": {"context": "from typing import Type\n\nfrom federatedscope.core.trainers import GeneralTorchTrainer\nfrom federatedscope.attack.auxiliary.utils import get_data_property\n\n\ndef wrap_ActivePIATrainer(", "metadata": {"fpath_tuple": ["alibaba_FederatedScope", "federatedscope", "attack", "trainer", "PIA_trainer.py"], "line_no": 7, "task_id": "alibaba_FederatedScope/28", "start_line_no": 0, "end_line_no": 7, "window_size": 20, "context_start_lineno": 0, "repo": "alibaba_FederatedScope"}}, "top_k_context": [{"context": "from federatedscope.core.trainers.utils import get_random\nfrom federatedscope.core.trainers.torch_trainer import GeneralTorchTrainer\nfrom typing import Type\nfrom copy import deepcopy\n\nimport numpy as np\nimport torch\n\n\ndef wrap_nbafl_trainer(", "metadata": [{"fpath_tuple": ["alibaba_FederatedScope", "federatedscope", "core", "trainers", "trainer_nbafl.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 10, "window_size": 20, "repo": "alibaba_FederatedScope", "slice_size": 10}], "sim_score": 0.4897959183673469}, {"context": "import logging\nfrom typing import Type\n\nimport torch\n\nfrom federatedscope.core.trainers import GeneralTorchTrainer\nfrom federatedscope.core.data.wrap_dataset import WrapDataset\nfrom federatedscope.attack.auxiliary.MIA_get_target_data import get_target_data\n\nlogger = logging.getLogger(__name__)\n\n", "metadata": [{"fpath_tuple": ["alibaba_FederatedScope", "federatedscope", "attack", "trainer", "MIA_invert_gradient_trainer.py"], "line_no": 2, "start_line_no": 0, "end_line_no": 12, "window_size": 20, "repo": "alibaba_FederatedScope", "slice_size": 10}], "sim_score": 0.46296296296296297}, {"context": "import logging\nfrom typing import Type\n\nimport torch\n\nfrom federatedscope.core.trainers import GeneralTorchTrainer\nfrom federatedscope.core.data.wrap_dataset import WrapDataset\nfrom federatedscope.attack.auxiliary.MIA_get_target_data import get_target_data\n\nlogger = logging.getLogger(__name__)", "metadata": [{"fpath_tuple": ["alibaba_FederatedScope", "federatedscope", "attack", "trainer", "MIA_invert_gradient_trainer.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 10, "window_size": 20, "repo": "alibaba_FederatedScope", "slice_size": 10}], "sim_score": 0.4444444444444444}, {"context": "import logging\nfrom typing import Type\n\nfrom federatedscope.core.trainers import GeneralTorchTrainer\nfrom federatedscope.attack.privacy_attacks.GAN_based_attack import GANCRA\n\nlogger = logging.getLogger(__name__)\n\n\ndef wrap_GANTrainer(", "metadata": [{"fpath_tuple": ["alibaba_FederatedScope", "federatedscope", "attack", "trainer", "GAN_trainer.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 10, "window_size": 20, "repo": "alibaba_FederatedScope", "slice_size": 10}], "sim_score": 0.4423076923076923}, {"context": "import logging\nfrom typing import Type\nimport numpy as np\n\nfrom federatedscope.core.trainers import GeneralTorchTrainer\n\nlogger = logging.getLogger(__name__)\n\n\ndef wrap_benignTrainer(", "metadata": [{"fpath_tuple": ["alibaba_FederatedScope", "federatedscope", "attack", "trainer", "benign_trainer.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 10, "window_size": 20, "repo": "alibaba_FederatedScope", "slice_size": 10}], "sim_score": 0.44}, {"context": "import copy\nimport logging\n\nimport torch\n\nfrom federatedscope.core.auxiliaries.optimizer_builder import get_optimizer\nfrom federatedscope.core.trainers.torch_trainer import GeneralTorchTrainer\nfrom federatedscope.core.optimizer import wrap_regularized_optimizer\nfrom federatedscope.core.trainers.utils import calculate_batch_epoch_num\nfrom typing import Type", "metadata": [{"fpath_tuple": ["alibaba_FederatedScope", "federatedscope", "core", "trainers", "trainer_Ditto.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 10, "window_size": 20, "repo": "alibaba_FederatedScope", "slice_size": 10}], "sim_score": 0.4230769230769231}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# test/test_helpers.py\n# --------------------------------------------------\n# from torchrl.trainers.helpers.envs import (\n#     EnvConfig,\n#     initialize_observation_norm_transforms,\n#     retrieve_observation_norms_state_dict,\n# )\n# from torchrl.trainers.helpers.losses import A2CLossConfig, make_a2c_loss\n# from torchrl.trainers.helpers.models import (\n#     A2CModelConfig,\n#     DDPGModelConfig,\n#     DiscreteModelConfig,\n#     DreamerConfig,\n#     make_a2c_model,\n#     make_ddpg_actor,\n#     make_dqn_actor,\n#     make_dreamer,\n#     make_ppo_model,\n#     make_redq_model,\n#     make_sac_model,\n#     PPOModelConfig,\n#     REDQModelConfig,\n# --------------------------------------------------\n# the below code fragment can be found in:\n# test/test_helpers.py\n# --------------------------------------------------\n#     initialize_observation_norm_transforms,\n#     retrieve_observation_norms_state_dict,\n# )\n# from torchrl.trainers.helpers.losses import A2CLossConfig, make_a2c_loss\n# from torchrl.trainers.helpers.models import (\n#     A2CModelConfig,\n#     DDPGModelConfig,\n#     DiscreteModelConfig,\n#     DreamerConfig,\n#     make_a2c_model,\n#     make_ddpg_actor,\n#     make_dqn_actor,\n#     make_dreamer,\n#     make_ppo_model,\n#     make_redq_model,\n#     make_sac_model,\n#     PPOModelConfig,\n#     REDQModelConfig,\n#     SACModelConfig,\n# )\n# --------------------------------------------------\n# the below code fragment can be found in:\n# test/test_helpers.py\n# --------------------------------------------------\n# )\n# from torchrl.trainers.helpers.losses import A2CLossConfig, make_a2c_loss\n# from torchrl.trainers.helpers.models import (\n#     A2CModelConfig,\n#     DDPGModelConfig,\n#     DiscreteModelConfig,\n#     DreamerConfig,\n#     make_a2c_model,\n#     make_ddpg_actor,\n#     make_dqn_actor,\n#     make_dreamer,\n#     make_ppo_model,\n#     make_redq_model,\n#     make_sac_model,\n#     PPOModelConfig,\n#     REDQModelConfig,\n#     SACModelConfig,\n# )\n# \n# TORCH_VERSION = version.parse(torch.__version__)\n# --------------------------------------------------\n# the below code fragment can be found in:\n# test/test_helpers.py\n# --------------------------------------------------\n# from torchrl.trainers.helpers.models import (\n#     A2CModelConfig,\n#     DDPGModelConfig,\n#     DiscreteModelConfig,\n#     DreamerConfig,\n#     make_a2c_model,\n#     make_ddpg_actor,\n#     make_dqn_actor,\n#     make_dreamer,\n#     make_ppo_model,\n#     make_redq_model,\n#     make_sac_model,\n#     PPOModelConfig,\n#     REDQModelConfig,\n#     SACModelConfig,\n# )\n# \n# TORCH_VERSION = version.parse(torch.__version__)\n# if TORCH_VERSION < version.parse(\"1.12.0\"):\n#     UNSQUEEZE_SINGLETON = True\n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\n# Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom .collectors import (\n    make_collector_offpolicy,\n    make_collector_onpolicy,\n    sync_async_collector,\n    sync_sync_collector,\n)\nfrom .envs import (\n    correct_for_frame_skip,\n    get_stats_random_rollout,\n    parallel_env_constructor,\n    transformed_env_constructor,\n)\nfrom .logger import LoggerConfig\nfrom .losses import (\n    make_a2c_loss,\n    make_ddpg_loss,\n    make_dqn_loss,\n    make_ppo_loss,\n    make_redq_loss,\n    make_sac_loss,\n    make_target_updater,\n)\nfrom .models import (\n    make_a2c_model,\n    make_ddpg_actor,\n    make_dqn_actor,\n    make_dreamer,\n    make_ppo_model,\n    make_redq_model,\n    make_sac_model,\n)", "metadata": {"task_id": "pytorch_rl/7", "ground_truth": "from .replay_buffer import make_replay_buffer", "fpath_tuple": ["pytorch_rl", "torchrl", "trainers", "helpers", "__init__.py"], "context_start_lineno": 0, "line_no": 36, "query_window": {"context": ")\nfrom .logger import LoggerConfig\nfrom .losses import (\n    make_a2c_loss,\n    make_ddpg_loss,\n    make_dqn_loss,\n    make_ppo_loss,\n    make_redq_loss,\n    make_sac_loss,\n    make_target_updater,\n)\nfrom .models import (\n    make_a2c_model,\n    make_ddpg_actor,\n    make_dqn_actor,\n    make_dreamer,\n    make_ppo_model,\n    make_redq_model,\n    make_sac_model,\n)", "metadata": {"fpath_tuple": ["pytorch_rl", "torchrl", "trainers", "helpers", "__init__.py"], "line_no": 36, "task_id": "pytorch_rl/7", "start_line_no": 16, "end_line_no": 36, "window_size": 20, "context_start_lineno": 0, "repo": "pytorch_rl"}}, "top_k_context": [{"context": ")\nfrom torchrl.trainers.helpers.losses import A2CLossConfig, make_a2c_loss\nfrom torchrl.trainers.helpers.models import (\n    A2CModelConfig,\n    DDPGModelConfig,\n    DiscreteModelConfig,\n    DreamerConfig,\n    make_a2c_model,\n    make_ddpg_actor,\n    make_dqn_actor,\n    make_dreamer,\n    make_ppo_model,\n    make_redq_model,\n    make_sac_model,\n    PPOModelConfig,\n    REDQModelConfig,\n    SACModelConfig,\n)\n\nTORCH_VERSION = version.parse(torch.__version__)", "metadata": [{"fpath_tuple": ["pytorch_rl", "test", "test_helpers.py"], "line_no": 56, "start_line_no": 46, "end_line_no": 66, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}], "sim_score": 0.3888888888888889}, {"context": "    initialize_observation_norm_transforms,\n    retrieve_observation_norms_state_dict,\n)\nfrom torchrl.trainers.helpers.losses import A2CLossConfig, make_a2c_loss\nfrom torchrl.trainers.helpers.models import (\n    A2CModelConfig,\n    DDPGModelConfig,\n    DiscreteModelConfig,\n    DreamerConfig,\n    make_a2c_model,\n    make_ddpg_actor,\n    make_dqn_actor,\n    make_dreamer,\n    make_ppo_model,\n    make_redq_model,\n    make_sac_model,\n    PPOModelConfig,\n    REDQModelConfig,\n    SACModelConfig,\n)", "metadata": [{"fpath_tuple": ["pytorch_rl", "test", "test_helpers.py"], "line_no": 54, "start_line_no": 44, "end_line_no": 64, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}], "sim_score": 0.3888888888888889}, {"context": "from torchrl.trainers.helpers.envs import (\n    EnvConfig,\n    initialize_observation_norm_transforms,\n    retrieve_observation_norms_state_dict,\n)\nfrom torchrl.trainers.helpers.losses import A2CLossConfig, make_a2c_loss\nfrom torchrl.trainers.helpers.models import (\n    A2CModelConfig,\n    DDPGModelConfig,\n    DiscreteModelConfig,\n    DreamerConfig,\n    make_a2c_model,\n    make_ddpg_actor,\n    make_dqn_actor,\n    make_dreamer,\n    make_ppo_model,\n    make_redq_model,\n    make_sac_model,\n    PPOModelConfig,\n    REDQModelConfig,", "metadata": [{"fpath_tuple": ["pytorch_rl", "test", "test_helpers.py"], "line_no": 52, "start_line_no": 42, "end_line_no": 62, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}], "sim_score": 0.3783783783783784}, {"context": "from torchrl.modules.tensordict_module.common import _has_functorch\nfrom torchrl.trainers.helpers import transformed_env_constructor\nfrom torchrl.trainers.helpers.envs import (\n    EnvConfig,\n    initialize_observation_norm_transforms,\n    retrieve_observation_norms_state_dict,\n)\nfrom torchrl.trainers.helpers.losses import A2CLossConfig, make_a2c_loss\nfrom torchrl.trainers.helpers.models import (\n    A2CModelConfig,\n    DDPGModelConfig,\n    DiscreteModelConfig,\n    DreamerConfig,\n    make_a2c_model,\n    make_ddpg_actor,\n    make_dqn_actor,\n    make_dreamer,\n    make_ppo_model,\n    make_redq_model,\n    make_sac_model,", "metadata": [{"fpath_tuple": ["pytorch_rl", "test", "test_helpers.py"], "line_no": 50, "start_line_no": 40, "end_line_no": 60, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}], "sim_score": 0.3333333333333333}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# ding/policy/cql.py\n# ding/policy/sac.py\n# --------------------------------------------------\n#                 self._alpha_optim.zero_grad()\n#                 loss_dict['alpha_loss'].backward()\n#                 self._alpha_optim.step()\n#                 self._alpha = max(0, self._alpha)\n# \n#         loss_dict['total_loss'] = sum(loss_dict.values())\n# \n#         info_dict = {}\n#         if self._value_network:\n#             info_dict['cur_lr_v'] = self._optimizer_value.defaults['lr']\n# \n#         # =============\n#         # after update\n#         # =============\n#         self._forward_learn_cnt += 1\n#         # target update\n#         self._target_model.update(self._learn_model.state_dict())\n#         return {\n#             'cur_lr_q': self._optimizer_q.defaults['lr'],\n#             'cur_lr_p': self._optimizer_policy.defaults['lr'],\n# --------------------------------------------------\n# the below code fragment can be found in:\n# ding/policy/masac.py\n# --------------------------------------------------\n#                 self._alpha_optim.step()\n#                 self._alpha = max(0, self._alpha)\n#                 self._alpha.data = torch.where(self._alpha > 0, self._alpha, torch.zeros_like(self._alpha)).requires_grad_()\n# \n#         loss_dict['total_loss'] = sum(loss_dict.values())\n# \n#         info_dict = {}\n#         # if self._value_network:\n#         #    info_dict['cur_lr_v'] = self._optimizer_value.defaults['lr']\n# \n#         # =============\n#         # after update\n#         # =============\n#         self._forward_learn_cnt += 1\n#         # target update\n#         self._target_model.update(self._learn_model.state_dict())\n#         return {\n#             'cur_lr_q': self._optimizer_q.defaults['lr'],\n#             'cur_lr_p': self._optimizer_policy.defaults['lr'],\n#             'priority': td_error_per_sample.abs().tolist(),\n# --------------------------------------------------\n# the below code fragment can be found in:\n# ding/policy/cql.py\n# ding/policy/sac.py\n# --------------------------------------------------\n#                 self._alpha_optim.step()\n#                 self._alpha = max(0, self._alpha)\n# \n#         loss_dict['total_loss'] = sum(loss_dict.values())\n# \n#         info_dict = {}\n#         if self._value_network:\n#             info_dict['cur_lr_v'] = self._optimizer_value.defaults['lr']\n# \n#         # =============\n#         # after update\n#         # =============\n#         self._forward_learn_cnt += 1\n#         # target update\n#         self._target_model.update(self._learn_model.state_dict())\n#         return {\n#             'cur_lr_q': self._optimizer_q.defaults['lr'],\n#             'cur_lr_p': self._optimizer_policy.defaults['lr'],\n#             'priority': td_error_per_sample.abs().tolist(),\n#             'td_error': td_error_per_sample.detach().mean().item(),\n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\n.shape[1])\n        act_repeat = data['action'].unsqueeze(1).repeat(1, self._num_actions, 1).view(\n            data['action'].shape[0] * self._num_actions, data['action'].shape[1]\n        )\n        q_pred = self._get_q_value({'obs': obs_repeat, 'action': act_repeat})\n        q_rand = self._get_q_value({'obs': obs_repeat, 'action': random_actions_tensor})\n        # q2_rand = self._get_q_value(obs, random_actions_tensor, network=self.qf2)\n        q_curr_actions = self._get_q_value({'obs': obs_repeat, 'action': curr_actions_tensor})\n        # q2_curr_actions = self._get_tensor_values(obs, curr_actions_tensor, network=self.qf2)\n        q_next_actions = self._get_q_value({'obs': obs_repeat, 'action': new_curr_actions_tensor})\n        # q2_next_actions = self._get_tensor_values(obs, new_curr_actions_tensor, network=self.qf2)\n\n        cat_q1 = torch.stack([q_rand[0], q_pred[0], q_next_actions[0], q_curr_actions[0]], 1)\n        cat_q2 = torch.stack([q_rand[1], q_pred[1], q_next_actions[1], q_curr_actions[1]], 1)\n        std_q1 = torch.std(cat_q1, dim=1)\n        std_q2 = torch.std(cat_q2, dim=1)\n        if self._min_q_version == 3:\n            # importance sammpled version\n            random_density = np.log(0.5 ** curr_actions_tensor.shape[-1])\n            cat_q1 = torch.stack(\n                [\n                    q_rand[0] - random_density, q_next_actions[0] - new_log_pis.detach(),\n                    q_curr_actions[0] - curr_log_pis.detach()\n                ], 1\n            )\n            cat_q2 = torch.stack(\n                [\n                    q_rand[1] - random_density, q_next_actions[1] - new_log_pis.detach(),\n                    q_curr_actions[1] - curr_log_pis.detach()\n                ], 1\n            )\n\n        min_qf1_loss = torch.logsumexp(cat_q1, dim=1).mean() * self._min_q_weight\n        min_qf2_loss = torch.logsumexp(cat_q2, dim=1).mean() * self._min_q_weight\n        \"\"\"Subtract the log likelihood of data\"\"\"\n        min_qf1_loss = min_qf1_loss - q_pred[0].mean() * self._min_q_weight\n        min_qf2_loss = min_qf2_loss - q_pred[1].mean() * self._min_q_weight\n\n        if self._with_lagrange:\n            alpha_prime = torch.clamp(self.log_alpha_prime.exp(), min=0.0, max=1000000.0)\n            min_qf1_loss = alpha_prime * (min_qf1_loss - self.target_action_gap)\n            min_qf2_loss = alpha_prime * (min_qf2_loss - self.target_action_gap)\n\n            self.alpha_prime_optimizer.zero_grad()\n            alpha_prime_loss = (-min_qf1_loss - min_qf2_loss) * 0.5\n            alpha_prime_loss.backward(retain_graph=True)\n            self.alpha_prime_optimizer.step()\n\n        loss_dict['critic_loss'] += min_qf1_loss\n        if self._twin_critic:\n            loss_dict['twin_critic_loss'] += min_qf2_loss\n\n        # update q network\n        self._optimizer_q.zero_grad()\n        loss_dict['critic_loss'].backward(retain_graph=True)\n        if self._twin_critic:\n            loss_dict['twin_critic_loss'].backward()\n        self._optimizer_q.step()\n\n        # evaluate to get action distribution\n        (mu, sigma) = self._learn_model.forward(data['obs'], mode='compute_actor')['logit']\n        dist = Independent(Normal(mu, sigma), 1)\n        pred = dist.rsample()\n        action = torch.tanh(pred)\n        y = 1 - action.pow(2) + 1e-6\n        log_prob = dist.log_prob(pred).unsqueeze(-1)\n        log_prob = log_prob - torch.log(y).sum(-1, keepdim=True)\n\n        eval_data = {'obs': obs, 'action': action}\n        new_q_value = self._learn_model.forward(eval_data, mode='compute_critic')['q_value']\n        if self._twin_critic:\n            new_q_value = torch.min(new_q_value[0], new_q_value[1])\n\n        # =================\n        # value network\n        # =================\n        # compute value loss\n        if self._value_network:\n            # new_q_value: (bs, ), log_prob: (bs, act_shape) -> target_v_value: (bs, )\n            target_v_value = (new_q_value.unsqueeze(-1) - self._alpha * log_prob).mean(dim=-1)\n            loss_dict['value_loss'] = F.mse_loss(v_value, target_v_value.detach())\n\n            # update value network\n            self._optimizer_value.zero_grad()\n            loss_dict['value_loss'].backward()\n            self._optimizer_value.step()\n\n        # =================\n        # policy network\n        # =================\n        # compute policy loss\n        policy_loss = (self._alpha * log_prob - new_q_value.unsqueeze(-1)).mean()\n\n        loss_dict['policy_loss'] = policy_loss\n\n        # update policy network\n        self._optimizer_policy.zero_grad()\n        loss_dict['policy_loss'].backward()\n        self._optimizer_policy.step()\n\n        # compute alpha loss\n        if self._auto_alpha:\n            if self._log_space:\n                log_prob = log_prob + self._target_entropy\n                loss_dict['alpha_loss'] = -(self._log_alpha * log_prob.detach()).mean()\n\n                self._alpha_optim.zero_grad()\n                loss_dict['alpha_loss'].backward()\n                self._alpha_optim.step()\n                self._alpha = self._log_alpha.detach().exp()\n            else:\n                log_prob = log_prob + self._target_entropy\n                loss_dict['alpha_loss'] = -(self._alpha * log_prob.detach()).mean()\n\n                self._alpha_optim.zero_grad()\n                loss_dict['alpha_loss'].backward()\n                self._alpha_optim.step()\n                self._alpha = max(0, self._alpha)\n\n        loss_dict['total_loss'] = sum(loss_dict.values())\n\n        info_dict = {}\n        if self._value_network:\n            info_dict['cur_lr_v'] = self._optimizer_value.defaults['lr']\n\n        # =============\n        # after update\n        # =============\n        self._forward_learn_cnt += 1\n        # target update\n        self._target_model.update(self._learn_model.state_dict())\n        return {\n            'cur_lr_q': self._optimizer_q.defaults['lr'],\n            'cur_lr_p': self._optimizer_policy.defaults['lr'],", "metadata": {"task_id": "opendilab_ACE/154", "ground_truth": "            'priority': td_error_per_sample.abs().tolist(),", "fpath_tuple": ["opendilab_ACE", "ding", "policy", "cql.py"], "context_start_lineno": 372, "line_no": 506, "query_window": {"context": "                self._alpha_optim.zero_grad()\n                loss_dict['alpha_loss'].backward()\n                self._alpha_optim.step()\n                self._alpha = max(0, self._alpha)\n\n        loss_dict['total_loss'] = sum(loss_dict.values())\n\n        info_dict = {}\n        if self._value_network:\n            info_dict['cur_lr_v'] = self._optimizer_value.defaults['lr']\n\n        # =============\n        # after update\n        # =============\n        self._forward_learn_cnt += 1\n        # target update\n        self._target_model.update(self._learn_model.state_dict())\n        return {\n            'cur_lr_q': self._optimizer_q.defaults['lr'],\n            'cur_lr_p': self._optimizer_policy.defaults['lr'],", "metadata": {"fpath_tuple": ["opendilab_ACE", "ding", "policy", "cql.py"], "line_no": 506, "task_id": "opendilab_ACE/154", "start_line_no": 486, "end_line_no": 506, "window_size": 20, "context_start_lineno": 372, "repo": "opendilab_ACE"}}, "top_k_context": [{"context": "                self._alpha_optim.zero_grad()\n                loss_dict['alpha_loss'].backward()\n                self._alpha_optim.step()\n                self._alpha = max(0, self._alpha)\n\n        loss_dict['total_loss'] = sum(loss_dict.values())\n\n        info_dict = {}\n        if self._value_network:\n            info_dict['cur_lr_v'] = self._optimizer_value.defaults['lr']\n\n        # =============\n        # after update\n        # =============\n        self._forward_learn_cnt += 1\n        # target update\n        self._target_model.update(self._learn_model.state_dict())\n        return {\n            'cur_lr_q': self._optimizer_q.defaults['lr'],\n            'cur_lr_p': self._optimizer_policy.defaults['lr'],", "metadata": [{"fpath_tuple": ["opendilab_ACE", "ding", "policy", "cql.py"], "line_no": 496, "start_line_no": 486, "end_line_no": 506, "window_size": 20, "repo": "opendilab_ACE", "slice_size": 10}, {"fpath_tuple": ["opendilab_ACE", "ding", "policy", "sac.py"], "line_no": 410, "start_line_no": 400, "end_line_no": 420, "window_size": 20, "repo": "opendilab_ACE", "slice_size": 10}], "sim_score": 1.0}, {"context": "                self._alpha_optim.zero_grad()\n                loss_dict['alpha_loss'].backward()\n                self._alpha_optim.step()\n                self._alpha = max(0, self._alpha)\n                self._alpha.data = torch.where(self._alpha > 0, self._alpha, torch.zeros_like(self._alpha)).requires_grad_()\n\n        loss_dict['total_loss'] = sum(loss_dict.values())\n\n        info_dict = {}\n        # if self._value_network:\n        #    info_dict['cur_lr_v'] = self._optimizer_value.defaults['lr']\n\n        # =============\n        # after update\n        # =============\n        self._forward_learn_cnt += 1\n        # target update\n        self._target_model.update(self._learn_model.state_dict())\n        return {\n            'cur_lr_q': self._optimizer_q.defaults['lr'],", "metadata": [{"fpath_tuple": ["opendilab_ACE", "ding", "policy", "masac.py"], "line_no": 374, "start_line_no": 364, "end_line_no": 384, "window_size": 20, "repo": "opendilab_ACE", "slice_size": 10}], "sim_score": 0.8395061728395061}, {"context": "                loss_dict['alpha_loss'] = -(self._alpha * log_prob.detach()).mean()\n\n                self._alpha_optim.zero_grad()\n                loss_dict['alpha_loss'].backward()\n                self._alpha_optim.step()\n                self._alpha = max(0, self._alpha)\n\n        loss_dict['total_loss'] = sum(loss_dict.values())\n\n        info_dict = {}\n        if self._value_network:\n            info_dict['cur_lr_v'] = self._optimizer_value.defaults['lr']\n\n        # =============\n        # after update\n        # =============\n        self._forward_learn_cnt += 1\n        # target update\n        self._target_model.update(self._learn_model.state_dict())\n        return {", "metadata": [{"fpath_tuple": ["opendilab_ACE", "ding", "policy", "cql.py"], "line_no": 494, "start_line_no": 484, "end_line_no": 504, "window_size": 20, "repo": "opendilab_ACE", "slice_size": 10}, {"fpath_tuple": ["opendilab_ACE", "ding", "policy", "sac.py"], "line_no": 408, "start_line_no": 398, "end_line_no": 418, "window_size": 20, "repo": "opendilab_ACE", "slice_size": 10}], "sim_score": 0.810126582278481}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# tutorials/sphinx-tutorials/torchrl_demo.py\n# --------------------------------------------------\n#     TransformedEnv,\n# )\n# \n# base_env = GymEnv(\"Pendulum-v1\", frame_skip=3, from_pixels=True, pixels_only=False)\n# env = TransformedEnv(base_env, Compose(NoopResetEnv(3), ToTensorImage()))\n# env.append_transform(ObservationNorm(in_keys=[\"pixels\"], loc=2, scale=1))\n# \n# ###############################################################################\n# \n# env.reset()\n# \n# ###############################################################################\n# \n# print(\"env: \", env)\n# print(\"last transform parent: \", env.transform[2].parent)\n# \n# ###############################################################################\n# # Vectorized Environments\n# # ------------------------------\n# \n# --------------------------------------------------\n# the below code fragment can be found in:\n# tutorials/sphinx-tutorials/torchrl_demo.py\n# --------------------------------------------------\n# \n# base_env = GymEnv(\"Pendulum-v1\", frame_skip=3, from_pixels=True, pixels_only=False)\n# env = TransformedEnv(base_env, Compose(NoopResetEnv(3), ToTensorImage()))\n# env.append_transform(ObservationNorm(in_keys=[\"pixels\"], loc=2, scale=1))\n# \n# ###############################################################################\n# \n# env.reset()\n# \n# ###############################################################################\n# \n# print(\"env: \", env)\n# print(\"last transform parent: \", env.transform[2].parent)\n# \n# ###############################################################################\n# # Vectorized Environments\n# # ------------------------------\n# \n# from torchrl.envs import ParallelEnv\n# \n# --------------------------------------------------\n# the below code fragment can be found in:\n# tutorials/sphinx-tutorials/torchrl_demo.py\n# --------------------------------------------------\n# \n# ###############################################################################\n# \n# env.reset()\n# \n# ###############################################################################\n# \n# print(\"env: \", env)\n# print(\"last transform parent: \", env.transform[2].parent)\n# \n# ###############################################################################\n# # Vectorized Environments\n# # ------------------------------\n# \n# from torchrl.envs import ParallelEnv\n# \n# base_env = ParallelEnv(\n#     4,\n#     lambda: GymEnv(\"Pendulum-v1\", frame_skip=3, from_pixels=True, pixels_only=False),\n# )\n# --------------------------------------------------\n# the below code fragment can be found in:\n# tutorials/sphinx-tutorials/torchrl_demo.py\n# --------------------------------------------------\n#     StepCounter,\n#     ToTensorImage,\n#     TransformedEnv,\n# )\n# \n# base_env = GymEnv(\"Pendulum-v1\", frame_skip=3, from_pixels=True, pixels_only=False)\n# env = TransformedEnv(base_env, Compose(NoopResetEnv(3), ToTensorImage()))\n# env.append_transform(ObservationNorm(in_keys=[\"pixels\"], loc=2, scale=1))\n# \n# ###############################################################################\n# \n# env.reset()\n# \n# ###############################################################################\n# \n# print(\"env: \", env)\n# print(\"last transform parent: \", env.transform[2].parent)\n# \n# ###############################################################################\n# # Vectorized Environments\n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\n# -*- coding: utf-8 -*-\n\"\"\"\nCoding a pixel-based DQN using TorchRL\n=======================================\n\"\"\"\n\n##############################################################################\n# This tutorial will guide you through the steps to code DQN to solve the\n# CartPole task from scratch. DQN\n# (`Deep Q-Learning <https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf>`_) was\n# the founding work in deep reinforcement learning. On a high level, the\n# algorithm is quite simple: Q-learning consists in learning a table of\n# state-action values in such a way that, when facing any particular state,\n# we know which action to pick just by searching for the action with the\n# highest value. This simple setting requires the actions and states to be\n# discretizable. DQN uses a neural network that maps state-actions pairs to\n# a certain value, which amortizes the cost of storing and exploring all the\n# possible states: if a state has not been seen in the past, we can still pass\n# it through our neural network and get an interpolated value for each of the\n# actions available.\n#\n# In this tutorial, you will learn:\n#\n# - how to build an environment in TorchRL, including transforms (e.g. data\n#   normalization, frame concatenation, resizing and turning to grayscale)\n#   and parallel execution;\n# - how to design a QValue actor, i.e. an actor that esitmates the action\n#   values and picks up the action with the highest estimated return;\n# - how to collect data from your environment efficiently and store them\n#   in a replay buffer;\n# - how to store trajectories (and not transitions) in your replay buffer),\n#   and how to estimate returns using TD(lambda);\n# - how to make a module functional and use ;\n# - and finally how to evaluate your model.\n#\n# This tutorial assumes the reader is familiar with some of TorchRL\n# primitives, such as ``TensorDict`` and ``TensorDictModules``, although it\n# should be sufficiently transparent to be understood without a deep\n# understanding of these classes.\n#\n# We do not aim at giving a SOTA implementation of the algorithm, but rather\n# to provide a high-level illustration of TorchRL features in the context\n# of this algorithm.\n\n# sphinx_gallery_start_ignore\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n# sphinx_gallery_end_ignore\n\nimport torch\nimport tqdm\nfrom functorch import vmap\nfrom IPython import display\nfrom matplotlib import pyplot as plt\nfrom tensordict import TensorDict\nfrom tensordict.nn import get_functional\nfrom torch import nn\nfrom torchrl.collectors import MultiaSyncDataCollector\nfrom torchrl.data import LazyMemmapStorage, TensorDictReplayBuffer\nfrom torchrl.envs import EnvCreator, ParallelEnv\nfrom torchrl.envs.libs.gym import GymEnv\nfrom torchrl.envs.transforms import (\n    CatFrames,\n    CatTensors,\n    Compose,\n    GrayScale,\n    ObservationNorm,\n    Resize,\n    ToTensorImage,\n    TransformedEnv,\n)\nfrom torchrl.envs.utils import set_exploration_mode, step_mdp\nfrom torchrl.modules import DuelingCnnDQNet, EGreedyWrapper, QValueActor\n\n\ndef is_notebook() -> bool:\n    try:\n        shell = get_ipython().__class__.__name__\n        if shell == \"ZMQInteractiveShell\":\n            return True  # Jupyter notebook or qtconsole\n        elif shell == \"TerminalInteractiveShell\":\n            return False  # Terminal running IPython\n        else:\n            return False  # Other type (?)\n    except NameError:\n        return False  # Probably standard Python interpreter\n\n\n###############################################################################\n# Hyperparameters\n# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n# Let's start with our hyperparameters. This is a totally arbitrary list of\n# hyperparams that we found to work well in practice. Hopefully the performance\n# of the algorithm should not be too sentitive to slight variations of these.\n\n# hyperparams\n\n# the learning rate of the optimizer\nlr = 2e-3\n# the beta parameters of Adam\nbetas = (0.9, 0.999)\n# gamma decay factor\ngamma = 0.99\n# lambda decay factor (see second the part with TD(lambda)\nlmbda = 0.95\n# total frames collected in the environment. In other implementations, the user defines a maximum number of episodes.\n# This is harder to do with our data collectors since they return batches of N collected frames, where N is a constant.\n# However, one can easily get the same restriction on number of episodes by breaking the training loop when a certain number\n# episodes has been collected.\ntotal_frames = 500\n# Random frames used to initialize the replay buffer.\ninit_random_frames = 100\n# Frames in each batch collected.\nframes_per_batch = 32\n# Optimization steps per batch collected\nn_optim = 4\n# Frames sampled from the replay buffer at each optimization step\nbatch_size = 32\n# Size of the replay buffer in terms of frames\nbuffer_size = min(total_frames, 100000)\n# Number of environments run in parallel in each data collector\nn_workers = 1\n\ndevice = \"cuda:0\" if torch.cuda.device_count() > 0 else \"cpu\"\n\n# Smooth target network update decay parameter. This loosely corresponds to a 1/(1-tau) interval with hard target network update\ntau = 0.005\n\n# Initial and final value of the epsilon factor in Epsilon-greedy exploration (notice that since our policy is deterministic exploration is crucial)\neps_greedy_val = 0.1\neps_greedy_val_env = 0.05\n\n# To speed up learning, we set the bias of the last layer of our value network to a predefined value\ninit_bias = 20.0\n\n###############################################################################\n# **Note**: for fast rendering of the tutorial ``total_frames`` hyperparameter\n# was set to a very low number. To get a reasonable performance, use a greater\n# value e.g. 500000\n#\n# Building the environment\n# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n# Our environment builder has three arguments:\n#\n# - parallel: determines whether multiple environments have to be run in\n#   parallel. We stack the transforms after the ParallelEnv to take advantage\n#   of vectorization of the operations on device, although this would\n#   techinally work with every single environment attached to its own set of\n#   transforms.\n# - mean and standard deviation: we normalize the observations (images)\n#   with two parameters computed from a random rollout in the environment.\n\n\ndef make_env(parallel=False, m=0, s=1):\n\n    if parallel:\n        base_env = ParallelEnv(\n            n_workers,\n            EnvCreator(\n                lambda: GymEnv(\n                    \"CartPole-v1\", from_pixels=True, pixels_only=True, device=device\n                )\n            ),\n        )\n    else:\n        base_env = GymEnv(\n            \"CartPole-v1\", from_pixels=True, pixels_only=True, device=device\n        )\n\n    env = TransformedEnv(\n        base_env,\n        Compose(\n            ToTensorImage(),\n            GrayScale(),\n            Resize(64, 64),\n            ObservationNorm(in_keys=[\"pixels\"], loc=m, scale=s, standard_normal=True),\n            CatFrames(4, in_keys=[\"pixels\"], dim=-3),\n        ),\n    )\n    return env\n\n\n###############################################################################\n# Compute normalizing constants:\n\ndummy_env = make_env()\nv = dummy_env.transform[3].parent.reset()[\"pixels\"]\nm, s = v.mean().item(), v.std().item()", "metadata": {"task_id": "pytorch_rl/113", "ground_truth": "m, s", "fpath_tuple": ["pytorch_rl", "tutorials", "sphinx-tutorials", "coding_dqn.py"], "context_start_lineno": 0, "line_no": 189, "query_window": {"context": "\n    env = TransformedEnv(\n        base_env,\n        Compose(\n            ToTensorImage(),\n            GrayScale(),\n            Resize(64, 64),\n            ObservationNorm(in_keys=[\"pixels\"], loc=m, scale=s, standard_normal=True),\n            CatFrames(4, in_keys=[\"pixels\"], dim=-3),\n        ),\n    )\n    return env\n\n\n###############################################################################\n# Compute normalizing constants:\n\ndummy_env = make_env()\nv = dummy_env.transform[3].parent.reset()[\"pixels\"]\nm, s = v.mean().item(), v.std().item()", "metadata": {"fpath_tuple": ["pytorch_rl", "tutorials", "sphinx-tutorials", "coding_dqn.py"], "line_no": 189, "task_id": "pytorch_rl/113", "start_line_no": 169, "end_line_no": 189, "window_size": 20, "context_start_lineno": 0, "repo": "pytorch_rl"}}, "top_k_context": [{"context": "    NoopResetEnv,\n    ObservationNorm,\n    StepCounter,\n    ToTensorImage,\n    TransformedEnv,\n)\n\nbase_env = GymEnv(\"Pendulum-v1\", frame_skip=3, from_pixels=True, pixels_only=False)\nenv = TransformedEnv(base_env, Compose(NoopResetEnv(3), ToTensorImage()))\nenv.append_transform(ObservationNorm(in_keys=[\"pixels\"], loc=2, scale=1))\n\n###############################################################################\n\nenv.reset()\n\n###############################################################################\n\nprint(\"env: \", env)\nprint(\"last transform parent: \", env.transform[2].parent)\n", "metadata": [{"fpath_tuple": ["pytorch_rl", "tutorials", "sphinx-tutorials", "torchrl_demo.py"], "line_no": 340, "start_line_no": 330, "end_line_no": 350, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}], "sim_score": 0.38461538461538464}, {"context": "env = TransformedEnv(base_env, Compose(NoopResetEnv(3), ToTensorImage()))\nenv.append_transform(ObservationNorm(in_keys=[\"pixels\"], loc=2, scale=1))\n\n###############################################################################\n\nenv.reset()\n\n###############################################################################\n\nprint(\"env: \", env)\nprint(\"last transform parent: \", env.transform[2].parent)\n\n###############################################################################\n# Vectorized Environments\n# ------------------------------\n\nfrom torchrl.envs import ParallelEnv\n\nbase_env = ParallelEnv(\n    4,", "metadata": [{"fpath_tuple": ["pytorch_rl", "tutorials", "sphinx-tutorials", "torchrl_demo.py"], "line_no": 348, "start_line_no": 338, "end_line_no": 358, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}], "sim_score": 0.3793103448275862}, {"context": "    TransformedEnv,\n)\n\nbase_env = GymEnv(\"Pendulum-v1\", frame_skip=3, from_pixels=True, pixels_only=False)\nenv = TransformedEnv(base_env, Compose(NoopResetEnv(3), ToTensorImage()))\nenv.append_transform(ObservationNorm(in_keys=[\"pixels\"], loc=2, scale=1))\n\n###############################################################################\n\nenv.reset()\n\n###############################################################################\n\nprint(\"env: \", env)\nprint(\"last transform parent: \", env.transform[2].parent)\n\n###############################################################################\n# Vectorized Environments\n# ------------------------------\n", "metadata": [{"fpath_tuple": ["pytorch_rl", "tutorials", "sphinx-tutorials", "torchrl_demo.py"], "line_no": 344, "start_line_no": 334, "end_line_no": 354, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}], "sim_score": 0.375}, {"context": "    StepCounter,\n    ToTensorImage,\n    TransformedEnv,\n)\n\nbase_env = GymEnv(\"Pendulum-v1\", frame_skip=3, from_pixels=True, pixels_only=False)\nenv = TransformedEnv(base_env, Compose(NoopResetEnv(3), ToTensorImage()))\nenv.append_transform(ObservationNorm(in_keys=[\"pixels\"], loc=2, scale=1))\n\n###############################################################################\n\nenv.reset()\n\n###############################################################################\n\nprint(\"env: \", env)\nprint(\"last transform parent: \", env.transform[2].parent)\n\n###############################################################################\n# Vectorized Environments", "metadata": [{"fpath_tuple": ["pytorch_rl", "tutorials", "sphinx-tutorials", "torchrl_demo.py"], "line_no": 342, "start_line_no": 332, "end_line_no": 352, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}], "sim_score": 0.375}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# examples/text_to_image/train_text_to_image.py\n# --------------------------------------------------\n# import os\n# import random\n# from pathlib import Path\n# from typing import Optional\n# \n# import numpy as np\n# import torch\n# import torch.nn.functional as F\n# import torch.utils.checkpoint\n# \n# import datasets\n# import diffusers\n# import transformers\n# from accelerate import Accelerator\n# from accelerate.logging import get_logger\n# from accelerate.utils import set_seed\n# from datasets import load_dataset\n# from diffusers import AutoencoderKL, DDPMScheduler, StableDiffusionPipeline, UNet2DConditionModel\n# from diffusers.optimization import get_scheduler\n# from diffusers.training_utils import EMAModel\n# --------------------------------------------------\n# the below code fragment can be found in:\n# examples/text_to_image/train_text_to_image.py\n# --------------------------------------------------\n# from pathlib import Path\n# from typing import Optional\n# \n# import numpy as np\n# import torch\n# import torch.nn.functional as F\n# import torch.utils.checkpoint\n# \n# import datasets\n# import diffusers\n# import transformers\n# from accelerate import Accelerator\n# from accelerate.logging import get_logger\n# from accelerate.utils import set_seed\n# from datasets import load_dataset\n# from diffusers import AutoencoderKL, DDPMScheduler, StableDiffusionPipeline, UNet2DConditionModel\n# from diffusers.optimization import get_scheduler\n# from diffusers.training_utils import EMAModel\n# from diffusers.utils import check_min_version\n# from diffusers.utils.import_utils import is_xformers_available\n# --------------------------------------------------\n# the below code fragment can be found in:\n# examples/research_projects/dreambooth_inpaint/train_dreambooth_inpaint.py\n# --------------------------------------------------\n# from pathlib import Path\n# from typing import Optional\n# \n# import numpy as np\n# import torch\n# import torch.nn.functional as F\n# import torch.utils.checkpoint\n# from torch.utils.data import Dataset\n# \n# from accelerate import Accelerator\n# from accelerate.logging import get_logger\n# from accelerate.utils import set_seed\n# from diffusers import (\n#     AutoencoderKL,\n#     DDPMScheduler,\n#     StableDiffusionInpaintPipeline,\n#     StableDiffusionPipeline,\n#     UNet2DConditionModel,\n# )\n# from diffusers.optimization import get_scheduler\n# --------------------------------------------------\n# the below code fragment can be found in:\n# examples/research_projects/dreambooth_inpaint/train_dreambooth_inpaint.py\n# --------------------------------------------------\n# \n# import numpy as np\n# import torch\n# import torch.nn.functional as F\n# import torch.utils.checkpoint\n# from torch.utils.data import Dataset\n# \n# from accelerate import Accelerator\n# from accelerate.logging import get_logger\n# from accelerate.utils import set_seed\n# from diffusers import (\n#     AutoencoderKL,\n#     DDPMScheduler,\n#     StableDiffusionInpaintPipeline,\n#     StableDiffusionPipeline,\n#     UNet2DConditionModel,\n# )\n# from diffusers.optimization import get_scheduler\n# from diffusers.utils import check_min_version\n# from huggingface_hub import HfFolder, Repository, create_repo, whoami\n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\nimport argparse\nimport itertools\nimport math\nimport os\nimport random\nfrom pathlib import Path\nfrom typing import Optional\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nimport torch.utils.checkpoint\nfrom torch.utils.data import Dataset\n\nimport intel_extension_for_pytorch as ipex\nimport PIL\nfrom accelerate import Accelerator\nfrom accelerate.logging import get_logger\nfrom accelerate.utils import set_seed\nfrom diffusers import AutoencoderKL, DDPMScheduler, PNDMScheduler, StableDiffusionPipeline, UNet2DConditionModel\nfrom diffusers.optimization import get_scheduler", "metadata": {"task_id": "huggingface_diffusers/36", "ground_truth": "from diffusers.pipelines.stable_diffusion import StableDiffusionSafetyChecker", "fpath_tuple": ["huggingface_diffusers", "examples", "research_projects", "intel_opts", "textual_inversion", "textual_inversion_bf16.py"], "context_start_lineno": 0, "line_no": 21, "query_window": {"context": "import itertools\nimport math\nimport os\nimport random\nfrom pathlib import Path\nfrom typing import Optional\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nimport torch.utils.checkpoint\nfrom torch.utils.data import Dataset\n\nimport intel_extension_for_pytorch as ipex\nimport PIL\nfrom accelerate import Accelerator\nfrom accelerate.logging import get_logger\nfrom accelerate.utils import set_seed\nfrom diffusers import AutoencoderKL, DDPMScheduler, PNDMScheduler, StableDiffusionPipeline, UNet2DConditionModel\nfrom diffusers.optimization import get_scheduler", "metadata": {"fpath_tuple": ["huggingface_diffusers", "examples", "research_projects", "intel_opts", "textual_inversion", "textual_inversion_bf16.py"], "line_no": 21, "task_id": "huggingface_diffusers/36", "start_line_no": 1, "end_line_no": 21, "window_size": 20, "context_start_lineno": 0, "repo": "huggingface_diffusers"}}, "top_k_context": [{"context": "from pathlib import Path\nfrom typing import Optional\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nimport torch.utils.checkpoint\nfrom torch.utils.data import Dataset\n\nfrom accelerate import Accelerator\nfrom accelerate.logging import get_logger\nfrom accelerate.utils import set_seed\nfrom diffusers import (\n    AutoencoderKL,\n    DDPMScheduler,\n    StableDiffusionInpaintPipeline,\n    StableDiffusionPipeline,\n    UNet2DConditionModel,\n)\nfrom diffusers.optimization import get_scheduler", "metadata": [{"fpath_tuple": ["huggingface_diffusers", "examples", "research_projects", "dreambooth_inpaint", "train_dreambooth_inpaint.py"], "line_no": 16, "start_line_no": 6, "end_line_no": 26, "window_size": 20, "repo": "huggingface_diffusers", "slice_size": 10}], "sim_score": 0.7333333333333333}, {"context": "import os\nimport random\nfrom pathlib import Path\nfrom typing import Optional\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nimport torch.utils.checkpoint\nfrom torch.utils.data import Dataset\n\nfrom accelerate import Accelerator\nfrom accelerate.logging import get_logger\nfrom accelerate.utils import set_seed\nfrom diffusers import (\n    AutoencoderKL,\n    DDPMScheduler,\n    StableDiffusionInpaintPipeline,\n    StableDiffusionPipeline,\n    UNet2DConditionModel,", "metadata": [{"fpath_tuple": ["huggingface_diffusers", "examples", "research_projects", "dreambooth_inpaint", "train_dreambooth_inpaint.py"], "line_no": 14, "start_line_no": 4, "end_line_no": 24, "window_size": 20, "repo": "huggingface_diffusers", "slice_size": 10}], "sim_score": 0.7191011235955056}, {"context": "import os\nimport random\nfrom pathlib import Path\nfrom typing import Optional\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nimport torch.utils.checkpoint\n\nimport datasets\nimport diffusers\nimport transformers\nfrom accelerate import Accelerator\nfrom accelerate.logging import get_logger\nfrom accelerate.utils import set_seed\nfrom datasets import load_dataset\nfrom diffusers import AutoencoderKL, DDPMScheduler, StableDiffusionPipeline, UNet2DConditionModel\nfrom diffusers.optimization import get_scheduler\nfrom diffusers.training_utils import EMAModel", "metadata": [{"fpath_tuple": ["huggingface_diffusers", "examples", "text_to_image", "train_text_to_image.py"], "line_no": 28, "start_line_no": 18, "end_line_no": 38, "window_size": 20, "repo": "huggingface_diffusers", "slice_size": 10}], "sim_score": 0.7021276595744681}, {"context": "import logging\nimport math\nimport os\nimport random\nfrom pathlib import Path\nfrom typing import Optional\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nimport torch.utils.checkpoint\n\nimport datasets\nimport diffusers\nimport transformers\nfrom accelerate import Accelerator\nfrom accelerate.logging import get_logger\nfrom accelerate.utils import set_seed\nfrom datasets import load_dataset\nfrom diffusers import AutoencoderKL, DDPMScheduler, StableDiffusionPipeline, UNet2DConditionModel", "metadata": [{"fpath_tuple": ["huggingface_diffusers", "examples", "text_to_image", "train_text_to_image.py"], "line_no": 26, "start_line_no": 16, "end_line_no": 36, "window_size": 20, "repo": "huggingface_diffusers", "slice_size": 10}], "sim_score": 0.7}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# scripts/convert_vq_diffusion_to_diffusers.py\n# --------------------------------------------------\n#         type=str,\n#         required=True,\n#         help=\"Path to the vqvae checkpoint to convert.\",\n#     )\n# \n#     parser.add_argument(\n#         \"--vqvae_original_config_file\",\n#         default=None,\n#         type=str,\n#         required=True,\n#         help=\"The YAML config file corresponding to the original architecture for the vqvae.\",\n#     )\n# \n#     parser.add_argument(\n#         \"--checkpoint_path\", default=None, type=str, required=True, help=\"Path to the checkpoint to convert.\"\n#     )\n# \n#     parser.add_argument(\n#         \"--original_config_file\",\n#         default=None,\n# --------------------------------------------------\n# the below code fragment can be found in:\n# scripts/convert_vq_diffusion_to_diffusers.py\n# --------------------------------------------------\n#         help=\"Path to the vqvae checkpoint to convert.\",\n#     )\n# \n#     parser.add_argument(\n#         \"--vqvae_original_config_file\",\n#         default=None,\n#         type=str,\n#         required=True,\n#         help=\"The YAML config file corresponding to the original architecture for the vqvae.\",\n#     )\n# \n#     parser.add_argument(\n#         \"--checkpoint_path\", default=None, type=str, required=True, help=\"Path to the checkpoint to convert.\"\n#     )\n# \n#     parser.add_argument(\n#         \"--original_config_file\",\n#         default=None,\n#         type=str,\n#         required=True,\n# --------------------------------------------------\n# the below code fragment can be found in:\n# scripts/convert_vq_diffusion_to_diffusers.py\n# --------------------------------------------------\n# \n#     parser.add_argument(\n#         \"--vqvae_original_config_file\",\n#         default=None,\n#         type=str,\n#         required=True,\n#         help=\"The YAML config file corresponding to the original architecture for the vqvae.\",\n#     )\n# \n#     parser.add_argument(\n#         \"--checkpoint_path\", default=None, type=str, required=True, help=\"Path to the checkpoint to convert.\"\n#     )\n# \n#     parser.add_argument(\n#         \"--original_config_file\",\n#         default=None,\n#         type=str,\n#         required=True,\n#         help=\"The YAML config file corresponding to the original architecture.\",\n#     )\n# --------------------------------------------------\n# the below code fragment can be found in:\n# scripts/convert_vq_diffusion_to_diffusers.py\n# --------------------------------------------------\n#         \"--vqvae_original_config_file\",\n#         default=None,\n#         type=str,\n#         required=True,\n#         help=\"The YAML config file corresponding to the original architecture for the vqvae.\",\n#     )\n# \n#     parser.add_argument(\n#         \"--checkpoint_path\", default=None, type=str, required=True, help=\"Path to the checkpoint to convert.\"\n#     )\n# \n#     parser.add_argument(\n#         \"--original_config_file\",\n#         default=None,\n#         type=str,\n#         required=True,\n#         help=\"The YAML config file corresponding to the original architecture.\",\n#     )\n# \n#     parser.add_argument(\"--dump_path\", default=None, type=str, required=True, help=\"Path to the output model.\")\n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\n\n            f\"{diffusers_attention_prefix}.to_out.0.weight\": checkpoint[f\"{attention_prefix}.proj_out.weight\"][\n                :, :, 0\n            ],\n            f\"{diffusers_attention_prefix}.to_out.0.bias\": checkpoint[f\"{attention_prefix}.proj_out.bias\"],\n        }\n    )\n\n    return diffusers_checkpoint\n\n\n# TODO maybe document and/or can do more efficiently (build indices in for loop and extract once for each split?)\ndef split_attentions(*, weight, bias, split, chunk_size):\n    weights = [None] * split\n    biases = [None] * split\n\n    weights_biases_idx = 0\n\n    for starting_row_index in range(0, weight.shape[0], chunk_size):\n        row_indices = torch.arange(starting_row_index, starting_row_index + chunk_size)\n\n        weight_rows = weight[row_indices, :]\n        bias_rows = bias[row_indices]\n\n        if weights[weights_biases_idx] is None:\n            assert weights[weights_biases_idx] is None\n            weights[weights_biases_idx] = weight_rows\n            biases[weights_biases_idx] = bias_rows\n        else:\n            assert weights[weights_biases_idx] is not None\n            weights[weights_biases_idx] = torch.concat([weights[weights_biases_idx], weight_rows])\n            biases[weights_biases_idx] = torch.concat([biases[weights_biases_idx], bias_rows])\n\n        weights_biases_idx = (weights_biases_idx + 1) % split\n\n    return weights, biases\n\n\n# done unet utils\n\n\n# Driver functions\n\n\ndef text_encoder():\n    print(\"loading CLIP text encoder\")\n\n    clip_name = \"openai/clip-vit-large-patch14\"\n\n    # sets pad_value to 0\n    pad_token = \"!\"\n\n    tokenizer_model = CLIPTokenizer.from_pretrained(clip_name, pad_token=pad_token, device_map=\"auto\")\n\n    assert tokenizer_model.convert_tokens_to_ids(pad_token) == 0\n\n    text_encoder_model = CLIPTextModelWithProjection.from_pretrained(\n        clip_name,\n        # `CLIPTextModel` does not support device_map=\"auto\"\n        # device_map=\"auto\"\n    )\n\n    print(\"done loading CLIP text encoder\")\n\n    return text_encoder_model, tokenizer_model\n\n\ndef prior(*, args, checkpoint_map_location):\n    print(\"loading prior\")\n\n    prior_checkpoint = torch.load(args.prior_checkpoint_path, map_location=checkpoint_map_location)\n    prior_checkpoint = prior_checkpoint[\"state_dict\"]\n\n    clip_stats_checkpoint = torch.load(args.clip_stat_path, map_location=checkpoint_map_location)\n\n    prior_model = prior_model_from_original_config()\n\n    prior_diffusers_checkpoint = prior_original_checkpoint_to_diffusers_checkpoint(\n        prior_model, prior_checkpoint, clip_stats_checkpoint\n    )\n\n    del prior_checkpoint\n    del clip_stats_checkpoint\n\n    load_checkpoint_to_model(prior_diffusers_checkpoint, prior_model, strict=True)\n\n    print(\"done loading prior\")\n\n    return prior_model\n\n\ndef decoder(*, args, checkpoint_map_location):\n    print(\"loading decoder\")\n\n    decoder_checkpoint = torch.load(args.decoder_checkpoint_path, map_location=checkpoint_map_location)\n    decoder_checkpoint = decoder_checkpoint[\"state_dict\"]\n\n    decoder_model = decoder_model_from_original_config()\n\n    decoder_diffusers_checkpoint = decoder_original_checkpoint_to_diffusers_checkpoint(\n        decoder_model, decoder_checkpoint\n    )\n\n    # text proj interlude\n\n    # The original decoder implementation includes a set of parameters that are used\n    # for creating the `encoder_hidden_states` which are what the U-net is conditioned\n    # on. The diffusers conditional unet directly takes the encoder_hidden_states. We pull\n    # the parameters into the UnCLIPTextProjModel class\n    text_proj_model = text_proj_from_original_config()\n\n    text_proj_checkpoint = text_proj_original_checkpoint_to_diffusers_checkpoint(decoder_checkpoint)\n\n    load_checkpoint_to_model(text_proj_checkpoint, text_proj_model, strict=True)\n\n    # done text proj interlude\n\n    del decoder_checkpoint\n\n    load_checkpoint_to_model(decoder_diffusers_checkpoint, decoder_model, strict=True)\n\n    print(\"done loading decoder\")\n\n    return decoder_model, text_proj_model\n\n\ndef super_res_unet(*, args, checkpoint_map_location):\n    print(\"loading super resolution unet\")\n\n    super_res_checkpoint = torch.load(args.super_res_unet_checkpoint_path, map_location=checkpoint_map_location)\n    super_res_checkpoint = super_res_checkpoint[\"state_dict\"]\n\n    # model_first_steps\n\n    super_res_first_model = super_res_unet_first_steps_model_from_original_config()\n\n    super_res_first_steps_checkpoint = super_res_unet_first_steps_original_checkpoint_to_diffusers_checkpoint(\n        super_res_first_model, super_res_checkpoint\n    )\n\n    # model_last_step\n    super_res_last_model = super_res_unet_last_step_model_from_original_config()\n\n    super_res_last_step_checkpoint = super_res_unet_last_step_original_checkpoint_to_diffusers_checkpoint(\n        super_res_last_model, super_res_checkpoint\n    )\n\n    del super_res_checkpoint\n\n    load_checkpoint_to_model(super_res_first_steps_checkpoint, super_res_first_model, strict=True)\n\n    load_checkpoint_to_model(super_res_last_step_checkpoint, super_res_last_model, strict=True)\n\n    print(\"done loading super resolution unet\")\n\n    return super_res_first_model, super_res_last_model\n\n\ndef load_checkpoint_to_model(checkpoint, model, strict=False):\n    with tempfile.NamedTemporaryFile() as file:\n        torch.save(checkpoint, file.name)\n        del checkpoint\n        if strict:\n            model.load_state_dict(torch.load(file.name), strict=True)\n        else:\n            load_checkpoint_and_dispatch(model, file.name, device_map=\"auto\")\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument(\"--dump_path\", default=None, type=str, required=True, help=\"Path to the output model.\")\n\n    parser.add_argument(\n        \"--prior_checkpoint_path\",\n        default=None,\n        type=str,\n        required=True,\n        help=\"Path to the prior checkpoint to convert.\",\n    )\n\n    parser.add_argument(\n        \"--decoder_checkpoint_path\",\n        default=None,\n        type=str,\n        required=True,\n        help=\"Path to the decoder checkpoint to convert.\",\n    )\n\n    parser.add_argument(\n        \"--super_res_unet_checkpoint_path\",\n        default=None,\n        type=str,\n        required=True,\n        help=\"Path to the super resolution checkpoint to convert.\",\n    )\n\n    parser.add_argument(\n        \"--clip_stat_path\", default=None, type=str, required=True, help=\"Path to the clip stats checkpoint to convert.\"\n    )\n\n    parser.add_argument(", "metadata": {"task_id": "huggingface_diffusers/125", "ground_truth": "        \"--checkpoint_load_device\",", "fpath_tuple": ["huggingface_diffusers", "scripts", "convert_kakao_brain_unclip_to_diffusers.py"], "context_start_lineno": 875, "line_no": 1077, "query_window": {"context": "        \"--decoder_checkpoint_path\",\n        default=None,\n        type=str,\n        required=True,\n        help=\"Path to the decoder checkpoint to convert.\",\n    )\n\n    parser.add_argument(\n        \"--super_res_unet_checkpoint_path\",\n        default=None,\n        type=str,\n        required=True,\n        help=\"Path to the super resolution checkpoint to convert.\",\n    )\n\n    parser.add_argument(\n        \"--clip_stat_path\", default=None, type=str, required=True, help=\"Path to the clip stats checkpoint to convert.\"\n    )\n\n    parser.add_argument(", "metadata": {"fpath_tuple": ["huggingface_diffusers", "scripts", "convert_kakao_brain_unclip_to_diffusers.py"], "line_no": 1077, "task_id": "huggingface_diffusers/125", "start_line_no": 1057, "end_line_no": 1077, "window_size": 20, "context_start_lineno": 875, "repo": "huggingface_diffusers"}}, "top_k_context": [{"context": "\n    parser.add_argument(\n        \"--vqvae_original_config_file\",\n        default=None,\n        type=str,\n        required=True,\n        help=\"The YAML config file corresponding to the original architecture for the vqvae.\",\n    )\n\n    parser.add_argument(\n        \"--checkpoint_path\", default=None, type=str, required=True, help=\"Path to the checkpoint to convert.\"\n    )\n\n    parser.add_argument(\n        \"--original_config_file\",\n        default=None,\n        type=str,\n        required=True,\n        help=\"The YAML config file corresponding to the original architecture.\",\n    )", "metadata": [{"fpath_tuple": ["huggingface_diffusers", "scripts", "convert_vq_diffusion_to_diffusers.py"], "line_no": 722, "start_line_no": 712, "end_line_no": 732, "window_size": 20, "repo": "huggingface_diffusers", "slice_size": 10}], "sim_score": 0.53125}, {"context": "        help=\"Path to the vqvae checkpoint to convert.\",\n    )\n\n    parser.add_argument(\n        \"--vqvae_original_config_file\",\n        default=None,\n        type=str,\n        required=True,\n        help=\"The YAML config file corresponding to the original architecture for the vqvae.\",\n    )\n\n    parser.add_argument(\n        \"--checkpoint_path\", default=None, type=str, required=True, help=\"Path to the checkpoint to convert.\"\n    )\n\n    parser.add_argument(\n        \"--original_config_file\",\n        default=None,\n        type=str,\n        required=True,", "metadata": [{"fpath_tuple": ["huggingface_diffusers", "scripts", "convert_vq_diffusion_to_diffusers.py"], "line_no": 720, "start_line_no": 710, "end_line_no": 730, "window_size": 20, "repo": "huggingface_diffusers", "slice_size": 10}], "sim_score": 0.53125}, {"context": "        type=str,\n        required=True,\n        help=\"Path to the vqvae checkpoint to convert.\",\n    )\n\n    parser.add_argument(\n        \"--vqvae_original_config_file\",\n        default=None,\n        type=str,\n        required=True,\n        help=\"The YAML config file corresponding to the original architecture for the vqvae.\",\n    )\n\n    parser.add_argument(\n        \"--checkpoint_path\", default=None, type=str, required=True, help=\"Path to the checkpoint to convert.\"\n    )\n\n    parser.add_argument(\n        \"--original_config_file\",\n        default=None,", "metadata": [{"fpath_tuple": ["huggingface_diffusers", "scripts", "convert_vq_diffusion_to_diffusers.py"], "line_no": 718, "start_line_no": 708, "end_line_no": 728, "window_size": 20, "repo": "huggingface_diffusers", "slice_size": 10}], "sim_score": 0.53125}, {"context": "        \"--vqvae_checkpoint_path\",\n        default=None,\n        type=str,\n        required=True,\n        help=\"Path to the vqvae checkpoint to convert.\",\n    )\n\n    parser.add_argument(\n        \"--vqvae_original_config_file\",\n        default=None,\n        type=str,\n        required=True,\n        help=\"The YAML config file corresponding to the original architecture for the vqvae.\",\n    )\n\n    parser.add_argument(\n        \"--checkpoint_path\", default=None, type=str, required=True, help=\"Path to the checkpoint to convert.\"\n    )\n\n    parser.add_argument(", "metadata": [{"fpath_tuple": ["huggingface_diffusers", "scripts", "convert_vq_diffusion_to_diffusers.py"], "line_no": 716, "start_line_no": 706, "end_line_no": 726, "window_size": 20, "repo": "huggingface_diffusers", "slice_size": 10}], "sim_score": 0.53125}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# torchrl/envs/common.py\n# --------------------------------------------------\n#             raise ValueError(\n#                 f\"The value of spec.shape ({value.shape}) must match the env batch size ({self.batch_size}).\"\n#             )\n#         self.__dict__[\"_input_spec\"] = value\n# \n#     @property\n#     def reward_spec(self) -> TensorSpec:\n#         return self._reward_spec\n# \n#     @reward_spec.setter\n#     def reward_spec(self, value: TensorSpec) -> None:\n#         if not hasattr(value, \"shape\"):\n#             raise TypeError(\n#                 f\"reward_spec of type {type(value)} do not have a shape \" f\"attribute.\"\n#             )\n#         if value.shape[: len(self.batch_size)] != self.batch_size:\n#             raise ValueError(\"The value of spec.shape must match the env batch size.\")\n#         if len(value.shape) == 0:\n#             raise RuntimeError(\n#                 \"the reward_spec shape cannot be empty (this error\"\n# --------------------------------------------------\n# the below code fragment can be found in:\n# torchrl/envs/common.py\n# --------------------------------------------------\n#             )\n#         self.__dict__[\"_input_spec\"] = value\n# \n#     @property\n#     def reward_spec(self) -> TensorSpec:\n#         return self._reward_spec\n# \n#     @reward_spec.setter\n#     def reward_spec(self, value: TensorSpec) -> None:\n#         if not hasattr(value, \"shape\"):\n#             raise TypeError(\n#                 f\"reward_spec of type {type(value)} do not have a shape \" f\"attribute.\"\n#             )\n#         if value.shape[: len(self.batch_size)] != self.batch_size:\n#             raise ValueError(\"The value of spec.shape must match the env batch size.\")\n#         if len(value.shape) == 0:\n#             raise RuntimeError(\n#                 \"the reward_spec shape cannot be empty (this error\"\n#                 \" usually comes from trying to set a reward_spec\"\n#                 \" with a null number of dimensions. Try using a multidimensional\"\n# --------------------------------------------------\n# the below code fragment can be found in:\n# torchrl/envs/common.py\n# --------------------------------------------------\n# \n#     @property\n#     def reward_spec(self) -> TensorSpec:\n#         return self._reward_spec\n# \n#     @reward_spec.setter\n#     def reward_spec(self, value: TensorSpec) -> None:\n#         if not hasattr(value, \"shape\"):\n#             raise TypeError(\n#                 f\"reward_spec of type {type(value)} do not have a shape \" f\"attribute.\"\n#             )\n#         if value.shape[: len(self.batch_size)] != self.batch_size:\n#             raise ValueError(\"The value of spec.shape must match the env batch size.\")\n#         if len(value.shape) == 0:\n#             raise RuntimeError(\n#                 \"the reward_spec shape cannot be empty (this error\"\n#                 \" usually comes from trying to set a reward_spec\"\n#                 \" with a null number of dimensions. Try using a multidimensional\"\n#                 \" spec instead, for instance with a singleton dimension at the tail).\"\n#             )\n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\nenv_kwargs, dict):\n            create_env_kwargs = [\n                deepcopy(create_env_kwargs) for _ in range(num_workers)\n            ]\n\n        self.policy_proof = policy_proof\n        self.num_workers = num_workers\n        self.create_env_fn = create_env_fn\n        self.create_env_kwargs = create_env_kwargs\n        self.env_input_keys = env_input_keys\n        self.pin_memory = pin_memory\n        self.selected_keys = selected_keys\n        self.excluded_keys = excluded_keys\n        self.share_individual_td = bool(share_individual_td)\n        self._share_memory = shared_memory\n        self._memmap = memmap\n        self.allow_step_when_done = allow_step_when_done\n        if self._share_memory and self._memmap:\n            raise RuntimeError(\n                \"memmap and shared memory are mutually exclusive features.\"\n            )\n        self._batch_size = None\n        self.__dict__[\"_observation_spec\"] = None\n        self.__dict__[\"_input_spec\"] = None\n        self.__dict__[\"_reward_spec\"] = None\n        self._device = None\n        self._dummy_env_str = None\n        self._seeds = None\n        # self._prepare_dummy_env(create_env_fn, create_env_kwargs)\n        self._get_metadata(create_env_fn, create_env_kwargs)\n\n    def _get_metadata(\n        self, create_env_fn: List[Callable], create_env_kwargs: List[Dict]\n    ):\n        if self._single_task:\n            # if EnvCreator, the metadata are already there\n            meta_data = get_env_metadata(create_env_fn[0], create_env_kwargs[0])\n            self.meta_data = meta_data.expand(\n                *(self.num_workers, *meta_data.batch_size)\n            )\n        else:\n            n_tasks = len(create_env_fn)\n            self.meta_data = []\n            for i in range(n_tasks):\n                self.meta_data.append(\n                    get_env_metadata(create_env_fn[i], create_env_kwargs[i])\n                )\n        self._set_properties()\n\n    def update_kwargs(self, kwargs: Union[dict, List[dict]]) -> None:\n        \"\"\"Updates the kwargs of each environment given a dictionary or a list of dictionaries.\n\n        Args:\n            kwargs (dict or list of dict): new kwargs to use with the environments\n\n        \"\"\"\n        if isinstance(kwargs, dict):\n            for _kwargs in self.create_env_kwargs:\n                _kwargs.update(kwargs)\n        else:\n            for _kwargs, _new_kwargs in zip(self.create_env_kwargs, kwargs):\n                _kwargs.update(_new_kwargs)\n\n    def _set_properties(self):\n        meta_data = self.meta_data\n        if self._single_task:\n            self._batch_size = meta_data.batch_size\n            observation_spec = meta_data.specs[\"observation_spec\"]\n\n            self.observation_spec = observation_spec\n\n            reward_spec = meta_data.specs[\"reward_spec\"]\n            self.reward_spec = reward_spec\n\n            input_spec = meta_data.specs[\"input_spec\"]\n            self.input_spec = input_spec\n\n            self._dummy_env_str = meta_data.env_str\n            self._device = meta_data.device\n            self._env_tensordict = meta_data.tensordict\n            self._batch_locked = meta_data.batch_locked\n        else:\n            self._batch_size = torch.Size([self.num_workers, *meta_data[0].batch_size])\n            self._device = meta_data[0].device\n            # TODO: check that all action_spec and reward spec match (issue #351)\n\n            reward_spec = meta_data[0].specs[\"reward_spec\"]\n            reward_spec = reward_spec.expand(self.num_workers, *reward_spec.shape)\n            self.reward_spec = reward_spec\n\n            _observation_spec = {}\n            for md in meta_data:\n                _observation_spec.update(dict(**md.specs[\"observation_spec\"]))\n            observation_spec = CompositeSpec(\n                **_observation_spec, shape=meta_data[0].batch_size\n            )\n            observation_spec = observation_spec.expand(\n                self.num_workers, *observation_spec.shape\n            )\n            self.observation_spec = observation_spec\n\n            _input_spec = {}\n            for md in meta_data:\n                _input_spec.update(dict(**md.specs[\"input_spec\"]))\n            input_spec = CompositeSpec(**_input_spec, shape=meta_data[0].batch_size)\n            input_spec = input_spec.expand(self.num_workers, *input_spec.shape)\n            self.input_spec = input_spec\n\n            self._dummy_env_str = str(meta_data[0])\n            self._env_tensordict = torch.stack(\n                [meta_data.tensordict for meta_data in meta_data], 0\n            )\n            self._batch_locked = meta_data[0].batch_locked\n\n    def state_dict(self) -> OrderedDict:\n        raise NotImplementedError\n\n    def load_state_dict(self, state_dict: OrderedDict) -> None:\n        raise NotImplementedError\n\n    @property\n    def batch_size(self) -> TensorSpec:\n        if \"_batch_size\" not in self.__dir__():\n            raise AttributeError(\"_batch_size is not initialized\")\n        if self._batch_size is None:\n            self._set_properties()\n        return self._batch_size\n\n    @property\n    def device(self) -> torch.device:\n        if self._device is None:\n            self._set_properties()\n        return self._device\n\n    @device.setter\n    def device(self, value: DEVICE_TYPING) -> None:\n        self.to(value)\n\n    @property\n    def observation_spec(self) -> TensorSpec:\n        if self._observation_spec is None:\n            self._set_properties()\n        return self._observation_spec\n\n    @observation_spec.setter\n    def observation_spec(self, value: TensorSpec) -> None:\n        if not isinstance(value, CompositeSpec) and value is not None:\n            raise TypeError(\"The type of an observation_spec must be Composite.\")\n        self.__dict__[\"_observation_spec\"] = value\n\n    @property\n    def input_spec(self) -> TensorSpec:\n        if self._input_spec is None:\n            self._set_properties()\n        return self._input_spec\n\n    @input_spec.setter\n    def input_spec(self, value: TensorSpec) -> None:\n        if not isinstance(value, CompositeSpec) and value is not None:\n            raise TypeError(\"The type of an input_spec must be Composite.\")\n        self.__dict__[\"_input_spec\"] = value\n\n    @property\n    def reward_spec(self) -> TensorSpec:\n        if self._reward_spec is None:\n            self._set_properties()\n        return self._reward_spec\n\n    @reward_spec.setter\n    def reward_spec(self, value: TensorSpec) -> None:\n        if not hasattr(value, \"shape\") and value is not None:\n            raise TypeError(\n                f\"reward_spec of type {type(value)} do not have a shape \" f\"attribute.\"\n            )\n        if value is not None and len(value.shape) == 0:\n            raise RuntimeError(\n                \"the reward_spec shape cannot be empty (this error\"\n                \" usually comes from trying to set a reward_spec\"", "metadata": {"task_id": "pytorch_rl/102", "ground_truth": "                \" with a null number of dimensions. Try using a multidimensional\"", "fpath_tuple": ["pytorch_rl", "torchrl", "envs", "vec_env.py"], "context_start_lineno": 180, "line_no": 358, "query_window": {"context": "        if not isinstance(value, CompositeSpec) and value is not None:\n            raise TypeError(\"The type of an input_spec must be Composite.\")\n        self.__dict__[\"_input_spec\"] = value\n\n    @property\n    def reward_spec(self) -> TensorSpec:\n        if self._reward_spec is None:\n            self._set_properties()\n        return self._reward_spec\n\n    @reward_spec.setter\n    def reward_spec(self, value: TensorSpec) -> None:\n        if not hasattr(value, \"shape\") and value is not None:\n            raise TypeError(\n                f\"reward_spec of type {type(value)} do not have a shape \" f\"attribute.\"\n            )\n        if value is not None and len(value.shape) == 0:\n            raise RuntimeError(\n                \"the reward_spec shape cannot be empty (this error\"\n                \" usually comes from trying to set a reward_spec\"", "metadata": {"fpath_tuple": ["pytorch_rl", "torchrl", "envs", "vec_env.py"], "line_no": 358, "task_id": "pytorch_rl/102", "start_line_no": 338, "end_line_no": 358, "window_size": 20, "context_start_lineno": 180, "repo": "pytorch_rl"}}, "top_k_context": [{"context": "            )\n        self.__dict__[\"_input_spec\"] = value\n\n    @property\n    def reward_spec(self) -> TensorSpec:\n        return self._reward_spec\n\n    @reward_spec.setter\n    def reward_spec(self, value: TensorSpec) -> None:\n        if not hasattr(value, \"shape\"):\n            raise TypeError(\n                f\"reward_spec of type {type(value)} do not have a shape \" f\"attribute.\"\n            )\n        if value.shape[: len(self.batch_size)] != self.batch_size:\n            raise ValueError(\"The value of spec.shape must match the env batch size.\")\n        if len(value.shape) == 0:\n            raise RuntimeError(\n                \"the reward_spec shape cannot be empty (this error\"\n                \" usually comes from trying to set a reward_spec\"\n                \" with a null number of dimensions. Try using a multidimensional\"", "metadata": [{"fpath_tuple": ["pytorch_rl", "torchrl", "envs", "common.py"], "line_no": 332, "start_line_no": 322, "end_line_no": 342, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}], "sim_score": 0.7256637168141593}, {"context": "            raise ValueError(\n                f\"The value of spec.shape ({value.shape}) must match the env batch size ({self.batch_size}).\"\n            )\n        self.__dict__[\"_input_spec\"] = value\n\n    @property\n    def reward_spec(self) -> TensorSpec:\n        return self._reward_spec\n\n    @reward_spec.setter\n    def reward_spec(self, value: TensorSpec) -> None:\n        if not hasattr(value, \"shape\"):\n            raise TypeError(\n                f\"reward_spec of type {type(value)} do not have a shape \" f\"attribute.\"\n            )\n        if value.shape[: len(self.batch_size)] != self.batch_size:\n            raise ValueError(\"The value of spec.shape must match the env batch size.\")\n        if len(value.shape) == 0:\n            raise RuntimeError(\n                \"the reward_spec shape cannot be empty (this error\"", "metadata": [{"fpath_tuple": ["pytorch_rl", "torchrl", "envs", "common.py"], "line_no": 330, "start_line_no": 320, "end_line_no": 340, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}], "sim_score": 0.7037037037037037}, {"context": "            raise TypeError(\"The type of an input_spec must be Composite.\")\n        if value.shape[: len(self.batch_size)] != self.batch_size:\n            raise ValueError(\n                f\"The value of spec.shape ({value.shape}) must match the env batch size ({self.batch_size}).\"\n            )\n        self.__dict__[\"_input_spec\"] = value\n\n    @property\n    def reward_spec(self) -> TensorSpec:\n        return self._reward_spec\n\n    @reward_spec.setter\n    def reward_spec(self, value: TensorSpec) -> None:\n        if not hasattr(value, \"shape\"):\n            raise TypeError(\n                f\"reward_spec of type {type(value)} do not have a shape \" f\"attribute.\"\n            )\n        if value.shape[: len(self.batch_size)] != self.batch_size:\n            raise ValueError(\"The value of spec.shape must match the env batch size.\")\n        if len(value.shape) == 0:", "metadata": [{"fpath_tuple": ["pytorch_rl", "torchrl", "envs", "common.py"], "line_no": 328, "start_line_no": 318, "end_line_no": 338, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}], "sim_score": 0.6666666666666666}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# tests/test_scheduler_flax.py\n# --------------------------------------------------\n#             scheduler = scheduler_class(**scheduler_config)\n#             state = scheduler.create_state()\n# \n#             sample, _ = self.dummy_sample\n#             residual = 0.1 * sample\n# \n#             if num_inference_steps is not None and hasattr(scheduler, \"set_timesteps\"):\n#                 state = scheduler.set_timesteps(state, num_inference_steps)\n#             elif num_inference_steps is not None and not hasattr(scheduler, \"set_timesteps\"):\n#                 kwargs[\"num_inference_steps\"] = num_inference_steps\n# \n#             output_0 = scheduler.step(state, residual, 0, sample, **kwargs).prev_sample\n#             output_1 = scheduler.step(state, residual, 1, sample, **kwargs).prev_sample\n# \n#             self.assertEqual(output_0.shape, sample.shape)\n#             self.assertEqual(output_0.shape, output_1.shape)\n# \n#     def test_timesteps(self):\n#         for timesteps in [100, 500, 1000]:\n#             self.check_over_configs(num_train_timesteps=timesteps)\n# --------------------------------------------------\n# the below code fragment can be found in:\n# tests/test_scheduler_flax.py\n# --------------------------------------------------\n#             scheduler_config = self.get_scheduler_config()\n#             scheduler = scheduler_class(**scheduler_config)\n#             state = scheduler.create_state()\n# \n#             sample, key = self.dummy_sample\n#             residual = 0.1 * sample\n# \n#             if num_inference_steps is not None and hasattr(scheduler, \"set_timesteps\"):\n#                 state = scheduler.set_timesteps(state, num_inference_steps)\n#             elif num_inference_steps is not None and not hasattr(scheduler, \"set_timesteps\"):\n#                 kwargs[\"num_inference_steps\"] = num_inference_steps\n# \n#             output_0 = scheduler.step(state, residual, 0, sample, key, **kwargs).prev_sample\n#             output_1 = scheduler.step(state, residual, 1, sample, key, **kwargs).prev_sample\n# \n#             self.assertEqual(output_0.shape, sample.shape)\n#             self.assertEqual(output_0.shape, output_1.shape)\n# \n#     def test_scheduler_outputs_equivalence(self):\n#         def set_nan_tensor_to_zero(t):\n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\nsize = 4\n        num_channels = 3\n        height = 8\n        width = 8\n\n        num_elems = batch_size * num_channels * height * width\n        sample = torch.arange(num_elems)\n        sample = sample.reshape(num_channels, height, width, batch_size)\n        sample = sample / num_elems\n        sample = sample.permute(3, 0, 1, 2)\n\n        return sample\n\n    def dummy_model(self):\n        def model(sample, t, *args):\n            return sample * t / (t + 1)\n\n        return model\n\n    def get_scheduler_config(self, **kwargs):\n        config = {\n            \"num_train_timesteps\": 2000,\n            \"snr\": 0.15,\n            \"sigma_min\": 0.01,\n            \"sigma_max\": 1348,\n            \"sampling_eps\": 1e-5,\n        }\n\n        config.update(**kwargs)\n        return config\n\n    def check_over_configs(self, time_step=0, **config):\n        kwargs = dict(self.forward_default_kwargs)\n\n        for scheduler_class in self.scheduler_classes:\n            sample = self.dummy_sample\n            residual = 0.1 * sample\n\n            scheduler_config = self.get_scheduler_config(**config)\n            scheduler = scheduler_class(**scheduler_config)\n\n            with tempfile.TemporaryDirectory() as tmpdirname:\n                scheduler.save_config(tmpdirname)\n                new_scheduler = scheduler_class.from_pretrained(tmpdirname)\n\n            output = scheduler.step_pred(\n                residual, time_step, sample, generator=torch.manual_seed(0), **kwargs\n            ).prev_sample\n            new_output = new_scheduler.step_pred(\n                residual, time_step, sample, generator=torch.manual_seed(0), **kwargs\n            ).prev_sample\n\n            assert torch.sum(torch.abs(output - new_output)) < 1e-5, \"Scheduler outputs are not identical\"\n\n            output = scheduler.step_correct(residual, sample, generator=torch.manual_seed(0), **kwargs).prev_sample\n            new_output = new_scheduler.step_correct(\n                residual, sample, generator=torch.manual_seed(0), **kwargs\n            ).prev_sample\n\n            assert torch.sum(torch.abs(output - new_output)) < 1e-5, \"Scheduler correction are not identical\"\n\n    def check_over_forward(self, time_step=0, **forward_kwargs):\n        kwargs = dict(self.forward_default_kwargs)\n        kwargs.update(forward_kwargs)\n\n        for scheduler_class in self.scheduler_classes:\n            sample = self.dummy_sample\n            residual = 0.1 * sample\n\n            scheduler_config = self.get_scheduler_config()\n            scheduler = scheduler_class(**scheduler_config)\n\n            with tempfile.TemporaryDirectory() as tmpdirname:\n                scheduler.save_config(tmpdirname)\n                new_scheduler = scheduler_class.from_pretrained(tmpdirname)\n\n            output = scheduler.step_pred(\n                residual, time_step, sample, generator=torch.manual_seed(0), **kwargs\n            ).prev_sample\n            new_output = new_scheduler.step_pred(\n                residual, time_step, sample, generator=torch.manual_seed(0), **kwargs\n            ).prev_sample\n\n            assert torch.sum(torch.abs(output - new_output)) < 1e-5, \"Scheduler outputs are not identical\"\n\n            output = scheduler.step_correct(residual, sample, generator=torch.manual_seed(0), **kwargs).prev_sample\n            new_output = new_scheduler.step_correct(\n                residual, sample, generator=torch.manual_seed(0), **kwargs\n            ).prev_sample\n\n            assert torch.sum(torch.abs(output - new_output)) < 1e-5, \"Scheduler correction are not identical\"\n\n    def test_timesteps(self):\n        for timesteps in [10, 100, 1000]:\n            self.check_over_configs(num_train_timesteps=timesteps)\n\n    def test_sigmas(self):\n        for sigma_min, sigma_max in zip([0.0001, 0.001, 0.01], [1, 100, 1000]):\n            self.check_over_configs(sigma_min=sigma_min, sigma_max=sigma_max)\n\n    def test_time_indices(self):\n        for t in [0.1, 0.5, 0.75]:\n            self.check_over_forward(time_step=t)\n\n    def test_full_loop_no_noise(self):\n        kwargs = dict(self.forward_default_kwargs)\n\n        scheduler_class = self.scheduler_classes[0]\n        scheduler_config = self.get_scheduler_config()\n        scheduler = scheduler_class(**scheduler_config)\n\n        num_inference_steps = 3\n\n        model = self.dummy_model()\n        sample = self.dummy_sample_deter\n\n        scheduler.set_sigmas(num_inference_steps)\n        scheduler.set_timesteps(num_inference_steps)\n        generator = torch.manual_seed(0)\n\n        for i, t in enumerate(scheduler.timesteps):\n            sigma_t = scheduler.sigmas[i]\n\n            for _ in range(scheduler.config.correct_steps):\n                with torch.no_grad():\n                    model_output = model(sample, sigma_t)\n                sample = scheduler.step_correct(model_output, sample, generator=generator, **kwargs).prev_sample\n\n            with torch.no_grad():\n                model_output = model(sample, sigma_t)\n\n            output = scheduler.step_pred(model_output, t, sample, generator=generator, **kwargs)\n            sample, _ = output.prev_sample, output.prev_sample_mean\n\n        result_sum = torch.sum(torch.abs(sample))\n        result_mean = torch.mean(torch.abs(sample))\n\n        assert np.isclose(result_sum.item(), 14372758528.0)\n        assert np.isclose(result_mean.item(), 18714530.0)\n\n    def test_step_shape(self):\n        kwargs = dict(self.forward_default_kwargs)\n\n        num_inference_steps = kwargs.pop(\"num_inference_steps\", None)\n\n        for scheduler_class in self.scheduler_classes:\n            scheduler_config = self.get_scheduler_config()\n            scheduler = scheduler_class(**scheduler_config)\n\n            sample = self.dummy_sample\n            residual = 0.1 * sample\n\n            if num_inference_steps is not None and hasattr(scheduler, \"set_timesteps\"):\n                scheduler.set_timesteps(num_inference_steps)\n            elif num_inference_steps is not None and not hasattr(scheduler, \"set_timesteps\"):\n                kwargs[\"num_inference_steps\"] = num_inference_steps\n\n            output_0 = scheduler.step_pred(residual, 0, sample, generator=torch.manual_seed(0), **kwargs).prev_sample\n            output_1 = scheduler.step_pred(residual, 1, sample, generator=torch.manual_seed(0), **kwargs).prev_sample\n\n            self.assertEqual(output_0.shape, sample.shape)\n            self.assertEqual(output_0.shape, output_1.shape)\n\n\nclass LMSDiscreteSchedulerTest(SchedulerCommonTest):\n    scheduler_classes = (LMSDiscreteScheduler,)", "metadata": {"task_id": "huggingface_diffusers/139", "ground_truth": "    num_inference_steps = 10", "fpath_tuple": ["huggingface_diffusers", "tests", "test_scheduler.py"], "context_start_lineno": 1484, "line_no": 1650, "query_window": {"context": "            scheduler_config = self.get_scheduler_config()\n            scheduler = scheduler_class(**scheduler_config)\n\n            sample = self.dummy_sample\n            residual = 0.1 * sample\n\n            if num_inference_steps is not None and hasattr(scheduler, \"set_timesteps\"):\n                scheduler.set_timesteps(num_inference_steps)\n            elif num_inference_steps is not None and not hasattr(scheduler, \"set_timesteps\"):\n                kwargs[\"num_inference_steps\"] = num_inference_steps\n\n            output_0 = scheduler.step_pred(residual, 0, sample, generator=torch.manual_seed(0), **kwargs).prev_sample\n            output_1 = scheduler.step_pred(residual, 1, sample, generator=torch.manual_seed(0), **kwargs).prev_sample\n\n            self.assertEqual(output_0.shape, sample.shape)\n            self.assertEqual(output_0.shape, output_1.shape)\n\n\nclass LMSDiscreteSchedulerTest(SchedulerCommonTest):\n    scheduler_classes = (LMSDiscreteScheduler,)", "metadata": {"fpath_tuple": ["huggingface_diffusers", "tests", "test_scheduler.py"], "line_no": 1650, "task_id": "huggingface_diffusers/139", "start_line_no": 1630, "end_line_no": 1650, "window_size": 20, "context_start_lineno": 1484, "repo": "huggingface_diffusers"}}, "top_k_context": [{"context": "\n        for scheduler_class in self.scheduler_classes:\n            scheduler_config = self.get_scheduler_config()\n            scheduler = scheduler_class(**scheduler_config)\n            state = scheduler.create_state()\n\n            sample, key = self.dummy_sample\n            residual = 0.1 * sample\n\n            if num_inference_steps is not None and hasattr(scheduler, \"set_timesteps\"):\n                state = scheduler.set_timesteps(state, num_inference_steps)\n            elif num_inference_steps is not None and not hasattr(scheduler, \"set_timesteps\"):\n                kwargs[\"num_inference_steps\"] = num_inference_steps\n\n            output_0 = scheduler.step(state, residual, 0, sample, key, **kwargs).prev_sample\n            output_1 = scheduler.step(state, residual, 1, sample, key, **kwargs).prev_sample\n\n            self.assertEqual(output_0.shape, sample.shape)\n            self.assertEqual(output_0.shape, output_1.shape)\n", "metadata": [{"fpath_tuple": ["huggingface_diffusers", "tests", "test_scheduler_flax.py"], "line_no": 170, "start_line_no": 160, "end_line_no": 180, "window_size": 20, "repo": "huggingface_diffusers", "slice_size": 10}], "sim_score": 0.6868686868686869}, {"context": "        for scheduler_class in self.scheduler_classes:\n            scheduler_config = self.get_scheduler_config()\n            scheduler = scheduler_class(**scheduler_config)\n            state = scheduler.create_state()\n\n            sample, _ = self.dummy_sample\n            residual = 0.1 * sample\n\n            if num_inference_steps is not None and hasattr(scheduler, \"set_timesteps\"):\n                state = scheduler.set_timesteps(state, num_inference_steps)\n            elif num_inference_steps is not None and not hasattr(scheduler, \"set_timesteps\"):\n                kwargs[\"num_inference_steps\"] = num_inference_steps\n\n            output_0 = scheduler.step(state, residual, 0, sample, **kwargs).prev_sample\n            output_1 = scheduler.step(state, residual, 1, sample, **kwargs).prev_sample\n\n            self.assertEqual(output_0.shape, sample.shape)\n            self.assertEqual(output_0.shape, output_1.shape)\n\n    def test_timesteps(self):", "metadata": [{"fpath_tuple": ["huggingface_diffusers", "tests", "test_scheduler_flax.py"], "line_no": 534, "start_line_no": 524, "end_line_no": 544, "window_size": 20, "repo": "huggingface_diffusers", "slice_size": 10}], "sim_score": 0.6764705882352942}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# torchrl/envs/libs/brax.py\n# --------------------------------------------------\n#         state = _tree_reshape(state, self.batch_size)\n#         state = _object_to_tensordict(state, self.device, self.batch_size)\n# \n#         # build result\n#         reward = state.get(\"reward\").view(*self.reward_spec.shape)\n#         done = state.get(\"done\").bool().view(*self.reward_spec.shape)\n#         tensordict_out = TensorDict(\n#             source={\n#                 \"observation\": state.get(\"obs\"),\n#                 \"reward\": reward,\n#                 \"done\": done,\n#                 \"state\": state,\n#             },\n#             batch_size=self.batch_size,\n#             device=self.device,\n#             _run_checks=False,\n#         )\n#         return tensordict_out\n# \n#     def _step_without_grad(self, tensordict: TensorDictBase):\n# --------------------------------------------------\n# the below code fragment can be found in:\n# torchrl/envs/libs/brax.py\n# --------------------------------------------------\n#         tensordict_out = TensorDict(\n#             source={\n#                 \"observation\": state.get(\"obs\"),\n#                 \"reward\": reward,\n#                 \"done\": done,\n#                 \"state\": state,\n#             },\n#             batch_size=self.batch_size,\n#             device=self.device,\n#             _run_checks=False,\n#         )\n#         return tensordict_out\n# \n#     def _step_without_grad(self, tensordict: TensorDictBase):\n# \n#         # convert tensors to ndarrays\n#         state = _tensordict_to_object(tensordict.get(\"state\"), self._state_example)\n#         action = _tensor_to_ndarray(tensordict.get(\"action\"))\n# \n#         # flatten batch size\n# --------------------------------------------------\n# the below code fragment can be found in:\n# torchrl/envs/libs/brax.py\n# --------------------------------------------------\n#                 \"observation\": state.get(\"obs\"),\n#                 \"reward\": reward,\n#                 \"done\": done,\n#                 \"state\": state,\n#             },\n#             batch_size=self.batch_size,\n#             device=self.device,\n#             _run_checks=False,\n#         )\n#         return tensordict_out\n# \n#     def _step_without_grad(self, tensordict: TensorDictBase):\n# \n#         # convert tensors to ndarrays\n#         state = _tensordict_to_object(tensordict.get(\"state\"), self._state_example)\n#         action = _tensor_to_ndarray(tensordict.get(\"action\"))\n# \n#         # flatten batch size\n#         state = _tree_flatten(state, self.batch_size)\n#         action = _tree_flatten(action, self.batch_size)\n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\nbatch_size)\n\n        # call env step with jit and vmap\n        next_state = self._vmap_jit_env_step(state, action)\n\n        # reshape batch size and convert ndarrays to tensors\n        next_state = _tree_reshape(next_state, self.batch_size)\n        next_state = _object_to_tensordict(next_state, self.device, self.batch_size)\n\n        # build result\n        reward = next_state.get(\"reward\").view(self.reward_spec.shape)\n        done = next_state.get(\"done\").bool().view(self.reward_spec.shape)\n        tensordict_out = TensorDict(\n            source={\n                \"observation\": next_state.get(\"obs\"),\n                \"reward\": reward,\n                \"done\": done,\n                \"state\": next_state,\n            },\n            batch_size=self.batch_size,\n            device=self.device,\n            _run_checks=False,\n        )\n        return tensordict_out\n\n    def _step_with_grad(self, tensordict: TensorDictBase):\n\n        # convert tensors to ndarrays\n        action = tensordict.get(\"action\")\n        state = tensordict.get(\"state\")\n        qp_keys = list(state.get(\"qp\").keys())\n        qp_values = list(state.get(\"qp\").values())\n\n        # call env step with autograd function\n        next_state_nograd, next_obs, next_reward, *next_qp_values = _BraxEnvStep.apply(\n            self, state, action, *qp_values\n        )\n\n        # extract done values: we assume a shape identical to reward\n        next_done = next_state_nograd.get(\"done\").bool().view(*self.reward_spec.shape)\n\n        # merge with tensors with grad function\n        next_state = next_state_nograd\n        next_state[\"obs\"] = next_obs\n        next_state[\"reward\"] = next_reward.view(*self.reward_spec.shape)\n        next_state[\"qp\"].update(dict(zip(qp_keys, next_qp_values)))\n\n        # build result\n        tensordict_out = TensorDict(\n            source={\n                \"observation\": next_obs,\n                \"reward\": next_reward,\n                \"done\": next_done,\n                \"state\": next_state,\n            },\n            batch_size=self.batch_size,\n            device=self.device,\n            _run_checks=False,\n        )\n        return tensordict_out\n\n    def _step(\n        self,\n        tensordict: TensorDictBase,\n    ) -> TensorDictBase:\n\n        if self.requires_grad:\n            return self._step_with_grad(tensordict)\n        else:\n            return self._step_without_grad(tensordict)\n\n\nclass BraxEnv(BraxWrapper):\n    \"\"\"Google Brax environment wrapper.\n\n    Examples:\n        >>> env = BraxEnv(env_name=\"ant\")\n        >>> td = env.rand_step()\n        >>> print(td)\n        >>> print(env.available_envs)\n\n    \"\"\"\n\n    def __init__(self, env_name, **kwargs):\n        kwargs[\"env_name\"] = env_name\n        super().__init__(**kwargs)\n\n    def _build_env(\n        self,\n        env_name: str,\n        **kwargs,\n    ) -> \"brax.envs.env.Env\":\n        if not _has_brax:\n            raise RuntimeError(\n                f\"brax not found, unable to create {env_name}. \"\n                f\"Consider downloading and installing brax from\"\n                f\" {self.git_url}\"\n            ) from IMPORT_ERR\n        from_pixels = kwargs.pop(\"from_pixels\", False)\n        pixels_only = kwargs.pop(\"pixels_only\", True)\n        requires_grad = kwargs.pop(\"requires_grad\", False)\n        assert not kwargs\n        self.wrapper_frame_skip = 1\n        env = self.lib.envs.get_environment(env_name, **kwargs)\n        return super()._build_env(\n            env,\n            pixels_only=pixels_only,\n            from_pixels=from_pixels,\n            requires_grad=requires_grad,\n        )\n\n    @property\n    def env_name(self):\n        return self._constructor_kwargs[\"env_name\"]\n\n    def _check_kwargs(self, kwargs: Dict):\n        if \"env_name\" not in kwargs:\n            raise TypeError(\"Expected 'env_name' to be part of kwargs\")\n\n    def __repr__(self) -> str:\n        return f\"{self.__class__.__name__}(env={self.env_name}, batch_size={self.batch_size}, device={self.device})\"\n\n\nclass _BraxEnvStep(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, env: BraxWrapper, state, action, *qp_values):\n\n        # convert tensors to ndarrays\n        state = _tensordict_to_object(state, env._state_example)\n        action = _tensor_to_ndarray(action)\n\n        # flatten batch size\n        state = _tree_flatten(state, env.batch_size)\n        action = _tree_flatten(action, env.batch_size)\n\n        # call vjp with jit and vmap\n        next_state, vjp_fn = jax.vjp(env._vmap_jit_env_step, state, action)\n\n        # reshape batch size\n        next_state = _tree_reshape(next_state, env.batch_size)\n\n        # convert ndarrays to tensors\n        next_state = _object_to_tensordict(\n            next_state, device=env.device, batch_size=env.batch_size\n        )\n\n        # save context\n        ctx.vjp_fn = vjp_fn\n        ctx.next_state = next_state\n        ctx.env = env\n\n        return (\n            next_state,  # no gradient\n            next_state[\"obs\"],\n            next_state[\"reward\"],\n            *next_state[\"qp\"].values(),\n        )\n\n    @staticmethod\n    def backward(ctx, _, grad_next_obs, grad_next_reward, *grad_next_qp_values):\n\n        # build gradient tensordict with zeros in fields with no grad\n        grad_next_state = TensorDict(\n            source={\n                \"qp\": dict(zip(ctx.next_state[\"qp\"].keys(), grad_next_qp_values)),\n                \"obs\": grad_next_obs,\n                \"reward\": grad_next_reward,\n                \"done\": torch.zeros_like(ctx.next_state[\"done\"]),\n                \"metrics\": {\n                    k: torch.zeros_like(v) for k, v in ctx.next_state[\"metrics\"].items()\n                },\n                \"info\": {\n                    k: torch.zeros_like(v) for k, v in ctx.next_state[\"info\"].items()\n                },\n            },\n            device=ctx.env.device,\n            batch_size=ctx.env.batch_size,\n            _run_checks=False,\n        )\n\n        # convert tensors to ndarrays\n        grad_next_state = _tensordict_to_object(grad_next_state, ctx.env._state_example)\n\n        # flatten batch size\n        grad_next_state = _tree_flatten(grad_next_state, ctx.env.batch_size)\n\n        # call vjp to get gradients\n        grad_state, grad_action = ctx.vjp_fn(grad_next_state)\n\n        # reshape batch size\n        grad_state = _tree_reshape(grad_state, ctx.env.batch_size)", "metadata": {"task_id": "pytorch_rl/14", "ground_truth": "        grad_action = _tree_reshape(grad_action, ctx.env.batch_size)", "fpath_tuple": ["pytorch_rl", "torchrl", "envs", "libs", "brax.py"], "context_start_lineno": 207, "line_no": 398, "query_window": {"context": "                \"info\": {\n                    k: torch.zeros_like(v) for k, v in ctx.next_state[\"info\"].items()\n                },\n            },\n            device=ctx.env.device,\n            batch_size=ctx.env.batch_size,\n            _run_checks=False,\n        )\n\n        # convert tensors to ndarrays\n        grad_next_state = _tensordict_to_object(grad_next_state, ctx.env._state_example)\n\n        # flatten batch size\n        grad_next_state = _tree_flatten(grad_next_state, ctx.env.batch_size)\n\n        # call vjp to get gradients\n        grad_state, grad_action = ctx.vjp_fn(grad_next_state)\n\n        # reshape batch size\n        grad_state = _tree_reshape(grad_state, ctx.env.batch_size)", "metadata": {"fpath_tuple": ["pytorch_rl", "torchrl", "envs", "libs", "brax.py"], "line_no": 398, "task_id": "pytorch_rl/14", "start_line_no": 378, "end_line_no": 398, "window_size": 20, "context_start_lineno": 207, "repo": "pytorch_rl"}}, "top_k_context": [{"context": "        tensordict_out = TensorDict(\n            source={\n                \"observation\": state.get(\"obs\"),\n                \"reward\": reward,\n                \"done\": done,\n                \"state\": state,\n            },\n            batch_size=self.batch_size,\n            device=self.device,\n            _run_checks=False,\n        )\n        return tensordict_out\n\n    def _step_without_grad(self, tensordict: TensorDictBase):\n\n        # convert tensors to ndarrays\n        state = _tensordict_to_object(tensordict.get(\"state\"), self._state_example)\n        action = _tensor_to_ndarray(tensordict.get(\"action\"))\n\n        # flatten batch size", "metadata": [{"fpath_tuple": ["pytorch_rl", "torchrl", "envs", "libs", "brax.py"], "line_no": 196, "start_line_no": 186, "end_line_no": 206, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}], "sim_score": 0.4260869565217391}, {"context": "        reward = state.get(\"reward\").view(*self.reward_spec.shape)\n        done = state.get(\"done\").bool().view(*self.reward_spec.shape)\n        tensordict_out = TensorDict(\n            source={\n                \"observation\": state.get(\"obs\"),\n                \"reward\": reward,\n                \"done\": done,\n                \"state\": state,\n            },\n            batch_size=self.batch_size,\n            device=self.device,\n            _run_checks=False,\n        )\n        return tensordict_out\n\n    def _step_without_grad(self, tensordict: TensorDictBase):\n\n        # convert tensors to ndarrays\n        state = _tensordict_to_object(tensordict.get(\"state\"), self._state_example)\n        action = _tensor_to_ndarray(tensordict.get(\"action\"))", "metadata": [{"fpath_tuple": ["pytorch_rl", "torchrl", "envs", "libs", "brax.py"], "line_no": 194, "start_line_no": 184, "end_line_no": 204, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}], "sim_score": 0.3770491803278688}, {"context": "\n        # reshape batch size\n        state = _tree_reshape(state, self.batch_size)\n        state = _object_to_tensordict(state, self.device, self.batch_size)\n\n        # build result\n        reward = state.get(\"reward\").view(*self.reward_spec.shape)\n        done = state.get(\"done\").bool().view(*self.reward_spec.shape)\n        tensordict_out = TensorDict(\n            source={\n                \"observation\": state.get(\"obs\"),\n                \"reward\": reward,\n                \"done\": done,\n                \"state\": state,\n            },\n            batch_size=self.batch_size,\n            device=self.device,\n            _run_checks=False,\n        )\n        return tensordict_out", "metadata": [{"fpath_tuple": ["pytorch_rl", "torchrl", "envs", "libs", "brax.py"], "line_no": 188, "start_line_no": 178, "end_line_no": 198, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}], "sim_score": 0.34210526315789475}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# test/test_transforms.py\n# --------------------------------------------------\n#     @pytest.mark.parametrize(\"device\", get_available_devices())\n#     @pytest.mark.parametrize(\"batch\", [[], [4], [6, 4]])\n#     def test_binarized_reward(self, device, batch):\n#         torch.manual_seed(0)\n#         br = BinarizeReward()\n#         reward = torch.randn(*batch, 1, device=device)\n#         reward_copy = reward.clone()\n#         misc = torch.randn(*batch, 1, device=device)\n#         misc_copy = misc.clone()\n# \n#         td = TensorDict(\n#             {\"misc\": misc, \"reward\": reward},\n#             batch,\n#             device=device,\n#         )\n#         br(td)\n#         assert (td[\"reward\"] != reward_copy).all()\n#         assert (td[\"misc\"] == misc_copy).all()\n#         assert (torch.count_nonzero(td[\"reward\"]) == torch.sum(reward_copy > 0)).all()\n# \n# --------------------------------------------------\n# the below code fragment can be found in:\n# test/test_transforms.py\n# --------------------------------------------------\n#         br = BinarizeReward()\n#         reward = torch.randn(*batch, 1, device=device)\n#         reward_copy = reward.clone()\n#         misc = torch.randn(*batch, 1, device=device)\n#         misc_copy = misc.clone()\n# \n#         td = TensorDict(\n#             {\"misc\": misc, \"reward\": reward},\n#             batch,\n#             device=device,\n#         )\n#         br(td)\n#         assert (td[\"reward\"] != reward_copy).all()\n#         assert (td[\"misc\"] == misc_copy).all()\n#         assert (torch.count_nonzero(td[\"reward\"]) == torch.sum(reward_copy > 0)).all()\n# \n#     @pytest.mark.parametrize(\"batch\", [[], [2], [2, 4]])\n#     @pytest.mark.parametrize(\"scale\", [0.1, 10])\n#     @pytest.mark.parametrize(\"loc\", [1, 5])\n#     @pytest.mark.parametrize(\"keys\", [None, [\"reward_1\"]])\n# --------------------------------------------------\n# the below code fragment can be found in:\n# test/test_transforms.py\n# --------------------------------------------------\n#     def test_step_counter(self, max_steps, device, batch, reset_workers):\n#         torch.manual_seed(0)\n#         step_counter = StepCounter(max_steps)\n#         td = TensorDict(\n#             {\"done\": torch.zeros(*batch, 1, dtype=torch.bool)}, batch, device=device\n#         )\n#         if reset_workers:\n#             td.set(\"_reset\", torch.randn(batch) < 0)\n#         step_counter.reset(td)\n#         assert not torch.all(td.get(\"step_count\"))\n#         i = 0\n#         while max_steps is None or i < max_steps:\n#             step_counter._step(td)\n#             i += 1\n#             assert torch.all(td.get(\"step_count\") == i), (td.get(\"step_count\"), i)\n#             if max_steps is None:\n#                 break\n#         if max_steps is not None:\n#             assert torch.all(td.get(\"step_count\") == max_steps)\n#             assert torch.all(td.get(\"done\"))\n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\n)}, batch_size=[3])\n        td = tensordict_module(td)\n        assert td.shape == torch.Size([3])\n        assert \"input\" in td.keys()\n        assert \"out_1\" in td.keys()\n        assert \"out_2\" in td.keys()\n        assert \"out_3\" in td.keys()\n        assert td.get(\"out_3\").shape == torch.Size([3, 2])\n\n        # Using \"_\" key to ignore some output\n        tensordict_module = SafeModule(\n            MultiHeadLinear(5, 4, 3, 2),\n            in_keys=[\"input\"],\n            out_keys=[\"_\", \"_\", \"out_3\"],\n        )\n        td = TensorDict({\"input\": torch.randn(3, 5)}, batch_size=[3])\n        td = tensordict_module(td)\n        assert td.shape == torch.Size([3])\n        assert \"input\" in td.keys()\n        assert \"out_3\" in td.keys()\n        assert \"_\" not in td.keys()\n        assert td.get(\"out_3\").shape == torch.Size([3, 2])\n\n    def test_spec_key_warning(self):\n        class MultiHeadLinear(nn.Module):\n            def __init__(self, in_1, out_1, out_2):\n                super().__init__()\n                self.linear_1 = nn.Linear(in_1, out_1)\n                self.linear_2 = nn.Linear(in_1, out_2)\n\n            def forward(self, x):\n                return self.linear_1(x), self.linear_2(x)\n\n        spec_dict = {\n            \"_\": UnboundedContinuousTensorSpec((4,)),\n            \"out_2\": UnboundedContinuousTensorSpec((3,)),\n        }\n\n        # warning due to \"_\" in spec keys\n        with pytest.warns(UserWarning, match='got a spec with key \"_\"'):\n            tensordict_module = SafeModule(\n                MultiHeadLinear(5, 4, 3),\n                in_keys=[\"input\"],\n                out_keys=[\"_\", \"out_2\"],\n                spec=CompositeSpec(**spec_dict),\n            )\n\n    @pytest.mark.parametrize(\"safe\", [True, False])\n    @pytest.mark.parametrize(\"spec_type\", [None, \"bounded\", \"unbounded\"])\n    @pytest.mark.parametrize(\"lazy\", [True, False])\n    def test_stateful(self, safe, spec_type, lazy):\n        torch.manual_seed(0)\n        param_multiplier = 1\n        if lazy:\n            net = nn.LazyLinear(4 * param_multiplier)\n        else:\n            net = nn.Linear(3, 4 * param_multiplier)\n\n        if spec_type is None:\n            spec = None\n        elif spec_type == \"bounded\":\n            spec = BoundedTensorSpec(-0.1, 0.1, 4)\n        elif spec_type == \"unbounded\":\n            spec = UnboundedContinuousTensorSpec(4)\n\n        if safe and spec is None:\n            with pytest.raises(\n                RuntimeError,\n                match=\"is not a valid configuration as the tensor specs are not \"\n                \"specified\",\n            ):\n                tensordict_module = SafeModule(\n                    module=net,\n                    spec=spec,\n                    in_keys=[\"in\"],\n                    out_keys=[\"out\"],\n                    safe=safe,\n                )\n            return\n        else:\n            tensordict_module = SafeModule(\n                module=net,\n                spec=spec,\n                in_keys=[\"in\"],\n                out_keys=[\"out\"],\n                safe=safe,\n            )\n\n        td = TensorDict({\"in\": torch.randn(3, 3)}, [3])\n        tensordict_module(td)\n        assert td.shape == torch.Size([3])\n        assert td.get(\"out\").shape == torch.Size([3, 4])\n\n        # test bounds\n        if not safe and spec_type == \"bounded\":\n            assert ((td.get(\"out\") > 0.1) | (td.get(\"out\") < -0.1)).any()\n        elif safe and spec_type == \"bounded\":\n            assert ((td.get(\"out\") < 0.1) | (td.get(\"out\") > -0.1)).all()\n\n    @pytest.mark.parametrize(\"safe\", [True, False])\n    @pytest.mark.parametrize(\"spec_type\", [None, \"bounded\", \"unbounded\"])\n    @pytest.mark.parametrize(\"out_keys\", [[\"loc\", \"scale\"], [\"loc_1\", \"scale_1\"]])\n    @pytest.mark.parametrize(\"lazy\", [True, False])\n    @pytest.mark.parametrize(\"exp_mode\", [\"mode\", \"random\", None])\n    def test_stateful_probabilistic(self, safe, spec_type, lazy, exp_mode, out_keys):\n        torch.manual_seed(0)\n        param_multiplier = 2\n        if lazy:\n            net = nn.LazyLinear(4 * param_multiplier)\n        else:\n            net = nn.Linear(3, 4 * param_multiplier)\n\n        in_keys = [\"in\"]\n        net = SafeModule(\n            module=NormalParamWrapper(net),\n            spec=None,\n            in_keys=in_keys,\n            out_keys=out_keys,\n        )\n\n        if spec_type is None:\n            spec = None\n        elif spec_type == \"bounded\":\n            spec = BoundedTensorSpec(-0.1, 0.1, 4)\n        elif spec_type == \"unbounded\":\n            spec = UnboundedContinuousTensorSpec(4)\n        else:\n            raise NotImplementedError\n\n        kwargs = {\"distribution_class\": TanhNormal}\n        if out_keys == [\"loc\", \"scale\"]:\n            dist_in_keys = [\"loc\", \"scale\"]\n        elif out_keys == [\"loc_1\", \"scale_1\"]:\n            dist_in_keys = {\"loc\": \"loc_1\", \"scale\": \"scale_1\"}\n        else:\n            raise NotImplementedError\n\n        if safe and spec is None:\n            with pytest.raises(\n                RuntimeError,\n                match=\"is not a valid configuration as the tensor specs are not \"\n                \"specified\",\n            ):\n                prob_module = SafeProbabilisticModule(\n                    in_keys=dist_in_keys,\n                    out_keys=[\"out\"],\n                    spec=spec,\n                    safe=safe,\n                    **kwargs,\n                )\n            return\n        else:\n            prob_module = SafeProbabilisticModule(\n                in_keys=dist_in_keys,\n                out_keys=[\"out\"],\n                spec=spec,\n                safe=safe,\n                **kwargs,\n            )\n\n        tensordict_module = SafeProbabilisticSequential(net, prob_module)\n        td = TensorDict({\"in\": torch.randn(3, 3)}, [3])\n        with set_exploration_mode(exp_mode):\n            tensordict_module(td)\n        assert td.shape == torch.Size([3])\n        assert td.get(\"out\").shape == torch.Size([3, 4])\n\n        # test bounds\n        if not safe and spec_type == \"bounded\":\n            assert ((td.get(\"out\") > 0.1) | (td.get(\"out\") < -0.1)).any()\n        elif safe and spec_type == \"bounded\":\n            assert ((td.get(\"out\") < 0.1) | (td.get(\"out\") > -0.1)).all()\n\n    @pytest.mark.parametrize(\"safe\", [True, False])\n    @pytest.mark.parametrize(\"spec_type\", [None, \"bounded\", \"unbounded\"])\n    def test_functional(self, safe, spec_type):\n        torch.manual_seed(0)", "metadata": {"task_id": "pytorch_rl/38", "ground_truth": "        param_multiplier = 1", "fpath_tuple": ["pytorch_rl", "test", "test_tensordictmodules.py"], "context_start_lineno": 55, "line_no": 232, "query_window": {"context": "                **kwargs,\n            )\n\n        tensordict_module = SafeProbabilisticSequential(net, prob_module)\n        td = TensorDict({\"in\": torch.randn(3, 3)}, [3])\n        with set_exploration_mode(exp_mode):\n            tensordict_module(td)\n        assert td.shape == torch.Size([3])\n        assert td.get(\"out\").shape == torch.Size([3, 4])\n\n        # test bounds\n        if not safe and spec_type == \"bounded\":\n            assert ((td.get(\"out\") > 0.1) | (td.get(\"out\") < -0.1)).any()\n        elif safe and spec_type == \"bounded\":\n            assert ((td.get(\"out\") < 0.1) | (td.get(\"out\") > -0.1)).all()\n\n    @pytest.mark.parametrize(\"safe\", [True, False])\n    @pytest.mark.parametrize(\"spec_type\", [None, \"bounded\", \"unbounded\"])\n    def test_functional(self, safe, spec_type):\n        torch.manual_seed(0)", "metadata": {"fpath_tuple": ["pytorch_rl", "test", "test_tensordictmodules.py"], "line_no": 232, "task_id": "pytorch_rl/38", "start_line_no": 212, "end_line_no": 232, "window_size": 20, "context_start_lineno": 55, "repo": "pytorch_rl"}}, "top_k_context": [{"context": "    @pytest.mark.parametrize(\"max_steps\", [None, 1, 5, 50])\n    @pytest.mark.parametrize(\"reset_workers\", [True, False])\n    def test_step_counter(self, max_steps, device, batch, reset_workers):\n        torch.manual_seed(0)\n        step_counter = StepCounter(max_steps)\n        td = TensorDict(\n            {\"done\": torch.zeros(*batch, 1, dtype=torch.bool)}, batch, device=device\n        )\n        if reset_workers:\n            td.set(\"_reset\", torch.randn(batch) < 0)\n        step_counter.reset(td)\n        assert not torch.all(td.get(\"step_count\"))\n        i = 0\n        while max_steps is None or i < max_steps:\n            step_counter._step(td)\n            i += 1\n            assert torch.all(td.get(\"step_count\") == i), (td.get(\"step_count\"), i)\n            if max_steps is None:\n                break\n        if max_steps is not None:", "metadata": [{"fpath_tuple": ["pytorch_rl", "test", "test_transforms.py"], "line_no": 1828, "start_line_no": 1818, "end_line_no": 1838, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}], "sim_score": 0.37254901960784315}, {"context": "    def test_binarized_reward(self, device, batch):\n        torch.manual_seed(0)\n        br = BinarizeReward()\n        reward = torch.randn(*batch, 1, device=device)\n        reward_copy = reward.clone()\n        misc = torch.randn(*batch, 1, device=device)\n        misc_copy = misc.clone()\n\n        td = TensorDict(\n            {\"misc\": misc, \"reward\": reward},\n            batch,\n            device=device,\n        )\n        br(td)\n        assert (td[\"reward\"] != reward_copy).all()\n        assert (td[\"misc\"] == misc_copy).all()\n        assert (torch.count_nonzero(td[\"reward\"]) == torch.sum(reward_copy > 0)).all()\n\n    @pytest.mark.parametrize(\"batch\", [[], [2], [2, 4]])\n    @pytest.mark.parametrize(\"scale\", [0.1, 10])", "metadata": [{"fpath_tuple": ["pytorch_rl", "test", "test_transforms.py"], "line_no": 1622, "start_line_no": 1612, "end_line_no": 1632, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}], "sim_score": 0.3716216216216216}, {"context": "                assert tensordict[key, \"b\"] is not None\n\n    @pytest.mark.parametrize(\"device\", get_available_devices())\n    @pytest.mark.parametrize(\"batch\", [[], [4], [6, 4]])\n    def test_binarized_reward(self, device, batch):\n        torch.manual_seed(0)\n        br = BinarizeReward()\n        reward = torch.randn(*batch, 1, device=device)\n        reward_copy = reward.clone()\n        misc = torch.randn(*batch, 1, device=device)\n        misc_copy = misc.clone()\n\n        td = TensorDict(\n            {\"misc\": misc, \"reward\": reward},\n            batch,\n            device=device,\n        )\n        br(td)\n        assert (td[\"reward\"] != reward_copy).all()\n        assert (td[\"misc\"] == misc_copy).all()", "metadata": [{"fpath_tuple": ["pytorch_rl", "test", "test_transforms.py"], "line_no": 1618, "start_line_no": 1608, "end_line_no": 1628, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}], "sim_score": 0.3691275167785235}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# test/mocking_classes.py\n# --------------------------------------------------\n#                 shape=batch_size,\n#             )\n#         cls._reward_spec = reward_spec\n#         cls._observation_spec = observation_spec\n#         cls._input_spec = input_spec\n#         cls.from_pixels = from_pixels\n#         return super().__new__(*args, **kwargs)\n# \n#     def _get_in_obs(self, obs):\n#         return obs\n# \n#     def _get_out_obs(self, obs):\n#         return obs\n# \n#     def _reset(self, tensordict: TensorDictBase) -> TensorDictBase:\n#         self.counter += 1\n#         self.step_count = 0\n#         # state = torch.zeros(self.size) + self.counter\n#         if tensordict is None:\n#             tensordict = TensorDict({}, self.batch_size, device=self.device)\n# --------------------------------------------------\n# the below code fragment can be found in:\n# test/mocking_classes.py\n# --------------------------------------------------\n#         done = torch.zeros_like(done).all(-1).unsqueeze(-1)\n#         tensordict.set(\"reward\", reward.to(torch.get_default_dtype()))\n#         tensordict.set(\"done\", done)\n#         return tensordict\n# \n# \n# class ContinuousActionVecMockEnv(_MockEnv):\n#     @classmethod\n#     def __new__(\n#         cls,\n#         *args,\n#         observation_spec=None,\n#         action_spec=None,\n#         input_spec=None,\n#         reward_spec=None,\n#         from_pixels=False,\n#         **kwargs,\n#     ):\n#         batch_size = kwargs.setdefault(\"batch_size\", torch.Size([]))\n#         size = cls.size = 7\n# --------------------------------------------------\n# the below code fragment can be found in:\n# test/mocking_classes.py\n# --------------------------------------------------\n#             cls._out_key = \"observation_orig\"\n#             input_spec = CompositeSpec(\n#                 **{\n#                     cls._out_key: observation_spec[\"observation\"],\n#                     \"action\": action_spec,\n#                 },\n#                 shape=batch_size,\n#             )\n#         cls._reward_spec = reward_spec\n#         cls._observation_spec = observation_spec\n#         cls._input_spec = input_spec\n#         cls.from_pixels = from_pixels\n#         return super().__new__(*args, **kwargs)\n# \n#     def _get_in_obs(self, obs):\n#         return obs\n# \n#     def _get_out_obs(self, obs):\n#         return obs\n# \n# --------------------------------------------------\n# the below code fragment can be found in:\n# test/mocking_classes.py\n# --------------------------------------------------\n#                 **{\n#                     cls._out_key: observation_spec[\"observation\"],\n#                     \"action\": action_spec,\n#                 },\n#                 shape=batch_size,\n#             )\n#         cls._reward_spec = reward_spec\n#         cls._observation_spec = observation_spec\n#         cls._input_spec = input_spec\n#         cls.from_pixels = from_pixels\n#         return super().__new__(*args, **kwargs)\n# \n#     def _get_in_obs(self, obs):\n#         return obs\n# \n#     def _get_out_obs(self, obs):\n#         return obs\n# \n#     def _reset(self, tensordict: TensorDictBase) -> TensorDictBase:\n#         self.counter += 1\n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\nset(\"done\", done)\n        return tensordict\n\n    def _obs_step(self, obs, a):\n        return obs + a / self.maxstep\n\n\nclass DiscreteActionVecPolicy:\n    in_keys = [\"observation\"]\n    out_keys = [\"action\"]\n\n    def _get_in_obs(self, tensordict):\n        obs = tensordict.get(*self.in_keys)\n        return obs\n\n    def __call__(self, tensordict):\n        obs = self._get_in_obs(tensordict)\n        max_obs = (obs == obs.max(dim=-1, keepdim=True)[0]).cumsum(-1).argmax(-1)\n        k = tensordict.get(*self.in_keys).shape[-1]\n        max_obs = (max_obs + 1) % k\n        action = torch.nn.functional.one_hot(max_obs, k)\n        tensordict.set(*self.out_keys, action)\n        return tensordict\n\n\nclass DiscreteActionConvMockEnv(DiscreteActionVecMockEnv):\n    @classmethod\n    def __new__(\n        cls,\n        *args,\n        observation_spec=None,\n        action_spec=None,\n        input_spec=None,\n        reward_spec=None,\n        from_pixels=True,\n        **kwargs,\n    ):\n        batch_size = kwargs.setdefault(\"batch_size\", torch.Size([]))\n        if observation_spec is None:\n            cls.out_key = \"pixels\"\n            observation_spec = CompositeSpec(\n                pixels=UnboundedContinuousTensorSpec(\n                    shape=torch.Size([*batch_size, 1, 7, 7])\n                ),\n                pixels_orig=UnboundedContinuousTensorSpec(\n                    shape=torch.Size([*batch_size, 1, 7, 7])\n                ),\n                shape=batch_size,\n            )\n        if action_spec is None:\n            action_spec = OneHotDiscreteTensorSpec(7, shape=(*batch_size, 7))\n        if reward_spec is None:\n            reward_spec = UnboundedContinuousTensorSpec(shape=(*batch_size, 1))\n\n        if input_spec is None:\n            cls._out_key = \"pixels_orig\"\n            input_spec = CompositeSpec(\n                **{\n                    cls._out_key: observation_spec[\"pixels_orig\"],\n                    \"action\": action_spec,\n                },\n                shape=batch_size,\n            )\n        return super().__new__(\n            *args,\n            observation_spec=observation_spec,\n            action_spec=action_spec,\n            reward_spec=reward_spec,\n            input_spec=input_spec,\n            from_pixels=from_pixels,\n            **kwargs,\n        )\n\n    def _get_out_obs(self, obs):\n        obs = torch.diag_embed(obs, 0, -2, -1).unsqueeze(0)\n        return obs\n\n    def _get_in_obs(self, obs):\n        return obs.diagonal(0, -1, -2).squeeze()\n\n\nclass DiscreteActionConvMockEnvNumpy(DiscreteActionConvMockEnv):\n    @classmethod\n    def __new__(\n        cls,\n        *args,\n        observation_spec=None,\n        action_spec=None,\n        input_spec=None,\n        reward_spec=None,\n        from_pixels=True,\n        categorical_action_encoding=False,\n        **kwargs,\n    ):\n        batch_size = kwargs.setdefault(\"batch_size\", torch.Size([]))\n        if observation_spec is None:\n            cls.out_key = \"pixels\"\n            observation_spec = CompositeSpec(\n                pixels=UnboundedContinuousTensorSpec(\n                    shape=torch.Size([*batch_size, 7, 7, 3])\n                ),\n                pixels_orig=UnboundedContinuousTensorSpec(\n                    shape=torch.Size([*batch_size, 7, 7, 3])\n                ),\n                shape=batch_size,\n            )\n        if action_spec is None:\n            action_spec_cls = (\n                DiscreteTensorSpec\n                if categorical_action_encoding\n                else OneHotDiscreteTensorSpec\n            )\n            action_spec = action_spec_cls(7, shape=(*batch_size, 7))\n        if input_spec is None:\n            cls._out_key = \"pixels_orig\"\n            input_spec = CompositeSpec(\n                **{\n                    cls._out_key: observation_spec[\"pixels_orig\"],\n                    \"action\": action_spec,\n                },\n                shape=batch_size,\n            )\n\n        return super().__new__(\n            *args,\n            observation_spec=observation_spec,\n            action_spec=action_spec,\n            reward_spec=reward_spec,\n            input_spec=input_spec,\n            from_pixels=from_pixels,\n            categorical_action_encoding=categorical_action_encoding,\n            **kwargs,\n        )\n\n    def _get_out_obs(self, obs):\n        obs = torch.diag_embed(obs, 0, -2, -1).unsqueeze(-1)\n        obs = obs.expand(*obs.shape[:-1], 3)\n        return obs\n\n    def _get_in_obs(self, obs):\n        return obs.diagonal(0, -2, -3)[..., 0, :]\n\n    def _obs_step(self, obs, a):\n        return obs + a.unsqueeze(-1) / self.maxstep\n\n\nclass ContinuousActionConvMockEnv(ContinuousActionVecMockEnv):\n    @classmethod\n    def __new__(\n        cls,\n        *args,\n        observation_spec=None,\n        action_spec=None,\n        input_spec=None,\n        reward_spec=None,\n        from_pixels=True,\n        pixel_shape=None,\n        **kwargs,\n    ):\n        batch_size = kwargs.setdefault(\"batch_size\", torch.Size([]))\n        if pixel_shape is None:\n            pixel_shape = [1, 7, 7]\n        if observation_spec is None:\n            cls.out_key = \"pixels\"\n            observation_spec = CompositeSpec(\n                pixels=UnboundedContinuousTensorSpec(\n                    shape=torch.Size([*batch_size, *pixel_shape])\n                ),\n                pixels_orig=UnboundedContinuousTensorSpec(\n                    shape=torch.Size([*batch_size, *pixel_shape])\n                ),\n                shape=batch_size,\n            )\n\n        if action_spec is None:\n            action_spec = BoundedTensorSpec(-1, 1, [*batch_size, pixel_shape[-1]])\n\n        if reward_spec is None:\n            reward_spec = UnboundedContinuousTensorSpec(shape=(*batch_size, 1))\n        if input_spec is None:\n            cls._out_key = \"pixels_orig\"\n            input_spec = CompositeSpec(\n                **{cls._out_key: observation_spec[\"pixels\"], \"action\": action_spec},\n                shape=batch_size,\n            )\n        return super().__new__(\n            *args,\n            observation_spec=observation_spec,\n            action_spec=action_spec,\n            reward_spec=reward_spec,\n            input_spec=input_spec,\n            from_pixels=from_pixels,\n            **kwargs,\n        )\n\n    def _get_out_obs(self, obs):\n        obs = torch.diag_embed(obs, 0, -2, -1)\n        return obs\n\n    def _get_in_obs(self, obs):\n        return obs.diagonal(0, -1, -2).squeeze()\n\n\nclass ContinuousActionConvMockEnvNumpy(ContinuousActionConvMockEnv):\n    @classmethod\n    def __new__(", "metadata": {"task_id": "pytorch_rl/147", "ground_truth": "        cls,", "fpath_tuple": ["pytorch_rl", "test", "mocking_classes.py"], "context_start_lineno": 512, "line_no": 718, "query_window": {"context": "            *args,\n            observation_spec=observation_spec,\n            action_spec=action_spec,\n            reward_spec=reward_spec,\n            input_spec=input_spec,\n            from_pixels=from_pixels,\n            **kwargs,\n        )\n\n    def _get_out_obs(self, obs):\n        obs = torch.diag_embed(obs, 0, -2, -1)\n        return obs\n\n    def _get_in_obs(self, obs):\n        return obs.diagonal(0, -1, -2).squeeze()\n\n\nclass ContinuousActionConvMockEnvNumpy(ContinuousActionConvMockEnv):\n    @classmethod\n    def __new__(", "metadata": {"fpath_tuple": ["pytorch_rl", "test", "mocking_classes.py"], "line_no": 718, "task_id": "pytorch_rl/147", "start_line_no": 698, "end_line_no": 718, "window_size": 20, "context_start_lineno": 512, "repo": "pytorch_rl"}}, "top_k_context": [{"context": "            cls._out_key = \"observation_orig\"\n            input_spec = CompositeSpec(\n                **{\n                    cls._out_key: observation_spec[\"observation\"],\n                    \"action\": action_spec,\n                },\n                shape=batch_size,\n            )\n        cls._reward_spec = reward_spec\n        cls._observation_spec = observation_spec\n        cls._input_spec = input_spec\n        cls.from_pixels = from_pixels\n        return super().__new__(*args, **kwargs)\n\n    def _get_in_obs(self, obs):\n        return obs\n\n    def _get_out_obs(self, obs):\n        return obs\n", "metadata": [{"fpath_tuple": ["pytorch_rl", "test", "mocking_classes.py"], "line_no": 470, "start_line_no": 460, "end_line_no": 480, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}], "sim_score": 0.4489795918367347}, {"context": "\n        if input_spec is None:\n            cls._out_key = \"observation_orig\"\n            input_spec = CompositeSpec(\n                **{\n                    cls._out_key: observation_spec[\"observation\"],\n                    \"action\": action_spec,\n                },\n                shape=batch_size,\n            )\n        cls._reward_spec = reward_spec\n        cls._observation_spec = observation_spec\n        cls._input_spec = input_spec\n        cls.from_pixels = from_pixels\n        return super().__new__(*args, **kwargs)\n\n    def _get_in_obs(self, obs):\n        return obs\n\n    def _get_out_obs(self, obs):", "metadata": [{"fpath_tuple": ["pytorch_rl", "test", "mocking_classes.py"], "line_no": 468, "start_line_no": 458, "end_line_no": 478, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}], "sim_score": 0.43564356435643564}, {"context": "        reward = done.any(-1).unsqueeze(-1)\n        # set done to False\n        done = torch.zeros_like(done).all(-1).unsqueeze(-1)\n        tensordict.set(\"reward\", reward.to(torch.get_default_dtype()))\n        tensordict.set(\"done\", done)\n        return tensordict\n\n\nclass ContinuousActionVecMockEnv(_MockEnv):\n    @classmethod\n    def __new__(\n        cls,\n        *args,\n        observation_spec=None,\n        action_spec=None,\n        input_spec=None,\n        reward_spec=None,\n        from_pixels=False,\n        **kwargs,\n    ):", "metadata": [{"fpath_tuple": ["pytorch_rl", "test", "mocking_classes.py"], "line_no": 424, "start_line_no": 414, "end_line_no": 434, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}], "sim_score": 0.42727272727272725}, {"context": "                    \"action\": action_spec,\n                },\n                shape=batch_size,\n            )\n        cls._reward_spec = reward_spec\n        cls._observation_spec = observation_spec\n        cls._input_spec = input_spec\n        cls.from_pixels = from_pixels\n        return super().__new__(*args, **kwargs)\n\n    def _get_in_obs(self, obs):\n        return obs\n\n    def _get_out_obs(self, obs):\n        return obs\n\n    def _reset(self, tensordict: TensorDictBase) -> TensorDictBase:\n        self.counter += 1\n        self.step_count = 0\n        # state = torch.zeros(self.size) + self.counter", "metadata": [{"fpath_tuple": ["pytorch_rl", "test", "mocking_classes.py"], "line_no": 474, "start_line_no": 464, "end_line_no": 484, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}], "sim_score": 0.41818181818181815}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# metrics/bleu/bleu.py\n# --------------------------------------------------\n# Neither intelligibility nor grammatical correctness are not taken into account.\n# \"\"\"\n# \n# _KWARGS_DESCRIPTION = \"\"\"\n# Computes BLEU score of translated segments against one or more references.\n# Args:\n#     predictions: list of translations to score.\n#     references: list of lists of or just a list of references for each translation.\n#     tokenizer : approach used for tokenizing `predictions` and `references`.\n#         The default tokenizer is `tokenizer_13a`, a minimal tokenization approach that is equivalent to `mteval-v13a`, used by WMT.\n#         This can be replaced by any function that takes a string as input and returns a list of tokens as output.\n#     max_order: Maximum n-gram order to use when computing BLEU score.\n#     smooth: Whether or not to apply Lin et al. 2004 smoothing.\n# Returns:\n#     'bleu': bleu score,\n#     'precisions': geometric mean of n-gram precisions,\n#     'brevity_penalty': brevity penalty,\n#     'length_ratio': ratio of lengths,\n#     'translation_length': translation_length,\n#     'reference_length': reference_length\n# --------------------------------------------------\n# the below code fragment can be found in:\n# metrics/bleu/bleu.py\n# --------------------------------------------------\n# Computes BLEU score of translated segments against one or more references.\n# Args:\n#     predictions: list of translations to score.\n#     references: list of lists of or just a list of references for each translation.\n#     tokenizer : approach used for tokenizing `predictions` and `references`.\n#         The default tokenizer is `tokenizer_13a`, a minimal tokenization approach that is equivalent to `mteval-v13a`, used by WMT.\n#         This can be replaced by any function that takes a string as input and returns a list of tokens as output.\n#     max_order: Maximum n-gram order to use when computing BLEU score.\n#     smooth: Whether or not to apply Lin et al. 2004 smoothing.\n# Returns:\n#     'bleu': bleu score,\n#     'precisions': geometric mean of n-gram precisions,\n#     'brevity_penalty': brevity penalty,\n#     'length_ratio': ratio of lengths,\n#     'translation_length': translation_length,\n#     'reference_length': reference_length\n# Examples:\n# \n#     >>> predictions = [\"hello there general kenobi\", \"foo bar foobar\"]\n#     >>> references = [\n# --------------------------------------------------\n# the below code fragment can be found in:\n# metrics/bleu/bleu.py\n# --------------------------------------------------\n# \n# _KWARGS_DESCRIPTION = \"\"\"\n# Computes BLEU score of translated segments against one or more references.\n# Args:\n#     predictions: list of translations to score.\n#     references: list of lists of or just a list of references for each translation.\n#     tokenizer : approach used for tokenizing `predictions` and `references`.\n#         The default tokenizer is `tokenizer_13a`, a minimal tokenization approach that is equivalent to `mteval-v13a`, used by WMT.\n#         This can be replaced by any function that takes a string as input and returns a list of tokens as output.\n#     max_order: Maximum n-gram order to use when computing BLEU score.\n#     smooth: Whether or not to apply Lin et al. 2004 smoothing.\n# Returns:\n#     'bleu': bleu score,\n#     'precisions': geometric mean of n-gram precisions,\n#     'brevity_penalty': brevity penalty,\n#     'length_ratio': ratio of lengths,\n#     'translation_length': translation_length,\n#     'reference_length': reference_length\n# Examples:\n# \n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\n# Copyright 2020 The HuggingFace Datasets Authors and the current dataset script contributor.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"NLTK's NIST implementation on both the sentence and corpus level\"\"\"\nfrom typing import Dict, Optional\n\nimport datasets\nimport nltk\nfrom datasets import Sequence, Value\n\n\ntry:\n    nltk.data.find(\"perluniprops\")\nexcept LookupError:\n    nltk.download(\"perluniprops\", quiet=True)  # NISTTokenizer requirement\n\nfrom nltk.tokenize.nist import NISTTokenizer\nfrom nltk.translate.nist_score import corpus_nist, sentence_nist\n\nimport evaluate\n\n\n_CITATION = \"\"\"\\\n@inproceedings{10.5555/1289189.1289273,\n    author = {Doddington, George},\n    title = {Automatic Evaluation of Machine Translation Quality Using N-Gram Co-Occurrence Statistics},\n    year = {2002},\n    publisher = {Morgan Kaufmann Publishers Inc.},\n    address = {San Francisco, CA, USA},\n    booktitle = {Proceedings of the Second International Conference on Human Language Technology Research},\n    pages = {138\u2013145},\n    numpages = {8},\n    location = {San Diego, California},\n    series = {HLT '02}\n}\n\"\"\"\n\n_DESCRIPTION = \"\"\"\\\nDARPA commissioned NIST to develop an MT evaluation facility based on the BLEU\nscore. The official script used by NIST to compute BLEU and NIST score is\nmteval-14.pl. The main differences are:\n\n - BLEU uses geometric mean of the ngram precisions, NIST uses arithmetic mean.\n - NIST has a different brevity penalty\n - NIST score from mteval-14.pl has a self-contained tokenizer (in the Hugging Face implementation we rely on NLTK's\nimplementation of the NIST-specific tokenizer)\n\"\"\"\n\n\n_KWARGS_DESCRIPTION = \"\"\"", "metadata": {"task_id": "huggingface_evaluate/199", "ground_truth": "Computes NIST score of translated segments against one or more references.", "fpath_tuple": ["huggingface_evaluate", "metrics", "nist_mt", "nist_mt.py"], "context_start_lineno": 0, "line_no": 60, "query_window": {"context": "    pages = {138\u2013145},\n    numpages = {8},\n    location = {San Diego, California},\n    series = {HLT '02}\n}\n\"\"\"\n\n_DESCRIPTION = \"\"\"\\\nDARPA commissioned NIST to develop an MT evaluation facility based on the BLEU\nscore. The official script used by NIST to compute BLEU and NIST score is\nmteval-14.pl. The main differences are:\n\n - BLEU uses geometric mean of the ngram precisions, NIST uses arithmetic mean.\n - NIST has a different brevity penalty\n - NIST score from mteval-14.pl has a self-contained tokenizer (in the Hugging Face implementation we rely on NLTK's\nimplementation of the NIST-specific tokenizer)\n\"\"\"\n\n\n_KWARGS_DESCRIPTION = \"\"\"", "metadata": {"fpath_tuple": ["huggingface_evaluate", "metrics", "nist_mt", "nist_mt.py"], "line_no": 60, "task_id": "huggingface_evaluate/199", "start_line_no": 40, "end_line_no": 60, "window_size": 20, "context_start_lineno": 0, "repo": "huggingface_evaluate"}}, "top_k_context": [{"context": "Neither intelligibility nor grammatical correctness are not taken into account.\n\"\"\"\n\n_KWARGS_DESCRIPTION = \"\"\"\nComputes BLEU score of translated segments against one or more references.\nArgs:\n    predictions: list of translations to score.\n    references: list of lists of or just a list of references for each translation.\n    tokenizer : approach used for tokenizing `predictions` and `references`.\n        The default tokenizer is `tokenizer_13a`, a minimal tokenization approach that is equivalent to `mteval-v13a`, used by WMT.\n        This can be replaced by any function that takes a string as input and returns a list of tokens as output.\n    max_order: Maximum n-gram order to use when computing BLEU score.\n    smooth: Whether or not to apply Lin et al. 2004 smoothing.\nReturns:\n    'bleu': bleu score,\n    'precisions': geometric mean of n-gram precisions,\n    'brevity_penalty': brevity penalty,\n    'length_ratio': ratio of lengths,\n    'translation_length': translation_length,\n    'reference_length': reference_length", "metadata": [{"fpath_tuple": ["huggingface_evaluate", "metrics", "bleu", "bleu.py"], "line_no": 62, "start_line_no": 52, "end_line_no": 72, "window_size": 20, "repo": "huggingface_evaluate", "slice_size": 10}], "sim_score": 0.20772946859903382}, {"context": "\n_KWARGS_DESCRIPTION = \"\"\"\nComputes BLEU score of translated segments against one or more references.\nArgs:\n    predictions: list of translations to score.\n    references: list of lists of or just a list of references for each translation.\n    tokenizer : approach used for tokenizing `predictions` and `references`.\n        The default tokenizer is `tokenizer_13a`, a minimal tokenization approach that is equivalent to `mteval-v13a`, used by WMT.\n        This can be replaced by any function that takes a string as input and returns a list of tokens as output.\n    max_order: Maximum n-gram order to use when computing BLEU score.\n    smooth: Whether or not to apply Lin et al. 2004 smoothing.\nReturns:\n    'bleu': bleu score,\n    'precisions': geometric mean of n-gram precisions,\n    'brevity_penalty': brevity penalty,\n    'length_ratio': ratio of lengths,\n    'translation_length': translation_length,\n    'reference_length': reference_length\nExamples:\n", "metadata": [{"fpath_tuple": ["huggingface_evaluate", "metrics", "bleu", "bleu.py"], "line_no": 64, "start_line_no": 54, "end_line_no": 74, "window_size": 20, "repo": "huggingface_evaluate", "slice_size": 10}], "sim_score": 0.20707070707070707}, {"context": "Scores are calculated for individual translated segments\u2014generally sentences\u2014by comparing them with a set of good quality reference translations.\nThose scores are then averaged over the whole corpus to reach an estimate of the translation's overall quality.\nNeither intelligibility nor grammatical correctness are not taken into account.\n\"\"\"\n\n_KWARGS_DESCRIPTION = \"\"\"\nComputes BLEU score of translated segments against one or more references.\nArgs:\n    predictions: list of translations to score.\n    references: list of lists of or just a list of references for each translation.\n    tokenizer : approach used for tokenizing `predictions` and `references`.\n        The default tokenizer is `tokenizer_13a`, a minimal tokenization approach that is equivalent to `mteval-v13a`, used by WMT.\n        This can be replaced by any function that takes a string as input and returns a list of tokens as output.\n    max_order: Maximum n-gram order to use when computing BLEU score.\n    smooth: Whether or not to apply Lin et al. 2004 smoothing.\nReturns:\n    'bleu': bleu score,\n    'precisions': geometric mean of n-gram precisions,\n    'brevity_penalty': brevity penalty,\n    'length_ratio': ratio of lengths,", "metadata": [{"fpath_tuple": ["huggingface_evaluate", "metrics", "bleu", "bleu.py"], "line_no": 60, "start_line_no": 50, "end_line_no": 70, "window_size": 20, "repo": "huggingface_evaluate", "slice_size": 10}], "sim_score": 0.2}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# metrics/wiki_split/wiki_split.py\n# --------------------------------------------------\n#     def _info(self):\n#         return evaluate.MetricInfo(\n#             description=_DESCRIPTION,\n#             citation=_CITATION,\n#             inputs_description=_KWARGS_DESCRIPTION,\n#             features=[\n#                 datasets.Features(\n#                     {\n#                         \"predictions\": datasets.Value(\"string\", id=\"sequence\"),\n#                         \"references\": datasets.Sequence(datasets.Value(\"string\", id=\"sequence\"), id=\"references\"),\n#                     }\n#                 ),\n#                 datasets.Features(\n#                     {\n#                         \"predictions\": datasets.Value(\"string\", id=\"sequence\"),\n#                         \"references\": datasets.Value(\"string\", id=\"sequence\"),\n#                     }\n#                 ),\n#             ],\n#             codebase_urls=[\n# --------------------------------------------------\n# the below code fragment can be found in:\n# metrics/charcut_mt/charcut_mt.py\n# --------------------------------------------------\n# @evaluate.utils.file_utils.add_start_docstrings(_DESCRIPTION, _KWARGS_DESCRIPTION)\n# class Charcut(evaluate.Metric):\n#     \"\"\"Character-based MT evaluation.\"\"\"\n# \n#     def _info(self):\n#         return evaluate.MetricInfo(\n#             # This is the description that will appear on the modules page.\n#             module_type=\"metric\",\n#             description=_DESCRIPTION,\n#             citation=_CITATION,\n#             inputs_description=_KWARGS_DESCRIPTION,\n#             # This defines the format of each prediction and reference\n#             features=[\n#                 datasets.Features(\n#                     {\"predictions\": Value(\"string\", id=\"prediction\"), \"references\": Value(\"string\", id=\"reference\")}\n#                 ),\n#             ],\n#             # Homepage of the module for documentation\n#             homepage=\"https://github.com/BramVanroy/CharCut\",\n#             # Additional links to the codebase or references\n# --------------------------------------------------\n# the below code fragment can be found in:\n# metrics/nist_mt/nist_mt.py\n# --------------------------------------------------\n# @evaluate.utils.file_utils.add_start_docstrings(_DESCRIPTION, _KWARGS_DESCRIPTION)\n# class NistMt(evaluate.Metric):\n#     \"\"\"A wrapper around NLTK's NIST implementation.\"\"\"\n# \n#     def _info(self):\n#         return evaluate.MetricInfo(\n#             module_type=\"metric\",\n#             description=_DESCRIPTION,\n#             citation=_CITATION,\n#             inputs_description=_KWARGS_DESCRIPTION,\n#             features=[\n#                 datasets.Features(\n#                     {\n#                         \"predictions\": Value(\"string\", id=\"prediction\"),\n#                         \"references\": Sequence(Value(\"string\", id=\"reference\"), id=\"references\"),\n#                     }\n#                 ),\n#                 datasets.Features(\n#                     {\n#                         \"predictions\": Value(\"string\", id=\"prediction\"),\n# --------------------------------------------------\n# the below code fragment can be found in:\n# metrics/nist_mt/nist_mt.py\n# --------------------------------------------------\n#     \"\"\"A wrapper around NLTK's NIST implementation.\"\"\"\n# \n#     def _info(self):\n#         return evaluate.MetricInfo(\n#             module_type=\"metric\",\n#             description=_DESCRIPTION,\n#             citation=_CITATION,\n#             inputs_description=_KWARGS_DESCRIPTION,\n#             features=[\n#                 datasets.Features(\n#                     {\n#                         \"predictions\": Value(\"string\", id=\"prediction\"),\n#                         \"references\": Sequence(Value(\"string\", id=\"reference\"), id=\"references\"),\n#                     }\n#                 ),\n#                 datasets.Features(\n#                     {\n#                         \"predictions\": Value(\"string\", id=\"prediction\"),\n#                         \"references\": Value(\"string\", id=\"reference\"),\n#                     }\n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\n# Copyright 2020 The HuggingFace Datasets Authors and the current dataset script contributor.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"CharacTER metric, a character-based TER variant, for machine translation.\"\"\"\nimport math\nfrom statistics import mean, median\nfrom typing import Iterable, List, Union\n\nimport cer\nimport datasets\nfrom cer import calculate_cer\nfrom datasets import Sequence, Value\n\nimport evaluate\n\n\n_CITATION = \"\"\"\\\n@inproceedings{wang-etal-2016-character,\n    title = \"{C}harac{T}er: Translation Edit Rate on Character Level\",\n    author = \"Wang, Weiyue  and\n      Peter, Jan-Thorsten  and\n      Rosendahl, Hendrik  and\n      Ney, Hermann\",\n    booktitle = \"Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers\",\n    month = aug,\n    year = \"2016\",\n    address = \"Berlin, Germany\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/W16-2342\",\n    doi = \"10.18653/v1/W16-2342\",\n    pages = \"505--510\",\n}\n\"\"\"\n\n_DESCRIPTION = \"\"\"\\\nCharacTer is a character-level metric inspired by the commonly applied translation edit rate (TER). It is\ndefined as the minimum number of character edits required to adjust a hypothesis, until it completely matches the\nreference, normalized by the length of the hypothesis sentence. CharacTer calculates the character level edit\ndistance while performing the shift edit on word level. Unlike the strict matching criterion in TER, a hypothesis\nword is considered to match a reference word and could be shifted, if the edit distance between them is below a\nthreshold value. The Levenshtein distance between the reference and the shifted hypothesis sequence is computed on the\ncharacter level. In addition, the lengths of hypothesis sequences instead of reference sequences are used for\nnormalizing the edit distance, which effectively counters the issue that shorter translations normally achieve lower\nTER.\"\"\"\n\n_KWARGS_DESCRIPTION = \"\"\"\nCalculates how good the predictions are in terms of the CharacTER metric given some references.\nArgs:\n    predictions: a list of predictions to score. Each prediction should be a string with\n     tokens separated by spaces.\n    references: a list of references for each prediction. You can also pass multiple references for each prediction,\n     so a list and in that list a sublist for each prediction for its related references. When multiple references are\n     given, the lowest (best) score is returned for that prediction-references pair.\n     Each reference should be a string with tokens separated by spaces.\n    aggregate: one of \"mean\", \"sum\", \"median\" to indicate how the scores of individual sentences should be\n     aggregated\n    return_all_scores: a boolean, indicating whether in addition to the aggregated score, also all individual\n     scores should be returned\nReturns:\n    cer_score: an aggregated score across all the items, based on 'aggregate'\n    cer_scores: (optionally, if 'return_all_scores' evaluates to True) a list of all scores, one per ref/hyp pair\nExamples:\n    >>> character_mt = evaluate.load(\"character\")\n    >>> preds = [\"this week the saudis denied information published in the new york times\"]\n    >>> refs = [\"saudi arabia denied this week information published in the american new york times\"]\n    >>> character_mt.compute(references=refs, predictions=preds)\n    {'cer_score': 0.36619718309859156}\n    >>> preds = [\"this week the saudis denied information published in the new york times\",\n    ...          \"this is in fact an estimate\"]\n    >>> refs = [\"saudi arabia denied this week information published in the american new york times\",\n    ...         \"this is actually an estimate\"]\n    >>> character_mt.compute(references=refs, predictions=preds, aggregate=\"sum\", return_all_scores=True)\n    {'cer_score': 0.6254564423578508, 'cer_scores': [0.36619718309859156, 0.25925925925925924]}\n    >>> preds = [\"this week the saudis denied information published in the new york times\"]\n    >>> refs = [[\"saudi arabia denied this week information published in the american new york times\",\n    ...          \"the saudis have denied new information published in the ny times\"]]\n    >>> character_mt.compute(references=refs, predictions=preds, aggregate=\"median\", return_all_scores=True)\n    {'cer_score': 0.36619718309859156, 'cer_scores': [0.36619718309859156]}\n\"\"\"\n\n\n@evaluate.utils.file_utils.add_start_docstrings(_DESCRIPTION, _KWARGS_DESCRIPTION)\nclass Character(evaluate.Metric):\n    \"\"\"CharacTer is a character-level metric inspired by the commonly applied translation edit rate (TER).\"\"\"\n\n    def _info(self):\n        return evaluate.MetricInfo(\n            module_type=\"metric\",\n            description=_DESCRIPTION,\n            citation=_CITATION,\n            inputs_description=_KWARGS_DESCRIPTION,\n            features=[\n                datasets.Features(\n                    {\"predictions\": Value(\"string\", id=\"prediction\"), \"references\": Value(\"string\", id=\"reference\")}\n                ),\n                datasets.Features(\n                    {\n                        \"predictions\": Value(\"string\", id=\"prediction\"),", "metadata": {"task_id": "huggingface_evaluate/192", "ground_truth": "                        \"references\": Sequence(Value(\"string\", id=\"reference\"), id=\"references\"),", "fpath_tuple": ["huggingface_evaluate", "metrics", "character", "character.py"], "context_start_lineno": 0, "line_no": 108, "query_window": {"context": "\"\"\"\n\n\n@evaluate.utils.file_utils.add_start_docstrings(_DESCRIPTION, _KWARGS_DESCRIPTION)\nclass Character(evaluate.Metric):\n    \"\"\"CharacTer is a character-level metric inspired by the commonly applied translation edit rate (TER).\"\"\"\n\n    def _info(self):\n        return evaluate.MetricInfo(\n            module_type=\"metric\",\n            description=_DESCRIPTION,\n            citation=_CITATION,\n            inputs_description=_KWARGS_DESCRIPTION,\n            features=[\n                datasets.Features(\n                    {\"predictions\": Value(\"string\", id=\"prediction\"), \"references\": Value(\"string\", id=\"reference\")}\n                ),\n                datasets.Features(\n                    {\n                        \"predictions\": Value(\"string\", id=\"prediction\"),", "metadata": {"fpath_tuple": ["huggingface_evaluate", "metrics", "character", "character.py"], "line_no": 108, "task_id": "huggingface_evaluate/192", "start_line_no": 88, "end_line_no": 108, "window_size": 20, "context_start_lineno": 0, "repo": "huggingface_evaluate"}}, "top_k_context": [{"context": "@evaluate.utils.file_utils.add_start_docstrings(_DESCRIPTION, _KWARGS_DESCRIPTION)\nclass NistMt(evaluate.Metric):\n    \"\"\"A wrapper around NLTK's NIST implementation.\"\"\"\n\n    def _info(self):\n        return evaluate.MetricInfo(\n            module_type=\"metric\",\n            description=_DESCRIPTION,\n            citation=_CITATION,\n            inputs_description=_KWARGS_DESCRIPTION,\n            features=[\n                datasets.Features(\n                    {\n                        \"predictions\": Value(\"string\", id=\"prediction\"),\n                        \"references\": Sequence(Value(\"string\", id=\"reference\"), id=\"references\"),\n                    }\n                ),\n                datasets.Features(\n                    {\n                        \"predictions\": Value(\"string\", id=\"prediction\"),", "metadata": [{"fpath_tuple": ["huggingface_evaluate", "metrics", "nist_mt", "nist_mt.py"], "line_no": 94, "start_line_no": 84, "end_line_no": 104, "window_size": 20, "repo": "huggingface_evaluate", "slice_size": 10}], "sim_score": 0.6371681415929203}, {"context": "\n\n@evaluate.utils.file_utils.add_start_docstrings(_DESCRIPTION, _KWARGS_DESCRIPTION)\nclass NistMt(evaluate.Metric):\n    \"\"\"A wrapper around NLTK's NIST implementation.\"\"\"\n\n    def _info(self):\n        return evaluate.MetricInfo(\n            module_type=\"metric\",\n            description=_DESCRIPTION,\n            citation=_CITATION,\n            inputs_description=_KWARGS_DESCRIPTION,\n            features=[\n                datasets.Features(\n                    {\n                        \"predictions\": Value(\"string\", id=\"prediction\"),\n                        \"references\": Sequence(Value(\"string\", id=\"reference\"), id=\"references\"),\n                    }\n                ),\n                datasets.Features(", "metadata": [{"fpath_tuple": ["huggingface_evaluate", "metrics", "nist_mt", "nist_mt.py"], "line_no": 92, "start_line_no": 82, "end_line_no": 102, "window_size": 20, "repo": "huggingface_evaluate", "slice_size": 10}], "sim_score": 0.6371681415929203}, {"context": "\n\n@evaluate.utils.file_utils.add_start_docstrings(_DESCRIPTION, _KWARGS_DESCRIPTION)\nclass Charcut(evaluate.Metric):\n    \"\"\"Character-based MT evaluation.\"\"\"\n\n    def _info(self):\n        return evaluate.MetricInfo(\n            # This is the description that will appear on the modules page.\n            module_type=\"metric\",\n            description=_DESCRIPTION,\n            citation=_CITATION,\n            inputs_description=_KWARGS_DESCRIPTION,\n            # This defines the format of each prediction and reference\n            features=[\n                datasets.Features(\n                    {\"predictions\": Value(\"string\", id=\"prediction\"), \"references\": Value(\"string\", id=\"reference\")}\n                ),\n            ],\n            # Homepage of the module for documentation", "metadata": [{"fpath_tuple": ["huggingface_evaluate", "metrics", "charcut_mt", "charcut_mt.py"], "line_no": 72, "start_line_no": 62, "end_line_no": 82, "window_size": 20, "repo": "huggingface_evaluate", "slice_size": 10}], "sim_score": 0.6129032258064516}, {"context": "@evaluate.utils.file_utils.add_start_docstrings(_DESCRIPTION, _KWARGS_DESCRIPTION)\nclass WikiSplit(evaluate.Metric):\n    def _info(self):\n        return evaluate.MetricInfo(\n            description=_DESCRIPTION,\n            citation=_CITATION,\n            inputs_description=_KWARGS_DESCRIPTION,\n            features=[\n                datasets.Features(\n                    {\n                        \"predictions\": datasets.Value(\"string\", id=\"sequence\"),\n                        \"references\": datasets.Sequence(datasets.Value(\"string\", id=\"sequence\"), id=\"references\"),\n                    }\n                ),\n                datasets.Features(\n                    {\n                        \"predictions\": datasets.Value(\"string\", id=\"sequence\"),\n                        \"references\": datasets.Value(\"string\", id=\"sequence\"),\n                    }\n                ),", "metadata": [{"fpath_tuple": ["huggingface_evaluate", "metrics", "wiki_split", "wiki_split.py"], "line_no": 332, "start_line_no": 322, "end_line_no": 342, "window_size": 20, "repo": "huggingface_evaluate", "slice_size": 10}], "sim_score": 0.5887850467289719}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# torchrl/envs/libs/jumanji.py\n# --------------------------------------------------\n#         >>> env = JumanjiEnv(env_name=\"Snake-6x6-v0\", frame_skip=4)\n#         >>> td = env.rand_step()\n#         >>> print(td)\n#         >>> print(env.available_envs)\n#     \"\"\"\n# \n#     def __init__(self, env_name, **kwargs):\n#         kwargs[\"env_name\"] = env_name\n#         super().__init__(**kwargs)\n# \n#     def _build_env(\n#         self,\n#         env_name: str,\n#         **kwargs,\n#     ) -> \"jumanji.env.Environment\":\n#         if not _has_jumanji:\n#             raise RuntimeError(\n#                 f\"jumanji not found, unable to create {env_name}. \"\n#                 f\"Consider installing jumanji. More info:\"\n#                 f\" {self.git_url}.\"\n# --------------------------------------------------\n# the below code fragment can be found in:\n# torchrl/envs/libs/jumanji.py\n# --------------------------------------------------\n#         >>> print(td)\n#         >>> print(env.available_envs)\n#     \"\"\"\n# \n#     def __init__(self, env_name, **kwargs):\n#         kwargs[\"env_name\"] = env_name\n#         super().__init__(**kwargs)\n# \n#     def _build_env(\n#         self,\n#         env_name: str,\n#         **kwargs,\n#     ) -> \"jumanji.env.Environment\":\n#         if not _has_jumanji:\n#             raise RuntimeError(\n#                 f\"jumanji not found, unable to create {env_name}. \"\n#                 f\"Consider installing jumanji. More info:\"\n#                 f\" {self.git_url}.\"\n#             ) from IMPORT_ERR\n#         from_pixels = kwargs.pop(\"from_pixels\", False)\n# --------------------------------------------------\n# the below code fragment can be found in:\n# torchrl/envs/libs/jumanji.py\n# --------------------------------------------------\n#     \"\"\"\n# \n#     def __init__(self, env_name, **kwargs):\n#         kwargs[\"env_name\"] = env_name\n#         super().__init__(**kwargs)\n# \n#     def _build_env(\n#         self,\n#         env_name: str,\n#         **kwargs,\n#     ) -> \"jumanji.env.Environment\":\n#         if not _has_jumanji:\n#             raise RuntimeError(\n#                 f\"jumanji not found, unable to create {env_name}. \"\n#                 f\"Consider installing jumanji. More info:\"\n#                 f\" {self.git_url}.\"\n#             ) from IMPORT_ERR\n#         from_pixels = kwargs.pop(\"from_pixels\", False)\n#         pixels_only = kwargs.pop(\"pixels_only\", True)\n#         assert not kwargs\n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\n \"brax.envs.env.Env\"):\n        key = jax.random.PRNGKey(0)\n        state = env.reset(key)\n        state_dict = _object_to_tensordict(state, self.device, batch_size=())\n        state_spec = _extract_spec(state_dict).expand(self.batch_size)\n        return state_spec\n\n    def _make_specs(self, env: \"brax.envs.env.Env\") -> None:  # noqa: F821\n        self.input_spec = CompositeSpec(\n            action=BoundedTensorSpec(\n                minimum=-1,\n                maximum=1,\n                shape=(\n                    *self.batch_size,\n                    env.action_size,\n                ),\n                device=self.device,\n            ),\n            shape=self.batch_size,\n        )\n        self.reward_spec = UnboundedContinuousTensorSpec(\n            shape=[\n                *self.batch_size,\n                1,\n            ],\n            device=self.device,\n        )\n        self.observation_spec = CompositeSpec(\n            observation=UnboundedContinuousTensorSpec(\n                shape=(\n                    *self.batch_size,\n                    env.observation_size,\n                ),\n                device=self.device,\n            ),\n            shape=self.batch_size,\n        )\n        # extract state spec from instance\n        self.state_spec = self._make_state_spec(env)\n        self.input_spec[\"state\"] = self.state_spec\n\n    def _make_state_example(self):\n        key = jax.random.PRNGKey(0)\n        keys = jax.random.split(key, self.batch_size.numel())\n        state = self._vmap_jit_env_reset(jax.numpy.stack(keys))\n        state = _tree_reshape(state, self.batch_size)\n        return state\n\n    def _init_env(self) -> Optional[int]:\n        self._key = None\n        self._vmap_jit_env_reset = jax.vmap(jax.jit(self._env.reset))\n        self._vmap_jit_env_step = jax.vmap(jax.jit(self._env.step))\n        self._state_example = self._make_state_example()\n\n    def _set_seed(self, seed: int):\n        if seed is None:\n            raise Exception(\"Brax requires an integer seed.\")\n        self._key = jax.random.PRNGKey(seed)\n\n    def _reset(self, tensordict: TensorDictBase = None, **kwargs) -> TensorDictBase:\n\n        # generate random keys\n        self._key, *keys = jax.random.split(self._key, 1 + self.numel())\n\n        # call env reset with jit and vmap\n        state = self._vmap_jit_env_reset(jax.numpy.stack(keys))\n\n        # reshape batch size\n        state = _tree_reshape(state, self.batch_size)\n        state = _object_to_tensordict(state, self.device, self.batch_size)\n\n        # build result\n        reward = state.get(\"reward\").view(*self.reward_spec.shape)\n        done = state.get(\"done\").bool().view(*self.reward_spec.shape)\n        tensordict_out = TensorDict(\n            source={\n                \"observation\": state.get(\"obs\"),\n                \"reward\": reward,\n                \"done\": done,\n                \"state\": state,\n            },\n            batch_size=self.batch_size,\n            device=self.device,\n            _run_checks=False,\n        )\n        return tensordict_out\n\n    def _step_without_grad(self, tensordict: TensorDictBase):\n\n        # convert tensors to ndarrays\n        state = _tensordict_to_object(tensordict.get(\"state\"), self._state_example)\n        action = _tensor_to_ndarray(tensordict.get(\"action\"))\n\n        # flatten batch size\n        state = _tree_flatten(state, self.batch_size)\n        action = _tree_flatten(action, self.batch_size)\n\n        # call env step with jit and vmap\n        next_state = self._vmap_jit_env_step(state, action)\n\n        # reshape batch size and convert ndarrays to tensors\n        next_state = _tree_reshape(next_state, self.batch_size)\n        next_state = _object_to_tensordict(next_state, self.device, self.batch_size)\n\n        # build result\n        reward = next_state.get(\"reward\").view(self.reward_spec.shape)\n        done = next_state.get(\"done\").bool().view(self.reward_spec.shape)\n        tensordict_out = TensorDict(\n            source={\n                \"observation\": next_state.get(\"obs\"),\n                \"reward\": reward,\n                \"done\": done,\n                \"state\": next_state,\n            },\n            batch_size=self.batch_size,\n            device=self.device,\n            _run_checks=False,\n        )\n        return tensordict_out\n\n    def _step_with_grad(self, tensordict: TensorDictBase):\n\n        # convert tensors to ndarrays\n        action = tensordict.get(\"action\")\n        state = tensordict.get(\"state\")\n        qp_keys = list(state.get(\"qp\").keys())\n        qp_values = list(state.get(\"qp\").values())\n\n        # call env step with autograd function\n        next_state_nograd, next_obs, next_reward, *next_qp_values = _BraxEnvStep.apply(\n            self, state, action, *qp_values\n        )\n\n        # extract done values: we assume a shape identical to reward\n        next_done = next_state_nograd.get(\"done\").bool().view(*self.reward_spec.shape)\n\n        # merge with tensors with grad function\n        next_state = next_state_nograd\n        next_state[\"obs\"] = next_obs\n        next_state[\"reward\"] = next_reward.view(*self.reward_spec.shape)\n        next_state[\"qp\"].update(dict(zip(qp_keys, next_qp_values)))\n\n        # build result\n        tensordict_out = TensorDict(\n            source={\n                \"observation\": next_obs,\n                \"reward\": next_reward,\n                \"done\": next_done,\n                \"state\": next_state,\n            },\n            batch_size=self.batch_size,\n            device=self.device,\n            _run_checks=False,\n        )\n        return tensordict_out\n\n    def _step(\n        self,\n        tensordict: TensorDictBase,\n    ) -> TensorDictBase:\n\n        if self.requires_grad:\n            return self._step_with_grad(tensordict)\n        else:\n            return self._step_without_grad(tensordict)\n\n\nclass BraxEnv(BraxWrapper):\n    \"\"\"Google Brax environment wrapper.\n\n    Examples:\n        >>> env = BraxEnv(env_name=\"ant\")\n        >>> td = env.rand_step()\n        >>> print(td)\n        >>> print(env.available_envs)\n\n    \"\"\"\n\n    def __init__(self, env_name, **kwargs):\n        kwargs[\"env_name\"] = env_name\n        super().__init__(**kwargs)\n\n    def _build_env(\n        self,\n        env_name: str,\n        **kwargs,\n    ) -> \"brax.envs.env.Env\":\n        if not _has_brax:\n            raise RuntimeError(\n                f\"brax not found, unable to create {env_name}. \"\n                f\"Consider downloading and installing brax from\"\n                f\" {self.git_url}\"\n            ) from IMPORT_ERR", "metadata": {"task_id": "pytorch_rl/37", "ground_truth": "        from_pixels = kwargs.pop(\"from_pixels\", False)", "fpath_tuple": ["pytorch_rl", "torchrl", "envs", "libs", "brax.py"], "context_start_lineno": 112, "line_no": 305, "query_window": {"context": "        >>> print(td)\n        >>> print(env.available_envs)\n\n    \"\"\"\n\n    def __init__(self, env_name, **kwargs):\n        kwargs[\"env_name\"] = env_name\n        super().__init__(**kwargs)\n\n    def _build_env(\n        self,\n        env_name: str,\n        **kwargs,\n    ) -> \"brax.envs.env.Env\":\n        if not _has_brax:\n            raise RuntimeError(\n                f\"brax not found, unable to create {env_name}. \"\n                f\"Consider downloading and installing brax from\"\n                f\" {self.git_url}\"\n            ) from IMPORT_ERR", "metadata": {"fpath_tuple": ["pytorch_rl", "torchrl", "envs", "libs", "brax.py"], "line_no": 305, "task_id": "pytorch_rl/37", "start_line_no": 285, "end_line_no": 305, "window_size": 20, "context_start_lineno": 112, "repo": "pytorch_rl"}}, "top_k_context": [{"context": "        >>> print(td)\n        >>> print(env.available_envs)\n    \"\"\"\n\n    def __init__(self, env_name, **kwargs):\n        kwargs[\"env_name\"] = env_name\n        super().__init__(**kwargs)\n\n    def _build_env(\n        self,\n        env_name: str,\n        **kwargs,\n    ) -> \"jumanji.env.Environment\":\n        if not _has_jumanji:\n            raise RuntimeError(\n                f\"jumanji not found, unable to create {env_name}. \"\n                f\"Consider installing jumanji. More info:\"\n                f\" {self.git_url}.\"\n            ) from IMPORT_ERR\n        from_pixels = kwargs.pop(\"from_pixels\", False)", "metadata": [{"fpath_tuple": ["pytorch_rl", "torchrl", "envs", "libs", "jumanji.py"], "line_no": 332, "start_line_no": 322, "end_line_no": 342, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}], "sim_score": 0.7368421052631579}, {"context": "        >>> env = JumanjiEnv(env_name=\"Snake-6x6-v0\", frame_skip=4)\n        >>> td = env.rand_step()\n        >>> print(td)\n        >>> print(env.available_envs)\n    \"\"\"\n\n    def __init__(self, env_name, **kwargs):\n        kwargs[\"env_name\"] = env_name\n        super().__init__(**kwargs)\n\n    def _build_env(\n        self,\n        env_name: str,\n        **kwargs,\n    ) -> \"jumanji.env.Environment\":\n        if not _has_jumanji:\n            raise RuntimeError(\n                f\"jumanji not found, unable to create {env_name}. \"\n                f\"Consider installing jumanji. More info:\"\n                f\" {self.git_url}.\"", "metadata": [{"fpath_tuple": ["pytorch_rl", "torchrl", "envs", "libs", "jumanji.py"], "line_no": 330, "start_line_no": 320, "end_line_no": 340, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}], "sim_score": 0.6601941747572816}, {"context": "\n    Examples:\n        >>> env = JumanjiEnv(env_name=\"Snake-6x6-v0\", frame_skip=4)\n        >>> td = env.rand_step()\n        >>> print(td)\n        >>> print(env.available_envs)\n    \"\"\"\n\n    def __init__(self, env_name, **kwargs):\n        kwargs[\"env_name\"] = env_name\n        super().__init__(**kwargs)\n\n    def _build_env(\n        self,\n        env_name: str,\n        **kwargs,\n    ) -> \"jumanji.env.Environment\":\n        if not _has_jumanji:\n            raise RuntimeError(\n                f\"jumanji not found, unable to create {env_name}. \"", "metadata": [{"fpath_tuple": ["pytorch_rl", "torchrl", "envs", "libs", "jumanji.py"], "line_no": 328, "start_line_no": 318, "end_line_no": 338, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}], "sim_score": 0.6530612244897959}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# vizier/_src/algorithms/optimizers/eagle_strategy.py\n# --------------------------------------------------\n#         pure_categorical_perturbation_factor=self.config.pure_categorical_perturbation_factor,\n#     )\n#     self._n_features = self._param_handler.n_features\n#     if self.config.pool_size > 0:\n#       # This allow to override the pool size computation.\n#       self.pool_size = self.config.pool_size\n#     else:\n#       self.pool_size = self._compute_pool_size()\n#     logging.info(\"Pool size: %d\", self.pool_size)\n#     if self.batch_size == -1:\n#       # This configuration updates all the fireflies in each iteration.\n#       self.batch_size = self.pool_size\n#     self._batch_id = 0\n#     self._iterations = 0\n#     self._batch_slice = np.s_[0 : self.batch_size]\n#     self._perturbations = (\n#         np.ones(\n#             self.pool_size,\n#         )\n#         * self.config.perturbation\n# --------------------------------------------------\n# the below code fragment can be found in:\n# vizier/_src/algorithms/optimizers/eagle_strategy.py\n# --------------------------------------------------\n#       # This allow to override the pool size computation.\n#       self.pool_size = self.config.pool_size\n#     else:\n#       self.pool_size = self._compute_pool_size()\n#     logging.info(\"Pool size: %d\", self.pool_size)\n#     if self.batch_size == -1:\n#       # This configuration updates all the fireflies in each iteration.\n#       self.batch_size = self.pool_size\n#     self._batch_id = 0\n#     self._iterations = 0\n#     self._batch_slice = np.s_[0 : self.batch_size]\n#     self._perturbations = (\n#         np.ones(\n#             self.pool_size,\n#         )\n#         * self.config.perturbation\n#     )\n#     self._last_suggested_features = None\n#     self._perturbation_factors = self._param_handler.perturbation_factors\n#     # Use priors to populate Eagle state\n# --------------------------------------------------\n# the below code fragment can be found in:\n# vizier/_src/algorithms/optimizers/eagle_strategy.py\n# --------------------------------------------------\n#     self._n_features = self._param_handler.n_features\n#     if self.config.pool_size > 0:\n#       # This allow to override the pool size computation.\n#       self.pool_size = self.config.pool_size\n#     else:\n#       self.pool_size = self._compute_pool_size()\n#     logging.info(\"Pool size: %d\", self.pool_size)\n#     if self.batch_size == -1:\n#       # This configuration updates all the fireflies in each iteration.\n#       self.batch_size = self.pool_size\n#     self._batch_id = 0\n#     self._iterations = 0\n#     self._batch_slice = np.s_[0 : self.batch_size]\n#     self._perturbations = (\n#         np.ones(\n#             self.pool_size,\n#         )\n#         * self.config.perturbation\n#     )\n#     self._last_suggested_features = None\n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\nwards.shape) > 1:\n      raise ValueError(\"prior rewards is expected to be 1D array!\")\n\n    # Reverse the order of prior trials to assign more weight to recent trials.\n    self.prior_features = np.flip(self.prior_features, axis=-1)\n    self.prior_rewards = np.flip(self.prior_rewards, axis=-1)\n\n    self._features = np.zeros((0, self._n_features))\n    # Fill pool with random features.\n    n_random_flies = int(\n        self.pool_size * (1 - self.config.prior_trials_pool_pct)\n    )\n    self._features = self._param_handler.random_features(\n        n_random_flies, self._n_features\n    )\n    pool_left_space = self.pool_size - n_random_flies\n\n    if self.prior_features.shape[0] < pool_left_space:\n      # Less prior trials than left space. Take all prior trials for the pool.\n      self._features = np.concatenate([self._features, self.prior_features])\n      # Randomize the rest of the pool fireflies.\n      random_features = self._param_handler.random_features(\n          self.pool_size - len(self._features), self._n_features\n      )\n      self._features = np.concatenate([self._features, random_features])\n    else:\n      # More prior trials than left space. Iteratively populate the pool.\n      tmp_features = self.prior_features[:pool_left_space]\n      tmp_rewards = self.prior_rewards[:pool_left_space]\n      for i in range(pool_left_space, self.prior_features.shape[0]):\n        ind = np.argmin(\n            np.sum(np.square(self.prior_features[i] - tmp_features), axis=-1)\n        )\n        if tmp_rewards[ind] < self.prior_rewards[i]:\n          # Only take the prior trials features. Rewards obtain during update.\n          tmp_features[ind] = self.prior_features[i]\n          tmp_rewards[ind] = self.prior_rewards[i]\n      self._features = np.concatenate([self._features, tmp_features])\n\n  @property\n  def suggestion_batch_size(self) -> int:\n    \"\"\"The number of suggestions returned at each call of 'suggest'.\"\"\"\n    return self.batch_size\n\n  def suggest(self) -> np.ndarray:\n    \"\"\"Suggest new mutated and perturbed features.\n\n    After initializing, at each call `batch_size` fireflies are mutated to\n    generate new features using pulls (attraction/repulsion) from all other\n    fireflies in the pool.\n\n    Returns:\n      suggested batch features: (batch_size, n_features)\n    \"\"\"\n    if self._iterations < self.pool_size // self.batch_size:\n      # The strategy is still initializing. Return the random/prior features.\n      new_features = self._features[self._batch_slice]\n    else:\n      mutated_features = self._create_features()\n      perturbations = self._create_perturbations()\n      new_features = mutated_features + perturbations\n\n    new_features = self._param_handler.sample_categorical(new_features)\n    suggested_features = np.clip(new_features, 0, 1)\n    # Save the suggested features to be used in update.\n    self._last_suggested_features = suggested_features\n    return suggested_features\n\n  def _increment_batch(self):\n    \"\"\"Increment the batch of fireflies features are generate from.\"\"\"\n    self._batch_id = (self._batch_id + 1) % (self.pool_size // self.batch_size)\n    start_batch = self._batch_id * self.batch_size\n    end_batch = (self._batch_id + 1) * self.batch_size\n    self._batch_slice = np.s_[start_batch:end_batch]\n\n  def _create_features(self) -> np.ndarray:\n    \"\"\"Create new batch of mutated and perturbed features.\n\n    Returns:\n      batch features: (batch_size, n_features)\n    \"\"\"\n    features_diffs, dists = self._compute_features_diffs_and_dists()\n    scaled_directions = self._compute_scaled_directions()\n    features_changes = self._compute_features_changes(\n        features_diffs, dists, scaled_directions\n    )\n    return self._features[self._batch_slice] + features_changes\n\n  def _compute_features_diffs_and_dists(self) -> tuple[np.ndarray, np.ndarray]:\n    \"\"\"Compute the features difference and distances.\n\n    The computation is done between the 'batch_size' fireflies and all\n    other fireflies in the pool.\n\n    features_diff[i, j, :] := features[j, :] - features[i, :]\n    features_dist[i, j, :] := distance between fly 'j' and fly 'i'\n\n    Returns:\n      feature differences: (batch_size, pool_size, n_features)\n      features distances: (batch_size, pool_size)\n    \"\"\"\n    shape = (self.batch_size,) + self._features.shape\n    features_diffs = np.broadcast_to(self._features, shape) - np.expand_dims(\n        self._features[self._batch_slice], 1\n    )\n    dists = np.sum(np.square(features_diffs), axis=-1)\n    return features_diffs, dists\n\n  def _compute_scaled_directions(self) -> np.ndarray:\n    \"\"\"Compute the scaled direction for applying pull between two flies.\n\n    scaled_directions[i,j] := direction of force applied by fly 'j' on fly 'i'.\n\n    Note that to compute 'directions' we might perform subtract with removed\n    flies with having value of -np.inf. Moreover, we might even subtract between\n    two removed flies which will result in np.nan. Both cases are handled when\n    computing the actual feautre changes applying a relevant mask.\n\n    Returns:\n      scaled directions: (batch_size, pool_size)\n    \"\"\"\n    shape = (self.batch_size,) + self._rewards.shape\n    directions = np.broadcast_to(self._rewards, shape) - np.expand_dims(\n        self._rewards[self._batch_slice], -1\n    )\n\n    scaled_directions = np.where(\n        directions >= 0, self.config.gravity, -self.config.negative_gravity\n    )\n    return scaled_directions\n\n  def _compute_features_changes(\n      self,\n      features_diffs: np.ndarray,\n      dists: np.ndarray,\n      scaled_directions: np.ndarray,\n  ) -> np.ndarray:\n    \"\"\"Compute the firefly features changes due to mutation.\n\n    The pool fireflies forces (pull/push) are being normalized to ensure the\n    combined force doesn't throw the firefly too far. Mathematically, the\n    normalization guarantees that the combined normalized force is within the\n    simplex constructed by the unnormalized forces and therefore within bounds.\n\n    Arguments:\n      features_diffs: (batch_size, pool_size, n_features)\n      dists: (batch_size, pool_size)\n      scaled_directions: (batch_size, pool_size)\n\n    Returns:\n      feature changes: (batch_size, feature_n)\n    \"\"\"\n    # Normalize the distance by the number of features.\n    force = np.exp(-self.config.visibility * dists / self._n_features * 10)\n    scaled_force = np.expand_dims(scaled_directions * force, -1)\n    # Handle removed fireflies without updated rewards.\n    inf_indx = np.isinf(self._rewards)\n    if np.sum(inf_indx) == self.pool_size:\n      logging.warning(\n          (\n              \"All firefly were recently removed. This Shouldn't happen.\"\n              \"Pool Features:\\n%sPool rewards:\\n%s\"", "metadata": {"task_id": "google_vizier/173", "ground_truth": "          ),", "fpath_tuple": ["google_vizier", "vizier", "_src", "algorithms", "optimizers", "eagle_strategy.py"], "context_start_lineno": 307, "line_no": 469, "query_window": {"context": "    simplex constructed by the unnormalized forces and therefore within bounds.\n\n    Arguments:\n      features_diffs: (batch_size, pool_size, n_features)\n      dists: (batch_size, pool_size)\n      scaled_directions: (batch_size, pool_size)\n\n    Returns:\n      feature changes: (batch_size, feature_n)\n    \"\"\"\n    # Normalize the distance by the number of features.\n    force = np.exp(-self.config.visibility * dists / self._n_features * 10)\n    scaled_force = np.expand_dims(scaled_directions * force, -1)\n    # Handle removed fireflies without updated rewards.\n    inf_indx = np.isinf(self._rewards)\n    if np.sum(inf_indx) == self.pool_size:\n      logging.warning(\n          (\n              \"All firefly were recently removed. This Shouldn't happen.\"\n              \"Pool Features:\\n%sPool rewards:\\n%s\"", "metadata": {"fpath_tuple": ["google_vizier", "vizier", "_src", "algorithms", "optimizers", "eagle_strategy.py"], "line_no": 469, "task_id": "google_vizier/173", "start_line_no": 449, "end_line_no": 469, "window_size": 20, "context_start_lineno": 307, "repo": "google_vizier"}}, "top_k_context": [{"context": "        pure_categorical_perturbation_factor=self.config.pure_categorical_perturbation_factor,\n    )\n    self._n_features = self._param_handler.n_features\n    if self.config.pool_size > 0:\n      # This allow to override the pool size computation.\n      self.pool_size = self.config.pool_size\n    else:\n      self.pool_size = self._compute_pool_size()\n    logging.info(\"Pool size: %d\", self.pool_size)\n    if self.batch_size == -1:\n      # This configuration updates all the fireflies in each iteration.\n      self.batch_size = self.pool_size\n    self._batch_id = 0\n    self._iterations = 0\n    self._batch_slice = np.s_[0 : self.batch_size]\n    self._perturbations = (\n        np.ones(\n            self.pool_size,\n        )\n        * self.config.perturbation", "metadata": [{"fpath_tuple": ["google_vizier", "vizier", "_src", "algorithms", "optimizers", "eagle_strategy.py"], "line_no": 258, "start_line_no": 248, "end_line_no": 268, "window_size": 20, "repo": "google_vizier", "slice_size": 10}], "sim_score": 0.23529411764705882}, {"context": "    self._n_features = self._param_handler.n_features\n    if self.config.pool_size > 0:\n      # This allow to override the pool size computation.\n      self.pool_size = self.config.pool_size\n    else:\n      self.pool_size = self._compute_pool_size()\n    logging.info(\"Pool size: %d\", self.pool_size)\n    if self.batch_size == -1:\n      # This configuration updates all the fireflies in each iteration.\n      self.batch_size = self.pool_size\n    self._batch_id = 0\n    self._iterations = 0\n    self._batch_slice = np.s_[0 : self.batch_size]\n    self._perturbations = (\n        np.ones(\n            self.pool_size,\n        )\n        * self.config.perturbation\n    )\n    self._last_suggested_features = None", "metadata": [{"fpath_tuple": ["google_vizier", "vizier", "_src", "algorithms", "optimizers", "eagle_strategy.py"], "line_no": 260, "start_line_no": 250, "end_line_no": 270, "window_size": 20, "repo": "google_vizier", "slice_size": 10}], "sim_score": 0.23333333333333334}, {"context": "        rng=self._rng,\n        categorical_perturbation_factor=self.config.categorical_perturbation_factor,\n        pure_categorical_perturbation_factor=self.config.pure_categorical_perturbation_factor,\n    )\n    self._n_features = self._param_handler.n_features\n    if self.config.pool_size > 0:\n      # This allow to override the pool size computation.\n      self.pool_size = self.config.pool_size\n    else:\n      self.pool_size = self._compute_pool_size()\n    logging.info(\"Pool size: %d\", self.pool_size)\n    if self.batch_size == -1:\n      # This configuration updates all the fireflies in each iteration.\n      self.batch_size = self.pool_size\n    self._batch_id = 0\n    self._iterations = 0\n    self._batch_slice = np.s_[0 : self.batch_size]\n    self._perturbations = (\n        np.ones(\n            self.pool_size,", "metadata": [{"fpath_tuple": ["google_vizier", "vizier", "_src", "algorithms", "optimizers", "eagle_strategy.py"], "line_no": 256, "start_line_no": 246, "end_line_no": 266, "window_size": 20, "repo": "google_vizier", "slice_size": 10}], "sim_score": 0.2229299363057325}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# federatedscope/core/feature/vfl/preprocess/min_max_norm.py\n# --------------------------------------------------\n# \n#     # Merge train & val & test\n#     merged_feat, _ = merge_splits_feat(worker.data)\n# \n#     feat_min = np.min(merged_feat, axis=0)\n#     feat_max = np.max(merged_feat, axis=0)\n# \n#     # If max == min, it will be replaced with 0.0\n#     for col_i in range(len(feat_min)):\n#         if feat_min[col_i] == feat_max[col_i]:\n#             feat_max[col_i] = np.inf\n# \n#     for split in ['train_data', 'val_data', 'test_data']:\n#         if hasattr(worker.data, split):\n#             split_data = getattr(worker.data, split)\n#             if split_data is not None and 'x' in split_data:\n#                 split_data['x'] = \\\n#                     (split_data['x'] - feat_min) / (feat_max - feat_min)\n#     worker._init_data_related_var()\n#     return worker\n# --------------------------------------------------\n# the below code fragment can be found in:\n# federatedscope/core/feature/vfl/preprocess/min_max_norm.py\n# --------------------------------------------------\n# \n#     # If max == min, it will be replaced with 0.0\n#     for col_i in range(len(feat_min)):\n#         if feat_min[col_i] == feat_max[col_i]:\n#             feat_max[col_i] = np.inf\n# \n#     for split in ['train_data', 'val_data', 'test_data']:\n#         if hasattr(worker.data, split):\n#             split_data = getattr(worker.data, split)\n#             if split_data is not None and 'x' in split_data:\n#                 split_data['x'] = \\\n#                     (split_data['x'] - feat_min) / (feat_max - feat_min)\n#     worker._init_data_related_var()\n#     return worker\n# \n# \n# def wrap_min_max_norm_client(worker):\n#     return wrap_min_max_norm(worker)\n# \n# \n# --------------------------------------------------\n# the below code fragment can be found in:\n# federatedscope/core/feature/vfl/preprocess/min_max_norm.py\n# --------------------------------------------------\n#     for col_i in range(len(feat_min)):\n#         if feat_min[col_i] == feat_max[col_i]:\n#             feat_max[col_i] = np.inf\n# \n#     for split in ['train_data', 'val_data', 'test_data']:\n#         if hasattr(worker.data, split):\n#             split_data = getattr(worker.data, split)\n#             if split_data is not None and 'x' in split_data:\n#                 split_data['x'] = \\\n#                     (split_data['x'] - feat_min) / (feat_max - feat_min)\n#     worker._init_data_related_var()\n#     return worker\n# \n# \n# def wrap_min_max_norm_client(worker):\n#     return wrap_min_max_norm(worker)\n# \n# \n# def wrap_min_max_norm_server(worker):\n#     return wrap_min_max_norm(worker)\n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\nimport logging\nimport numpy as np\n\nlogger = logging.getLogger(__name__)\n\n\ndef merge_splits_feat(data):\n    \"\"\"\n\n    Args:\n        data: ``federatedscope.core.data.ClientData`` with Tabular format.\n\n    Returns:\n        Merged data feature/x.\n    \"\"\"\n    merged_feat = None\n    merged_y = None\n    for split in ['train_data', 'val_data', 'test_data']:\n        if hasattr(data, split):\n            split_data = getattr(data, split)\n            if split_data is not None and 'x' in split_data:\n                if merged_feat is None:\n                    merged_feat = split_data['x']\n                else:\n                    merged_feat = \\\n                        np.concatenate((merged_feat, split_data['x']), axis=0)\n            if split_data is not None and 'y' in split_data:\n                if merged_y is None:\n                    merged_y = split_data['y']\n                else:\n                    merged_y = \\\n                        np.concatenate((merged_y, split_data['y']), axis=0)\n    return merged_feat, merged_y\n\n\ndef vfl_binning(feat, num_bins, strategy='uniform'):\n    \"\"\"\n\n    Args:\n        feat: feature to be binned, which must be 2D numpy.array\n        num_bins: list for bins\n        strategy: binning strategy, ``'uniform'`` or ``'quantile'``\n\n    Returns:\n        Bin edges for binning\n    \"\"\"\n    num_features = feat.shape[1]\n    bin_edges = np.zeros(num_features, dtype=object)\n\n    for i in range(num_features):\n        col = feat[:, i]\n        col_min, col_max = np.min(col), np.max(col)\n        if col_min == col_max:\n            logger.warning(\n                f'Feature {i} is constant and will be replaced with 0.')\n            bin_edges[i] = np.array([-np.inf, np.inf])\n            continue\n        if strategy == 'uniform':\n            bin_edges[i] = np.linspace(col_min, col_max, num_bins[i] + 1)\n        elif strategy == 'quantile':\n            quantiles = np.linspace(0, 100, num_bins[i] + 1)\n            bin_edges[i] = np.asarray(np.percentile(col, quantiles))\n\n    return bin_edges\n\n\ndef secure_builder(cfg):\n    if cfg.feat_engr.secure.type == 'encrypt':\n        if cfg.feat_engr.secure.encrypt.type == 'dummy':\n            from federatedscope.core.secure.encrypt.dummy_encrypt import \\", "metadata": {"task_id": "alibaba_FederatedScope/101", "ground_truth": "                DummyEncryptKeypair", "fpath_tuple": ["alibaba_FederatedScope", "federatedscope", "core", "feature", "utils.py"], "context_start_lineno": 0, "line_no": 70, "query_window": {"context": "        col = feat[:, i]\n        col_min, col_max = np.min(col), np.max(col)\n        if col_min == col_max:\n            logger.warning(\n                f'Feature {i} is constant and will be replaced with 0.')\n            bin_edges[i] = np.array([-np.inf, np.inf])\n            continue\n        if strategy == 'uniform':\n            bin_edges[i] = np.linspace(col_min, col_max, num_bins[i] + 1)\n        elif strategy == 'quantile':\n            quantiles = np.linspace(0, 100, num_bins[i] + 1)\n            bin_edges[i] = np.asarray(np.percentile(col, quantiles))\n\n    return bin_edges\n\n\ndef secure_builder(cfg):\n    if cfg.feat_engr.secure.type == 'encrypt':\n        if cfg.feat_engr.secure.encrypt.type == 'dummy':\n            from federatedscope.core.secure.encrypt.dummy_encrypt import \\", "metadata": {"fpath_tuple": ["alibaba_FederatedScope", "federatedscope", "core", "feature", "utils.py"], "line_no": 70, "task_id": "alibaba_FederatedScope/101", "start_line_no": 50, "end_line_no": 70, "window_size": 20, "context_start_lineno": 0, "repo": "alibaba_FederatedScope"}}, "top_k_context": [{"context": "\n    # If max == min, it will be replaced with 0.0\n    for col_i in range(len(feat_min)):\n        if feat_min[col_i] == feat_max[col_i]:\n            feat_max[col_i] = np.inf\n\n    for split in ['train_data', 'val_data', 'test_data']:\n        if hasattr(worker.data, split):\n            split_data = getattr(worker.data, split)\n            if split_data is not None and 'x' in split_data:\n                split_data['x'] = \\\n                    (split_data['x'] - feat_min) / (feat_max - feat_min)\n    worker._init_data_related_var()\n    return worker\n\n\ndef wrap_min_max_norm_client(worker):\n    return wrap_min_max_norm(worker)\n\n", "metadata": [{"fpath_tuple": ["alibaba_FederatedScope", "federatedscope", "core", "feature", "vfl", "preprocess", "min_max_norm.py"], "line_no": 34, "start_line_no": 24, "end_line_no": 44, "window_size": 20, "repo": "alibaba_FederatedScope", "slice_size": 10}], "sim_score": 0.2857142857142857}, {"context": "    feat_min = np.min(merged_feat, axis=0)\n    feat_max = np.max(merged_feat, axis=0)\n\n    # If max == min, it will be replaced with 0.0\n    for col_i in range(len(feat_min)):\n        if feat_min[col_i] == feat_max[col_i]:\n            feat_max[col_i] = np.inf\n\n    for split in ['train_data', 'val_data', 'test_data']:\n        if hasattr(worker.data, split):\n            split_data = getattr(worker.data, split)\n            if split_data is not None and 'x' in split_data:\n                split_data['x'] = \\\n                    (split_data['x'] - feat_min) / (feat_max - feat_min)\n    worker._init_data_related_var()\n    return worker\n\n\ndef wrap_min_max_norm_client(worker):\n    return wrap_min_max_norm(worker)", "metadata": [{"fpath_tuple": ["alibaba_FederatedScope", "federatedscope", "core", "feature", "vfl", "preprocess", "min_max_norm.py"], "line_no": 32, "start_line_no": 22, "end_line_no": 42, "window_size": 20, "repo": "alibaba_FederatedScope", "slice_size": 10}], "sim_score": 0.2777777777777778}, {"context": "    \"\"\"\n    logger.info('Start to execute min-max scaling.')\n\n    # Merge train & val & test\n    merged_feat, _ = merge_splits_feat(worker.data)\n\n    feat_min = np.min(merged_feat, axis=0)\n    feat_max = np.max(merged_feat, axis=0)\n\n    # If max == min, it will be replaced with 0.0\n    for col_i in range(len(feat_min)):\n        if feat_min[col_i] == feat_max[col_i]:\n            feat_max[col_i] = np.inf\n\n    for split in ['train_data', 'val_data', 'test_data']:\n        if hasattr(worker.data, split):\n            split_data = getattr(worker.data, split)\n            if split_data is not None and 'x' in split_data:\n                split_data['x'] = \\\n                    (split_data['x'] - feat_min) / (feat_max - feat_min)", "metadata": [{"fpath_tuple": ["alibaba_FederatedScope", "federatedscope", "core", "feature", "vfl", "preprocess", "min_max_norm.py"], "line_no": 26, "start_line_no": 16, "end_line_no": 36, "window_size": 20, "repo": "alibaba_FederatedScope", "slice_size": 10}], "sim_score": 0.26973684210526316}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# fortuna/prob_model/classification.py\n# fortuna/prob_model/regression.py\n# --------------------------------------------------\n#         calib_data_loader: Optional[DataLoader] = None,\n#         fit_config: FitConfig = FitConfig(),\n#         calib_config: CalibConfig = CalibConfig(),\n#         **fit_kwargs,\n#     ) -> Dict[str, Status]:\n#         self._check_output_dim(train_data_loader)\n#         return super().train(\n#             train_data_loader,\n#             val_data_loader,\n#             calib_data_loader,\n#             fit_config,\n#             calib_config,\n#             **fit_kwargs,\n#         )\n# \n#     def calibrate(\n#         self,\n#         calib_data_loader: DataLoader,\n#         val_data_loader: Optional[DataLoader] = None,\n#         calib_config: CalibConfig = CalibConfig(),\n# --------------------------------------------------\n# the below code fragment can be found in:\n# fortuna/prob_model/classification.py\n# fortuna/prob_model/regression.py\n# --------------------------------------------------\n#         calib_config: CalibConfig = CalibConfig(),\n#         **fit_kwargs,\n#     ) -> Dict[str, Status]:\n#         self._check_output_dim(train_data_loader)\n#         return super().train(\n#             train_data_loader,\n#             val_data_loader,\n#             calib_data_loader,\n#             fit_config,\n#             calib_config,\n#             **fit_kwargs,\n#         )\n# \n#     def calibrate(\n#         self,\n#         calib_data_loader: DataLoader,\n#         val_data_loader: Optional[DataLoader] = None,\n#         calib_config: CalibConfig = CalibConfig(),\n#     ) -> Status:\n#         \"\"\"\n# --------------------------------------------------\n# the below code fragment can be found in:\n# fortuna/prob_model/classification.py\n# fortuna/prob_model/regression.py\n# --------------------------------------------------\n#     ) -> Dict[str, Status]:\n#         self._check_output_dim(train_data_loader)\n#         return super().train(\n#             train_data_loader,\n#             val_data_loader,\n#             calib_data_loader,\n#             fit_config,\n#             calib_config,\n#             **fit_kwargs,\n#         )\n# \n#     def calibrate(\n#         self,\n#         calib_data_loader: DataLoader,\n#         val_data_loader: Optional[DataLoader] = None,\n#         calib_config: CalibConfig = CalibConfig(),\n#     ) -> Status:\n#         \"\"\"\n#         Calibrate the probabilistic classifier.\n# \n# --------------------------------------------------\n# the below code fragment can be found in:\n# fortuna/prob_model/classification.py\n# fortuna/prob_model/regression.py\n# --------------------------------------------------\n#             val_data_loader,\n#             calib_data_loader,\n#             fit_config,\n#             calib_config,\n#             **fit_kwargs,\n#         )\n# \n#     def calibrate(\n#         self,\n#         calib_data_loader: DataLoader,\n#         val_data_loader: Optional[DataLoader] = None,\n#         calib_config: CalibConfig = CalibConfig(),\n#     ) -> Status:\n#         \"\"\"\n#         Calibrate the probabilistic classifier.\n# \n#         Parameters\n#         ----------\n#         calib_data_loader : DataLoader\n#             A calibration data loader.\n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\nimport abc\nimport logging\nfrom typing import Callable, Dict, Optional\n\nimport jax\nimport jax.numpy as jnp\n\nfrom fortuna.calibration.state import CalibState\nfrom fortuna.data.loader import DataLoader\nfrom fortuna.prob_model.calib_config.base import CalibConfig\nfrom fortuna.prob_model.fit_config import FitConfig\nfrom fortuna.prob_model.prob_model_calibrator import (\n    JittedProbModelCalibrator, MultiDeviceProbModelCalibrator,\n    ProbModelCalibrator)\nfrom fortuna.typing import Array, Path, Status\nfrom fortuna.utils.data import check_data_loader_is_not_random\nfrom fortuna.utils.device import select_trainer_given_devices\nfrom fortuna.utils.random import RandomNumberGenerator\n\n\nclass ProbModel(abc.ABC):\n    \"\"\"\n    Abstract probabilistic model class.\n    \"\"\"\n\n    def __init__(self, seed: int = 0):\n        self.rng = RandomNumberGenerator(seed=seed)\n        self.__set_rng()\n\n    def __set_rng(self):\n        self.model_manager.rng = self.rng\n        self.output_calib_manager.rng = self.rng\n        self.prob_output_layer.rng = self.rng\n        self.prior.rng = self.rng\n        self.likelihood.rng = self.rng\n        self.joint.rng = self.rng\n        self.posterior.rng = self.rng\n        self.predictive.rng = self.rng\n\n    def train(\n        self,\n        train_data_loader: DataLoader,\n        val_data_loader: Optional[DataLoader] = None,\n        calib_data_loader: Optional[DataLoader] = None,\n        fit_config: FitConfig = FitConfig(),\n        calib_config: CalibConfig = CalibConfig(),\n        map_fit_config: Optional[FitConfig] = None,\n    ) -> Dict[str, Status]:\n        \"\"\"\n        Train the probabilistic model. This involves fitting the posterior distribution and calibrating the\n        probabilistic model. Calibration is performed only if (1) `calib_data_loader` is passed and (2) the\n        probabilistic model contains any calibrator.\n\n        Parameters\n        ----------\n        train_data_loader : DataLoader\n            A training data loader.\n        val_data_loader : DataLoader\n            A validation data loader. This is used to validate both posterior fitting and calibration.\n        calib_data_loader : DataLoader\n            A calibration data loader. If this is not passed, no calibration is performed.\n        fit_config : FitConfig\n            An object to configure the posterior distribution fitting.\n        calib_config : CalibConfig\n            An object to configure the calibration.\n        map_fit_config : Optional[FitConfig] = None\n            An object to configure a preliminary posterior distribution fitting via the Maximum-A-Posteriori (MAP)\n            method.\n            The fit methods of several supported posterior approximations, like the ones of\n            :class:`~fortuna.prob_model.posterior.swag.swag_posterior.SWAGPosterior` and\n            :class:`~fortuna.prob_model.posterior.laplace.laplace_posterior.LaplacePosterior`, start from a preliminary\n            run of MAP, which can be configured via this object. If the method does not start from MAP, this argument is\n            ignored.\n\n        Returns\n        -------\n        Dict[str, Status]\n            Status objects for both posterior fitting and calibration.\n\n        \"\"\"\n        logging.info(\"Fit the posterior distribution...\")\n        fit_status = self.posterior.fit(\n            train_data_loader=train_data_loader,\n            val_data_loader=val_data_loader,\n            fit_config=fit_config,\n            map_fit_config=map_fit_config,\n        )\n        logging.info(\"Fit completed.\")\n\n        calib_status = None\n        if calib_data_loader:\n            calib_status = self.calibrate(\n                calib_data_loader=calib_data_loader,\n                val_data_loader=val_data_loader,\n                calib_config=calib_config,\n            )\n            logging.info(\"Calibration completed.\")\n        return dict(fit_status=fit_status, calib_status=calib_status)\n\n    def _calibrate(\n        self,\n        calib_data_loader: DataLoader,", "metadata": {"task_id": "awslabs_fortuna/128", "ground_truth": "        uncertainty_fn: Callable[[jnp.ndarray, jnp.ndarray, Array], jnp.ndarray],", "fpath_tuple": ["awslabs_fortuna", "fortuna", "prob_model", "base.py"], "context_start_lineno": 0, "line_no": 102, "query_window": {"context": "            train_data_loader=train_data_loader,\n            val_data_loader=val_data_loader,\n            fit_config=fit_config,\n            map_fit_config=map_fit_config,\n        )\n        logging.info(\"Fit completed.\")\n\n        calib_status = None\n        if calib_data_loader:\n            calib_status = self.calibrate(\n                calib_data_loader=calib_data_loader,\n                val_data_loader=val_data_loader,\n                calib_config=calib_config,\n            )\n            logging.info(\"Calibration completed.\")\n        return dict(fit_status=fit_status, calib_status=calib_status)\n\n    def _calibrate(\n        self,\n        calib_data_loader: DataLoader,", "metadata": {"fpath_tuple": ["awslabs_fortuna", "fortuna", "prob_model", "base.py"], "line_no": 102, "task_id": "awslabs_fortuna/128", "start_line_no": 82, "end_line_no": 102, "window_size": 20, "context_start_lineno": 0, "repo": "awslabs_fortuna"}}, "top_k_context": [{"context": "        return super().train(\n            train_data_loader,\n            val_data_loader,\n            calib_data_loader,\n            fit_config,\n            calib_config,\n            **fit_kwargs,\n        )\n\n    def calibrate(\n        self,\n        calib_data_loader: DataLoader,\n        val_data_loader: Optional[DataLoader] = None,\n        calib_config: CalibConfig = CalibConfig(),\n    ) -> Status:\n        \"\"\"\n        Calibrate the probabilistic classifier.\n\n        Parameters\n        ----------", "metadata": [{"fpath_tuple": ["awslabs_fortuna", "fortuna", "prob_model", "classification.py"], "line_no": 156, "start_line_no": 146, "end_line_no": 166, "window_size": 20, "repo": "awslabs_fortuna", "slice_size": 10}, {"fpath_tuple": ["awslabs_fortuna", "fortuna", "prob_model", "regression.py"], "line_no": 162, "start_line_no": 152, "end_line_no": 172, "window_size": 20, "repo": "awslabs_fortuna", "slice_size": 10}], "sim_score": 0.40540540540540543}, {"context": "        calib_config: CalibConfig = CalibConfig(),\n        **fit_kwargs,\n    ) -> Dict[str, Status]:\n        self._check_output_dim(train_data_loader)\n        return super().train(\n            train_data_loader,\n            val_data_loader,\n            calib_data_loader,\n            fit_config,\n            calib_config,\n            **fit_kwargs,\n        )\n\n    def calibrate(\n        self,\n        calib_data_loader: DataLoader,\n        val_data_loader: Optional[DataLoader] = None,\n        calib_config: CalibConfig = CalibConfig(),\n    ) -> Status:\n        \"\"\"", "metadata": [{"fpath_tuple": ["awslabs_fortuna", "fortuna", "prob_model", "classification.py"], "line_no": 152, "start_line_no": 142, "end_line_no": 162, "window_size": 20, "repo": "awslabs_fortuna", "slice_size": 10}, {"fpath_tuple": ["awslabs_fortuna", "fortuna", "prob_model", "regression.py"], "line_no": 158, "start_line_no": 148, "end_line_no": 168, "window_size": 20, "repo": "awslabs_fortuna", "slice_size": 10}], "sim_score": 0.3972602739726027}, {"context": "        calib_data_loader: Optional[DataLoader] = None,\n        fit_config: FitConfig = FitConfig(),\n        calib_config: CalibConfig = CalibConfig(),\n        **fit_kwargs,\n    ) -> Dict[str, Status]:\n        self._check_output_dim(train_data_loader)\n        return super().train(\n            train_data_loader,\n            val_data_loader,\n            calib_data_loader,\n            fit_config,\n            calib_config,\n            **fit_kwargs,\n        )\n\n    def calibrate(\n        self,\n        calib_data_loader: DataLoader,\n        val_data_loader: Optional[DataLoader] = None,\n        calib_config: CalibConfig = CalibConfig(),", "metadata": [{"fpath_tuple": ["awslabs_fortuna", "fortuna", "prob_model", "classification.py"], "line_no": 150, "start_line_no": 140, "end_line_no": 160, "window_size": 20, "repo": "awslabs_fortuna", "slice_size": 10}, {"fpath_tuple": ["awslabs_fortuna", "fortuna", "prob_model", "regression.py"], "line_no": 156, "start_line_no": 146, "end_line_no": 166, "window_size": 20, "repo": "awslabs_fortuna", "slice_size": 10}], "sim_score": 0.3972602739726027}, {"context": "        train_data_loader: DataLoader,\n        val_data_loader: Optional[DataLoader] = None,\n        calib_data_loader: Optional[DataLoader] = None,\n        fit_config: FitConfig = FitConfig(),\n        calib_config: CalibConfig = CalibConfig(),\n        **fit_kwargs,\n    ) -> Dict[str, Status]:\n        self._check_output_dim(train_data_loader)\n        return super().train(\n            train_data_loader,\n            val_data_loader,\n            calib_data_loader,\n            fit_config,\n            calib_config,\n            **fit_kwargs,\n        )\n\n    def calibrate(\n        self,\n        calib_data_loader: DataLoader,", "metadata": [{"fpath_tuple": ["awslabs_fortuna", "fortuna", "prob_model", "classification.py"], "line_no": 148, "start_line_no": 138, "end_line_no": 158, "window_size": 20, "repo": "awslabs_fortuna", "slice_size": 10}, {"fpath_tuple": ["awslabs_fortuna", "fortuna", "prob_model", "regression.py"], "line_no": 154, "start_line_no": 144, "end_line_no": 164, "window_size": 20, "repo": "awslabs_fortuna", "slice_size": 10}], "sim_score": 0.3972602739726027}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# federatedscope/core/splitters/graph/scaffold_lda_splitter.py\n# --------------------------------------------------\n#         else:\n#             scaffolds[scaffold].append(idx)\n#     # Sort from largest to smallest scaffold sets\n#     scaffolds = {key: sorted(value) for key, value in scaffolds.items()}\n#     scaffold_list = [\n#         list(scaffold_set)\n#         for (scaffold,\n#              scaffold_set) in sorted(scaffolds.items(),\n#                                      key=lambda x: (len(x[1]), x[1][0]),\n#                                      reverse=True)\n#     ]\n#     label = np.zeros(len(dataset))\n#     for i in range(len(scaffold_list)):\n#         label[scaffold_list[i]] = i + 1\n#     label = torch.LongTensor(label)\n#     # Split data to list\n#     idx_slice = dirichlet_distribution_noniid_slice(label, client_num, alpha)\n#     return idx_slice\n# \n# \n# --------------------------------------------------\n# the below code fragment can be found in:\n# federatedscope/core/splitters/graph/randchunk_splitter.py\n# --------------------------------------------------\n#     def __init__(self, client_num):\n#         BaseSplitter.__init__(self, client_num)\n# \n#     def __call__(self, dataset, **kwargs):\n#         data_list = []\n#         dataset = [ds for ds in dataset]\n#         num_graph = len(dataset)\n# \n#         # Split dataset\n#         num_graph = len(dataset)\n#         min_size = min(50, int(num_graph / self.client_num))\n# \n#         for i in range(self.client_num):\n#             data_list.append(dataset[i * min_size:(i + 1) * min_size])\n#         for graph in dataset[self.client_num * min_size:]:\n#             client_idx = np.random.randint(low=0, high=self.client_num,\n#                                            size=1)[0]\n#             data_list[client_idx].append(graph)\n# \n#         return data_list\n# --------------------------------------------------\n# the below code fragment can be found in:\n# federatedscope/core/splitters/graph/scaffold_lda_splitter.py\n# --------------------------------------------------\n#     # Sort from largest to smallest scaffold sets\n#     scaffolds = {key: sorted(value) for key, value in scaffolds.items()}\n#     scaffold_list = [\n#         list(scaffold_set)\n#         for (scaffold,\n#              scaffold_set) in sorted(scaffolds.items(),\n#                                      key=lambda x: (len(x[1]), x[1][0]),\n#                                      reverse=True)\n#     ]\n#     label = np.zeros(len(dataset))\n#     for i in range(len(scaffold_list)):\n#         label[scaffold_list[i]] = i + 1\n#     label = torch.LongTensor(label)\n#     # Split data to list\n#     idx_slice = dirichlet_distribution_noniid_slice(label, client_num, alpha)\n#     return idx_slice\n# \n# \n# class ScaffoldLdaSplitter(BaseSplitter):\n#     \"\"\"\n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\nimport numpy as np\n\n\ndef _split_according_to_prior(label, client_num, prior):\n    assert client_num == len(prior)\n    classes = len(np.unique(label))\n    assert classes == len(np.unique(np.concatenate(prior, 0)))\n\n    # counting\n    frequency = np.zeros(shape=(client_num, classes))\n    for idx, client_prior in enumerate(prior):\n        for each in client_prior:\n            frequency[idx][each] += 1\n    sum_frequency = np.sum(frequency, axis=0)\n\n    idx_slice = [[] for _ in range(client_num)]\n    for k in range(classes):\n        idx_k = np.where(label == k)[0]\n        np.random.shuffle(idx_k)\n        nums_k = np.ceil(frequency[:, k] / sum_frequency[k] *\n                         len(idx_k)).astype(int)\n        while len(idx_k) < np.sum(nums_k):\n            random_client = np.random.choice(range(client_num))\n            if nums_k[random_client] > 0:\n                nums_k[random_client] -= 1\n        assert len(idx_k) == np.sum(nums_k)\n        idx_slice = [\n            idx_j + idx.tolist() for idx_j, idx in zip(\n                idx_slice, np.split(idx_k,\n                                    np.cumsum(nums_k)[:-1]))\n        ]\n\n    for i in range(len(idx_slice)):\n        np.random.shuffle(idx_slice[i])\n    return idx_slice\n\n\ndef dirichlet_distribution_noniid_slice(label,\n                                        client_num,\n                                        alpha,\n                                        min_size=1,\n                                        prior=None):\n    r\"\"\"Get sample index list for each client from the Dirichlet distribution.\n    https://github.com/FedML-AI/FedML/blob/master/fedml_core/non_iid", "metadata": {"task_id": "alibaba_FederatedScope/0", "ground_truth": "    partition/noniid_partition.py", "fpath_tuple": ["alibaba_FederatedScope", "federatedscope", "core", "splitters", "utils.py"], "context_start_lineno": 0, "line_no": 44, "query_window": {"context": "                nums_k[random_client] -= 1\n        assert len(idx_k) == np.sum(nums_k)\n        idx_slice = [\n            idx_j + idx.tolist() for idx_j, idx in zip(\n                idx_slice, np.split(idx_k,\n                                    np.cumsum(nums_k)[:-1]))\n        ]\n\n    for i in range(len(idx_slice)):\n        np.random.shuffle(idx_slice[i])\n    return idx_slice\n\n\ndef dirichlet_distribution_noniid_slice(label,\n                                        client_num,\n                                        alpha,\n                                        min_size=1,\n                                        prior=None):\n    r\"\"\"Get sample index list for each client from the Dirichlet distribution.\n    https://github.com/FedML-AI/FedML/blob/master/fedml_core/non_iid", "metadata": {"fpath_tuple": ["alibaba_FederatedScope", "federatedscope", "core", "splitters", "utils.py"], "line_no": 44, "task_id": "alibaba_FederatedScope/0", "start_line_no": 24, "end_line_no": 44, "window_size": 20, "context_start_lineno": 0, "repo": "alibaba_FederatedScope"}}, "top_k_context": [{"context": "        else:\n            scaffolds[scaffold].append(idx)\n    # Sort from largest to smallest scaffold sets\n    scaffolds = {key: sorted(value) for key, value in scaffolds.items()}\n    scaffold_list = [\n        list(scaffold_set)\n        for (scaffold,\n             scaffold_set) in sorted(scaffolds.items(),\n                                     key=lambda x: (len(x[1]), x[1][0]),\n                                     reverse=True)\n    ]\n    label = np.zeros(len(dataset))\n    for i in range(len(scaffold_list)):\n        label[scaffold_list[i]] = i + 1\n    label = torch.LongTensor(label)\n    # Split data to list\n    idx_slice = dirichlet_distribution_noniid_slice(label, client_num, alpha)\n    return idx_slice\n\n", "metadata": [{"fpath_tuple": ["alibaba_FederatedScope", "federatedscope", "core", "splitters", "graph", "scaffold_lda_splitter.py"], "line_no": 142, "start_line_no": 132, "end_line_no": 152, "window_size": 20, "repo": "alibaba_FederatedScope", "slice_size": 10}], "sim_score": 0.3006535947712418}, {"context": "        dataset = [ds for ds in dataset]\n        num_graph = len(dataset)\n\n        # Split dataset\n        num_graph = len(dataset)\n        min_size = min(50, int(num_graph / self.client_num))\n\n        for i in range(self.client_num):\n            data_list.append(dataset[i * min_size:(i + 1) * min_size])\n        for graph in dataset[self.client_num * min_size:]:\n            client_idx = np.random.randint(low=0, high=self.client_num,\n                                           size=1)[0]\n            data_list[client_idx].append(graph)\n\n        return data_list", "metadata": [{"fpath_tuple": ["alibaba_FederatedScope", "federatedscope", "core", "splitters", "graph", "randchunk_splitter.py"], "line_no": 28, "start_line_no": 18, "end_line_no": 33, "window_size": 20, "repo": "alibaba_FederatedScope", "slice_size": 10}], "sim_score": 0.29850746268656714}, {"context": "        if scaffold not in scaffolds:\n            scaffolds[scaffold] = [idx]\n        else:\n            scaffolds[scaffold].append(idx)\n    # Sort from largest to smallest scaffold sets\n    scaffolds = {key: sorted(value) for key, value in scaffolds.items()}\n    scaffold_list = [\n        list(scaffold_set)\n        for (scaffold,\n             scaffold_set) in sorted(scaffolds.items(),\n                                     key=lambda x: (len(x[1]), x[1][0]),\n                                     reverse=True)\n    ]\n    label = np.zeros(len(dataset))\n    for i in range(len(scaffold_list)):\n        label[scaffold_list[i]] = i + 1\n    label = torch.LongTensor(label)\n    # Split data to list\n    idx_slice = dirichlet_distribution_noniid_slice(label, client_num, alpha)\n    return idx_slice", "metadata": [{"fpath_tuple": ["alibaba_FederatedScope", "federatedscope", "core", "splitters", "graph", "scaffold_lda_splitter.py"], "line_no": 140, "start_line_no": 130, "end_line_no": 150, "window_size": 20, "repo": "alibaba_FederatedScope", "slice_size": 10}], "sim_score": 0.2967741935483871}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# torchrl/envs/gym_like.py\n# --------------------------------------------------\n# \n#             done, do_break = self.read_done(done)\n#             if do_break:\n#                 break\n# \n#         obs_dict = self.read_obs(obs)\n# \n#         if reward is None:\n#             reward = np.nan\n#         reward = self._to_tensor(reward, dtype=self.reward_spec.dtype)\n#         done = self._to_tensor(done, dtype=torch.bool)\n# \n#         tensordict_out = TensorDict(\n#             obs_dict, batch_size=tensordict.batch_size, device=self.device\n#         )\n# \n#         tensordict_out.set(\"reward\", reward)\n#         tensordict_out.set(\"done\", done)\n#         if self.info_dict_reader is not None and info is not None:\n#             self.info_dict_reader(info, tensordict_out)\n# --------------------------------------------------\n# the below code fragment can be found in:\n# test/mocking_classes.py\n# --------------------------------------------------\n#         tensordict.set(self.out_key, self._get_out_obs(obs))\n#         tensordict.set(self._out_key, self._get_out_obs(obs))\n# \n#         done = torch.isclose(obs, torch.ones_like(obs) * (self.counter + 1))\n#         while done.shape != tensordict.shape:\n#             done = done.any(-1)\n#         done = reward = done.unsqueeze(-1)\n#         tensordict.set(\"reward\", reward.to(torch.get_default_dtype()))\n#         tensordict.set(\"done\", done)\n#         return tensordict\n# \n#     def _obs_step(self, obs, a):\n#         return obs + a / self.maxstep\n# \n# \n# class DiscreteActionVecPolicy:\n#     in_keys = [\"observation\"]\n#     out_keys = [\"action\"]\n# \n#     def _get_in_obs(self, tensordict):\n# --------------------------------------------------\n# the below code fragment can be found in:\n# test/mocking_classes.py\n# --------------------------------------------------\n# \n#         obs = self._obs_step(self._get_in_obs(tensordict.get(self._out_key)), a)\n#         tensordict = tensordict.select()  # empty tensordict\n# \n#         tensordict.set(self.out_key, self._get_out_obs(obs))\n#         tensordict.set(self._out_key, self._get_out_obs(obs))\n# \n#         done = torch.isclose(obs, torch.ones_like(obs) * (self.counter + 1))\n#         while done.shape != tensordict.shape:\n#             done = done.any(-1)\n#         done = reward = done.unsqueeze(-1)\n#         tensordict.set(\"reward\", reward.to(torch.get_default_dtype()))\n#         tensordict.set(\"done\", done)\n#         return tensordict\n# \n#     def _obs_step(self, obs, a):\n#         return obs + a / self.maxstep\n# \n# \n# class DiscreteActionVecPolicy:\n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\n be passed (in-place copy\n        # for tensors that are part of computational graphs will result in an error).\n        # It can also lead to inconsistencies when calling rollout.\n        cls._inplace_update = _inplace_update\n        cls._batch_locked = _batch_locked\n        cls._device = None\n        return super().__new__(cls)\n\n    def __setattr__(self, key, value):\n        if key in (\"_input_spec\", \"_observation_spec\", \"_action_spec\", \"_reward_spec\"):\n            raise AttributeError(\n                \"To set an environment spec, please use `env.observation_spec = obs_spec` (without the leading\"\n                \" underscore).\"\n            )\n        return super().__setattr__(key, value)\n\n    @property\n    def batch_locked(self) -> bool:\n        \"\"\"Whether the environnement can be used with a batch size different from the one it was initialized with or not.\n\n        If True, the env needs to be used with a tensordict having the same batch size as the env.\n        batch_locked is an immutable property.\n        \"\"\"\n        return self._batch_locked\n\n    @batch_locked.setter\n    def batch_locked(self, value: bool) -> None:\n        raise RuntimeError(\"batch_locked is a read-only property\")\n\n    @property\n    def run_type_checks(self) -> bool:\n        return self._run_type_checks\n\n    @run_type_checks.setter\n    def run_type_checks(self, run_type_checks: bool) -> None:\n        self._run_type_checks = run_type_checks\n\n    @property\n    def batch_size(self) -> TensorSpec:\n        if (\"_batch_size\" not in self.__dir__()) and (\n            \"_batch_size\" not in self.__class__.__dict__\n        ):\n            self._batch_size = torch.Size([])\n        return self._batch_size\n\n    @batch_size.setter\n    def batch_size(self, value: torch.Size) -> None:\n        self._batch_size = torch.Size(value)\n\n    @property\n    def action_spec(self) -> TensorSpec:\n        return self.input_spec[\"action\"]\n\n    @action_spec.setter\n    def action_spec(self, value: TensorSpec) -> None:\n        if self._input_spec is None:\n            self.input_spec = CompositeSpec(action=value, shape=self.batch_size)\n        else:\n            self.input_spec[\"action\"] = value\n\n    @property\n    def input_spec(self) -> TensorSpec:\n        return self._input_spec\n\n    @input_spec.setter\n    def input_spec(self, value: TensorSpec) -> None:\n        if not isinstance(value, CompositeSpec):\n            raise TypeError(\"The type of an input_spec must be Composite.\")\n        if value.shape[: len(self.batch_size)] != self.batch_size:\n            raise ValueError(\n                f\"The value of spec.shape ({value.shape}) must match the env batch size ({self.batch_size}).\"\n            )\n        self.__dict__[\"_input_spec\"] = value\n\n    @property\n    def reward_spec(self) -> TensorSpec:\n        return self._reward_spec\n\n    @reward_spec.setter\n    def reward_spec(self, value: TensorSpec) -> None:\n        if not hasattr(value, \"shape\"):\n            raise TypeError(\n                f\"reward_spec of type {type(value)} do not have a shape \" f\"attribute.\"\n            )\n        if value.shape[: len(self.batch_size)] != self.batch_size:\n            raise ValueError(\"The value of spec.shape must match the env batch size.\")\n        if len(value.shape) == 0:\n            raise RuntimeError(\n                \"the reward_spec shape cannot be empty (this error\"\n                \" usually comes from trying to set a reward_spec\"\n                \" with a null number of dimensions. Try using a multidimensional\"\n                \" spec instead, for instance with a singleton dimension at the tail).\"\n            )\n        self.__dict__[\"_reward_spec\"] = value\n\n    @property\n    def observation_spec(self) -> TensorSpec:\n        return self._observation_spec\n\n    @observation_spec.setter\n    def observation_spec(self, value: TensorSpec) -> None:\n        if not isinstance(value, CompositeSpec):\n            raise TypeError(\"The type of an observation_spec must be Composite.\")\n        elif value.shape[: len(self.batch_size)] != self.batch_size:\n            raise ValueError(\n                f\"The value of spec.shape ({value.shape}) must match the env batch size ({self.batch_size}).\"\n            )\n        self.__dict__[\"_observation_spec\"] = value\n\n    def step(self, tensordict: TensorDictBase) -> TensorDictBase:\n        \"\"\"Makes a step in the environment.\n\n        Step accepts a single argument, tensordict, which usually carries an 'action' key which indicates the action\n        to be taken.\n        Step will call an out-place private method, _step, which is the method to be re-written by EnvBase subclasses.\n\n        Args:\n            tensordict (TensorDictBase): Tensordict containing the action to be taken.\n\n        Returns:\n            the input tensordict, modified in place with the resulting observations, done state and reward\n            (+ others if needed).\n\n        \"\"\"\n        # sanity check\n        self._assert_tensordict_shape(tensordict)\n\n        tensordict.lock()  # make sure _step does not modify the tensordict\n        tensordict_out = self._step(tensordict)\n        if tensordict_out is tensordict:\n            raise RuntimeError(\n                \"EnvBase._step should return outplace changes to the input \"\n                \"tensordict. Consider emptying the TensorDict first (e.g. tensordict.empty() or \"\n                \"tensordict.select()) inside _step before writing new tensors onto this new instance.\"\n            )\n        tensordict.unlock()\n\n        obs_keys = set(self.observation_spec.keys())\n        tensordict_out_select = tensordict_out.select(*obs_keys)\n        tensordict_out = tensordict_out.exclude(*obs_keys, inplace=True)\n        tensordict_out.set(\"next\", tensordict_out_select)\n\n        reward = tensordict_out.get(\"reward\")\n        # unsqueeze rewards if needed\n        # the input tensordict may have more leading dimensions than the batch_size\n        # e.g. in model-based contexts.\n        batch_size = self.batch_size\n        dims = len(batch_size)\n        leading_batch_size = (\n            tensordict_out.batch_size[:-dims] if dims else tensordict_out.shape\n        )\n        expected_reward_shape = torch.Size(\n            [*leading_batch_size, *self.reward_spec.shape]\n        )\n        actual_reward_shape = reward.shape\n        if actual_reward_shape != expected_reward_shape:\n            reward = reward.view(expected_reward_shape)\n            tensordict_out.set(\"reward\", reward)\n\n        done = tensordict_out.get(\"done\")\n        # unsqueeze done if needed\n        expected_done_shape = torch.Size([*leading_batch_size, *batch_size, 1])\n        actual_done_shape = done.shape\n        if actual_done_shape != expected_done_shape:\n            done = done.view(expected_done_shape)\n            tensordict_out.set(\"done\", done)\n\n        if self.run_type_checks:\n            for key in self._select_observation_keys(tensordict_out):\n                obs = tensordict_out.get(key)\n                self.observation_spec.type_check(obs, key)\n\n            if tensordict_out.get(\"reward\").dtype is not self.reward_spec.dtype:", "metadata": {"task_id": "pytorch_rl/40", "ground_truth": "                raise TypeError(", "fpath_tuple": ["pytorch_rl", "torchrl", "envs", "common.py"], "context_start_lineno": 251, "line_no": 424, "query_window": {"context": "        )\n        actual_reward_shape = reward.shape\n        if actual_reward_shape != expected_reward_shape:\n            reward = reward.view(expected_reward_shape)\n            tensordict_out.set(\"reward\", reward)\n\n        done = tensordict_out.get(\"done\")\n        # unsqueeze done if needed\n        expected_done_shape = torch.Size([*leading_batch_size, *batch_size, 1])\n        actual_done_shape = done.shape\n        if actual_done_shape != expected_done_shape:\n            done = done.view(expected_done_shape)\n            tensordict_out.set(\"done\", done)\n\n        if self.run_type_checks:\n            for key in self._select_observation_keys(tensordict_out):\n                obs = tensordict_out.get(key)\n                self.observation_spec.type_check(obs, key)\n\n            if tensordict_out.get(\"reward\").dtype is not self.reward_spec.dtype:", "metadata": {"fpath_tuple": ["pytorch_rl", "torchrl", "envs", "common.py"], "line_no": 424, "task_id": "pytorch_rl/40", "start_line_no": 404, "end_line_no": 424, "window_size": 20, "context_start_lineno": 251, "repo": "pytorch_rl"}}, "top_k_context": [{"context": "        tensordict = tensordict.to(self.device)\n        a = tensordict.get(\"action\")\n\n        obs = self._obs_step(self._get_in_obs(tensordict.get(self._out_key)), a)\n        tensordict = tensordict.select()  # empty tensordict\n\n        tensordict.set(self.out_key, self._get_out_obs(obs))\n        tensordict.set(self._out_key, self._get_out_obs(obs))\n\n        done = torch.isclose(obs, torch.ones_like(obs) * (self.counter + 1))\n        while done.shape != tensordict.shape:\n            done = done.any(-1)\n        done = reward = done.unsqueeze(-1)\n        tensordict.set(\"reward\", reward.to(torch.get_default_dtype()))\n        tensordict.set(\"done\", done)\n        return tensordict\n\n    def _obs_step(self, obs, a):\n        return obs + a / self.maxstep\n", "metadata": [{"fpath_tuple": ["pytorch_rl", "test", "mocking_classes.py"], "line_no": 508, "start_line_no": 498, "end_line_no": 518, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}], "sim_score": 0.4166666666666667}, {"context": "        tensordict = tensordict.select()  # empty tensordict\n\n        tensordict.set(self.out_key, self._get_out_obs(obs))\n        tensordict.set(self._out_key, self._get_out_obs(obs))\n\n        done = torch.isclose(obs, torch.ones_like(obs) * (self.counter + 1))\n        while done.shape != tensordict.shape:\n            done = done.any(-1)\n        done = reward = done.unsqueeze(-1)\n        tensordict.set(\"reward\", reward.to(torch.get_default_dtype()))\n        tensordict.set(\"done\", done)\n        return tensordict\n\n    def _obs_step(self, obs, a):\n        return obs + a / self.maxstep\n\n\nclass DiscreteActionVecPolicy:\n    in_keys = [\"observation\"]\n    out_keys = [\"action\"]", "metadata": [{"fpath_tuple": ["pytorch_rl", "test", "mocking_classes.py"], "line_no": 512, "start_line_no": 502, "end_line_no": 522, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}], "sim_score": 0.40869565217391307}, {"context": "            ):\n                done = torch.tensor([done], device=self.device)\n\n            done, do_break = self.read_done(done)\n            if do_break:\n                break\n\n        obs_dict = self.read_obs(obs)\n\n        if reward is None:\n            reward = np.nan\n        reward = self._to_tensor(reward, dtype=self.reward_spec.dtype)\n        done = self._to_tensor(done, dtype=torch.bool)\n\n        tensordict_out = TensorDict(\n            obs_dict, batch_size=tensordict.batch_size, device=self.device\n        )\n\n        tensordict_out.set(\"reward\", reward)\n        tensordict_out.set(\"done\", done)", "metadata": [{"fpath_tuple": ["pytorch_rl", "torchrl", "envs", "gym_like.py"], "line_no": 218, "start_line_no": 208, "end_line_no": 228, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}], "sim_score": 0.40816326530612246}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# ding/entry/tests/test_serial_entry.py\n# --------------------------------------------------\n#     config = [deepcopy(cartpole_r2d2_config), deepcopy(cartpole_r2d2_create_config)]\n#     config[0].policy.learn.update_per_collect = 1\n#     try:\n#         serial_pipeline(config, seed=0, max_iterations=1)\n#     except Exception:\n#         assert False, \"pipeline fail\"\n# \n# \n# @pytest.mark.unittest\n# def test_a2c_with_nstep_return():\n#     config = [deepcopy(cartpole_a2c_config), deepcopy(cartpole_a2c_create_config)]\n#     config[0].policy.learn.update_per_collect = 1\n#     config[0].policy.learn.nstep_return = config[0].policy.collect.nstep_return = True\n#     config[0].policy.collect.discount_factor = 0.9\n#     config[0].policy.collect.nstep = 3\n#     try:\n#         serial_pipeline(config, seed=0, max_iterations=1)\n#     except Exception:\n#         assert False, \"pipeline fail\"\n# \n# --------------------------------------------------\n# the below code fragment can be found in:\n# ding/entry/tests/test_serial_entry.py\n# --------------------------------------------------\n# @pytest.mark.unittest\n# def test_r2d2():\n#     config = [deepcopy(cartpole_r2d2_config), deepcopy(cartpole_r2d2_create_config)]\n#     config[0].policy.learn.update_per_collect = 1\n#     try:\n#         serial_pipeline(config, seed=0, max_iterations=1)\n#     except Exception:\n#         assert False, \"pipeline fail\"\n# \n# \n# @pytest.mark.unittest\n# def test_a2c_with_nstep_return():\n#     config = [deepcopy(cartpole_a2c_config), deepcopy(cartpole_a2c_create_config)]\n#     config[0].policy.learn.update_per_collect = 1\n#     config[0].policy.learn.nstep_return = config[0].policy.collect.nstep_return = True\n#     config[0].policy.collect.discount_factor = 0.9\n#     config[0].policy.collect.nstep = 3\n#     try:\n#         serial_pipeline(config, seed=0, max_iterations=1)\n#     except Exception:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# ding/entry/tests/test_serial_entry.py\n# --------------------------------------------------\n#     try:\n#         serial_pipeline(config, seed=0, max_iterations=1)\n#     except Exception:\n#         assert False, \"pipeline fail\"\n# \n# \n# @pytest.mark.unittest\n# def test_a2c_with_nstep_return():\n#     config = [deepcopy(cartpole_a2c_config), deepcopy(cartpole_a2c_create_config)]\n#     config[0].policy.learn.update_per_collect = 1\n#     config[0].policy.learn.nstep_return = config[0].policy.collect.nstep_return = True\n#     config[0].policy.collect.discount_factor = 0.9\n#     config[0].policy.collect.nstep = 3\n#     try:\n#         serial_pipeline(config, seed=0, max_iterations=1)\n#     except Exception:\n#         assert False, \"pipeline fail\"\n# \n# \n# @pytest.mark.unittest\n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\n_config, cooperative_navigation_wqmix_create_config  # noqa\nfrom dizoo.multiagent_particle.config import cooperative_navigation_vdn_config, cooperative_navigation_vdn_create_config  # noqa\nfrom dizoo.multiagent_particle.config import cooperative_navigation_coma_config, cooperative_navigation_coma_create_config  # noqa\nfrom dizoo.multiagent_particle.config import cooperative_navigation_collaq_config, cooperative_navigation_collaq_create_config  # noqa\nfrom dizoo.multiagent_particle.config import cooperative_navigation_atoc_config, cooperative_navigation_atoc_create_config  # noqa\nfrom dizoo.league_demo.league_demo_ppo_config import league_demo_ppo_config\nfrom dizoo.league_demo.selfplay_demo_ppo_main import main as selfplay_main\nfrom dizoo.league_demo.league_demo_ppo_main import main as league_main\nfrom dizoo.classic_control.pendulum.config.pendulum_sac_data_generation_default_config import pendulum_sac_data_genearation_default_config, pendulum_sac_data_genearation_default_create_config  # noqa\nfrom dizoo.classic_control.pendulum.config.pendulum_cql_config import pendulum_cql_default_config, pendulum_cql_default_create_config  # noqa\nfrom dizoo.classic_control.cartpole.config.cartpole_qrdqn_generation_data_config import cartpole_qrdqn_generation_data_config, cartpole_qrdqn_generation_data_create_config  # noqa\nfrom dizoo.classic_control.cartpole.config.cartpole_cql_config import cartpole_discrete_cql_config, cartpole_discrete_cql_create_config  # noqa\n\nwith open(\"./algo_record.log\", \"w+\") as f:\n    f.write(\"ALGO TEST STARTS\\n\")\n\n\n@pytest.mark.algotest\ndef test_dqn():\n    config = [deepcopy(cartpole_dqn_config), deepcopy(cartpole_dqn_create_config)]\n    try:\n        serial_pipeline(config, seed=0)\n    except Exception:\n        assert False, \"pipeline fail\"\n    with open(\"./algo_record.log\", \"a+\") as f:\n        f.write(\"1. dqn\\n\")\n\n\n@pytest.mark.algotest\ndef test_ddpg():\n    config = [deepcopy(pendulum_ddpg_config), deepcopy(pendulum_ddpg_create_config)]\n    try:\n        serial_pipeline(config, seed=0)\n    except Exception:\n        assert False, \"pipeline fail\"\n    with open(\"./algo_record.log\", \"a+\") as f:\n        f.write(\"2. ddpg\\n\")\n\n\n@pytest.mark.algotest\ndef test_td3():\n    config = [deepcopy(pendulum_td3_config), deepcopy(pendulum_td3_create_config)]\n    try:\n        serial_pipeline(config, seed=0)\n    except Exception:\n        assert False, \"pipeline fail\"\n    with open(\"./algo_record.log\", \"a+\") as f:\n        f.write(\"3. td3\\n\")\n\n\n@pytest.mark.algotest\ndef test_a2c():\n    config = [deepcopy(cartpole_a2c_config), deepcopy(cartpole_a2c_create_config)]\n    try:\n        serial_pipeline(config, seed=0)\n    except Exception:\n        assert False, \"pipeline fail\"\n    with open(\"./algo_record.log\", \"a+\") as f:\n        f.write(\"4. a2c\\n\")\n\n\n@pytest.mark.algotest\ndef test_rainbow():\n    config = [deepcopy(cartpole_rainbow_config), deepcopy(cartpole_rainbow_create_config)]\n    try:\n        serial_pipeline(config, seed=0)\n    except Exception:\n        assert False, \"pipeline fail\"\n    with open(\"./algo_record.log\", \"a+\") as f:\n        f.write(\"5. rainbow\\n\")\n\n\n@pytest.mark.algotest\ndef test_ppo():\n    config = [deepcopy(cartpole_ppo_config), deepcopy(cartpole_ppo_create_config)]\n    try:\n        ppo_main(config[0], seed=0)\n    except Exception:\n        assert False, \"pipeline fail\"\n    with open(\"./algo_record.log\", \"a+\") as f:\n        f.write(\"6. ppo\\n\")\n\n\n# @pytest.mark.algotest\ndef test_collaq():\n    config = [deepcopy(cooperative_navigation_collaq_config), deepcopy(cooperative_navigation_collaq_create_config)]\n    try:\n        serial_pipeline(config, seed=0)\n    except Exception:\n        assert False, \"pipeline fail\"\n    with open(\"./algo_record.log\", \"a+\") as f:\n        f.write(\"7. collaq\\n\")\n\n\n# @pytest.mark.algotest\ndef test_coma():\n    config = [deepcopy(cooperative_navigation_coma_config), deepcopy(cooperative_navigation_coma_create_config)]\n    try:\n        serial_pipeline(config, seed=0)\n    except Exception:\n        assert False, \"pipeline fail\"\n    with open(\"./algo_record.log\", \"a+\") as f:\n        f.write(\"8. coma\\n\")\n\n\n@pytest.mark.algotest\ndef test_sac():\n    config = [deepcopy(pendulum_sac_config), deepcopy(pendulum_sac_create_config)]\n    try:\n        serial_pipeline(config, seed=0)\n    except Exception:\n        assert False, \"pipeline fail\"\n    with open(\"./algo_record.log\", \"a+\") as f:\n        f.write(\"9. sac\\n\")\n\n\n@pytest.mark.algotest\ndef test_c51():\n    config = [deepcopy(cartpole_c51_config), deepcopy(cartpole_c51_create_config)]\n    try:\n        serial_pipeline(config, seed=0)\n    except Exception:\n        assert False, \"pipeline fail\"\n    with open(\"./algo_record.log\", \"a+\") as f:\n        f.write(\"10. c51\\n\")\n\n\n@pytest.mark.algotest\ndef test_r2d2():\n    config = [deepcopy(cartpole_r2d2_config), deepcopy(cartpole_r2d2_create_config)]\n    try:\n        serial_pipeline(config, seed=0)\n    except Exception:\n        assert False, \"pipeline fail\"\n    with open(\"./algo_record.log\", \"a+\") as f:\n        f.write(\"11. r2d2\\n\")\n\n\n@pytest.mark.algotest\ndef test_a2c_with_nstep_return():\n    config = [deepcopy(cartpole_a2c_config), deepcopy(cartpole_a2c_create_config)]\n    config[0].policy.learn.nstep_return = config[0].policy.collect.nstep_return = True\n    config[0].policy.collect.discount_factor = 0.9\n    config[0].policy.collect.nstep = 3\n    try:\n        serial_pipeline(config, seed=0)\n    except Exception:\n        assert False, \"pipeline fail\"\n    with open(\"./algo_record.log\", \"a+\") as f:\n        f.write(\"12. a2c with nstep return\\n\")\n\n\n# @pytest.mark.algotest", "metadata": {"task_id": "opendilab_ACE/83", "ground_truth": "def test_atoc():", "fpath_tuple": ["opendilab_ACE", "ding", "entry", "tests", "test_serial_entry_algo.py"], "context_start_lineno": 32, "line_no": 185, "query_window": {"context": "        assert False, \"pipeline fail\"\n    with open(\"./algo_record.log\", \"a+\") as f:\n        f.write(\"11. r2d2\\n\")\n\n\n@pytest.mark.algotest\ndef test_a2c_with_nstep_return():\n    config = [deepcopy(cartpole_a2c_config), deepcopy(cartpole_a2c_create_config)]\n    config[0].policy.learn.nstep_return = config[0].policy.collect.nstep_return = True\n    config[0].policy.collect.discount_factor = 0.9\n    config[0].policy.collect.nstep = 3\n    try:\n        serial_pipeline(config, seed=0)\n    except Exception:\n        assert False, \"pipeline fail\"\n    with open(\"./algo_record.log\", \"a+\") as f:\n        f.write(\"12. a2c with nstep return\\n\")\n\n\n# @pytest.mark.algotest", "metadata": {"fpath_tuple": ["opendilab_ACE", "ding", "entry", "tests", "test_serial_entry_algo.py"], "line_no": 185, "task_id": "opendilab_ACE/83", "start_line_no": 165, "end_line_no": 185, "window_size": 20, "context_start_lineno": 32, "repo": "opendilab_ACE"}}, "top_k_context": [{"context": "    config = [deepcopy(cartpole_r2d2_config), deepcopy(cartpole_r2d2_create_config)]\n    config[0].policy.learn.update_per_collect = 1\n    try:\n        serial_pipeline(config, seed=0, max_iterations=1)\n    except Exception:\n        assert False, \"pipeline fail\"\n\n\n@pytest.mark.unittest\ndef test_a2c_with_nstep_return():\n    config = [deepcopy(cartpole_a2c_config), deepcopy(cartpole_a2c_create_config)]\n    config[0].policy.learn.update_per_collect = 1\n    config[0].policy.learn.nstep_return = config[0].policy.collect.nstep_return = True\n    config[0].policy.collect.discount_factor = 0.9\n    config[0].policy.collect.nstep = 3\n    try:\n        serial_pipeline(config, seed=0, max_iterations=1)\n    except Exception:\n        assert False, \"pipeline fail\"\n", "metadata": [{"fpath_tuple": ["opendilab_ACE", "ding", "entry", "tests", "test_serial_entry.py"], "line_no": 168, "start_line_no": 158, "end_line_no": 178, "window_size": 20, "repo": "opendilab_ACE", "slice_size": 10}], "sim_score": 0.6435643564356436}, {"context": "\n\n@pytest.mark.unittest\ndef test_r2d2():\n    config = [deepcopy(cartpole_r2d2_config), deepcopy(cartpole_r2d2_create_config)]\n    config[0].policy.learn.update_per_collect = 1\n    try:\n        serial_pipeline(config, seed=0, max_iterations=1)\n    except Exception:\n        assert False, \"pipeline fail\"\n\n\n@pytest.mark.unittest\ndef test_a2c_with_nstep_return():\n    config = [deepcopy(cartpole_a2c_config), deepcopy(cartpole_a2c_create_config)]\n    config[0].policy.learn.update_per_collect = 1\n    config[0].policy.learn.nstep_return = config[0].policy.collect.nstep_return = True\n    config[0].policy.collect.discount_factor = 0.9\n    config[0].policy.collect.nstep = 3\n    try:", "metadata": [{"fpath_tuple": ["opendilab_ACE", "ding", "entry", "tests", "test_serial_entry.py"], "line_no": 164, "start_line_no": 154, "end_line_no": 174, "window_size": 20, "repo": "opendilab_ACE", "slice_size": 10}], "sim_score": 0.6435643564356436}, {"context": "@pytest.mark.unittest\ndef test_r2d2():\n    config = [deepcopy(cartpole_r2d2_config), deepcopy(cartpole_r2d2_create_config)]\n    config[0].policy.learn.update_per_collect = 1\n    try:\n        serial_pipeline(config, seed=0, max_iterations=1)\n    except Exception:\n        assert False, \"pipeline fail\"\n\n\n@pytest.mark.unittest\ndef test_a2c_with_nstep_return():\n    config = [deepcopy(cartpole_a2c_config), deepcopy(cartpole_a2c_create_config)]\n    config[0].policy.learn.update_per_collect = 1\n    config[0].policy.learn.nstep_return = config[0].policy.collect.nstep_return = True\n    config[0].policy.collect.discount_factor = 0.9\n    config[0].policy.collect.nstep = 3\n    try:\n        serial_pipeline(config, seed=0, max_iterations=1)\n    except Exception:", "metadata": [{"fpath_tuple": ["opendilab_ACE", "ding", "entry", "tests", "test_serial_entry.py"], "line_no": 166, "start_line_no": 156, "end_line_no": 176, "window_size": 20, "repo": "opendilab_ACE", "slice_size": 10}], "sim_score": 0.6435643564356436}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# src/diffusers/models/unet_2d_blocks.py\n# src/diffusers/models/unet_2d_blocks.py\n# --------------------------------------------------\n#         prev_output_channel: int,\n#         out_channels: int,\n#         temb_channels: int,\n#         dropout: float = 0.0,\n#         num_layers: int = 1,\n#         resnet_eps: float = 1e-6,\n#         resnet_time_scale_shift: str = \"default\",\n#         resnet_act_fn: str = \"swish\",\n#         resnet_groups: int = 32,\n#         resnet_pre_norm: bool = True,\n#         output_scale_factor=1.0,\n#         add_upsample=True,\n#     ):\n#         super().__init__()\n#         resnets = []\n# \n#         for i in range(num_layers):\n#             res_skip_channels = in_channels if (i == num_layers - 1) else out_channels\n#             resnet_in_channels = prev_output_channel if i == 0 else out_channels\n# \n# --------------------------------------------------\n# the below code fragment can be found in:\n# src/diffusers/models/unet_2d_blocks.py\n# src/diffusers/pipelines/versatile_diffusion/modeling_text_unet.py\n# --------------------------------------------------\n#         in_channels: int,\n#         out_channels: int,\n#         temb_channels: int,\n#         dropout: float = 0.0,\n#         num_layers: int = 1,\n#         resnet_eps: float = 1e-6,\n#         resnet_time_scale_shift: str = \"default\",\n#         resnet_act_fn: str = \"swish\",\n#         resnet_groups: int = 32,\n#         resnet_pre_norm: bool = True,\n#         output_scale_factor=1.0,\n#         add_downsample=True,\n#         downsample_padding=1,\n#     ):\n#         super().__init__()\n#         resnets = []\n# \n#         for i in range(num_layers):\n#             in_channels = in_channels if i == 0 else out_channels\n#             resnets.append(\n# --------------------------------------------------\n# the below code fragment can be found in:\n# src/diffusers/pipelines/versatile_diffusion/modeling_text_unet.py\n# --------------------------------------------------\n#         in_channels: int,\n#         prev_output_channel: int,\n#         out_channels: int,\n#         temb_channels: int,\n#         dropout: float = 0.0,\n#         num_layers: int = 1,\n#         resnet_eps: float = 1e-6,\n#         resnet_time_scale_shift: str = \"default\",\n#         resnet_act_fn: str = \"swish\",\n#         resnet_groups: int = 32,\n#         resnet_pre_norm: bool = True,\n#         output_scale_factor=1.0,\n#         add_upsample=True,\n#     ):\n#         super().__init__()\n#         resnets = []\n# \n#         for i in range(num_layers):\n#             res_skip_channels = in_channels if (i == num_layers - 1) else out_channels\n#             resnet_in_channels = prev_output_channel if i == 0 else out_channels\n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\nupsampler = FirUpsample2D(in_channels, out_channels=out_channels)\n        if add_upsample:\n            self.resnet_up = ResnetBlock2D(\n                in_channels=out_channels,\n                out_channels=out_channels,\n                temb_channels=temb_channels,\n                eps=resnet_eps,\n                groups=min(out_channels // 4, 32),\n                groups_out=min(out_channels // 4, 32),\n                dropout=dropout,\n                time_embedding_norm=resnet_time_scale_shift,\n                non_linearity=resnet_act_fn,\n                output_scale_factor=output_scale_factor,\n                pre_norm=resnet_pre_norm,\n                use_in_shortcut=True,\n                up=True,\n                kernel=\"fir\",\n            )\n            self.skip_conv = nn.Conv2d(out_channels, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            self.skip_norm = torch.nn.GroupNorm(\n                num_groups=min(out_channels // 4, 32), num_channels=out_channels, eps=resnet_eps, affine=True\n            )\n            self.act = nn.SiLU()\n        else:\n            self.resnet_up = None\n            self.skip_conv = None\n            self.skip_norm = None\n            self.act = None\n\n    def forward(self, hidden_states, res_hidden_states_tuple, temb=None, skip_sample=None):\n        for resnet in self.resnets:\n            # pop res hidden states\n            res_hidden_states = res_hidden_states_tuple[-1]\n            res_hidden_states_tuple = res_hidden_states_tuple[:-1]\n            hidden_states = torch.cat([hidden_states, res_hidden_states], dim=1)\n\n            hidden_states = resnet(hidden_states, temb)\n\n        hidden_states = self.attentions[0](hidden_states)\n\n        if skip_sample is not None:\n            skip_sample = self.upsampler(skip_sample)\n        else:\n            skip_sample = 0\n\n        if self.resnet_up is not None:\n            skip_sample_states = self.skip_norm(hidden_states)\n            skip_sample_states = self.act(skip_sample_states)\n            skip_sample_states = self.skip_conv(skip_sample_states)\n\n            skip_sample = skip_sample + skip_sample_states\n\n            hidden_states = self.resnet_up(hidden_states, temb)\n\n        return hidden_states, skip_sample\n\n\nclass SkipUpBlock2D(nn.Module):\n    def __init__(\n        self,\n        in_channels: int,\n        prev_output_channel: int,\n        out_channels: int,\n        temb_channels: int,\n        dropout: float = 0.0,\n        num_layers: int = 1,\n        resnet_eps: float = 1e-6,\n        resnet_time_scale_shift: str = \"default\",\n        resnet_act_fn: str = \"swish\",\n        resnet_pre_norm: bool = True,\n        output_scale_factor=np.sqrt(2.0),\n        add_upsample=True,\n        upsample_padding=1,\n    ):\n        super().__init__()\n        self.resnets = nn.ModuleList([])\n\n        for i in range(num_layers):\n            res_skip_channels = in_channels if (i == num_layers - 1) else out_channels\n            resnet_in_channels = prev_output_channel if i == 0 else out_channels\n\n            self.resnets.append(\n                ResnetBlock2D(\n                    in_channels=resnet_in_channels + res_skip_channels,\n                    out_channels=out_channels,\n                    temb_channels=temb_channels,\n                    eps=resnet_eps,\n                    groups=min((resnet_in_channels + res_skip_channels) // 4, 32),\n                    groups_out=min(out_channels // 4, 32),\n                    dropout=dropout,\n                    time_embedding_norm=resnet_time_scale_shift,\n                    non_linearity=resnet_act_fn,\n                    output_scale_factor=output_scale_factor,\n                    pre_norm=resnet_pre_norm,\n                )\n            )\n\n        self.upsampler = FirUpsample2D(in_channels, out_channels=out_channels)\n        if add_upsample:\n            self.resnet_up = ResnetBlock2D(\n                in_channels=out_channels,\n                out_channels=out_channels,\n                temb_channels=temb_channels,\n                eps=resnet_eps,\n                groups=min(out_channels // 4, 32),\n                groups_out=min(out_channels // 4, 32),\n                dropout=dropout,\n                time_embedding_norm=resnet_time_scale_shift,\n                non_linearity=resnet_act_fn,\n                output_scale_factor=output_scale_factor,\n                pre_norm=resnet_pre_norm,\n                use_in_shortcut=True,\n                up=True,\n                kernel=\"fir\",\n            )\n            self.skip_conv = nn.Conv2d(out_channels, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            self.skip_norm = torch.nn.GroupNorm(\n                num_groups=min(out_channels // 4, 32), num_channels=out_channels, eps=resnet_eps, affine=True\n            )\n            self.act = nn.SiLU()\n        else:\n            self.resnet_up = None\n            self.skip_conv = None\n            self.skip_norm = None\n            self.act = None\n\n    def forward(self, hidden_states, res_hidden_states_tuple, temb=None, skip_sample=None):\n        for resnet in self.resnets:\n            # pop res hidden states\n            res_hidden_states = res_hidden_states_tuple[-1]\n            res_hidden_states_tuple = res_hidden_states_tuple[:-1]\n            hidden_states = torch.cat([hidden_states, res_hidden_states], dim=1)\n\n            hidden_states = resnet(hidden_states, temb)\n\n        if skip_sample is not None:\n            skip_sample = self.upsampler(skip_sample)\n        else:\n            skip_sample = 0\n\n        if self.resnet_up is not None:\n            skip_sample_states = self.skip_norm(hidden_states)\n            skip_sample_states = self.act(skip_sample_states)\n            skip_sample_states = self.skip_conv(skip_sample_states)\n\n            skip_sample = skip_sample + skip_sample_states\n\n            hidden_states = self.resnet_up(hidden_states, temb)\n\n        return hidden_states, skip_sample\n\n\nclass ResnetUpsampleBlock2D(nn.Module):\n    def __init__(\n        self,\n        in_channels: int,\n        prev_output_channel: int,\n        out_channels: int,\n        temb_channels: int,\n        dropout: float = 0.0,\n        num_layers: int = 1,\n        resnet_eps: float = 1e-6,\n        resnet_time_scale_shift: str = \"default\",\n        resnet_act_fn: str = \"swish\",\n        resnet_groups: int = 32,\n        resnet_pre_norm: bool = True,\n        output_scale_factor=1.0,\n        add_upsample=True,\n    ):\n        super().__init__()\n        resnets = []\n\n        for i in range(num_layers):", "metadata": {"task_id": "huggingface_diffusers/46", "ground_truth": "            res_skip_channels = in_channels if (i == num_layers - 1) else out_channels", "fpath_tuple": ["huggingface_diffusers", "src", "diffusers", "models", "unet_2d_blocks.py"], "context_start_lineno": 1840, "line_no": 2013, "query_window": {"context": "    def __init__(\n        self,\n        in_channels: int,\n        prev_output_channel: int,\n        out_channels: int,\n        temb_channels: int,\n        dropout: float = 0.0,\n        num_layers: int = 1,\n        resnet_eps: float = 1e-6,\n        resnet_time_scale_shift: str = \"default\",\n        resnet_act_fn: str = \"swish\",\n        resnet_groups: int = 32,\n        resnet_pre_norm: bool = True,\n        output_scale_factor=1.0,\n        add_upsample=True,\n    ):\n        super().__init__()\n        resnets = []\n\n        for i in range(num_layers):", "metadata": {"fpath_tuple": ["huggingface_diffusers", "src", "diffusers", "models", "unet_2d_blocks.py"], "line_no": 2013, "task_id": "huggingface_diffusers/46", "start_line_no": 1993, "end_line_no": 2013, "window_size": 20, "context_start_lineno": 1840, "repo": "huggingface_diffusers"}}, "top_k_context": [{"context": "    def __init__(\n        self,\n        in_channels: int,\n        prev_output_channel: int,\n        out_channels: int,\n        temb_channels: int,\n        dropout: float = 0.0,\n        num_layers: int = 1,\n        resnet_eps: float = 1e-6,\n        resnet_time_scale_shift: str = \"default\",\n        resnet_act_fn: str = \"swish\",\n        resnet_groups: int = 32,\n        resnet_pre_norm: bool = True,\n        output_scale_factor=1.0,\n        add_upsample=True,\n    ):\n        super().__init__()\n        resnets = []\n\n        for i in range(num_layers):", "metadata": [{"fpath_tuple": ["huggingface_diffusers", "src", "diffusers", "pipelines", "versatile_diffusion", "modeling_text_unet.py"], "line_no": 968, "start_line_no": 958, "end_line_no": 978, "window_size": 20, "repo": "huggingface_diffusers", "slice_size": 10}], "sim_score": 1.0}, {"context": "    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        temb_channels: int,\n        dropout: float = 0.0,\n        num_layers: int = 1,\n        resnet_eps: float = 1e-6,\n        resnet_time_scale_shift: str = \"default\",\n        resnet_act_fn: str = \"swish\",\n        resnet_groups: int = 32,\n        resnet_pre_norm: bool = True,\n        output_scale_factor=1.0,\n        add_downsample=True,\n        downsample_padding=1,\n    ):\n        super().__init__()\n        resnets = []\n\n        for i in range(num_layers):", "metadata": [{"fpath_tuple": ["huggingface_diffusers", "src", "diffusers", "models", "unet_2d_blocks.py"], "line_no": 816, "start_line_no": 806, "end_line_no": 826, "window_size": 20, "repo": "huggingface_diffusers", "slice_size": 10}, {"fpath_tuple": ["huggingface_diffusers", "src", "diffusers", "pipelines", "versatile_diffusion", "modeling_text_unet.py"], "line_no": 756, "start_line_no": 746, "end_line_no": 766, "window_size": 20, "repo": "huggingface_diffusers", "slice_size": 10}], "sim_score": 0.9113924050632911}, {"context": "        self,\n        in_channels: int,\n        prev_output_channel: int,\n        out_channels: int,\n        temb_channels: int,\n        dropout: float = 0.0,\n        num_layers: int = 1,\n        resnet_eps: float = 1e-6,\n        resnet_time_scale_shift: str = \"default\",\n        resnet_act_fn: str = \"swish\",\n        resnet_groups: int = 32,\n        resnet_pre_norm: bool = True,\n        output_scale_factor=1.0,\n        add_upsample=True,\n    ):\n        super().__init__()\n        resnets = []\n\n        for i in range(num_layers):\n            res_skip_channels = in_channels if (i == num_layers - 1) else out_channels", "metadata": [{"fpath_tuple": ["huggingface_diffusers", "src", "diffusers", "models", "unet_2d_blocks.py"], "line_no": 1606, "start_line_no": 1596, "end_line_no": 1616, "window_size": 20, "repo": "huggingface_diffusers", "slice_size": 10}, {"fpath_tuple": ["huggingface_diffusers", "src", "diffusers", "models", "unet_2d_blocks.py"], "line_no": 2004, "start_line_no": 1994, "end_line_no": 2014, "window_size": 20, "repo": "huggingface_diffusers", "slice_size": 10}], "sim_score": 0.8705882352941177}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# torchrl/data/tensor_specs.py\n# --------------------------------------------------\n#             value = {selected_keys: value}\n#             selected_keys = [selected_keys]\n# \n#         for _key in self:\n#             if self[_key] is not None and (\n#                 selected_keys is None or _key in selected_keys\n#             ):\n#                 self._specs[_key].type_check(value[_key], _key)\n# \n#     def is_in(self, val: Union[dict, TensorDictBase]) -> bool:\n#         return all(\n#             [\n#                 item.is_in(val.get(key))\n#                 for (key, item) in self._specs.items()\n#                 if item is not None\n#             ]\n#         )\n# \n#     def project(self, val: TensorDictBase) -> TensorDictBase:\n#         for key, item in self.items():\n# --------------------------------------------------\n# the below code fragment can be found in:\n# torchrl/collectors/collectors.py\n# --------------------------------------------------\n#             create_env_kwargs (dictionary): kwargs for the env creator\n#             policy (TensorDictModule, optional): a policy to be used\n#             device (int, str or torch.device, optional): device where to place\n#                 the policy\n#             observation_spec (TensorSpec, optional): spec of the observations\n# \n#         \"\"\"\n#         # if create_env_fn is not None:\n#         #     if create_env_kwargs is None:\n#         #         create_env_kwargs = {}\n#         #     self.create_env_fn = create_env_fn\n#         #     if isinstance(create_env_fn, EnvBase):\n#         #         env = create_env_fn\n#         #     else:\n#         #         env = self.create_env_fn(**create_env_kwargs)\n#         # else:\n#         #     env = None\n# \n#         if policy is None:\n#             if not hasattr(self, \"env\") or self.env is None:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# torchrl/envs/gym_like.py\n# --------------------------------------------------\n#         if not isinstance(info_dict, dict) and len(self.keys):\n#             warnings.warn(\n#                 f\"Found an info_dict of type {type(info_dict)} \"\n#                 f\"but expected type or subtype `dict`.\"\n#             )\n#         for key in self.keys:\n#             if key in info_dict:\n#                 tensordict[key] = info_dict[key]\n#         return tensordict\n# \n#     @property\n#     def info_spec(self) -> Dict[str, TensorSpec]:\n#         return self._info_spec\n# \n# \n# class GymLikeEnv(_EnvWrapper):\n#     \"\"\"A gym-like env is an environment.\n# \n#     Its behaviour is similar to gym environments in what common methods (specifically reset and step) are expected to do.\n# \n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\n# Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom __future__ import annotations\n\nfrom collections import OrderedDict\nfrom typing import Callable, Dict, Optional, Union\n\nimport torch\nfrom tensordict.tensordict import TensorDictBase\n\nfrom torchrl.data.utils import CloudpickleWrapper\nfrom torchrl.envs.common import EnvBase, EnvMetaData\n\n\nclass EnvCreator:\n    \"\"\"Environment creator class.\n\n    EnvCreator is a generic environment creator class that can substitute\n    lambda functions when creating environments in multiprocessing contexts.\n    If the environment created on a subprocess must share information with the\n    main process (e.g. for the VecNorm transform), EnvCreator will pass the\n    pointers to the tensordicts in shared memory to each process such that\n    all of them are synchronised.\n\n    Args:\n        create_env_fn (callable): a callable that returns an EnvBase\n            instance.\n        create_env_kwargs (dict, optional): the kwargs of the env creator.\n        share_memory (bool, optional): if False, the resulting tensordict\n            from the environment won't be placed in shared memory.\n\n    Examples:\n        >>> # We create the same environment on 2 processes using VecNorm\n        >>> # and check that the discounted count of observations match on\n        >>> # both workers, even if one has not executed any step\n        >>> import time\n        >>> from torchrl.envs.libs.gym import GymEnv\n        >>> from torchrl.envs.transforms import VecNorm, TransformedEnv\n        >>> from torchrl.envs import EnvCreator\n        >>> from torch import multiprocessing as mp\n        >>> env_fn = lambda: TransformedEnv(GymEnv(\"Pendulum-v1\"), VecNorm())\n        >>> env_creator = EnvCreator(env_fn)\n        >>>\n        >>> def test_env1(env_creator):\n        ...     env = env_creator()\n        ...     tensordict = env.reset()\n        ...     for _ in range(10):\n        ...         env.rand_step(tensordict)\n        ...         if tensordict.get(\"done\"):\n        ...             tensordict = env.reset(tensordict)\n        ...     print(\"env 1: \", env.transform._td.get((\"next\", \"observation_count\")))\n        >>>\n        >>> def test_env2(env_creator):\n        ...     env = env_creator()\n        ...     time.sleep(5)\n        ...     print(\"env 2: \", env.transform._td.get((\"next\", \"observation_count\")))\n        >>>\n        >>> if __name__ == \"__main__\":\n        ...     ps = []\n        ...     p1 = mp.Process(target=test_env1, args=(env_creator,))\n        ...     p1.start()\n        ...     ps.append(p1)\n        ...     p2 = mp.Process(target=test_env2, args=(env_creator,))\n        ...     p2.start()\n        ...     ps.append(p1)\n        ...     for p in ps:\n        ...         p.join()\n        env 1:  tensor([11.9934])\n        env 2:  tensor([11.9934])\n    \"\"\"\n\n    def __init__(\n        self,\n        create_env_fn: Callable[..., EnvBase],\n        create_env_kwargs: Optional[Dict] = None,\n        share_memory: bool = True,\n    ) -> None:\n        if not isinstance(create_env_fn, EnvCreator):\n            self.create_env_fn = CloudpickleWrapper(create_env_fn)\n        else:\n            self.create_env_fn = create_env_fn\n\n        self.create_env_kwargs = (\n            create_env_kwargs if isinstance(create_env_kwargs, dict) else {}\n        )\n        self.initialized = False\n        self._meta_data = None\n        self._share_memory = share_memory\n        self.init_()\n\n    def share_memory(self, state_dict: OrderedDict) -> None:\n        for key, item in list(state_dict.items()):\n            if isinstance(item, (TensorDictBase,)):\n                if not item.is_shared():\n                    item.share_memory_()\n                else:\n                    print(\n                        f\"{self.env_type}: {item} is already shared\"\n                    )  # , deleting key')\n                    del state_dict[key]\n            elif isinstance(item, OrderedDict):\n                self.share_memory(item)\n            elif isinstance(item, torch.Tensor):\n                del state_dict[key]\n\n    @property\n    def meta_data(self):\n        if self._meta_data is None:\n            raise RuntimeError(\n                \"meta_data is None in EnvCreator. \" \"Make sure init_() has been called.\"\n            )\n        return self._meta_data\n\n    @meta_data.setter\n    def meta_data(self, value: EnvMetaData):\n        self._meta_data = value\n\n    def init_(self) -> EnvCreator:\n        shadow_env = self.create_env_fn(**self.create_env_kwargs)\n        tensordict = shadow_env.reset()\n        shadow_env.rand_step(tensordict)\n        self.env_type = type(shadow_env)\n        self._transform_state_dict = shadow_env.state_dict()\n        if self._share_memory:\n            self.share_memory(self._transform_state_dict)\n        self.initialized = True\n        self.meta_data = EnvMetaData.build_metadata_from_env(shadow_env)\n        shadow_env.close()\n        del shadow_env\n        return self\n\n    def __call__(self, **kwargs) -> EnvBase:\n        if not self.initialized:\n            raise RuntimeError(\"EnvCreator must be initialized before being called.\")\n        kwargs.update(self.create_env_kwargs)  # create_env_kwargs precedes\n        env = self.create_env_fn(**kwargs)\n        env.load_state_dict(self._transform_state_dict, strict=False)\n        return env\n\n    def state_dict(self) -> OrderedDict:\n        if self._transform_state_dict is None:\n            return OrderedDict()\n        return self._transform_state_dict\n\n    def load_state_dict(self, state_dict: OrderedDict) -> None:\n        if self._transform_state_dict is not None:\n            for key, item in state_dict.items():\n                item_to_update = self._transform_state_dict[key]\n                item_to_update.copy_(item)\n\n    def __repr__(self) -> str:\n        substr = \", \".join(\n            [f\"{key}: {type(item)}\" for key, item in self.create_env_kwargs]\n        )\n        return f\"EnvCreator({self.create_env_fn}({substr}))\"\n\n\ndef env_creator(fun: Callable) -> EnvCreator:\n    \"\"\"Helper function to call `EnvCreator`.\"\"\"\n    return EnvCreator(fun)\n\n\ndef get_env_metadata(\n    env_or_creator: Union[EnvBase, Callable], kwargs: Optional[Dict] = None\n):\n    \"\"\"Retrieves a EnvMetaData object from an env.\"\"\"\n    if isinstance(env_or_creator, (EnvBase,)):\n        return EnvMetaData.build_metadata_from_env(env_or_creator)\n    elif not isinstance(env_or_creator, EnvBase) and not isinstance(\n        env_or_creator, EnvCreator\n    ):\n        # then env is a creator", "metadata": {"task_id": "pytorch_rl/8", "ground_truth": "        if kwargs is None:", "fpath_tuple": ["pytorch_rl", "torchrl", "envs", "env_creator.py"], "context_start_lineno": 0, "line_no": 175, "query_window": {"context": "            [f\"{key}: {type(item)}\" for key, item in self.create_env_kwargs]\n        )\n        return f\"EnvCreator({self.create_env_fn}({substr}))\"\n\n\ndef env_creator(fun: Callable) -> EnvCreator:\n    \"\"\"Helper function to call `EnvCreator`.\"\"\"\n    return EnvCreator(fun)\n\n\ndef get_env_metadata(\n    env_or_creator: Union[EnvBase, Callable], kwargs: Optional[Dict] = None\n):\n    \"\"\"Retrieves a EnvMetaData object from an env.\"\"\"\n    if isinstance(env_or_creator, (EnvBase,)):\n        return EnvMetaData.build_metadata_from_env(env_or_creator)\n    elif not isinstance(env_or_creator, EnvBase) and not isinstance(\n        env_or_creator, EnvCreator\n    ):\n        # then env is a creator", "metadata": {"fpath_tuple": ["pytorch_rl", "torchrl", "envs", "env_creator.py"], "line_no": 175, "task_id": "pytorch_rl/8", "start_line_no": 155, "end_line_no": 175, "window_size": 20, "context_start_lineno": 0, "repo": "pytorch_rl"}}, "top_k_context": [{"context": "        self, info_dict: Dict[str, Any], tensordict: TensorDictBase\n    ) -> TensorDictBase:\n        if not isinstance(info_dict, dict) and len(self.keys):\n            warnings.warn(\n                f\"Found an info_dict of type {type(info_dict)} \"\n                f\"but expected type or subtype `dict`.\"\n            )\n        for key in self.keys:\n            if key in info_dict:\n                tensordict[key] = info_dict[key]\n        return tensordict\n\n    @property\n    def info_spec(self) -> Dict[str, TensorSpec]:\n        return self._info_spec\n\n\nclass GymLikeEnv(_EnvWrapper):\n    \"\"\"A gym-like env is an environment.\n", "metadata": [{"fpath_tuple": ["pytorch_rl", "torchrl", "envs", "gym_like.py"], "line_no": 90, "start_line_no": 80, "end_line_no": 100, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}], "sim_score": 0.323943661971831}, {"context": "            create_env_fn (Callable or list of callables): an env creator\n                function (or a list of creators)\n            create_env_kwargs (dictionary): kwargs for the env creator\n            policy (TensorDictModule, optional): a policy to be used\n            device (int, str or torch.device, optional): device where to place\n                the policy\n            observation_spec (TensorSpec, optional): spec of the observations\n\n        \"\"\"\n        # if create_env_fn is not None:\n        #     if create_env_kwargs is None:\n        #         create_env_kwargs = {}\n        #     self.create_env_fn = create_env_fn\n        #     if isinstance(create_env_fn, EnvBase):\n        #         env = create_env_fn\n        #     else:\n        #         env = self.create_env_fn(**create_env_kwargs)\n        # else:\n        #     env = None\n", "metadata": [{"fpath_tuple": ["pytorch_rl", "torchrl", "collectors", "collectors.py"], "line_no": 142, "start_line_no": 132, "end_line_no": 152, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}], "sim_score": 0.31851851851851853}, {"context": "    ):\n        if isinstance(value, torch.Tensor) and isinstance(selected_keys, str):\n            value = {selected_keys: value}\n            selected_keys = [selected_keys]\n\n        for _key in self:\n            if self[_key] is not None and (\n                selected_keys is None or _key in selected_keys\n            ):\n                self._specs[_key].type_check(value[_key], _key)\n\n    def is_in(self, val: Union[dict, TensorDictBase]) -> bool:\n        return all(\n            [\n                item.is_in(val.get(key))\n                for (key, item) in self._specs.items()\n                if item is not None\n            ]\n        )\n", "metadata": [{"fpath_tuple": ["pytorch_rl", "torchrl", "data", "tensor_specs.py"], "line_no": 1786, "start_line_no": 1776, "end_line_no": 1796, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}], "sim_score": 0.3181818181818182}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# tests/test_local_train_lr.py\n# tests/test_trainer_property.py\n# --------------------------------------------------\n# # Copyright (c) Alibaba, Inc. and its affiliates.\n# import unittest\n# \n# from federatedscope.core.auxiliaries.data_builder import get_data\n# from federatedscope.core.auxiliaries.utils import setup_seed\n# from federatedscope.core.auxiliaries.logging import update_logger\n# from federatedscope.core.configs.config import global_cfg\n# from federatedscope.core.auxiliaries.runner_builder import get_runner\n# from federatedscope.core.auxiliaries.worker_builder import get_server_cls, \\\n#     get_client_cls\n# \n# \n# --------------------------------------------------\n# the below code fragment can be found in:\n# tests/test_asyn_cifar10.py\n# tests/test_backdoor_attack.py\n# tests/test_CRA_gan_attack.py\n# tests/test_ditto.py\n# tests/test_efficient_simulation.py\n# tests/test_external_dataset.py\n# tests/test_fedem.py\n# tests/test_fedopt.py\n# tests/test_fedprox.py\n# tests/test_fedsageplus.py\n# tests/test_femnist.py\n# tests/test_finetune_lr.py\n# tests/test_global_train_lr.py\n# tests/test_graph_node_trainer.py\n# tests/test_mf.py\n# tests/test_MIA_gradient_ascent.py\n# tests/test_nbafl.py\n# tests/test_optimizer.py\n# tests/test_pfedme.py\n# tests/test_PIA_toy.py\n# tests/test_rec_IG_opt_attack.py\n# tests/test_rec_opt_attack.py\n# tests/test_toy_lr.py\n# tests/test_unseen_clients_lr.py\n# --------------------------------------------------\n# # Copyright (c) Alibaba, Inc. and its affiliates.\n# import unittest\n# \n# from federatedscope.core.auxiliaries.data_builder import get_data\n# from federatedscope.core.auxiliaries.utils import setup_seed\n# from federatedscope.core.auxiliaries.logging import update_logger\n# from federatedscope.core.configs.config import global_cfg\n# from federatedscope.core.auxiliaries.runner_builder import get_runner\n# from federatedscope.core.auxiliaries.worker_builder import get_server_cls, get_client_cls\n# \n# \n# class AsynCIFAR10Test(unittest.TestCase):\n# --------------------------------------------------\n# the below code fragment can be found in:\n# tests/test_vertical_fl.py\n# tests/test_xgb.py\n# --------------------------------------------------\n# # Copyright (c) Alibaba, Inc. and its affiliates.\n# import unittest\n# \n# from federatedscope.core.auxiliaries.data_builder import get_data\n# from federatedscope.core.auxiliaries.worker_builder import get_client_cls, get_server_cls\n# from federatedscope.core.auxiliaries.utils import setup_seed\n# from federatedscope.core.auxiliaries.logging import update_logger\n# from federatedscope.core.configs.config import global_cfg\n# from federatedscope.core.auxiliaries.runner_builder import get_runner\n# \n# \n# class vFLTest(unittest.TestCase):\n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\n# Copyright (c) Alibaba, Inc. and its affiliates.\nimport unittest\n\nfrom federatedscope.core.auxiliaries.data_builder import get_data\nfrom federatedscope.core.auxiliaries.utils import setup_seed\nfrom federatedscope.core.auxiliaries.logging import update_logger", "metadata": {"task_id": "alibaba_FederatedScope/117", "ground_truth": "from federatedscope.core.configs.config import global_cfg", "fpath_tuple": ["alibaba_FederatedScope", "tests", "test_unseen_clients_lr.py"], "context_start_lineno": 0, "line_no": 6, "query_window": {"context": "# Copyright (c) Alibaba, Inc. and its affiliates.\nimport unittest\n\nfrom federatedscope.core.auxiliaries.data_builder import get_data\nfrom federatedscope.core.auxiliaries.utils import setup_seed\nfrom federatedscope.core.auxiliaries.logging import update_logger", "metadata": {"fpath_tuple": ["alibaba_FederatedScope", "tests", "test_unseen_clients_lr.py"], "line_no": 6, "task_id": "alibaba_FederatedScope/117", "start_line_no": 0, "end_line_no": 6, "window_size": 20, "context_start_lineno": 0, "repo": "alibaba_FederatedScope"}}, "top_k_context": [{"context": "# Copyright (c) Alibaba, Inc. and its affiliates.\nimport unittest\n\nfrom federatedscope.core.auxiliaries.data_builder import get_data\nfrom federatedscope.core.auxiliaries.worker_builder import get_client_cls, get_server_cls\nfrom federatedscope.core.auxiliaries.utils import setup_seed\nfrom federatedscope.core.auxiliaries.logging import update_logger\nfrom federatedscope.core.configs.config import global_cfg\nfrom federatedscope.core.auxiliaries.runner_builder import get_runner\n", "metadata": [{"fpath_tuple": ["alibaba_FederatedScope", "tests", "test_vertical_fl.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 10, "window_size": 20, "repo": "alibaba_FederatedScope", "slice_size": 10}, {"fpath_tuple": ["alibaba_FederatedScope", "tests", "test_xgb.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 10, "window_size": 20, "repo": "alibaba_FederatedScope", "slice_size": 10}], "sim_score": 0.8043478260869565}, {"context": "# Copyright (c) Alibaba, Inc. and its affiliates.\nimport unittest\n\nfrom federatedscope.core.auxiliaries.data_builder import get_data\nfrom federatedscope.core.auxiliaries.utils import setup_seed\nfrom federatedscope.core.auxiliaries.logging import update_logger\nfrom federatedscope.core.configs.config import global_cfg\nfrom federatedscope.core.auxiliaries.runner_builder import get_runner\nfrom federatedscope.core.auxiliaries.worker_builder import get_server_cls, get_client_cls\n", "metadata": [{"fpath_tuple": ["alibaba_FederatedScope", "tests", "test_asyn_cifar10.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 10, "window_size": 20, "repo": "alibaba_FederatedScope", "slice_size": 10}, {"fpath_tuple": ["alibaba_FederatedScope", "tests", "test_backdoor_attack.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 10, "window_size": 20, "repo": "alibaba_FederatedScope", "slice_size": 10}, {"fpath_tuple": ["alibaba_FederatedScope", "tests", "test_CRA_gan_attack.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 10, "window_size": 20, "repo": "alibaba_FederatedScope", "slice_size": 10}, {"fpath_tuple": ["alibaba_FederatedScope", "tests", "test_ditto.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 10, "window_size": 20, "repo": "alibaba_FederatedScope", "slice_size": 10}, {"fpath_tuple": ["alibaba_FederatedScope", "tests", "test_efficient_simulation.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 10, "window_size": 20, "repo": "alibaba_FederatedScope", "slice_size": 10}, {"fpath_tuple": ["alibaba_FederatedScope", "tests", "test_external_dataset.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 10, "window_size": 20, "repo": "alibaba_FederatedScope", "slice_size": 10}, {"fpath_tuple": ["alibaba_FederatedScope", "tests", "test_fedem.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 10, "window_size": 20, "repo": "alibaba_FederatedScope", "slice_size": 10}, {"fpath_tuple": ["alibaba_FederatedScope", "tests", "test_fedopt.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 10, "window_size": 20, "repo": "alibaba_FederatedScope", "slice_size": 10}, {"fpath_tuple": ["alibaba_FederatedScope", "tests", "test_fedprox.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 10, "window_size": 20, "repo": "alibaba_FederatedScope", "slice_size": 10}, {"fpath_tuple": ["alibaba_FederatedScope", "tests", "test_fedsageplus.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 10, "window_size": 20, "repo": "alibaba_FederatedScope", "slice_size": 10}, {"fpath_tuple": ["alibaba_FederatedScope", "tests", "test_femnist.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 10, "window_size": 20, "repo": "alibaba_FederatedScope", "slice_size": 10}, {"fpath_tuple": ["alibaba_FederatedScope", "tests", "test_finetune_lr.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 10, "window_size": 20, "repo": "alibaba_FederatedScope", "slice_size": 10}, {"fpath_tuple": ["alibaba_FederatedScope", "tests", "test_global_train_lr.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 10, "window_size": 20, "repo": "alibaba_FederatedScope", "slice_size": 10}, {"fpath_tuple": ["alibaba_FederatedScope", "tests", "test_graph_node_trainer.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 10, "window_size": 20, "repo": "alibaba_FederatedScope", "slice_size": 10}, {"fpath_tuple": ["alibaba_FederatedScope", "tests", "test_mf.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 10, "window_size": 20, "repo": "alibaba_FederatedScope", "slice_size": 10}, {"fpath_tuple": ["alibaba_FederatedScope", "tests", "test_MIA_gradient_ascent.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 10, "window_size": 20, "repo": "alibaba_FederatedScope", "slice_size": 10}, {"fpath_tuple": ["alibaba_FederatedScope", "tests", "test_nbafl.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 10, "window_size": 20, "repo": "alibaba_FederatedScope", "slice_size": 10}, {"fpath_tuple": ["alibaba_FederatedScope", "tests", "test_optimizer.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 10, "window_size": 20, "repo": "alibaba_FederatedScope", "slice_size": 10}, {"fpath_tuple": ["alibaba_FederatedScope", "tests", "test_pfedme.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 10, "window_size": 20, "repo": "alibaba_FederatedScope", "slice_size": 10}, {"fpath_tuple": ["alibaba_FederatedScope", "tests", "test_PIA_toy.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 10, "window_size": 20, "repo": "alibaba_FederatedScope", "slice_size": 10}, {"fpath_tuple": ["alibaba_FederatedScope", "tests", "test_rec_IG_opt_attack.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 10, "window_size": 20, "repo": "alibaba_FederatedScope", "slice_size": 10}, {"fpath_tuple": ["alibaba_FederatedScope", "tests", "test_rec_opt_attack.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 10, "window_size": 20, "repo": "alibaba_FederatedScope", "slice_size": 10}, {"fpath_tuple": ["alibaba_FederatedScope", "tests", "test_toy_lr.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 10, "window_size": 20, "repo": "alibaba_FederatedScope", "slice_size": 10}, {"fpath_tuple": ["alibaba_FederatedScope", "tests", "test_unseen_clients_lr.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 10, "window_size": 20, "repo": "alibaba_FederatedScope", "slice_size": 10}], "sim_score": 0.8043478260869565}, {"context": "# Copyright (c) Alibaba, Inc. and its affiliates.\nimport unittest\n\nfrom federatedscope.core.auxiliaries.data_builder import get_data\nfrom federatedscope.core.auxiliaries.utils import setup_seed\nfrom federatedscope.core.auxiliaries.logging import update_logger\nfrom federatedscope.core.configs.config import global_cfg\nfrom federatedscope.core.auxiliaries.runner_builder import get_runner\nfrom federatedscope.core.auxiliaries.worker_builder import get_server_cls, \\\n    get_client_cls", "metadata": [{"fpath_tuple": ["alibaba_FederatedScope", "tests", "test_local_train_lr.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 10, "window_size": 20, "repo": "alibaba_FederatedScope", "slice_size": 10}, {"fpath_tuple": ["alibaba_FederatedScope", "tests", "test_trainer_property.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 10, "window_size": 20, "repo": "alibaba_FederatedScope", "slice_size": 10}], "sim_score": 0.7708333333333334}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# ding/worker/collector/zergling_parallel_collector.py\n# --------------------------------------------------\n#         finish_info = {\n#             'eval_flag': self._eval_flag,\n#             'env_num': self._env_num,\n#             'duration': duration,\n#             'train_iter': self._policy_iter,\n#             'collector_done': self._env_manager.done,\n#             'predefined_episode_count': self._predefined_episode_count,\n#             'real_episode_count': self._total_episode,\n#             'step_count': self._total_step,\n#             'sample_count': self._total_sample,\n#             'avg_time_per_episode': duration / max(1, self._total_episode),\n#             'avg_time_per_step': duration / self._total_step,\n#             'avg_time_per_train_sample': duration / max(1, self._total_sample),\n#             'avg_step_per_episode': self._total_step / max(1, self._total_episode),\n#             'avg_sample_per_episode': self._total_sample / max(1, self._total_episode),\n#             'reward_mean': np.mean(episode_result) if len(episode_result) > 0 else 0,\n#             'reward_std': np.std(episode_result) if len(episode_result) > 0 else 0,\n#             'reward_raw': episode_result,\n#             'finish_time': time.time()\n#         }\n# --------------------------------------------------\n# the below code fragment can be found in:\n# ding/worker/collector/zergling_parallel_collector.py\n# --------------------------------------------------\n#             'env_num': self._env_num,\n#             'duration': duration,\n#             'train_iter': self._policy_iter,\n#             'collector_done': self._env_manager.done,\n#             'predefined_episode_count': self._predefined_episode_count,\n#             'real_episode_count': self._total_episode,\n#             'step_count': self._total_step,\n#             'sample_count': self._total_sample,\n#             'avg_time_per_episode': duration / max(1, self._total_episode),\n#             'avg_time_per_step': duration / self._total_step,\n#             'avg_time_per_train_sample': duration / max(1, self._total_sample),\n#             'avg_step_per_episode': self._total_step / max(1, self._total_episode),\n#             'avg_sample_per_episode': self._total_sample / max(1, self._total_episode),\n#             'reward_mean': np.mean(episode_result) if len(episode_result) > 0 else 0,\n#             'reward_std': np.std(episode_result) if len(episode_result) > 0 else 0,\n#             'reward_raw': episode_result,\n#             'finish_time': time.time()\n#         }\n#         if not self._eval_flag:\n#             finish_info['collect_setting'] = self._cfg.collect_setting\n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\n        self._env_num = self._env_manager.env_num\n        self._predefined_episode_count = self._env_num * self._env_manager._episode_num\n\n    def _setup_env_manager(self, cfg: EasyDict) -> BaseEnvManager:\n        env_fn, collector_env_cfg, evaluator_env_cfg = get_vec_env_setting(cfg)\n        if self._eval_flag:\n            env_cfg = evaluator_env_cfg\n        else:\n            env_cfg = collector_env_cfg\n        env_manager = create_env_manager(cfg.manager, [partial(env_fn, cfg=c) for c in env_cfg])\n        return env_manager\n\n    def _start_thread(self) -> None:\n        # evaluator doesn't need to update policy periodically, only updating policy when starts\n        if not self._eval_flag:\n            self._update_policy_thread.start()\n\n    def _join_thread(self) -> None:\n        if not self._eval_flag:\n            self._update_policy_thread.join()\n            del self._update_policy_thread\n\n    # override\n    def close(self) -> None:\n        if self._end_flag:\n            return\n        self._end_flag = True\n        time.sleep(1)\n        if hasattr(self, '_env_manager'):\n            self._env_manager.close()\n        self._join_thread()\n\n    # override\n    def _policy_inference(self, obs: Dict[int, Any]) -> Dict[int, Any]:\n        env_ids = list(obs.keys())\n        if len(self._policy) > 1:\n            assert not self._eval_flag\n            obs = [{id: obs[id][i] for id in env_ids} for i in range(len(self._policy))]\n        else:\n            assert self._eval_flag\n            obs = [obs]\n        self._obs_pool.update(obs)\n        policy_outputs = []\n        for i in range(len(self._policy)):\n            if self._eval_flag:\n                policy_output = self._policy[i].forward(obs[i])\n            else:\n                policy_output = self._policy[i].forward(obs[i], **self._cfg.collect_setting)\n            policy_outputs.append(policy_output)\n        self._policy_output_pool.update(policy_outputs)\n        actions = {}\n        for env_id in env_ids:\n            action = [policy_outputs[i][env_id]['action'] for i in range(len(self._policy))]\n            action = torch.stack(action).squeeze()\n            actions[env_id] = action\n        return actions\n\n    # override\n    def _env_step(self, actions: Dict[int, Any]) -> Dict[int, Any]:\n        return self._env_manager.step(actions)\n\n    # override\n    def _process_timestep(self, timestep: Dict[int, namedtuple]) -> None:\n        for env_id, t in timestep.items():\n            if t.info.get('abnormal', False):\n                # If there is an abnormal timestep, reset all the related variables, also this env has been reset\n                for c in self._traj_buffer[env_id]:\n                    c.clear()\n                self._obs_pool.reset(env_id)\n                self._policy_output_pool.reset(env_id)\n                for p in self._policy:\n                    p.reset([env_id])\n                continue\n            self._total_step += 1\n            t = [BaseEnvTimestep(t.obs[i], t.reward[i], t.done, t.info) for i in range(len(self._policy))]\n            if t[0].done:\n                self._total_episode += 1\n            if not self._eval_flag:\n                for i in range(len(self._policy)):\n                    if self._policy_is_active[i]:\n                        # Only active policy will store transition into replay buffer.\n                        transition = self._policy[i].process_transition(\n                            self._obs_pool[env_id][i], self._policy_output_pool[env_id][i], t[i]\n                        )\n                        self._traj_buffer[env_id][i].append(transition)\n                full_indices = []\n                for i in range(len(self._traj_buffer[env_id])):\n                    if len(self._traj_buffer[env_id][i]) == self._traj_len:\n                        full_indices.append(i)\n                if t[0].done or len(full_indices) > 0:\n                    for i in full_indices:\n                        train_sample = self._policy[i].get_train_sample(self._traj_buffer[env_id][i])\n                        for s in train_sample:\n                            s = self._compressor(s)\n                            self._total_sample += 1\n                            metadata = self._get_metadata(s, env_id)\n                            self.send_stepdata(metadata['data_id'], s)\n                            self.send_metadata(metadata)\n                        self._traj_buffer[env_id][i].clear()\n            if t[0].done:\n                # env reset is done by env_manager automatically\n                self._obs_pool.reset(env_id)\n                self._policy_output_pool.reset(env_id)\n                for p in self._policy:\n                    p.reset([env_id])\n                reward = t[0].info['final_eval_reward']\n                # Only left player's reward will be recorded.\n                left_reward = reward[0]\n                if isinstance(left_reward, torch.Tensor):\n                    left_reward = left_reward.item()\n                self._episode_result[env_id].append(left_reward)\n                self.debug(\n                    \"Env {} finish episode, final reward: {}, collected episode: {}.\".format(\n                        env_id, reward, len(self._episode_result[env_id])\n                    )\n                )\n            self._total_step += 1\n        dones = [t.done for t in timestep.values()]\n        if any(dones):\n            collector_info = self._get_collector_info()\n            self.send_metadata(collector_info)\n\n    # override\n    def get_finish_info(self) -> dict:\n        duration = max(time.time() - self._start_time, 1e-8)\n        game_result = copy.deepcopy(self._episode_result)\n        for i, env_result in enumerate(game_result):\n            for j, rew in enumerate(env_result):\n                if rew < 0:\n                    game_result[i][j] = \"losses\"\n                elif rew == 0:\n                    game_result[i][j] = \"draws\"\n                else:\n                    game_result[i][j] = \"wins\"\n\n        finish_info = {\n            # 'finished_task': True,  # flag\n            'eval_flag': self._eval_flag,\n            # 'episode_num': self._episode_num,\n            'env_num': self._env_num,\n            'duration': duration,\n            'collector_done': self._env_manager.done,\n            'predefined_episode_count': self._predefined_episode_count,\n            'real_episode_count': self._total_episode,\n            'step_count': self._total_step,\n            'sample_count': self._total_sample,\n            'avg_time_per_episode': duration / max(1, self._total_episode),\n            'avg_time_per_step': duration / self._total_step,\n            'avg_time_per_train_sample': duration / max(1, self._total_sample),\n            'avg_step_per_episode': self._total_step / max(1, self._total_episode),\n            'avg_sample_per_episode': self._total_sample / max(1, self._total_episode),\n            'reward_mean': np.mean(self._episode_result),", "metadata": {"task_id": "opendilab_ACE/144", "ground_truth": "            'reward_std': np.std(self._episode_result),", "fpath_tuple": ["opendilab_ACE", "ding", "worker", "collector", "marine_parallel_collector.py"], "context_start_lineno": 115, "line_no": 267, "query_window": {"context": "                else:\n                    game_result[i][j] = \"wins\"\n\n        finish_info = {\n            # 'finished_task': True,  # flag\n            'eval_flag': self._eval_flag,\n            # 'episode_num': self._episode_num,\n            'env_num': self._env_num,\n            'duration': duration,\n            'collector_done': self._env_manager.done,\n            'predefined_episode_count': self._predefined_episode_count,\n            'real_episode_count': self._total_episode,\n            'step_count': self._total_step,\n            'sample_count': self._total_sample,\n            'avg_time_per_episode': duration / max(1, self._total_episode),\n            'avg_time_per_step': duration / self._total_step,\n            'avg_time_per_train_sample': duration / max(1, self._total_sample),\n            'avg_step_per_episode': self._total_step / max(1, self._total_episode),\n            'avg_sample_per_episode': self._total_sample / max(1, self._total_episode),\n            'reward_mean': np.mean(self._episode_result),", "metadata": {"fpath_tuple": ["opendilab_ACE", "ding", "worker", "collector", "marine_parallel_collector.py"], "line_no": 267, "task_id": "opendilab_ACE/144", "start_line_no": 247, "end_line_no": 267, "window_size": 20, "context_start_lineno": 115, "repo": "opendilab_ACE"}}, "top_k_context": [{"context": "        finish_info = {\n            'eval_flag': self._eval_flag,\n            'env_num': self._env_num,\n            'duration': duration,\n            'train_iter': self._policy_iter,\n            'collector_done': self._env_manager.done,\n            'predefined_episode_count': self._predefined_episode_count,\n            'real_episode_count': self._total_episode,\n            'step_count': self._total_step,\n            'sample_count': self._total_sample,\n            'avg_time_per_episode': duration / max(1, self._total_episode),\n            'avg_time_per_step': duration / self._total_step,\n            'avg_time_per_train_sample': duration / max(1, self._total_sample),\n            'avg_step_per_episode': self._total_step / max(1, self._total_episode),\n            'avg_sample_per_episode': self._total_sample / max(1, self._total_episode),\n            'reward_mean': np.mean(episode_result) if len(episode_result) > 0 else 0,\n            'reward_std': np.std(episode_result) if len(episode_result) > 0 else 0,\n            'reward_raw': episode_result,\n            'finish_time': time.time()\n        }", "metadata": [{"fpath_tuple": ["opendilab_ACE", "ding", "worker", "collector", "zergling_parallel_collector.py"], "line_no": 218, "start_line_no": 208, "end_line_no": 228, "window_size": 20, "repo": "opendilab_ACE", "slice_size": 10}], "sim_score": 0.5714285714285714}, {"context": "        duration = max(time.time() - self._start_time, 1e-8)\n        episode_result = sum(self._episode_result, [])\n        finish_info = {\n            'eval_flag': self._eval_flag,\n            'env_num': self._env_num,\n            'duration': duration,\n            'train_iter': self._policy_iter,\n            'collector_done': self._env_manager.done,\n            'predefined_episode_count': self._predefined_episode_count,\n            'real_episode_count': self._total_episode,\n            'step_count': self._total_step,\n            'sample_count': self._total_sample,\n            'avg_time_per_episode': duration / max(1, self._total_episode),\n            'avg_time_per_step': duration / self._total_step,\n            'avg_time_per_train_sample': duration / max(1, self._total_sample),\n            'avg_step_per_episode': self._total_step / max(1, self._total_episode),\n            'avg_sample_per_episode': self._total_sample / max(1, self._total_episode),\n            'reward_mean': np.mean(episode_result) if len(episode_result) > 0 else 0,\n            'reward_std': np.std(episode_result) if len(episode_result) > 0 else 0,\n            'reward_raw': episode_result,", "metadata": [{"fpath_tuple": ["opendilab_ACE", "ding", "worker", "collector", "zergling_parallel_collector.py"], "line_no": 216, "start_line_no": 206, "end_line_no": 226, "window_size": 20, "repo": "opendilab_ACE", "slice_size": 10}], "sim_score": 0.550561797752809}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# ding/utils/autolog/time_ctl.py\n# --------------------------------------------------\n# import time\n# from abc import ABCMeta, abstractmethod\n# from typing import Union\n# \n# from ..lock_helper import LockContext, LockContextType\n# \n# \n# class BaseTime(metaclass=ABCMeta):\n#     \"\"\"\n#     Overview:\n#         Abstract time interface\n#     \"\"\"\n# --------------------------------------------------\n# the below code fragment can be found in:\n# ding/interaction/exception/slave.py\n# --------------------------------------------------\n# from abc import ABCMeta\n# from enum import unique, IntEnum\n# from typing import Type\n# \n# import enum_tools\n# from requests import HTTPError\n# \n# from .base import ResponseException\n# from ..base import get_values_from_response\n# \n# \n# @enum_tools.documentation.document_enum\n# @unique\n# class SlaveErrorCode(IntEnum):\n#     \"\"\"\n#     Overview:\n#         Error code for slave end\n#     \"\"\"\n#     SUCCESS = 0  # doc: Slave request success\n# \n# --------------------------------------------------\n# the below code fragment can be found in:\n# ding/interaction/base/app.py\n# --------------------------------------------------\n# import json\n# from enum import IntEnum, unique\n# from functools import wraps\n# from typing import Mapping, Any, Type, Optional, Tuple, Union, Iterable, Callable\n# \n# import flask\n# import requests\n# from flask import jsonify\n# \n# \n# @unique\n# class CommonErrorCode(IntEnum):\n# --------------------------------------------------\n# the below code fragment can be found in:\n# ding/interaction/exception/slave.py\n# --------------------------------------------------\n# from abc import ABCMeta\n# from enum import unique, IntEnum\n# from typing import Type\n# \n# import enum_tools\n# from requests import HTTPError\n# \n# from .base import ResponseException\n# from ..base import get_values_from_response\n# \n# \n# @enum_tools.documentation.document_enum\n# @unique\n# class SlaveErrorCode(IntEnum):\n#     \"\"\"\n#     Overview:\n#         Error code for slave end\n#     \"\"\"\n# --------------------------------------------------\n# the below code fragment can be found in:\n# ding/interaction/exception/slave.py\n# --------------------------------------------------\n# from abc import ABCMeta\n# from enum import unique, IntEnum\n# from typing import Type\n# \n# import enum_tools\n# from requests import HTTPError\n# \n# from .base import ResponseException\n# from ..base import get_values_from_response\n# \n# \n# @enum_tools.documentation.document_enum\n# @unique\n# class SlaveErrorCode(IntEnum):\n#     \"\"\"\n#     Overview:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# ding/interaction/exception/master.py\n# ding/interaction/exception/slave.py\n# --------------------------------------------------\n# from abc import ABCMeta\n# from enum import unique, IntEnum\n# from typing import Type\n# \n# import enum_tools\n# from requests import HTTPError\n# \n# from .base import ResponseException\n# from ..base import get_values_from_response\n# \n# \n# @enum_tools.documentation.document_enum\n# @unique\n# class MasterErrorCode(IntEnum):\n# --------------------------------------------------\n# the below code fragment can be found in:\n# ding/interaction/exception/master.py\n# ding/interaction/exception/slave.py\n# --------------------------------------------------\n# from abc import ABCMeta\n# from enum import unique, IntEnum\n# from typing import Type\n# \n# import enum_tools\n# from requests import HTTPError\n# \n# from .base import ResponseException\n# from ..base import get_values_from_response\n# \n# \n# @enum_tools.documentation.document_enum\n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\nfrom abc import ABCMeta\nfrom enum import unique, IntEnum", "metadata": {"task_id": "opendilab_ACE/181", "ground_truth": "from typing import Type", "fpath_tuple": ["opendilab_ACE", "ding", "interaction", "exception", "master.py"], "context_start_lineno": 0, "line_no": 2, "query_window": {"context": "from abc import ABCMeta\nfrom enum import unique, IntEnum", "metadata": {"fpath_tuple": ["opendilab_ACE", "ding", "interaction", "exception", "master.py"], "line_no": 2, "task_id": "opendilab_ACE/181", "start_line_no": 0, "end_line_no": 2, "window_size": 20, "context_start_lineno": 0, "repo": "opendilab_ACE"}}, "top_k_context": [{"context": "from abc import ABCMeta\nfrom enum import unique, IntEnum\nfrom typing import Type\n\nimport enum_tools\nfrom requests import HTTPError\n\nfrom .base import ResponseException\nfrom ..base import get_values_from_response\n", "metadata": [{"fpath_tuple": ["opendilab_ACE", "ding", "interaction", "exception", "master.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 10, "window_size": 20, "repo": "opendilab_ACE", "slice_size": 10}, {"fpath_tuple": ["opendilab_ACE", "ding", "interaction", "exception", "slave.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 10, "window_size": 20, "repo": "opendilab_ACE", "slice_size": 10}], "sim_score": 0.4482758620689655}, {"context": "from abc import ABCMeta\nfrom enum import unique, IntEnum\nfrom typing import Type\n\nimport enum_tools\nfrom requests import HTTPError\n\nfrom .base import ResponseException\nfrom ..base import get_values_from_response\n\n\n@enum_tools.documentation.document_enum", "metadata": [{"fpath_tuple": ["opendilab_ACE", "ding", "interaction", "exception", "master.py"], "line_no": 2, "start_line_no": 0, "end_line_no": 12, "window_size": 20, "repo": "opendilab_ACE", "slice_size": 10}, {"fpath_tuple": ["opendilab_ACE", "ding", "interaction", "exception", "slave.py"], "line_no": 2, "start_line_no": 0, "end_line_no": 12, "window_size": 20, "repo": "opendilab_ACE", "slice_size": 10}], "sim_score": 0.37142857142857144}, {"context": "from abc import ABCMeta\nfrom enum import unique, IntEnum\nfrom typing import Type\n\nimport enum_tools\nfrom requests import HTTPError\n\nfrom .base import ResponseException\nfrom ..base import get_values_from_response\n\n\n@enum_tools.documentation.document_enum\n@unique\nclass SlaveErrorCode(IntEnum):", "metadata": [{"fpath_tuple": ["opendilab_ACE", "ding", "interaction", "exception", "slave.py"], "line_no": 4, "start_line_no": 0, "end_line_no": 14, "window_size": 20, "repo": "opendilab_ACE", "slice_size": 10}], "sim_score": 0.30952380952380953}, {"context": "from abc import ABCMeta\nfrom enum import unique, IntEnum\nfrom typing import Type\n\nimport enum_tools\nfrom requests import HTTPError\n\nfrom .base import ResponseException\nfrom ..base import get_values_from_response\n\n\n@enum_tools.documentation.document_enum\n@unique\nclass SlaveErrorCode(IntEnum):\n    \"\"\"\n    Overview:", "metadata": [{"fpath_tuple": ["opendilab_ACE", "ding", "interaction", "exception", "slave.py"], "line_no": 6, "start_line_no": 0, "end_line_no": 16, "window_size": 20, "repo": "opendilab_ACE", "slice_size": 10}], "sim_score": 0.2826086956521739}, {"context": "import json\nfrom enum import IntEnum, unique\nfrom functools import wraps\nfrom typing import Mapping, Any, Type, Optional, Tuple, Union, Iterable, Callable\n\nimport flask\nimport requests\nfrom flask import jsonify\n\n", "metadata": [{"fpath_tuple": ["opendilab_ACE", "ding", "interaction", "base", "app.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 10, "window_size": 20, "repo": "opendilab_ACE", "slice_size": 10}], "sim_score": 0.2571428571428571}, {"context": "from abc import ABCMeta\nfrom enum import unique, IntEnum\nfrom typing import Type\n\nimport enum_tools\nfrom requests import HTTPError\n\nfrom .base import ResponseException\nfrom ..base import get_values_from_response\n\n\n@enum_tools.documentation.document_enum\n@unique\nclass SlaveErrorCode(IntEnum):\n    \"\"\"\n    Overview:\n        Error code for slave end\n    \"\"\"", "metadata": [{"fpath_tuple": ["opendilab_ACE", "ding", "interaction", "exception", "slave.py"], "line_no": 8, "start_line_no": 0, "end_line_no": 18, "window_size": 20, "repo": "opendilab_ACE", "slice_size": 10}], "sim_score": 0.25}, {"context": "import time\nfrom abc import ABCMeta, abstractmethod\nfrom typing import Union\n\nfrom ..lock_helper import LockContext, LockContextType\n\n\nclass BaseTime(metaclass=ABCMeta):\n    \"\"\"\n    Overview:", "metadata": [{"fpath_tuple": ["opendilab_ACE", "ding", "utils", "autolog", "time_ctl.py"], "line_no": 0, "start_line_no": 0, "end_line_no": 10, "window_size": 20, "repo": "opendilab_ACE", "slice_size": 10}], "sim_score": 0.19047619047619047}], "window_size": 20, "slice_size": 10}}
{"prompt": "# Here are some relevant code fragments from other files of the repo:\n# --------------------------------------------------\n# the below code fragment can be found in:\n# examples/dreamer/dreamer_utils.py\n# --------------------------------------------------\n#                     step=collected_frames,\n#                 )\n#     # Compute observation reco\n#     if cfg.record_video and record._count % cfg.record_interval == 0:\n#         world_model_td = sampled_tensordict\n# \n#         true_pixels = recover_pixels(world_model_td[(\"next\", \"pixels\")], stats)\n# \n#         reco_pixels = recover_pixels(world_model_td[\"next\", \"reco_pixels\"], stats)\n#         with autocast(dtype=torch.float16):\n#             world_model_td = world_model_td.select(\"state\", \"belief\", \"reward\")\n#             world_model_td = model_based_env.rollout(\n#                 max_steps=true_pixels.shape[1],\n#                 policy=actor_model,\n#                 auto_reset=False,\n#                 tensordict=world_model_td[:, 0],\n#             )\n#         imagine_pxls = recover_pixels(\n#             model_based_env.decode_obs(world_model_td)[\"next\", \"reco_pixels\"],\n#             stats,\n# --------------------------------------------------\n# the below code fragment can be found in:\n# examples/dreamer/dreamer_utils.py\n# --------------------------------------------------\n#     # Compute observation reco\n#     if cfg.record_video and record._count % cfg.record_interval == 0:\n#         world_model_td = sampled_tensordict\n# \n#         true_pixels = recover_pixels(world_model_td[(\"next\", \"pixels\")], stats)\n# \n#         reco_pixels = recover_pixels(world_model_td[\"next\", \"reco_pixels\"], stats)\n#         with autocast(dtype=torch.float16):\n#             world_model_td = world_model_td.select(\"state\", \"belief\", \"reward\")\n#             world_model_td = model_based_env.rollout(\n#                 max_steps=true_pixels.shape[1],\n#                 policy=actor_model,\n#                 auto_reset=False,\n#                 tensordict=world_model_td[:, 0],\n#             )\n#         imagine_pxls = recover_pixels(\n#             model_based_env.decode_obs(world_model_td)[\"next\", \"reco_pixels\"],\n#             stats,\n#         )\n# \n# --------------------------------------------------\n# the below code fragment can be found in:\n# examples/dreamer/dreamer_utils.py\n# --------------------------------------------------\n#         world_model_td = sampled_tensordict\n# \n#         true_pixels = recover_pixels(world_model_td[(\"next\", \"pixels\")], stats)\n# \n#         reco_pixels = recover_pixels(world_model_td[\"next\", \"reco_pixels\"], stats)\n#         with autocast(dtype=torch.float16):\n#             world_model_td = world_model_td.select(\"state\", \"belief\", \"reward\")\n#             world_model_td = model_based_env.rollout(\n#                 max_steps=true_pixels.shape[1],\n#                 policy=actor_model,\n#                 auto_reset=False,\n#                 tensordict=world_model_td[:, 0],\n#             )\n#         imagine_pxls = recover_pixels(\n#             model_based_env.decode_obs(world_model_td)[\"next\", \"reco_pixels\"],\n#             stats,\n#         )\n# \n#         stacked_pixels = torch.cat([true_pixels, reco_pixels, imagine_pxls], dim=-1)\n#         if logger is not None:\n# --------------------------------------------------\n\"\"\"Based on the above, complete the following code:\"\"\"\n\n_tag = f\"Dreamer_{cfg.env_name}_policy_test\" if cfg.record_video else \"\"\n\n    key, init_env_steps, stats = None, None, None\n    if not cfg.vecnorm and cfg.norm_stats:\n        if not hasattr(cfg, \"init_env_steps\"):\n            raise AttributeError(\"init_env_steps missing from arguments.\")\n        key = (\"next\", \"pixels\") if cfg.from_pixels else (\"next\", \"observation_vector\")\n        init_env_steps = cfg.init_env_steps\n        stats = {\"loc\": None, \"scale\": None}\n    elif cfg.from_pixels:\n        stats = {\"loc\": 0.5, \"scale\": 0.5}\n    proof_env = transformed_env_constructor(\n        cfg=cfg, use_env_creator=False, stats=stats\n    )()\n    initialize_observation_norm_transforms(\n        proof_environment=proof_env, num_iter=init_env_steps, key=key\n    )\n    _, obs_norm_state_dict = retrieve_observation_norms_state_dict(proof_env)[0]\n    proof_env.close()\n\n    # Create the different components of dreamer\n    world_model, model_based_env, actor_model, value_model, policy = make_dreamer(\n        obs_norm_state_dict=obs_norm_state_dict,\n        cfg=cfg,\n        device=device,\n        use_decoder_in_env=True,\n        action_key=\"action\",\n        value_key=\"state_value\",\n        proof_environment=transformed_env_constructor(\n            cfg, stats={\"loc\": 0.0, \"scale\": 1.0}\n        )(),\n    )\n\n    # reward normalization\n    if cfg.normalize_rewards_online:\n        # if used the running statistics of the rewards are computed and the\n        # rewards used for training will be normalized based on these.\n        reward_normalizer = RewardNormalizer(\n            scale=cfg.normalize_rewards_online_scale,\n            decay=cfg.normalize_rewards_online_decay,\n        )\n    else:\n        reward_normalizer = None\n\n    # Losses\n    world_model_loss = DreamerModelLoss(world_model)\n    actor_loss = DreamerActorLoss(\n        actor_model,\n        value_model,\n        model_based_env,\n        imagination_horizon=cfg.imagination_horizon,\n    )\n    value_loss = DreamerValueLoss(value_model)\n\n    # Exploration noise to be added to the actions\n    if cfg.exploration == \"additive_gaussian\":\n        exploration_policy = AdditiveGaussianWrapper(\n            policy,\n            sigma_init=0.3,\n            sigma_end=0.3,\n        ).to(device)\n    elif cfg.exploration == \"ou_exploration\":\n        exploration_policy = OrnsteinUhlenbeckProcessWrapper(\n            policy,\n            annealing_num_steps=cfg.total_frames,\n        ).to(device)\n    elif cfg.exploration == \"\":\n        exploration_policy = policy.to(device)\n\n    action_dim_gsde, state_dim_gsde = None, None\n    create_env_fn = parallel_env_constructor(\n        cfg=cfg,\n        obs_norm_state_dict=obs_norm_state_dict,\n        action_dim_gsde=action_dim_gsde,\n        state_dim_gsde=state_dim_gsde,\n    )\n\n    # Create the replay buffer\n\n    collector = make_collector_offpolicy(\n        make_env=create_env_fn,\n        actor_model_explore=exploration_policy,\n        cfg=cfg,\n        # make_env_kwargs=[\n        #     {\"device\": device}\n        #     for device in cfg.collector_devices\n        # ],\n    )\n    print(\"collector:\", collector)\n\n    replay_buffer = make_replay_buffer(device, cfg)\n\n    record = Recorder(\n        record_frames=cfg.record_frames,\n        frame_skip=cfg.frame_skip,\n        policy_exploration=policy,\n        recorder=make_recorder_env(\n            cfg=cfg,\n            video_tag=video_tag,\n            obs_norm_state_dict=obs_norm_state_dict,\n            logger=logger,\n            create_env_fn=create_env_fn,\n        ),\n        record_interval=cfg.record_interval,\n        log_keys=cfg.recorder_log_keys,\n    )\n\n    final_seed = collector.set_seed(cfg.seed)\n    print(f\"init seed: {cfg.seed}, final seed: {final_seed}\")\n    # Training loop\n    collected_frames = 0\n    pbar = tqdm.tqdm(total=cfg.total_frames)\n    path = Path(\"./log\")\n    path.mkdir(exist_ok=True)\n\n    # optimizers\n    world_model_opt = torch.optim.Adam(world_model.parameters(), lr=cfg.world_model_lr)\n    actor_opt = torch.optim.Adam(actor_model.parameters(), lr=cfg.actor_value_lr)\n    value_opt = torch.optim.Adam(value_model.parameters(), lr=cfg.actor_value_lr)\n\n    scaler1 = GradScaler()\n    scaler2 = GradScaler()\n    scaler3 = GradScaler()\n\n    for i, tensordict in enumerate(collector):\n        cmpt = 0\n        if reward_normalizer is not None:\n            reward_normalizer.update_reward_stats(tensordict)\n        pbar.update(tensordict.numel())\n        current_frames = tensordict.numel()\n        collected_frames += current_frames\n\n        # Compared to the original paper, the replay buffer is not temporally sampled. We fill it with trajectories of length batch_length.\n        # To be closer to the paper, we would need to fill it with trajectories of lentgh 1000 and then sample subsequences of length batch_length.\n\n        # tensordict = tensordict.reshape(-1, cfg.batch_length)\n        print(tensordict.shape)\n        replay_buffer.extend(tensordict.cpu())\n        logger.log_scalar(\n            \"r_training\",\n            tensordict[\"reward\"].mean().detach().item(),\n            step=collected_frames,\n        )\n\n        if (i % cfg.record_interval) == 0:\n            do_log = True\n        else:\n            do_log = False\n\n        if collected_frames >= cfg.init_random_frames:\n            if i % cfg.record_interval == 0:\n                logger.log_scalar(\"cmpt\", cmpt)\n            for j in range(cfg.optim_steps_per_batch):\n                cmpt += 1\n                # sample from replay buffer\n                sampled_tensordict = replay_buffer.sample(cfg.batch_size).to(\n                    device, non_blocking=True\n                )\n                if reward_normalizer is not None:\n                    sampled_tensordict = reward_normalizer.normalize_reward(\n                        sampled_tensordict\n                    )\n                # update world model\n                with autocast(dtype=torch.float16):\n                    model_loss_td, sampled_tensordict = world_model_loss(\n                        sampled_tensordict\n                    )\n                    loss_world_model = (\n                        model_loss_td[\"loss_model_kl\"]\n                        + model_loss_td[\"loss_model_reco\"]\n                        + model_loss_td[\"loss_model_reward\"]\n                    )\n                    # If we are logging videos, we keep some frames.\n                    if (\n                        cfg.record_video\n                        and (record._count + 1) % cfg.record_interval == 0\n                    ):\n                        sampled_tensordict_save = (\n                            sampled_tensordict.select(\n                                \"next\" \"state\",", "metadata": {"task_id": "pytorch_rl/129", "ground_truth": "                                \"belief\",", "fpath_tuple": ["pytorch_rl", "examples", "dreamer", "dreamer.py"], "context_start_lineno": 96, "line_no": 276, "query_window": {"context": "                        sampled_tensordict\n                    )\n                # update world model\n                with autocast(dtype=torch.float16):\n                    model_loss_td, sampled_tensordict = world_model_loss(\n                        sampled_tensordict\n                    )\n                    loss_world_model = (\n                        model_loss_td[\"loss_model_kl\"]\n                        + model_loss_td[\"loss_model_reco\"]\n                        + model_loss_td[\"loss_model_reward\"]\n                    )\n                    # If we are logging videos, we keep some frames.\n                    if (\n                        cfg.record_video\n                        and (record._count + 1) % cfg.record_interval == 0\n                    ):\n                        sampled_tensordict_save = (\n                            sampled_tensordict.select(\n                                \"next\" \"state\",", "metadata": {"fpath_tuple": ["pytorch_rl", "examples", "dreamer", "dreamer.py"], "line_no": 276, "task_id": "pytorch_rl/129", "start_line_no": 256, "end_line_no": 276, "window_size": 20, "context_start_lineno": 96, "repo": "pytorch_rl"}}, "top_k_context": [{"context": "    # Compute observation reco\n    if cfg.record_video and record._count % cfg.record_interval == 0:\n        world_model_td = sampled_tensordict\n\n        true_pixels = recover_pixels(world_model_td[(\"next\", \"pixels\")], stats)\n\n        reco_pixels = recover_pixels(world_model_td[\"next\", \"reco_pixels\"], stats)\n        with autocast(dtype=torch.float16):\n            world_model_td = world_model_td.select(\"state\", \"belief\", \"reward\")\n            world_model_td = model_based_env.rollout(\n                max_steps=true_pixels.shape[1],\n                policy=actor_model,\n                auto_reset=False,\n                tensordict=world_model_td[:, 0],\n            )\n        imagine_pxls = recover_pixels(\n            model_based_env.decode_obs(world_model_td)[\"next\", \"reco_pixels\"],\n            stats,\n        )\n", "metadata": [{"fpath_tuple": ["pytorch_rl", "examples", "dreamer", "dreamer_utils.py"], "line_no": 322, "start_line_no": 312, "end_line_no": 332, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}], "sim_score": 0.45454545454545453}, {"context": "                    step=collected_frames,\n                )\n    # Compute observation reco\n    if cfg.record_video and record._count % cfg.record_interval == 0:\n        world_model_td = sampled_tensordict\n\n        true_pixels = recover_pixels(world_model_td[(\"next\", \"pixels\")], stats)\n\n        reco_pixels = recover_pixels(world_model_td[\"next\", \"reco_pixels\"], stats)\n        with autocast(dtype=torch.float16):\n            world_model_td = world_model_td.select(\"state\", \"belief\", \"reward\")\n            world_model_td = model_based_env.rollout(\n                max_steps=true_pixels.shape[1],\n                policy=actor_model,\n                auto_reset=False,\n                tensordict=world_model_td[:, 0],\n            )\n        imagine_pxls = recover_pixels(\n            model_based_env.decode_obs(world_model_td)[\"next\", \"reco_pixels\"],\n            stats,", "metadata": [{"fpath_tuple": ["pytorch_rl", "examples", "dreamer", "dreamer_utils.py"], "line_no": 320, "start_line_no": 310, "end_line_no": 330, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}], "sim_score": 0.448}, {"context": "                    key,\n                    value.detach().item(),\n                    step=collected_frames,\n                )\n    # Compute observation reco\n    if cfg.record_video and record._count % cfg.record_interval == 0:\n        world_model_td = sampled_tensordict\n\n        true_pixels = recover_pixels(world_model_td[(\"next\", \"pixels\")], stats)\n\n        reco_pixels = recover_pixels(world_model_td[\"next\", \"reco_pixels\"], stats)\n        with autocast(dtype=torch.float16):\n            world_model_td = world_model_td.select(\"state\", \"belief\", \"reward\")\n            world_model_td = model_based_env.rollout(\n                max_steps=true_pixels.shape[1],\n                policy=actor_model,\n                auto_reset=False,\n                tensordict=world_model_td[:, 0],\n            )\n        imagine_pxls = recover_pixels(", "metadata": [{"fpath_tuple": ["pytorch_rl", "examples", "dreamer", "dreamer_utils.py"], "line_no": 318, "start_line_no": 308, "end_line_no": 328, "window_size": 20, "repo": "pytorch_rl", "slice_size": 10}], "sim_score": 0.43410852713178294}], "window_size": 20, "slice_size": 10}}
