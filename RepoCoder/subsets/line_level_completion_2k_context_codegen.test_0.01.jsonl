{"prompt": "buffer(\"loc\", loc)\n        self.register_buffer(\"scale\", scale)\n\n    def init_stats(\n        self,\n        num_iter: int,\n        reduce_dim: Union[int, Tuple[int]] = 0,\n        cat_dim: Optional[int] = None,\n        key: Optional[str] = None,\n        keep_dims: Optional[Tuple[int]] = None,\n    ) -> None:\n        \"\"\"Initializes the loc and scale stats of the parent environment.\n\n        Normalization constant should ideally make the observation statistics approach\n        those of a standard Gaussian distribution. This method computes a location\n        and scale tensor that will empirically compute the mean and standard\n        deviation of a Gaussian distribution fitted on data generated randomly with\n        the parent environment for a given number of steps.\n\n        Args:\n            num_iter (int): number of random iterations to run in the environment.\n            reduce_dim (int or tuple of int, optional): dimension to compute the mean and std over.\n                Defaults to 0.\n            cat_dim (int, optional): dimension along which the batches collected will be concatenated.\n                It must be part equal to reduce_dim (if integer) or part of the reduce_dim tuple.\n                Defaults to the same value as reduce_dim.\n            key (str, optional): if provided, the summary statistics will be\n                retrieved from that key in the resulting tensordicts.\n                Otherwise, the first key in :obj:`ObservationNorm.in_keys` will be used.\n            keep_dims (tuple of int, optional): the dimensions to keep in the loc and scale.\n                For instance, one may want the location and scale to have shape [C, 1, 1]\n                when normalizing a 3D tensor over the last two dimensions, but not the\n                third. Defaults to None.\n\n        \"\"\"\n        if cat_dim is None:\n            cat_dim = reduce_dim\n            if not isinstance(cat_dim, int):\n                raise ValueError(\n                    \"cat_dim must be specified if reduce_dim is not an integer.\"\n                )\n        if (isinstance(reduce_dim, tuple) and cat_dim not in reduce_dim) or (\n            isinstance(reduce_dim, int) and cat_dim!= reduce_dim\n        ):\n            raise ValueError(\"cat_dim must be part of or equal to reduce_dim.\")\n        if self.loc is not None or self.scale is not None:\n            raise RuntimeError(\n                f\"Loc/Scale are already initialized: ({self.loc}, {self.scale})\"\n            )\n\n        if len(self.in_keys) > 1 and key is None:\n            raise RuntimeError(\n                \"Transform has multiple in_keys but no specific key was passed as an argument\"\n            )\n        key = self.in_keys[0] if key is None else key\n\n        def raise_initialization_exception(module):\n            if (\n                isinstance(module, ObservationNorm)\n                and module.scale is None\n                and module.loc is None\n            ):\n                raise RuntimeError(\n                    \"ObservationNorms need to be initialized in the right order.\"\n                    \"Trying to initialize an ObservationNorm \"\n                    \"while a parent ObservationNorm transform is still uninitialized\"\n                )\n\n        parent = self.parent\n        parent.apply(raise_initialization_exception)\n\n        collected_frames = 0\n        data = []\n        while collected_frames < num_iter:\n            tensordict = parent.rollout(max_steps=num_iter)\n            collected_frames += tensordict.numel()\n            data.append(tensordict.get(key))\n\n        data = torch.cat(data, cat_dim)\n        if isinstance(reduce_dim, int):\n            reduce_dim = [reduce_dim]\n        if keep_dims is not None:\n            if not all(k in reduce_dim for k in keep_dims):\n                raise ValueError(\"keep_dim elements must be part of reduce_dim list.\")\n        else:\n            keep_dims = []\n        loc = data.mean(reduce_dim, keepdim=True)\n        scale = data.std(reduce_dim, keepdim=True)\n        for r in sorted(reduce_dim, reverse=True):\n            if r not in keep_dims:\n                loc = loc.squeeze(r)\n                scale = scale.squeeze(r)\n\n        if not self.standard_normal:\n            scale = 1 / scale.clamp_min(self.eps)\n            loc = -loc * scale\n\n        if not torch.isfinite(loc).all():\n            raise RuntimeError(\"Non-finite values found in loc\")\n        if not torch.isfinite(scale).all():\n            raise RuntimeError(\"Non-finite values found in scale\")\n        self.register_buffer(\"loc\", loc)\n        self.register_buffer(\"scale\", scale.clamp_min(self.eps))\n\n    def _apply_transform(self, obs: torch.Tensor) -> torch.Tensor:\n        if self.loc is None or self.scale is None:\n            raise RuntimeError(\n                \"Loc/Scale have not been initialized. Either pass in values in the constructor \"\n                \"or call the init_stats method\"\n            )\n        if self.standard_normal:\n            loc = self.loc\n            scale = self.scale\n            return (obs - loc) / scale\n        else:\n            scale = self.scale\n            loc = self.loc\n            return obs * scale + loc\n\n    @_apply_to_composite\n    def transform_observation_spec(self, observation_spec: TensorSpec) -> TensorSpec:\n        space = observation_spec.space\n        if isinstance(space, ContinuousBox):\n            space.minimum = self._apply_transform(space.minimum)\n            space.maximum = self._apply_transform(space.maximum)\n        return observation_spec\n\n    def __repr__(self) -> str:\n        if self.loc.numel() == 1 and self.scale.numel() == 1:\n            return (\n                f\"{self.__class__.__name__}(\"\n                f\"loc={float(self.loc):4.4f}, scale\"\n                f\"={float(self.scale):4.4f}, keys={self.in_keys})\"\n            )\n        else:\n            return super().__repr__()\n\n\nclass CatFrames(ObservationTransform):\n    \"\"\"Concatenates successive observation frames into a single tensor.\n\n    This can, for instance, account for movement/velocity of the observed\n    feature. Proposed in \"Playing Atari with Deep Reinforcement Learning\" (\n    https://arxiv.org/pdf/1312.5602.pdf).\n\n    CatFrames is a stateful class and it can be reset to its native state by\n    calling the `reset()` method.\n\n    Args:\n        N (int): number of observation to concatenate.\n        dim (int): dimension along which concatenate the\n            observations. Should be negative, to ensure that it is compatible\n            with environments of different batch_size.\n        in_keys (list of int, optional): keys pointing to the frames that have\n            to be concatenated. Defaults to [\"pixels\"].\n        out_keys (list of int, optional): keys pointing to where the output\n            has to be written. Defaults to the value of `in_keys`.\n\n    \"\"\"\n\n    inplace = False\n    _CAT_DIM_ERR = (\n        \"dim must be > 0 to accomodate for tensordict of \"\n        \"different batch-sizes (since negative dims are batch invariant).\"\n    )\n\n    def __init__(\n        self,\n        N: int,\n        dim: int,\n        in_keys: Optional[Sequence[str]] = None,\n        out_keys: Optional[Sequence[str]] = None,\n    ):\n        if in_keys is None:\n            in_keys = IMAGE_KEYS", "metadata": {"task_id": "pytorch_rl/109", "ground_truth": "        super().__init__(in_keys=in_keys, out_keys=out_keys)", "fpath_tuple": ["pytorch_rl", "torchrl", "envs", "transforms", "transforms.py"], "context_start_lineno": 1347, "line_no": 1522}}
{"prompt": ":Stitched>True</GSpherical:Stitched>\n<GSpherical:StitchingSoftware>nerfstudio</GSpherical:StitchingSoftware>\n</rdf:SphericalVideo>\"\"\",\n        \"utf-8\",\n    )\n    insert_size = len(spherical_metadata) + 8 + 16\n    with open(output_filename, mode=\"r+b\") as mp4file:\n        try:\n            # get file size\n            mp4file_size = os.stat(output_filename).st_size\n\n            # find moov container (probably after ftyp, free, mdat)\n            while True:\n                pos = mp4file.tell()\n                size, tag = struct.unpack(\">I4s\", mp4file.read(8))\n                if tag == b\"moov\":\n                    break\n                mp4file.seek(pos + size)\n            # if moov isn't at end, bail\n            if pos + size!= mp4file_size:\n                # TODO: to support faststart, rewrite all stco offsets\n                raise Exception(\"moov container not at end of file\")\n            # go back and write inserted size\n            mp4file.seek(pos)\n            mp4file.write(struct.pack(\">I\", size + insert_size))\n            # go inside moov\n            mp4file.seek(pos + 8)\n            # find trak container (probably after mvhd)\n            while True:\n                pos = mp4file.tell()\n                size, tag = struct.unpack(\">I4s\", mp4file.read(8))\n                if tag == b\"trak\":\n                    break\n                mp4file.seek(pos + size)\n            # go back and write inserted size\n            mp4file.seek(pos)\n            mp4file.write(struct.pack(\">I\", size + insert_size))\n            # we need to read everything from end of trak to end of file in order to insert\n            # TODO: to support faststart, make more efficient (may load nearly all data)\n            mp4file.seek(pos + size)\n            rest_of_file = mp4file.read(mp4file_size - pos - size)\n            # go to end of trak (again)\n            mp4file.seek(pos + size)\n            # insert our uuid atom with spherical metadata\n            mp4file.write(struct.pack(\">I4s16s\", insert_size, b\"uuid\", spherical_uuid))\n            mp4file.write(spherical_metadata)\n            # write rest of file\n            mp4file.write(rest_of_file)\n        finally:\n            mp4file.close()\n\n\n@dataclass\nclass CropData:\n    \"\"\"Data for cropping an image.\"\"\"\n\n    background_color: TensorType[3] = torch.Tensor([0.0, 0.0, 0.0])\n    \"\"\"background color\"\"\"\n    center: TensorType[3] = torch.Tensor([0.0, 0.0, 0.0])\n    \"\"\"center of the crop\"\"\"\n    scale: TensorType[3] = torch.Tensor([2.0, 2.0, 2.0])\n    \"\"\"scale of the crop\"\"\"\n\n\ndef get_crop_from_json(camera_json: Dict[str, Any]) -> Optional[CropData]:\n    \"\"\"Load crop data from a camera path JSON\n\n    args:\n        camera_json: camera path data\n    returns:\n        Crop data\n    \"\"\"\n    if \"crop\" not in camera_json or camera_json[\"crop\"] is None:\n        return None\n\n    bg_color = camera_json[\"crop\"][\"crop_bg_color\"]\n\n    return CropData(\n        background_color=torch.Tensor([bg_color[\"r\"] / 255.0, bg_color[\"g\"] / 255.0, bg_color[\"b\"] / 255.0]),\n        center=torch.Tensor(camera_json[\"crop\"][\"crop_center\"]),\n        scale=torch.Tensor(camera_json[\"crop\"][\"crop_scale\"]),\n    )\n\n\n@dataclass\nclass RenderTrajectory:\n    \"\"\"Load a checkpoint, render a trajectory, and save to a video file.\"\"\"\n\n    load_config: Path\n    \"\"\"Path to config YAML file.\"\"\"\n    rendered_output_names: List[str] = field(default_factory=lambda: [\"rgb\"])\n    \"\"\"Name of the renderer outputs to use. rgb, depth, etc. concatenates them along y axis\"\"\"\n    traj: Literal[\"spiral\", \"filename\"] = \"spiral\"\n    \"\"\"Trajectory to render.\"\"\"\n    downscale_factor: int = 1\n    \"\"\"Scaling factor to apply to the camera image resolution.\"\"\"\n    camera_path_filename: Path = Path(\"camera_path.json\")\n    \"\"\"Filename of the camera path to render.\"\"\"\n    output_path: Path = Path(\"renders/output.mp4\")\n    \"\"\"Name of the output file.\"\"\"\n    seconds: float = 5.0\n    \"\"\"How long the video should be.\"\"\"\n    output_format: Literal[\"images\", \"video\"] = \"video\"\n    \"\"\"How to save output data.\"\"\"\n    eval_num_rays_per_chunk: Optional[int] = None\n    \"\"\"Specifies number of rays per chunk during eval.\"\"\"\n\n    def main(self) -> None:\n        \"\"\"Main function.\"\"\"\n        _, pipeline, _ = eval_setup(\n            self.load_config,\n            eval_num_rays_per_chunk=self.eval_num_rays_per_chunk,\n            test_mode=\"test\" if self.traj == \"spiral\" else \"inference\",\n        )\n\n        install_checks.check_ffmpeg_installed()\n\n        seconds = self.seconds\n        crop_data = None\n\n        # TODO(ethan): use camera information from parsing args\n        if self.traj == \"spiral\":\n            camera_start = pipeline.datamanager.eval_dataloader.get_camera(image_idx=0).flatten()\n            # TODO(ethan): pass in the up direction of the camera\n            camera_type = CameraType.PERSPECTIVE\n            camera_path = get_spiral_path(camera_start, steps=30, radius=0.1)\n        elif self.traj == \"filename\":\n            with open(self.camera_path_filename, \"r\", encoding=\"utf-8\") as f:\n                camera_path = json.load(f)\n            seconds = camera_path[\"seconds\"]\n            if \"camera_type\" not in camera_path:\n                camera_type = CameraType.PERSPECTIVE\n            elif camera_path[\"camera_type\"] == \"fisheye\":\n                camera_type = CameraType.FISHEYE\n            elif camera_path[\"camera_type\"] == \"equirectangular\":\n                camera_type = CameraType.EQUIRECTANGULAR\n            else:\n                camera_type = CameraType.PERSPECTIVE\n            crop_data = get_crop_from_json(camera_path)\n            camera_path = get_path_from_json(camera_path)\n        else:\n            assert_never(self.traj)\n\n        _render_trajectory_video(\n            pipeline,\n            camera_path,\n            output_filename=self.output_path,\n            rendered_output_names=self.rendered_output_names,\n            rendered_resolution_scaling_factor=1.0 / self.downscale_factor,\n            crop_data=crop_data,\n            seconds=seconds,\n            output_format=self.output_format,\n            camera_type=camera_type,\n        )\n\n\ndef entrypoint():\n    \"\"\"Entrypoint for use with pyproject scripts.\"\"\"\n    tyro.extras.set_accent_color(\"bright_yellow\")\n    tyro.cli(RenderTrajectory).main()\n\n\nif __name__ == \"__main__\":", "metadata": {"task_id": "nerfstudio-project_nerfstudio/28", "ground_truth": "    entrypoint()", "fpath_tuple": ["nerfstudio-project_nerfstudio", "scripts", "render.py"], "context_start_lineno": 165, "line_no": 328}}
{"prompt": ".item() - 201.9864) < 1e-2\n        assert abs(result_mean.item() - 0.2630) < 1e-3\n\n\nclass DDIMSchedulerTest(SchedulerCommonTest):\n    scheduler_classes = (DDIMScheduler,)\n    forward_default_kwargs = ((\"eta\", 0.0), (\"num_inference_steps\", 50))\n\n    def get_scheduler_config(self, **kwargs):\n        config = {\n            \"num_train_timesteps\": 1000,\n            \"beta_start\": 0.0001,\n            \"beta_end\": 0.02,\n            \"beta_schedule\": \"linear\",\n            \"clip_sample\": True,\n        }\n\n        config.update(**kwargs)\n        return config\n\n    def full_loop(self, **config):\n        scheduler_class = self.scheduler_classes[0]\n        scheduler_config = self.get_scheduler_config(**config)\n        scheduler = scheduler_class(**scheduler_config)\n\n        num_inference_steps, eta = 10, 0.0\n\n        model = self.dummy_model()\n        sample = self.dummy_sample_deter\n\n        scheduler.set_timesteps(num_inference_steps)\n\n        for t in scheduler.timesteps:\n            residual = model(sample, t)\n            sample = scheduler.step(residual, t, sample, eta).prev_sample\n\n        return sample\n\n    def test_timesteps(self):\n        for timesteps in [100, 500, 1000]:\n            self.check_over_configs(num_train_timesteps=timesteps)\n\n    def test_steps_offset(self):\n        for steps_offset in [0, 1]:\n            self.check_over_configs(steps_offset=steps_offset)\n\n        scheduler_class = self.scheduler_classes[0]\n        scheduler_config = self.get_scheduler_config(steps_offset=1)\n        scheduler = scheduler_class(**scheduler_config)\n        scheduler.set_timesteps(5)\n        assert torch.equal(scheduler.timesteps, torch.LongTensor([801, 601, 401, 201, 1]))\n\n    def test_betas(self):\n        for beta_start, beta_end in zip([0.0001, 0.001, 0.01, 0.1], [0.002, 0.02, 0.2, 2]):\n            self.check_over_configs(beta_start=beta_start, beta_end=beta_end)\n\n    def test_schedules(self):\n        for schedule in [\"linear\", \"squaredcos_cap_v2\"]:\n            self.check_over_configs(beta_schedule=schedule)\n\n    def test_prediction_type(self):\n        for prediction_type in [\"epsilon\", \"v_prediction\"]:\n            self.check_over_configs(prediction_type=prediction_type)\n\n    def test_clip_sample(self):\n        for clip_sample in [True, False]:\n            self.check_over_configs(clip_sample=clip_sample)\n\n    def test_time_indices(self):\n        for t in [1, 10, 49]:\n            self.check_over_forward(time_step=t)\n\n    def test_inference_steps(self):\n        for t, num_inference_steps in zip([1, 10, 50], [10, 50, 500]):\n            self.check_over_forward(time_step=t, num_inference_steps=num_inference_steps)\n\n    def test_eta(self):\n        for t, eta in zip([1, 10, 49], [0.0, 0.5, 1.0]):\n            self.check_over_forward(time_step=t, eta=eta)\n\n    def test_variance(self):\n        scheduler_class = self.scheduler_classes[0]\n        scheduler_config = self.get_scheduler_config()\n        scheduler = scheduler_class(**scheduler_config)\n\n        assert torch.sum(torch.abs(scheduler._get_variance(0, 0) - 0.0)) < 1e-5\n        assert torch.sum(torch.abs(scheduler._get_variance(420, 400) - 0.14771)) < 1e-5\n        assert torch.sum(torch.abs(scheduler._get_variance(980, 960) - 0.32460)) < 1e-5\n        assert torch.sum(torch.abs(scheduler._get_variance(0, 0) - 0.0)) < 1e-5\n        assert torch.sum(torch.abs(scheduler._get_variance(487, 486) - 0.00979)) < 1e-5\n        assert torch.sum(torch.abs(scheduler._get_variance(999, 998) - 0.02)) < 1e-5\n\n    def test_full_loop_no_noise(self):\n        sample = self.full_loop()\n\n        result_sum = torch.sum(torch.abs(sample))\n        result_mean = torch.mean(torch.abs(sample))\n\n        assert abs(result_sum.item() - 172.0067) < 1e-2\n        assert abs(result_mean.item() - 0.223967) < 1e-3\n\n    def test_full_loop_with_v_prediction(self):\n        sample = self.full_loop(prediction_type=\"v_prediction\")\n\n        result_sum = torch.sum(torch.abs(sample))\n        result_mean = torch.mean(torch.abs(sample))\n\n        assert abs(result_sum.item() - 52.5302) < 1e-2\n        assert abs(result_mean.item() - 0.0684) < 1e-3\n\n    def test_full_loop_with_set_alpha_to_one(self):\n        # We specify different beta, so that the first alpha is 0.99\n        sample = self.full_loop(set_alpha_to_one=True, beta_start=0.01)\n        result_sum = torch.sum(torch.abs(sample))\n        result_mean = torch.mean(torch.abs(sample))\n\n        assert abs(result_sum.item() - 149.8295) < 1e-2\n        assert abs(result_mean.item() - 0.1951) < 1e-3\n\n    def test_full_loop_with_no_set_alpha_to_one(self):\n        # We specify different beta, so that the first alpha is 0.99\n        sample = self.full_loop(set_alpha_to_one=False, beta_start=0.01)\n        result_sum = torch.sum(torch.abs(sample))\n        result_mean = torch.mean(torch.abs(sample))\n\n        assert abs(result_sum.item() - 149.0784) < 1e-2\n        assert abs(result_mean.item() - 0.1941) < 1e-3\n\n\nclass DPMSolverSinglestepSchedulerTest(SchedulerCommonTest):\n    scheduler_classes = (DPMSolverSinglestepScheduler,)\n    forward_default_kwargs = ((\"num_inference_steps\", 25),)\n\n    def get_scheduler_config(self, **kwargs):\n        config = {\n            \"num_train_timesteps\": 1000,\n            \"beta_start\": 0.0001,\n            \"beta_end\": 0.02,\n            \"beta_schedule\": \"linear\",\n            \"solver_order\": 2,\n            \"prediction_type\": \"epsilon\",", "metadata": {"task_id": "huggingface_diffusers/51", "ground_truth": "            \"thresholding\": False,", "fpath_tuple": ["huggingface_diffusers", "tests", "test_scheduler.py"], "context_start_lineno": 719, "line_no": 860}}
{"prompt": "import pytest\nimport numpy as np\nimport torch\nfrom collections import namedtuple\n\nfrom ding.utils.default_helper import lists_to_dicts, dicts_to_lists, squeeze, default_get, override, error_wrapper,\\\n    list_split, LimitedSpaceContainer, set_pkg_seed, deep_merge_dicts, deep_update, flatten_dict\n\n\n@pytest.mark.unittest\nclass TestDefaultHelper():\n\n    def test_lists_to_dicts(self):\n        set_pkg_seed(12)\n        with pytest.raises(ValueError):\n            lists_to_dicts([])\n        with pytest.raises(TypeError):\n            lists_to_dicts([1])\n        assert lists_to_dicts([{1: 1, 10: 3}, {1: 2, 10: 4}]) == {1: [1, 2], 10: [3, 4]}\n        T = namedtuple('T', ['location', 'race'])\n        data = [T({'x': 1, 'y': 2}, 'zerg') for _ in range(3)]\n        output = lists_to_dicts(data)\n        assert isinstance(output, T) and output.__class__ == T\n        assert len(output.location) == 3\n        data = [{'value': torch.randn(1), 'obs': {'scalar': torch.randn(4)}} for _ in range(3)]\n        output = lists_to_dicts(data, recursive=True)\n        assert isinstance(output, dict)\n        assert len(output['value']) == 3\n        assert len(output['obs']['scalar']) == 3\n\n    def test_dicts_to_lists(self):\n        assert dicts_to_lists({1: [1, 2], 10: [3, 4]}) == [{1: 1, 10: 3}, {1: 2, 10: 4}]\n\n    def test_squeeze(self):\n        assert squeeze((4, )) == 4\n        assert squeeze({'a': 4}) == 4\n        assert squeeze([1, 3]) == (1, 3)\n        data = np.random.randn(3)\n        output = squeeze(data)\n        assert (output == data).all()\n\n    def test_default_get(self):\n        assert default_get({}, 'a', default_value=1, judge_fn=lambda x: x < 2) == 1\n        assert default_get({}, 'a', default_fn=lambda: 1, judge_fn=lambda x: x < 2) == 1\n        with pytest.raises(AssertionError):\n            default_get({}, 'a', default_fn=lambda: 1, judge_fn=lambda x: x < 0)\n        assert default_get({'val': 1}, 'val', default_value=2) == 1\n\n    def test_override(self):\n\n        class foo(object):\n\n            def fun(self):\n                raise NotImplementedError\n\n        class foo1(foo):\n\n            @override(foo)\n            def fun(self):\n                return \"a\"\n\n        with pytest.raises(NameError):\n\n            class foo2(foo):\n\n                @override(foo)\n                def func(self):\n                    pass\n\n        with pytest.raises(NotImplementedError):\n            foo().fun()\n        foo1().fun()\n\n    def test_error_wrapper(self):\n\n        def good_ret(a, b=1):\n            return a + b\n\n        wrap_good_ret = error_wrapper(good_ret, 0)\n        assert good_ret(1) == wrap_good_ret(1)\n\n        def bad_ret(a, b=0):\n            return a / b\n\n        wrap_bad_ret = error_wrapper(bad_ret, 0)\n        assert wrap_bad_ret(1) == 0\n\n    def test_list_split(self):\n        data = [i for i in range(10)]\n        output, residual = list_split(data, step=4)\n        assert len(output) == 2\n        assert output[1] == [4, 5, 6, 7]\n        assert residual == [8, 9]\n        output, residual = list_split(data, step=5)\n        assert len(output) == 2\n        assert output[1] == [5, 6, 7, 8, 9]\n        assert residual is None\n\n\n@pytest.mark.unittest\nclass TestLimitedSpaceContainer():\n\n    def test_container(self):\n        container = LimitedSpaceContainer(0, 5)\n        first = container.acquire_space()\n        assert first\n        assert container.cur == 1\n        left = container.get_residual_space()\n        assert left == 4\n        assert container.cur == container.max_val == 5\n        no_space = container.acquire_space()\n        assert not no_space\n        container.increase_space()\n        six = container.acquire_space()\n        assert six\n        for i in range(6):\n            container.release_space()\n            assert container.cur == 5 - i\n        container.decrease_space()\n        assert container.max_val == 5\n\n\n@pytest.mark.unittest\nclass TestDict:\n\n    def test_deep_merge_dicts(self):\n        dict1 = {\n            'a': 3,\n            'b': {\n                'c': 3,\n                'd': {\n                    'e': 6,\n                    'f': 5,\n                }\n            }\n        }\n        dict2 = {\n            'b': {\n                'c': 5,\n                'd': 6,\n                'g': 4,\n            }\n        }\n        new_dict = deep_merge_dicts(dict1, dict2)\n        assert new_dict['a'] == 3\n        assert isinstance(new_dict['b'], dict)\n        assert new_dict['b']['c'] == 5\n        assert new_dict['b']['c'] == 5\n        assert new_dict['b']['g'] == 4\n\n    def test_deep_update(self):\n        dict1 = {\n            'a': 3,\n            'b': {\n                'c': 3,\n                'd': {\n                    'e': 6,\n                    'f': 5,\n                },\n                'z': 4,\n            }\n        }\n        dict2 = {\n            'b': {\n                'c': 5,", "metadata": {"task_id": "opendilab_ACE/118", "ground_truth": "                'd': 6,", "fpath_tuple": ["opendilab_ACE", "ding", "utils", "tests", "test_default_helper.py"], "context_start_lineno": 0, "line_no": 165}}
{"prompt": "Classifier(\n                model=MyModel(self.class_output_dim),\n                posterior_approximator=LaplacePosteriorApproximator(),\n                output_calibrator=ClassificationTemperatureScaler(),\n            )\n            # no save dir, no dump\n            status = prob_class.train(\n                train_data_loader=self.class_train_data_loader,\n                calib_data_loader=self.class_val_data_loader,\n                val_data_loader=self.class_val_data_loader,\n                map_fit_config=self.class_fit_config_nodir_nodump,\n                fit_config=self.class_fit_config_nodir_nodump,\n                calib_config=self.class_calib_config_nodir_nodump,\n            )\n            sample = prob_class.posterior.sample()\n\n            # no save dir but dump\n            with self.assertRaises(ValueError):\n                status = prob_class.train(\n                    train_data_loader=self.class_train_data_loader,\n                    calib_data_loader=self.class_val_data_loader,\n                    val_data_loader=self.class_val_data_loader,\n                    map_fit_config=self.class_fit_config_nodir_nodump,\n                    fit_config=self.class_fit_config_nodir_dump,\n                    calib_config=self.class_calib_config_nodir_nodump,\n                )\n\n            # save dir, no dump\n            status = prob_class.train(\n                train_data_loader=self.class_train_data_loader,\n                calib_data_loader=self.class_val_data_loader,\n                val_data_loader=self.class_val_data_loader,\n                map_fit_config=self.class_fit_config_nodir_nodump,\n                fit_config=self.class_fit_config_dir_nodump(tmp_dir),\n                calib_config=self.class_calib_config_nodir_nodump,\n            )\n            sample = prob_class.posterior.sample()\n            prob_class.posterior.load_state(tmp_dir)\n\n            # save dir and dump\n            status = prob_class.train(\n                train_data_loader=self.class_train_data_loader,\n                calib_data_loader=self.class_val_data_loader,\n                val_data_loader=self.class_val_data_loader,\n                map_fit_config=self.class_fit_config_nodir_nodump,\n                fit_config=self.class_fit_config_dir_dump(tmp_dir),\n                calib_config=self.class_calib_config_nodir_nodump,\n            )\n            sample = prob_class.posterior.sample()\n            prob_class.posterior.load_state(tmp_dir)\n\n            # restore from laplace\n            status = prob_class.train(\n                train_data_loader=self.class_train_data_loader,\n                calib_data_loader=self.class_val_data_loader,\n                val_data_loader=self.class_val_data_loader,\n                map_fit_config=self.class_fit_config_nodir_nodump,\n                fit_config=self.class_fit_config_restore(tmp_dir),\n                calib_config=self.class_calib_config_nodir_nodump,\n            )\n\n            # restore from map\n            prob_class_map = ProbClassifier(\n                model=MyModel(self.class_output_dim),\n                posterior_approximator=MAPPosteriorApproximator(),\n                output_calibrator=ClassificationTemperatureScaler(),\n            )\n            status = prob_class_map.train(\n                train_data_loader=self.class_train_data_loader,\n                calib_data_loader=self.class_val_data_loader,\n                val_data_loader=self.class_val_data_loader,\n                map_fit_config=self.class_fit_config_nodir_nodump,\n                fit_config=self.class_fit_config_dir_dump(tmp_dir),\n                calib_config=self.class_calib_config_nodir_nodump,\n            )\n            status = prob_class.train(\n                train_data_loader=self.class_train_data_loader,\n                calib_data_loader=self.class_val_data_loader,\n                val_data_loader=self.class_val_data_loader,\n                map_fit_config=self.class_fit_config_nodir_nodump,\n                fit_config=self.class_fit_config_restore(tmp_dir),\n                calib_config=self.class_calib_config_nodir_nodump,\n            )\n\n            # load state\n            prob_class.load_state(checkpoint_path=tmp_dir)\n\n            # save state\n            prob_class.save_state(checkpoint_path=tmp_dir)\n\n    def test_dryrun_reg_swag(self):\n        with tempfile.TemporaryDirectory() as tmp_dir:\n            prob_reg = ProbRegressor(\n                model=MyModel(self.reg_output_dim),\n                likelihood_log_variance_model=MyModel(self.reg_output_dim),\n                posterior_approximator=SWAGPosteriorApproximator(rank=2),\n                output_calibrator=RegressionTemperatureScaler(),\n            )\n            # no save dir, no dump\n            status = prob_reg.train(\n                train_data_loader=self.reg_train_data_loader,\n                calib_data_loader=self.reg_val_data_loader,\n                val_data_loader=self.reg_val_data_loader,\n                map_fit_config=self.reg_fit_config_nodir_nodump,\n                fit_config=self.reg_fit_config_nodir_nodump,\n                calib_config=self.reg_calib_config_nodir_nodump,\n            )\n            sample = prob_reg.posterior.sample()\n\n            # no save dir but dump\n            with self.assertRaises(ValueError):\n                status = prob_reg.train(\n                    train_data_loader=self.reg_train_data_loader,\n                    calib_data_loader=self.reg_val_data_loader,\n                    val_data_loader=self.reg_val_data_loader,\n                    map_fit_config=self.reg_fit_config_nodir_nodump,\n                    fit_config=self.reg_fit_config_nodir_dump,\n                    calib_config=self.reg_calib_config_nodir_nodump,\n                )\n\n            # save dir, no dump\n            status = prob_reg.train(\n                train_data_loader=self.reg_train_data_loader,\n                calib_data_loader=self.reg_val_data_loader,\n                val_data_loader=self.reg_val_data_loader,\n                map_fit_config=self.reg_fit_config_nodir_nodump,\n                fit_config=self.reg_fit_config_dir_nodump(tmp_dir),\n                calib_config=self.reg_calib_config_nodir_nodump,\n            )\n            sample = prob_reg.posterior.sample()\n            prob_reg.posterior.load_state(tmp_dir)\n\n            # save dir and dump", "metadata": {"task_id": "awslabs_fortuna/163", "ground_truth": "            status = prob_reg.train(", "fpath_tuple": ["awslabs_fortuna", "tests", "fortuna", "prob_model", "test_train.py"], "context_start_lineno": 674, "line_no": 807}}
{"prompt": "from typing import Optional\n\nimport jax.numpy as jnp\n\nfrom fortuna.typing import Array\n\n\nclass OneDimensionalUncertaintyConformalRegressor:\n    def score(\n        self, val_preds: Array, val_uncertainties: Array, val_targets: Array,\n    ) -> jnp.ndarray:\n        \"\"\"\n        Compute the conformal scores.\n\n        Parameters\n        ----------\n        val_preds: Array\n            A two-dimensional array of predictions over the validation data points.\n        val_uncertainties: Array\n            A two-dimensional array of uncertainty estimates (e.g. the standard deviation). The first\n            dimension is over the validation inputs. The second must have only one component.\n        val_targets: Array\n            A two-dimensional array of validation target variables.\n\n        Returns\n        -------\n        jnp.ndarray\n            Scores.\n        \"\"\"\n        if val_preds.ndim!= 2 or val_preds.shape[1]!= 1:\n            raise ValueError(\n                \"\"\"`val_preds` must be a two-dimensional array. The second dimension must have only one\n            component.\"\"\"\n            )\n        if val_uncertainties.ndim!= 2 or val_uncertainties.shape[1]!= 1:\n            raise ValueError(\n                \"\"\"`val_uncertainties` must be a two-dimensional array. The second dimension must have only\n            one component.\"\"\"\n            )", "metadata": {"task_id": "awslabs_fortuna/101", "ground_truth": "        if (val_uncertainties <= 0).any():", "fpath_tuple": ["awslabs_fortuna", "fortuna", "conformal", "regression", "onedim_uncertainty.py"], "context_start_lineno": 0, "line_no": 39}}
{"prompt": "import unittest\n\nimport jax.numpy as jnp\nfrom flax.core import FrozenDict\nfrom jax import random\n\nfrom fortuna.data.loader import DataLoader\nfrom fortuna.model.mlp import MLP\nfrom fortuna.model.model_manager.regression import RegressionModelManager\nfrom fortuna.output_calibrator.output_calib_manager.base import \\\n    OutputCalibManager\nfrom fortuna.prob_model.joint.base import Joint\nfrom fortuna.prob_model.likelihood.regression import RegressionLikelihood\nfrom fortuna.prob_model.prior import IsotropicGaussianPrior\nfrom fortuna.prob_output_layer.regression import RegressionProbOutputLayer\nfrom tests.make_data import make_array_random_data\n\n\nclass TestJoints(unittest.TestCase):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.shape_inputs = (3,)\n        self.n_inputs = 10\n        self.output_dim = 2\n        self.rng = random.PRNGKey(0)\n        self.joint = Joint(\n            prior=IsotropicGaussianPrior(),\n            likelihood=RegressionLikelihood(\n                model_manager=RegressionModelManager(\n                    model=MLP(output_dim=self.output_dim),\n                    likelihood_log_variance_model=MLP(output_dim=self.output_dim),\n                ),\n                prob_output_layer=RegressionProbOutputLayer(),\n                output_calib_manager=OutputCalibManager(output_calibrator=None),\n            ),\n        )\n\n        self.data_arr = DataLoader.from_array_data(\n            make_array_random_data(\n                n_data=self.n_inputs,\n                shape_inputs=self.shape_inputs,\n                output_dim=self.output_dim,", "metadata": {"task_id": "awslabs_fortuna/57", "ground_truth": "                output_type=\"continuous\",", "fpath_tuple": ["awslabs_fortuna", "tests", "fortuna", "prob_model", "test_joint.py"], "context_start_lineno": 0, "line_no": 42}}
{"prompt": "d;\n    # fy(x, y) = y * d(x, y) + 2 * p2 * x * y + p1 * (r(x, y) + 2 * y^2) - yd;\n    #\n    # We are looking for a solution that satisfies\n    # fx(x, y) = fy(x, y) = 0;\n    fx = d * x + 2 * p1 * x * y + p2 * (r + 2 * x * x) - xd\n    fy = d * y + 2 * p2 * x * y + p1 * (r + 2 * y * y) - yd\n\n    # Compute derivative of d over [x, y]\n    d_r = k1 + r * (2.0 * k2 + r * (3.0 * k3 + r * 4.0 * k4))\n    d_x = 2.0 * x * d_r\n    d_y = 2.0 * y * d_r\n\n    # Compute derivative of fx over x and y.\n    fx_x = d + d_x * x + 2.0 * p1 * y + 6.0 * p2 * x\n    fx_y = d_y * x + 2.0 * p1 * x + 2.0 * p2 * y\n\n    # Compute derivative of fy over x and y.\n    fy_x = d_x * y + 2.0 * p2 * y + 2.0 * p1 * x\n    fy_y = d + d_y * y + 2.0 * p2 * x + 6.0 * p1 * y\n\n    return fx, fy, fx_x, fx_y, fy_x, fy_y\n\n\n@torch.jit.script\ndef radial_and_tangential_undistort(\n    coords: torch.Tensor,\n    distortion_params: torch.Tensor,\n    eps: float = 1e-3,\n    max_iterations: int = 10,\n) -> torch.Tensor:\n    \"\"\"Computes undistorted coords given opencv distortion parameters.\n    Adapted from MultiNeRF\n    https://github.com/google-research/multinerf/blob/b02228160d3179300c7d499dca28cb9ca3677f32/internal/camera_utils.py#L477-L509\n\n    Args:\n        coords: The distorted coordinates.\n        distortion_params: The distortion parameters [k1, k2, k3, k4, p1, p2].\n        eps: The epsilon for the convergence.\n        max_iterations: The maximum number of iterations to perform.\n\n    Returns:\n        The undistorted coordinates.\n    \"\"\"\n\n    # Initialize from the distorted point.\n    x = coords[..., 0]\n    y = coords[..., 1]\n\n    for _ in range(max_iterations):\n        fx, fy, fx_x, fx_y, fy_x, fy_y = _compute_residual_and_jacobian(\n            x=x, y=y, xd=coords[..., 0], yd=coords[..., 1], distortion_params=distortion_params\n        )\n        denominator = fy_x * fx_y - fx_x * fy_y\n        x_numerator = fx * fy_y - fy * fx_y\n        y_numerator = fy * fx_x - fx * fy_x\n        step_x = torch.where(torch.abs(denominator) > eps, x_numerator / denominator, torch.zeros_like(denominator))\n        step_y = torch.where(torch.abs(denominator) > eps, y_numerator / denominator, torch.zeros_like(denominator))\n\n        x = x + step_x\n        y = y + step_y\n\n    return torch.stack([x, y], dim=-1)\n\n\ndef rotation_matrix(a: TensorType[3], b: TensorType[3]) -> TensorType[3, 3]:\n    \"\"\"Compute the rotation matrix that rotates vector a to vector b.\n\n    Args:\n        a: The vector to rotate.\n        b: The vector to rotate to.\n    Returns:\n        The rotation matrix.\n    \"\"\"\n    a = a / torch.linalg.norm(a)\n    b = b / torch.linalg.norm(b)\n    v = torch.cross(a, b)\n    c = torch.dot(a, b)\n    # If vectors are exactly opposite, we add a little noise to one of them\n    if c < -1 + 1e-8:\n        eps = (torch.rand(3) - 0.5) * 0.01\n        return rotation_matrix(a + eps, b)\n    s = torch.linalg.norm(v)\n    skew_sym_mat = torch.Tensor(\n        [\n            [0, -v[2], v[1]],\n            [v[2], 0, -v[0]],\n            [-v[1], v[0], 0],\n        ]\n    )\n    return torch.eye(3) + skew_sym_mat + skew_sym_mat @ skew_sym_mat * ((1 - c) / (s**2 + 1e-8))\n\n\ndef auto_orient_and_center_poses(\n    poses: TensorType[\"num_poses\":..., 4, 4], method: Literal[\"pca\", \"up\", \"none\"] = \"up\", center_poses: bool = True\n) -> Tuple[TensorType[\"num_poses\":..., 3, 4], TensorType[4, 4]]:\n    \"\"\"Orients and centers the poses. We provide two methods for orientation: pca and up.\n\n    pca: Orient the poses so that the principal component of the points is aligned with the axes.\n        This method works well when all of the cameras are in the same plane.\n    up: Orient the poses so that the average up vector is aligned with the z axis.\n        This method works well when images are not at arbitrary angles.\n\n\n    Args:\n        poses: The poses to orient.\n        method: The method to use for orientation.\n        center_poses: If True, the poses are centered around the origin.\n\n    Returns:\n        Tuple of the oriented poses and the transform matrix.\n    \"\"\"\n\n    translation = poses[..., :3, 3]\n\n    mean_translation = torch.mean(translation, dim=0)\n    translation_diff = translation - mean_translation\n\n    if center_poses:\n        translation = mean_translation\n    else:\n        translation = torch.zeros_like(mean_translation)\n\n    if method == \"pca\":\n        _, eigvec = torch.linalg.eigh(translation_diff.T @ translation_diff)\n        eigvec = torch.flip(eigvec, dims=(-1,))\n\n        if torch.linalg.det(eigvec) < 0:\n            eigvec[:, 2] = -eigvec[:, 2]\n\n        transform = torch.cat([eigvec, eigvec @ -translation[..., None]], dim=-1)\n        oriented_poses = transform @ poses\n\n        if oriented_poses.mean(axis=0)[2, 1] < 0:\n            oriented_poses[:, 1:3] = -1 * oriented_poses[:, 1:3]\n    elif method == \"up\":\n        up = torch.mean(poses[:, :3, 1], dim=0)\n        up = up / torch.linalg.norm(up)\n\n        rotation = rotation_matrix(up, torch.Tensor([0, 0, 1]))\n        transform = torch.cat([rotation, rotation @ -translation[..., None]], dim=-1)\n        oriented_poses = transform @ poses\n    elif method == \"none\":", "metadata": {"task_id": "nerfstudio-project_nerfstudio/85", "ground_truth": "        transform = torch.eye(4)", "fpath_tuple": ["nerfstudio-project_nerfstudio", "nerfstudio", "cameras", "camera_utils.py"], "context_start_lineno": 331, "line_no": 474}}
{"prompt": "from typing import Optional, Dict, Union\nimport copy\nimport torch\nimport torch.nn as nn\nfrom ding.utils import SequenceType, MODEL_REGISTRY\nfrom.vac import VAC\n\n\n@MODEL_REGISTRY.register('ppg')\nclass PPG(nn.Module):\n    mode = ['compute_actor', 'compute_critic', 'compute_actor_critic']\n\n    def __init__(\n            self,\n            obs_shape: Union[int, SequenceType],\n            action_shape: Union[int, SequenceType],\n            share_encoder: bool = True,\n            continuous: bool = False,\n            encoder_hidden_size_list: SequenceType = [128, 128, 64],\n            actor_head_hidden_size: int = 64,\n            actor_head_layer_num: int = 2,\n            critic_head_hidden_size: int = 64,\n            critic_head_layer_num: int = 1,\n            activation: Optional[nn.Module] = nn.ReLU(),\n            norm_type: Optional[str] = None,\n    ) -> None:\n        super(PPG, self).__init__()\n        self.actor_critic = VAC(\n            obs_shape, action_shape, share_encoder, continuous, encoder_hidden_size_list, actor_head_hidden_size,\n            actor_head_layer_num, critic_head_hidden_size, critic_head_layer_num, activation, norm_type\n        )\n        self.aux_critic = copy.deepcopy(self.actor_critic.critic)\n\n    def forward(self, inputs: Union[torch.Tensor, Dict], mode: str) -> Dict:\n        assert mode in self.mode, \"not support forward mode: {}/{}\".format(mode, self.mode)\n        return getattr(self, mode)(inputs)\n\n    def compute_actor(self, x: torch.Tensor) -> Dict:\n        \"\"\"\n        ReturnsKeys:\n            - necessary: ``logit``\n        \"\"\"", "metadata": {"task_id": "opendilab_ACE/108", "ground_truth": "        return self.actor_critic(x, mode='compute_actor')", "fpath_tuple": ["opendilab_ACE", "ding", "model", "template", "ppg.py"], "context_start_lineno": 0, "line_no": 42}}
{"prompt": "# Copyright 2022 The Nerfstudio Team. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\nDepth dataset.\n\"\"\"\n\nfrom typing import Dict\n\nfrom nerfstudio.data.dataparsers.base_dataparser import DataparserOutputs\nfrom nerfstudio.data.datasets.base_dataset import InputDataset\nfrom nerfstudio.data.utils.data_utils import get_depth_image_from_path\n\n\nclass DepthDataset(InputDataset):\n    \"\"\"Dataset that returns images and depths.\n\n    Args:\n        dataparser_outputs: description of where and how to read input images.\n        scale_factor: The scaling factor for the dataparser outputs.\n    \"\"\"\n\n    def __init__(self, dataparser_outputs: DataparserOutputs, scale_factor: float = 1.0):\n        super().__init__(dataparser_outputs, scale_factor)\n        assert (\n            \"depth_filenames\" in dataparser_outputs.metadata.keys()\n            and dataparser_outputs.metadata[\"depth_filenames\"] is not None\n        )\n        self.depth_filenames = self.metadata[\"depth_filenames\"]", "metadata": {"task_id": "nerfstudio-project_nerfstudio/9", "ground_truth": "        self.depth_unit_scale_factor = self.metadata[\"depth_unit_scale_factor\"]", "fpath_tuple": ["nerfstudio-project_nerfstudio", "nerfstudio", "data", "datasets", "depth_dataset.py"], "context_start_lineno": 0, "line_no": 40}}
{"prompt": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom __future__ import annotations\n\nimport os\nfrom collections import OrderedDict\nfrom copy import deepcopy\nfrom multiprocessing import connection\nfrom multiprocessing.synchronize import Lock as MpLock\nfrom time import sleep\nfrom typing import Any, Callable, Dict, List, Optional, Sequence, Union\nfrom warnings import warn\n\nimport torch\nfrom tensordict import TensorDict\nfrom tensordict.tensordict import LazyStackedTensorDict, TensorDictBase\nfrom torch import multiprocessing as mp\nfrom torchrl._utils import _check_for_faulty_process\nfrom torchrl.data import CompositeSpec, TensorSpec\nfrom torchrl.data.utils import CloudpickleWrapper, DEVICE_TYPING\nfrom torchrl.envs.common import EnvBase\nfrom torchrl.envs.env_creator import get_env_metadata\n\n\ndef _check_start(fun):\n    def decorated_fun(self: _BatchedEnv, *args, **kwargs):\n        if self.is_closed:", "metadata": {"task_id": "pytorch_rl/185", "ground_truth": "            self._create_td()", "fpath_tuple": ["pytorch_rl", "torchrl", "envs", "vec_env.py"], "context_start_lineno": 0, "line_no": 30}}
{"prompt": "from typing import Union, List\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom functools import reduce\nfrom ding.utils import list_split, MODEL_REGISTRY\nfrom ding.torch_utils.network.nn_module import fc_block, MLP\nfrom ding.torch_utils.network.transformer import ScaledDotProductAttention\nfrom.q_learning import DRQN\nfrom ding.model.template.qmix import Mixer\n\n\nclass MixerStar(nn.Module):\n    \"\"\"\n    Overview:\n        mixer network for Q_star in WQMIX, which mix up the independent q_value of\n        each agent to a total q_value and is diffrent from the Qmix's mixer network,\n        here the mixing network is a feedforward network with 3 hidden layers of 256 dim.\n    Interface:\n        __init__, forward\n    \"\"\"\n\n    def __init__(self, agent_num: int, state_dim: int, mixing_embed_dim: int) -> None:\n        \"\"\"\n        Overview:\n            initialize the mixer network of Q_star in WQMIX.\n        Arguments:\n            - agent_num (:obj:`int`): the number of agent\n            - state_dim(:obj:`int`): the dimension of global observation state\n            - mixing_embed_dim (:obj:`int`): the dimension of mixing state emdedding\n        \"\"\"\n        super(MixerStar, self).__init__()\n        self.agent_num = agent_num\n        self.state_dim = state_dim\n        self.embed_dim = mixing_embed_dim\n        self.input_dim = self.agent_num + self.state_dim  # shape N+A\n        non_lin = nn.ReLU()\n        self.net = nn.Sequential(\n            nn.Linear(self.input_dim, self.embed_dim), non_lin, nn.Linear(self.embed_dim, self.embed_dim), non_lin,\n            nn.Linear(self.embed_dim, self.embed_dim), non_lin, nn.Linear(self.embed_dim, 1)\n        )\n\n        # V(s) instead of a bias for the last layers\n        self.V = nn.Sequential(nn.Linear(self.state_dim, self.embed_dim), non_lin, nn.Linear(self.embed_dim, 1))\n\n    def forward(self, agent_qs: torch.FloatTensor, states: torch.FloatTensor) -> torch.FloatTensor:\n        \"\"\"\n        Overview:\n            forward computation graph of the mixer network for Q_star in WQMIX.\n        Arguments:\n            - agent_qs (:obj:`torch.FloatTensor`): the independent q_value of each agent\n            - states (:obj:`torch.FloatTensor`): the emdedding vector of global state\n        Returns:\n            - q_tot (:obj:`torch.FloatTensor`): the total mixed q_value\n        Shapes:\n            - agent_qs (:obj:`torch.FloatTensor`): :math:`(T,B, N)`, where T is timestep,\n              B is batch size, A is agent_num, N is obs_shape\n            - states (:obj:`torch.FloatTensor`): :math:`(T, B, M)`, where M is global_obs_shape\n            - q_tot (:obj:`torch.FloatTensor`): :math:`(T, B, )`\n        \"\"\"\n        # in below annotations about the shape of the variables, T is timestep,\n        # B is batch_size A is agent_num, N is obs_shape\uff0c for example,\n        # in 3s5z, we can set T=10, B=32, A=8, N=216\n        bs = agent_qs.shape[:-1]  # (T*B, A)\n        states = states.reshape(-1, self.state_dim)  # T*B, N),\n        agent_qs = agent_qs.reshape(-1, self.agent_num)  # (T, B, A) -> (T*B, A)\n        inputs = torch.cat([states, agent_qs], dim=1)  # (T*B, N) (T*B, A)-> (T*B, N+A)\n        advs = self.net(inputs)  # (T*B, 1)\n        vs = self.V(states)  # (T*B, 1)\n        y = advs + vs\n        q_tot = y.view(*bs)  # (T*B, 1) -> (T, B)\n\n        return q_tot\n\n\n@MODEL_REGISTRY.register('wqmix')\nclass WQMix(nn.Module):\n    \"\"\"\n    Overview:\n        WQMIX network, which is same as Qmix network\n    Interface:\n        __init__, forward, _setup_global_encoder\n    \"\"\"\n\n    def __init__(\n            self,\n            agent_num: int,\n            obs_shape: int,", "metadata": {"task_id": "opendilab_ACE/116", "ground_truth": "            global_obs_shape: int,", "fpath_tuple": ["opendilab_ACE", "ding", "model", "template", "wqmix.py"], "context_start_lineno": 0, "line_no": 88}}
{"prompt": "                 msg_type=None,\n                 sender=0,\n                 receiver=0,\n                 state=0,\n                 content='None',\n                 timestamp=0,\n                 strategy=None):\n        self._msg_type = msg_type\n        self._sender = sender\n        self._receiver = receiver\n        self._state = state\n        self._content = content\n        self._timestamp = timestamp\n        self._strategy = strategy\n\n    @property\n    def msg_type(self):\n        return self._msg_type\n\n    @msg_type.setter\n    def msg_type(self, value):\n        self._msg_type = value\n\n    @property\n    def sender(self):\n        return self._sender\n\n    @sender.setter\n    def sender(self, value):\n        self._sender = value\n\n    @property\n    def receiver(self):\n        return self._receiver\n\n    @receiver.setter\n    def receiver(self, value):\n        self._receiver = value\n\n    @property\n    def state(self):\n        return self._state\n\n    @state.setter\n    def state(self, value):\n        self._state = value\n\n    @property\n    def content(self):\n        return self._content\n\n    @content.setter\n    def content(self, value):\n        self._content = value\n\n    @property\n    def timestamp(self):\n        return self._timestamp\n\n    @timestamp.setter\n    def timestamp(self, value):\n        assert isinstance(value, int) or isinstance(value, float), \\\n            \"We only support an int or a float value for timestamp\"\n        self._timestamp = value\n\n    @property\n    def strategy(self):\n        return self._strategy\n\n    @strategy.setter\n    def strategy(self, value):\n        self._strategy = value\n\n    def __lt__(self, other):\n        if self.timestamp!= other.timestamp:\n            return self.timestamp < other.timestamp\n        else:\n            return self.state < other.state\n\n    def transform_to_list(self, x):\n        if isinstance(x, list) or isinstance(x, tuple):\n            return [self.transform_to_list(each_x) for each_x in x]\n        elif isinstance(x, dict):\n            for key in x.keys():\n                x[key] = self.transform_to_list(x[key])\n            return x\n        else:\n            if hasattr(x, 'tolist'):\n                return x.tolist()\n            else:\n                return x\n\n    def msg_to_json(self, to_list=False):\n        if to_list:\n            self.content = self.transform_to_list(self.content)\n\n        json_msg = {\n           'msg_type': self.msg_type,\n           'sender': self.sender,\n           'receiver': self.receiver,\n           'state': self.state,\n            'content': self.content,\n            'timestamp': self.timestamp,\n           'strategy': self.strategy,\n        }\n        return json.dumps(json_msg)\n\n    def json_to_msg(self, json_string):\n        json_msg = json.loads(json_string)\n        self.msg_type = json_msg['msg_type']\n        self.sender = json_msg['sender']\n        self.receiver = json_msg['receiver']\n        self.state = json_msg['state']\n        self.content = json_msg['content']\n        self.timestamp = json_msg['timestamp']\n        self.strategy = json_msg['strategy']\n\n    def create_by_type(self, value, nested=False):\n        if isinstance(value, dict):\n            if isinstance(list(value.keys())[0], str):\n                m_dict = gRPC_comm_manager_pb2.mDict_keyIsString()\n                key_type ='string'\n            else:\n                m_dict = gRPC_comm_manager_pb2.mDict_keyIsInt()\n                key_type = 'int'\n\n            for key in value.keys():\n                m_dict.dict_value[key].MergeFrom(\n                    self.create_by_type(value[key], nested=True))\n            if nested:\n                msg_value = gRPC_comm_manager_pb2.MsgValue()\n                if key_type =='string':\n                    msg_value.dict_msg_stringkey.MergeFrom(m_dict)\n                else:\n                    msg_value.dict_msg_intkey.MergeFrom(m_dict)\n                return msg_value\n            else:\n                return m_dict\n        elif isinstance(value, list) or isinstance(value, tuple):\n            m_list = gRPC_comm_manager_pb2.mList()\n            for each in value:\n                m_list.list_value.append(self.create_by_type(each,\n                                                             nested=True))\n            if nested:\n                msg_value = gRPC_comm_manager_pb2.MsgValue()\n                msg_value.list_msg.MergeFrom(m_list)\n                return msg_value\n            else:\n                return m_list\n        else:\n            m_single = gRPC_comm_manager_pb2.mSingle()\n            if type(value) in [int, np.int32]:\n                m_single.int_value = value\n            elif type(value) in [str]:\n                m_single.str_value = value\n            elif type(value) in [float, np.float32]:\n                m_single.float_value = value\n            else:\n                raise ValueError(\n                    'The data type {} has not been supported.'.format(\n                        type(value)))\n\n            if nested:\n                msg_value = gRPC_comm_manager_pb2.MsgValue()\n                msg_value.single_msg.MergeFrom(m_single)\n                return msg_value\n            else:\n                return m_single\n\n    def build_msg_value(self, value):\n        msg_value = gRPC_comm_manager_pb2.MsgValue()\n\n        if isinstance(value, list) or isinstance(value, tuple):\n            msg_value.list_msg.MergeFrom(self.create_by_type(value))\n        elif isinstance(value, dict):\n            if isinstance(list(value.keys())[0], str):\n                msg_value.dict_msg_stringkey.MergeFrom(\n                    self.create_by_type(value))\n            else:\n                msg_value.dict_msg_intkey.MergeFrom(self.create_by_type(value))\n        else:\n            msg_value.single_msg.MergeFrom(self.create_by_type(value))\n\n        return msg_value\n\n    def transform(self, to_list=False):\n        if to_list:\n            self.content = self.transform_to_list(self.content)\n\n        splited_msg = gRPC_comm_manager_pb2.MessageRequest()  # map/dict\n        splited_msg.msg['sender'].MergeFrom(self.build_msg_value(self.sender))\n        splited_msg.msg['receiver'].MergeFrom(\n            self.build_msg_value(self.receiver))\n        splited_msg.msg['state'].MergeFrom(self.build_msg_value(self.state))\n        splited_msg.msg['msg_type'].MergeFrom(\n            self.build_msg_value(self.msg_type))\n        splited_msg.msg['content'].MergeFrom(self.build_msg_value(\n            self.content))\n        splited_msg.msg['timestamp'].MergeFrom(", "metadata": {"task_id": "alibaba_FederatedScope/116", "ground_truth": "            self.build_msg_value(self.timestamp))", "fpath_tuple": ["alibaba_FederatedScope", "federatedscope", "core", "message.py"], "context_start_lineno": 19, "line_no": 218}}
{"prompt": "Object()\n\n        # mock add obj class to `diffusers`\n        setattr(diffusers, \"SchedulerObject\", SchedulerObject)\n        setattr(diffusers, \"SchedulerObject2\", SchedulerObject2)\n        setattr(diffusers, \"SchedulerObject3\", SchedulerObject3)\n        logger = logging.get_logger(\"diffusers.configuration_utils\")\n        logger.setLevel(diffusers.logging.INFO)\n\n        with tempfile.TemporaryDirectory() as tmpdirname:\n            obj.save_config(tmpdirname)\n\n            with CaptureLogger(logger) as cap_logger_1:\n                config = SchedulerObject.load_config(tmpdirname)\n                new_obj_1 = SchedulerObject.from_config(config)\n\n            with CaptureLogger(logger) as cap_logger_2:\n                config = SchedulerObject2.load_config(tmpdirname)\n                new_obj_2 = SchedulerObject2.from_config(config)\n\n            with CaptureLogger(logger) as cap_logger_3:\n                config = SchedulerObject3.load_config(tmpdirname)\n                new_obj_3 = SchedulerObject3.from_config(config)\n\n        assert new_obj_1.__class__ == SchedulerObject\n        assert new_obj_2.__class__ == SchedulerObject2\n        assert new_obj_3.__class__ == SchedulerObject3\n\n        assert cap_logger_1.out == \"\"\n        assert cap_logger_2.out == \"{'f'} was not found in config. Values will be initialized to default values.\\n\"\n        assert cap_logger_3.out == \"{'f'} was not found in config. Values will be initialized to default values.\\n\"\n\n\nclass SchedulerCommonTest(unittest.TestCase):\n    scheduler_classes = ()\n    forward_default_kwargs = ()\n\n    @property\n    def dummy_sample(self):\n        batch_size = 4\n        num_channels = 3\n        height = 8\n        width = 8\n\n        sample = torch.rand((batch_size, num_channels, height, width))\n\n        return sample\n\n    @property\n    def dummy_sample_deter(self):\n        batch_size = 4\n        num_channels = 3\n        height = 8\n        width = 8\n\n        num_elems = batch_size * num_channels * height * width\n        sample = torch.arange(num_elems)\n        sample = sample.reshape(num_channels, height, width, batch_size)\n        sample = sample / num_elems\n        sample = sample.permute(3, 0, 1, 2)\n\n        return sample\n\n    def get_scheduler_config(self):\n        raise NotImplementedError\n\n    def dummy_model(self):\n        def model(sample, t, *args):\n            return sample * t / (t + 1)\n\n        return model\n\n    def check_over_configs(self, time_step=0, **config):\n        kwargs = dict(self.forward_default_kwargs)\n\n        num_inference_steps = kwargs.pop(\"num_inference_steps\", None)\n\n        for scheduler_class in self.scheduler_classes:\n            # TODO(Suraj) - delete the following two lines once DDPM, DDIM, and PNDM have timesteps casted to float by default\n            if scheduler_class in (EulerAncestralDiscreteScheduler, EulerDiscreteScheduler, LMSDiscreteScheduler):\n                time_step = float(time_step)\n\n            scheduler_config = self.get_scheduler_config(**config)\n            scheduler = scheduler_class(**scheduler_config)\n\n            if scheduler_class == VQDiffusionScheduler:\n                num_vec_classes = scheduler_config[\"num_vec_classes\"]\n                sample = self.dummy_sample(num_vec_classes)\n                model = self.dummy_model(num_vec_classes)\n                residual = model(sample, time_step)\n            else:\n                sample = self.dummy_sample\n                residual = 0.1 * sample\n\n            with tempfile.TemporaryDirectory() as tmpdirname:\n                scheduler.save_config(tmpdirname)\n                new_scheduler = scheduler_class.from_pretrained(tmpdirname)\n\n            if num_inference_steps is not None and hasattr(scheduler, \"set_timesteps\"):\n                scheduler.set_timesteps(num_inference_steps)\n                new_scheduler.set_timesteps(num_inference_steps)\n            elif num_inference_steps is not None and not hasattr(scheduler, \"set_timesteps\"):\n                kwargs[\"num_inference_steps\"] = num_inference_steps\n\n            # Set the seed before step() as some schedulers are stochastic like EulerAncestralDiscreteScheduler, EulerDiscreteScheduler\n            if \"generator\" in set(inspect.signature(scheduler.step).parameters.keys()):\n                kwargs[\"generator\"] = torch.manual_seed(0)\n            output = scheduler.step(residual, time_step, sample, **kwargs).prev_sample\n\n            if \"generator\" in set(inspect.signature(scheduler.step).parameters.keys()):\n                kwargs[\"generator\"] = torch.manual_seed(0)\n            new_output = new_scheduler.step(residual, time_step, sample, **kwargs).prev_sample\n\n            assert torch.sum(torch.abs(output - new_output)) < 1e-5, \"Scheduler outputs are not identical\"\n\n    def check_over_forward(self, time_step=0, **forward_kwargs):\n        kwargs = dict(self.forward_default_kwargs)\n        kwargs.update(forward_kwargs)\n\n        num_inference_steps = kwargs.pop(\"num_inference_steps\", None)\n\n        for scheduler_class in self.scheduler_classes:\n            if scheduler_class in (EulerAncestralDiscreteScheduler, EulerDiscreteScheduler, LMSDiscreteScheduler):\n                time_step = float(time_step)\n\n            scheduler_config = self.get_scheduler_config()\n            scheduler = scheduler_class(**scheduler_config)\n\n            if scheduler_class == VQDiffusionScheduler:\n                num_vec_classes = scheduler_config[\"num_vec_classes\"]\n                sample = self.dummy_sample(num_vec_classes)\n                model = self.dummy_model(num_vec_classes)\n                residual = model(sample, time_step)\n            else:\n                sample = self.dummy_sample\n                residual = 0.1 * sample\n\n            with tempfile.TemporaryDirectory() as tmpdirname:\n                scheduler.save_config(tmpdirname)\n                new_scheduler = scheduler_class.from_pretrained(tmpdirname)\n\n            if num_inference_steps is not None and hasattr(scheduler, \"set_timesteps\"):\n                scheduler.set_timesteps(num_inference_steps)\n                new_scheduler.set_timesteps(num_inference_steps)\n            elif num_inference_steps is not None and not hasattr(scheduler, \"set_timesteps\"):\n                kwargs[\"num_inference_steps\"] = num_inference_steps\n\n            if \"generator\" in set(inspect.signature(scheduler.step).parameters.keys()):\n                kwargs[\"generator\"] = torch.manual_seed(0)", "metadata": {"task_id": "huggingface_diffusers/178", "ground_truth": "            output = scheduler.step(residual, time_step, sample, **kwargs).prev_sample", "fpath_tuple": ["huggingface_diffusers", "tests", "test_scheduler.py"], "context_start_lineno": 184, "line_no": 333}}
{"prompt": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\n\"\"\"\nSample latency benchmarking (using RPC)\n======================================\nA rough benchmark of sample latency using different storage types over the network using `torch.rpc`.\nRun this script with --rank=0 and --rank=1 flags set in separate processes - these ranks correspond to the trainer worker and buffer worker respectively, and both need to be initialised.\ne.g. to benchmark LazyMemmapStorage, run the following commands using either two separate shells or multiprocessing.\n    - python3 benchmark_sample_latency_over_rpc.py --rank=0 --storage=LazyMemmapStorage\n    - python3 benchmark_sample_latency_over_rpc.py --rank=1 --storage=LazyMemmapStorage\nThis code is based on examples/distributed/distributed_replay_buffer.py.\n\"\"\"\nimport argparse\nimport os\nimport pickle\nimport sys\nimport time\nimport timeit\nfrom datetime import datetime\n\nimport torch\nimport torch.distributed.rpc as rpc\nfrom tensordict import TensorDict\nfrom torchrl.data.replay_buffers import RemoteTensorDictReplayBuffer\nfrom torchrl.data.replay_buffers.samplers import RandomSampler\nfrom torchrl.data.replay_buffers.storages import (\n    LazyMemmapStorage,\n    LazyTensorStorage,\n    ListStorage,\n)\nfrom torchrl.data.replay_buffers.writers import RoundRobinWriter\n\nRETRY_LIMIT = 2\nRETRY_DELAY_SECS = 3\nREPLAY_BUFFER_NODE = \"ReplayBuffer\"\nTRAINER_NODE = \"Trainer\"\nTENSOR_SIZE = 3 * 86 * 86\nBUFFER_SIZE = 1001\nBATCH_SIZE = 256\nREPEATS = 1000\n\nstorage_options = {\n    \"LazyMemmapStorage\": LazyMemmapStorage,\n    \"LazyTensorStorage\": LazyTensorStorage,\n    \"ListStorage\": ListStorage,\n}\n\nstorage_arg_options = {\n    \"LazyMemmapStorage\": {\"scratch_dir\": \"/tmp/\", \"device\": torch.device(\"cpu\")},\n    \"LazyTensorStorage\": {},\n    \"ListStorage\": {},\n}\nparser = argparse.ArgumentParser(\n    description=\"RPC Replay Buffer Example\",\n    formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n)\n\nparser.add_argument(\n    \"--rank\",\n    type=int,\n    default=-1,\n    help=\"Node Rank [0 = Replay Buffer, 1 = Dummy Trainer, 2+ = Dummy Data Collector]\",\n)\n\nparser.add_argument(\n    \"--storage\",\n    type=str,", "metadata": {"task_id": "pytorch_rl/9", "ground_truth": "    default=\"LazyMemmapStorage\",", "fpath_tuple": ["pytorch_rl", "benchmarks", "storage", "benchmark_sample_latency_over_rpc.py"], "context_start_lineno": 0, "line_no": 70}}
{"prompt": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import annotations\n\n\"\"\"Setup for pip package.\"\"\"\nimport os\nimport sys\nfrom setuptools import find_namespace_packages\nfrom setuptools import setup\nfrom setuptools.command.build import build\n\n\ndef _get_version():\n  with open('vizier/__init__.py') as fp:\n    for line in fp:\n      if line.startswith('__version__'):\n        g = {}\n        exec(line, g)  # pylint: disable=exec-used\n        return g['__version__']\n    raise ValueError('`__version__` not defined in `vizier/__init__.py`')\n\n\ndef _strip_comments_from_line(s: str) -> str:\n  \"\"\"Parses a line of a requirements.txt file.\"\"\"\n  requirement, *_ = s.split('#')\n  return requirement.strip()\n\n\ndef _parse_requirements(requirements_txt_path: str) -> list[str]:\n  \"\"\"Returns a list of dependencies for setup() from requirements.txt.\"\"\"\n\n  # Currently a requirements.txt is being used to specify dependencies. In order\n  # to avoid specifying it in two places, we're going to use that file as the\n  # source of truth.\n  with open(requirements_txt_path) as fp:\n    # Parse comments.\n    lines = [_strip_comments_from_line(line) for line in fp.read().splitlines()]\n    # Remove empty lines and direct github repos (not allowed in PyPI setups)\n    return [l for l in lines if (l and 'github.com' not in l)]\n\n\nclass BuildCmd(build):\n  \"\"\"Custom installation script to build the protos.\"\"\"\n\n  def run(self):\n    current_path = os.path.dirname(os.path.realpath(__file__))\n    sys.stdout.write('current_path: {}'.format(current_path))\n    with os.scandir('.') as it:\n      for entry in it:\n        if entry.name.startswith('build_protos.sh'):\n          sys.stdout.write('{}'.format(entry))\n    if os.system('bash build_protos.sh'):\n      raise OSError('Failed to run build_protos.sh')\n    build.run(self)\n\n\n_VERSION = _get_version()\n\nsetup(\n    name='google-vizier',\n    version=_VERSION,\n    url='https://github.com/google/vizier',\n    license='Apache License 2.0',\n    author='Vizier Team',\n    description=(\n        'Open Source Vizier: Distributed service framework for blackbox'\n       'optimization and research.'\n    ),\n    long_description=open('README.md').read(),\n    long_description_content_type='text/markdown',\n    author_email='oss-vizier-dev@google.com',\n    # Contained modules and scripts.\n    packages=find_namespace_packages(\n        include=['vizier*'], exclude=['*_test.py', 'examples']\n    ),\n    install_requires=_parse_requirements('requirements.txt'),\n    extras_require={", "metadata": {"task_id": "google_vizier/64", "ground_truth": "        'jax': _parse_requirements('requirements-jax.txt'),", "fpath_tuple": ["google_vizier", "setup.py"], "context_start_lineno": 0, "line_no": 89}}
