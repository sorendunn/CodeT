{"prompt": ")\n            td_out.set(\"entropy\", entropy.mean().detach())  # for logging\n            td_out.set(\"loss_entropy\", -self.entropy_coef * entropy.mean())\n        if self.critic_coef:\n            loss_critic = self.loss_critic(tensordict)\n            td_out.set(\"loss_critic\", loss_critic.mean())\n        td_out.set(\"ESS\", ess.mean() / batch)\n        return td_out\n\n\nclass KLPENPPOLoss(PPOLoss):\n    \"\"\"KL Penalty PPO loss.\n\n    The KL penalty loss has the following formula:\n        loss = loss - beta * KL(old_policy, new_policy)\n    The \"beta\" parameter is adapted on-the-fly to match a target KL divergence between the new and old policy, thus\n    favouring a certain level of distancing between the two while still preventing them to be too much apart.\n\n    Args:\n        actor (SafeProbabilisticSequential): policy operator.\n        critic (ValueOperator): value operator.\n        advantage_key (str): the input tensordict key where the advantage is expected to be written.\n            default: \"advantage\"\n        dtarg (scalar): target KL divergence.\n        beta (scalar): initial KL divergence multiplier.\n            default: 1.0\n        increment (scalar): how much beta should be incremented if KL > dtarg. Valid range: increment >= 1.0\n            default: 2.0\n        decrement (scalar): how much beta should be decremented if KL < dtarg. Valid range: decrement <= 1.0\n            default: 0.5\n        entropy_bonus (bool): if True, an entropy bonus will be added to the loss to favour exploratory policies.\n        samples_mc_entropy (int): if the distribution retrieved from the policy operator does not have a closed form\n            formula for the entropy, a Monte-Carlo estimate will be used. samples_mc_entropy will control how many\n            samples will be used to compute this estimate.\n            default: 1\n        entropy_coef (scalar): entropy multiplier when computing the total loss.\n            default: 0.01\n        critic_coef (scalar): critic loss multiplier when computing the total loss.\n            default: 1.0\n        gamma (scalar): a discount factor for return computation.\n        loss_critic_type (str): loss function for the value discrepancy. Can be one of \"l1\", \"l2\" or \"smooth_l1\".\n        normalize_advantage (bool): if True, the advantage will be normalized before being used.\n            Defaults to True.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        actor: SafeProbabilisticSequential,\n        critic: SafeModule,\n        advantage_key=\"advantage\",\n        dtarg: float = 0.01,\n        beta: float = 1.0,\n        increment: float = 2,\n        decrement: float = 0.5,\n        samples_mc_kl: int = 1,\n        entropy_bonus: bool = True,\n        samples_mc_entropy: int = 1,\n        entropy_coef: float = 0.01,\n        critic_coef: float = 1.0,\n        gamma: float = 0.99,\n        loss_critic_type: str = \"smooth_l1\",\n        normalize_advantage: bool = True,\n        **kwargs,\n    ):\n        super(KLPENPPOLoss, self).__init__(\n            actor,\n            critic,\n            advantage_key,\n            entropy_bonus=entropy_bonus,\n            samples_mc_entropy=samples_mc_entropy,\n            entropy_coef=entropy_coef,\n            critic_coef=critic_coef,\n            gamma=gamma,\n            loss_critic_type=loss_critic_type,\n            normalize_advantage=normalize_advantage,\n            **kwargs,\n        )\n\n        self.dtarg = dtarg\n        self._beta_init = beta", "completion": "self.beta = beta\n        self.increment = increment\n        self.decrement = decrement\n        self.samples_mc_kl = samples_mc_kl\n\n    def kl_divergence(self, tensordict: TensorDict) -> torch.Tensor:\n        \"\"\"Compute the KL divergence between the old and new policy distributions.\n\n        Args:\n            tensordict (TensorDict): the input tensors.\n\n        Returns:\n            torch.Tensor: the KL divergence.\n\n        \"\"\"\n        old_dist = self.actor(tensordict, mode=\"old\")\n        new_dist = self.actor(tensordict, mode=\"new\")\n\n        kl_div = torch.distributions.kl_divergence(old_dist, new_dist)\n        kl_div = kl_div.mean()\n\n        return kl"}
{"prompt": "            camera_path_files = os.listdir(camera_path_dir)\n            all_path_dict = {}\n            for i in camera_path_files:\n                if i[-4:] == \"json\":\n                    all_path_dict[i[:-5]] = load_from_json(Path(os.path.join(camera_path_dir, i)))\n            self.vis[\"renderingState/all_camera_paths\"].write(all_path_dict)\n            self.vis[\"populate_paths_payload\"].delete()\n\n    def _check_webrtc_offer(self):\n        \"\"\"Check if there is a webrtc offer to respond to.\"\"\"\n        data = self.vis[\"webrtc/offer\"].read()\n        if data:\n            if self.webrtc_thread and self.webrtc_thread.is_alive():\n                # kill the previous thread if the webpage refreshes\n                self.kill_webrtc_signal = True\n                return\n\n            def loop_in_thread(loop):\n                asyncio.set_event_loop(loop)\n                loop.run_until_complete(self.send_webrtc_answer(data))\n\n            loop = asyncio.get_event_loop()\n            self.webrtc_thread = threading.Thread(target=loop_in_thread, args=(loop,))\n            self.webrtc_thread.daemon = True\n            self.webrtc_thread.start()\n            # remove the offer from the state tree\n            self.vis[\"webrtc/offer\"].delete()\n\n    def _update_render_aabb(self, graph):\n        \"\"\"\n        update the render aabb box for the viewer:\n\n        :param graph:\n        :return:\n        \"\"\"\n\n        crop_enabled = self.vis[\"renderingState/crop_enabled\"].read()\n        if crop_enabled!= self.prev_crop_enabled:\n            self.camera_moving = True\n            self.prev_crop_enabled = crop_enabled\n            self.prev_crop_bg_color = None\n            self.prev_crop_scale = None\n            self.prev_crop_center = None\n\n        if crop_enabled:\n            crop_scale = self.vis[\"renderingState/crop_scale\"].read()\n            crop_center = self.vis[\"renderingState/crop_center\"].read()\n            crop_bg_color = self.vis[\"renderingState/crop_bg_color\"].read()\n\n            if crop_bg_color!= self.prev_crop_bg_color:\n                self.camera_moving = True\n                self.prev_crop_bg_color = crop_bg_color\n\n            if crop_scale!= self.prev_crop_scale or crop_center!= self.prev_crop_center:\n                self.camera_moving = True\n                self.prev_crop_scale = crop_scale\n                self.prev_crop_center = crop_center\n\n                crop_scale = torch.tensor(crop_scale)\n                crop_center = torch.tensor(crop_center)\n\n                box_min = crop_center - crop_scale / 2.0\n                box_max = crop_center + crop_scale / 2.0\n\n                if isinstance(graph.render_aabb, SceneBox):\n                    graph.render_aabb.aabb[0] = box_min\n                    graph.render_aabb.aabb[1] = box_max\n                else:\n                    graph.render_aabb = SceneBox(aabb=torch.stack([box_min, box_max], dim=0))\n\n                # maybe should update only if true change?\n                json_ = graph.render_aabb.to_json()\n                self.vis[\"sceneState/sceneBox\"].write(json_)\n        else:\n            graph.render_aabb = None\n\n    def update_scene(self, trainer, step: int, graph: Model, num_rays_per_batch: int) -> None:\n        \"\"\"updates the scene based on the graph weights\n\n        Args:\n            step: iteration step of training\n            graph: the current checkpoint of the model\n        \"\"\"\n        has_temporal_distortion = getattr(graph, \"temporal_distortion\", None) is not None", "completion": "            self.vis[\"renderingState/has_temporal_distortion\"].write(has_temporal_distortion)"}
{"prompt": "\"  # ensure determinism for the device-dependent torch.Generator\n        components = self.get_dummy_components()\n        sd_pipe = StableDiffusionImg2ImgPipeline(**components)\n        sd_pipe = sd_pipe.to(device)\n        sd_pipe.set_progress_bar_config(disable=None)\n\n        inputs = self.get_dummy_inputs(device)\n        negative_prompt = \"french fries\"\n        output = sd_pipe(**inputs, negative_prompt=negative_prompt)\n        image = output.images\n        image_slice = image[0, -3:, -3:, -1]\n\n        assert image.shape == (1, 32, 32, 3)\n        expected_slice = np.array([0.4065, 0.3783, 0.4050, 0.5266, 0.4781, 0.4252, 0.4203, 0.4692, 0.4365])\n\n        assert np.abs(image_slice.flatten() - expected_slice).max() < 1e-3\n\n    def test_stable_diffusion_img2img_multiple_init_images(self):\n        device = \"cpu\"  # ensure determinism for the device-dependent torch.Generator\n        components = self.get_dummy_components()\n        sd_pipe = StableDiffusionImg2ImgPipeline(**components)\n        sd_pipe = sd_pipe.to(device)\n        sd_pipe.set_progress_bar_config(disable=None)\n\n        inputs = self.get_dummy_inputs(device)\n        inputs[\"prompt\"] = [inputs[\"prompt\"]] * 2\n        inputs[\"image\"] = inputs[\"image\"].repeat(2, 1, 1, 1)\n        image = sd_pipe(**inputs).images\n        image_slice = image[-1, -3:, -3:, -1]\n\n        assert image.shape == (2, 32, 32, 3)\n        expected_slice = np.array([0.5144, 0.4447, 0.4735, 0.6676, 0.5526, 0.5454, 0.645, 0.5149, 0.4689])\n\n        assert np.abs(image_slice.flatten() - expected_slice).max() < 1e-3\n\n    def test_stable_diffusion_img2img_k_lms(self):\n        device = \"cpu\"  # ensure determinism for the device-dependent torch.Generator\n        components = self.get_dummy_components()\n        components[\"scheduler\"] = LMSDiscreteScheduler(\n            beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\"\n        )\n        sd_pipe = StableDiffusionImg2ImgPipeline(**components)\n        sd_pipe = sd_pipe.to(device)\n        sd_pipe.set_progress_bar_config(disable=None)\n\n        inputs = self.get_dummy_inputs(device)\n        image = sd_pipe(**inputs).images\n        image_slice = image[0, -3:, -3:, -1]\n\n        assert image.shape == (1, 32, 32, 3)\n        expected_slice = np.array([0.4367, 0.4986, 0.4372, 0.6706, 0.5665, 0.444, 0.5864, 0.6019, 0.5203])\n\n        assert np.abs(image_slice.flatten() - expected_slice).max() < 1e-3\n\n    def test_stable_diffusion_img2img_num_images_per_prompt(self):\n        device = \"cpu\"  # ensure determinism for the device-dependent torch.Generator\n        components = self.get_dummy_components()\n        sd_pipe = StableDiffusionImg2ImgPipeline(**components)\n        sd_pipe = sd_pipe.to(device)\n        sd_pipe.set_progress_bar_config(disable=None)\n\n        # test num_images_per_prompt=1 (default)", "completion": "inputs[\"num_images_per_prompt\"] = 1\n        image = sd_pipe(**inputs).images\n        image_slice = image[0, -3:, -3:, -1]\n\n        assert image.shape == (1, 32, 32, 3)\n        expected_slice = np.array([0.4065, 0.3783, 0.4050, 0.5266, 0.4781, 0.4252, 0.4203, 0.4692, 0.4365])\n\n        assert np.abs(image_slice.flatten() - expected_slice).max() < 1e-3\n\n        # test num_images_per_prompt=2\n        inputs[\"num"}
{"prompt": "import time\nimport signal\nimport pytest\nimport torch\nimport numpy as np\n\nfrom..base_env_manager import BaseEnvManager, EnvState\n\n\n@pytest.mark.unittest\nclass TestBaseEnvManager:\n\n    def test_naive(self, setup_base_manager_cfg):\n        env_fn = setup_base_manager_cfg.pop('env_fn')\n        env_manager = BaseEnvManager(env_fn, setup_base_manager_cfg)\n        env_manager.seed([314 for _ in range(env_manager.env_num)])\n        assert env_manager._closed\n        obs = env_manager.launch(reset_param={i: {'stat':'stat_test'} for i in range(env_manager.env_num)})\n        assert all([env_manager._env_states[env_id] == EnvState.RUN for env_id in range(env_manager.env_num)])\n        # Test basic\n        name = env_manager._name\n        assert len(name) == env_manager.env_num\n        assert all([isinstance(n, str) for n in name])\n        name = env_manager.name\n        assert len(name) == env_manager.env_num\n        assert all([isinstance(n, str) for n in name])\n        assert env_manager._max_retry == 5\n        assert env_manager._reset_timeout == 10\n        assert all([s == 314 for s in env_manager._seed])\n        assert all([s =='stat_test'] for s in env_manager._stat)\n        # Test arribute\n        with pytest.raises(AttributeError):\n            _ = env_manager.xxx\n        with pytest.raises(RuntimeError):\n            env_manager.user_defined()\n        # Test step\n        count = 1\n        start_time = time.time()\n        while not env_manager.done:\n            env_id = env_manager.ready_obs.keys()\n            action = {i: np.random.randn(4) for i in env_id}", "completion": "            obs, reward, done, info = env_manager.step(action)\n            assert isinstance(obs, dict)\n            assert isinstance(reward, dict)\n            assert isinstance(done, dict)\n            assert isinstance(info, dict)\n            assert all([isinstance(o, np.ndarray) for o in obs.values()])\n            assert all([isinstance(r, float) for r in reward.values()])\n            assert all([isinstance(d, bool) for d in done.values()])\n            assert all([isinstance(i, dict) for i in info.values()])\n            count += 1\n        end_time = time.time()\n        assert env_manager.num_envs == env_manager.env_num\n        assert count == env_manager.episode_length\n        assert end_time - start"}
{"prompt": "rics_str = \"\"\n        for batch, outputs in zip(val_data_loader, val_outputs_loader):\n            val_losses_and_metrics_current_batch = self.val_step(\n                state, batch, outputs, fun, rng, val_dataset_size, metrics,\n            )\n            val_losses_and_metrics_epoch_all_steps.append(\n                val_losses_and_metrics_current_batch\n            )\n        # compute validation losses and metrics for the current epoch\n        val_losses_and_metrics_current_epoch = self.val_epoch_end(\n            val_losses_and_metrics_epoch_all_steps, state\n        )\n        # logging\n        if verbose:\n            val_epoch_metrics_str = \" | \".join(\n                [\n                    f\"{m}: {round(float(v), 5)}\"\n                    for m, v in val_losses_and_metrics_current_epoch.items()\n                ]\n            )\n        return val_losses_and_metrics_current_epoch, val_epoch_metrics_str\n\n    def val_step(\n        self,\n        state: CalibState,\n        batch: Batch,\n        outputs: Array,\n        fun: Callable,\n        rng: PRNGKeyArray,\n        n_data: int,\n        metrics: Optional[\n            Tuple[Callable[[jnp.ndarray, jnp.ndarray, Array], Array],...]\n        ] = None,\n    ) -> Dict[str, jnp.ndarray]:\n        val_loss, aux = self.val_loss_step(state, batch, outputs, fun, rng, n_data)\n        val_metrics = self.val_metrics_step(aux, batch, metrics)\n        return {\"val_loss\": val_loss, **val_metrics}\n\n    @abc.abstractmethod\n    def val_loss_step(\n        self,\n        state: CalibState,\n        batch: Batch,\n        outputs: Array,\n        fun: Callable,\n        rng: PRNGKeyArray,\n        n_data: int,\n    ) -> Tuple[jnp.ndarray, Dict[str, jnp.ndarray]]:\n        pass\n\n    def val_metrics_step(\n        self,\n        aux: Dict[str, jnp.ndarray],\n        batch: Batch,\n        metrics: Optional[\n            Tuple[Callable[[jnp.ndarray, jnp.ndarray, Array], Array],...]\n        ] = None,\n    ) -> Dict[str, jnp.ndarray]:\n        if metrics is not None:\n            val_metrics = self.compute_metrics(\n                self.predict_fn(aux[\"outputs\"]),\n                self.uncertainty_fn(aux[\"outputs\"]),\n                batch[1],\n                metrics,\n            )\n            return {f\"val_{m}\": v for m, v in val_metrics.items()}\n        else:\n            return {}\n\n    def training_epoch_end(\n        self, training_losses_and_metrics_current_epoch: List[Dict[str, jnp.ndarray]]\n    ) -> Dict[str, float]:\n        return self._get_mean_losses_and_metrics(\n            training_losses_and_metrics_current_epoch\n        )\n\n    def val_epoch_end(\n        self,\n        val_losses_and_metrics_current_epoch: List[Dict[str, jnp.ndarray]],\n        state: CalibState,\n    ) -> Dict[str, float]:\n        val_losses_and_metrics_current_epoch = self._get_mean_losses_and_metrics(\n            val_losses_and_metrics_current_epoch\n        )\n        # early stopping\n        improved = self.early_stopping_update(val_losses_and_metrics_current_epoch)\n        if improved and self.save_checkpoint_dir:", "completion": "```python\n            self.save_checkpoint(state)\n```"}
{"prompt": " key = random.split(rng)\n            epistemic_variances = self.epistemic_variance(\n                inputs_loader=inputs_loader,\n                n_posterior_samples=n_posterior_samples,\n                rng=key,\n                distribute=distribute,\n            )\n        return aleatoric_variances + epistemic_variances\n\n    def std(\n        self,\n        inputs_loader: InputsLoader,\n        n_posterior_samples: int = 30,\n        variances: Optional[jnp.ndarray] = None,\n        rng: Optional[PRNGKeyArray] = None,\n        distribute: bool = True,\n    ) -> jnp.ndarray:\n        r\"\"\"\n        Estimate the predictive standard deviation of the target variable, that is\n\n       .. math::\n            \\text{Var}_{Y|x, D}[Y],\n\n        where:\n         - :math:`x` is an observed input variable;\n         - :math:`Y` is a random target variable;\n         - :math:`\\mathcal{D}` is the observed training data set.\n\n        Parameters\n        ----------\n        inputs_loader : InputsLoader\n            A loader of input data points.\n        n_posterior_samples : int\n            Number of samples to draw from the posterior distribution for each input.\n        variances: Optional[jnp.ndarray]\n            An estimate of the predictive variance.\n        rng : Optional[PRNGKeyArray]\n            A random number generator. If not passed, this will be taken from the attributes of this class.\n        distribute: bool\n            Whether to distribute computation over multiple devices, if available.\n\n        Returns\n        -------\n        jnp.ndarray\n            An estimate of the predictive standard deviation for each input.\n        \"\"\"\n        if variances is None:\n            variances = self.variance(\n                inputs_loader=inputs_loader,\n                n_posterior_samples=n_posterior_samples,\n                rng=rng,\n                distribute=distribute,\n            )\n        return jnp.sqrt(variances)\n\n    @staticmethod\n    def _unshard_ensemble_arrays(arr: Array) -> Array:\n        arr = arr.swapaxes(1, 2)\n        arr = arr.reshape((arr.shape[0] * arr.shape[1],) + arr.shape[2:])\n        return arr.swapaxes(0, 1)\n\n    def _loop_fun_through_inputs_loader(\n        self,\n        fun: Callable,\n        inputs_loader: InputsLoader,\n        n_posterior_samples: int,\n        rng: PRNGKeyArray,\n        distribute: bool = True,\n        **kwargs\n    ) -> Array:\n        if distribute and jax.local_device_count() <= 1:\n            distribute = False\n\n        def fun2(_inputs):\n            return fun(_inputs, n_posterior_samples, rng, **kwargs)\n\n        if distribute:\n            inputs_loader = DeviceDimensionAugmentedInputsLoader(inputs_loader)\n            fun2 = pmap(fun2)\n            return jnp.concatenate(\n                [\n                    self.likelihood._unshard_array(fun2(inputs))\n                    for inputs in inputs_loader\n                ],\n                0,\n            )\n        fun2 = jit(fun2)\n        return jnp.concatenate([fun2(inputs) for inputs in inputs_loader], 0)\n\n    def _loop_fun_through_data_loader(\n        self,\n        fun: Callable,\n        data_loader: DataLoader,\n        n_posterior_samples: int,\n        rng: PRNGKeyArray,\n        distribute: bool = True,\n        **kwargs\n    ) -> Array:\n        if distribute and jax.local_device_count() <= 1:\n            distribute = False\n\n        def fun2(_batch):\n            return fun(_batch, n_posterior_samples, rng, **kwargs)\n\n        if distribute:", "completion": "            data_loader = DeviceDimensionAugmentedDataLoader(data_loader)\n            fun2 = pmap(fun2)\n            return jnp.concatenate(\n                [\n                    self.likelihood._unshard_array(fun2(batch))\n                    for batch in data_loader\n                ],\n                0,\n            )\n        fun2 = jit(fun2)\n        return jnp.concatenate([fun2(batch) for batch in data_loader], 0)"}
{"prompt": "from __future__ import annotations\n\nimport logging\nimport os\nimport pathlib\nfrom typing import List, Optional\n\nimport numpy as np\nfrom jax._src.prng import PRNGKeyArray\n\nfrom fortuna.data.loader import DataLoader\nfrom fortuna.prob_model.fit_config import FitConfig\nfrom fortuna.prob_model.joint.base import Joint\nfrom fortuna.prob_model.joint.state import JointState\nfrom fortuna.prob_model.posterior.base import Posterior\nfrom fortuna.prob_model.posterior.deep_ensemble import DEEP_ENSEMBLE_NAME\nfrom fortuna.prob_model.posterior.deep_ensemble.deep_ensemble_approximator import \\\n    DeepEnsemblePosteriorApproximator\nfrom fortuna.prob_model.posterior.deep_ensemble.deep_ensemble_repositories import \\\n    DeepEnsemblePosteriorStateRepository\nfrom fortuna.prob_model.posterior.map.map_posterior import MAPState\nfrom fortuna.prob_model.posterior.map.map_trainer import (\n    JittedMAPTrainer, MAPTrainer, MultiDeviceMAPTrainer)\nfrom fortuna.typing import Path, Status\nfrom fortuna.utils.device import select_trainer_given_devices\n\nlogger = logging.getLogger(__name__)\n\n\nclass DeepEnsemblePosterior(Posterior):\n    def __init__(\n        self, joint: Joint, posterior_approximator: DeepEnsemblePosteriorApproximator,\n    ):\n        \"\"\"\n        Deep ensemble approximate posterior class.\n\n        Parameters\n        ----------\n        joint: Joint\n            Joint distribution.\n        posterior_approximator: DeepEnsemble\n            Deep ensemble posterior approximator.\n        \"\"\"\n        super().__init__(joint=joint, posterior_approximator=posterior_approximator)\n\n    def __str__(self):\n        return DEEP_ENSEMBLE_NAME\n\n    def fit(\n        self,\n        train_data_loader: DataLoader,\n        val_data_loader: Optional[DataLoader] = None,\n        fit_config: FitConfig = FitConfig(),\n        **kwargs,\n    ) -> List[Status]:\n        if (\n            fit_config.checkpointer.dump_state is True\n            and not fit_config.checkpointer.save_checkpoint_dir\n        ):\n            raise ValueError(\n                \"`save_checkpoint_dir` must be passed when `dump_state` is set to True.\"\n            )\n\n        trainer_cls = select_trainer_given_devices(\n            devices=fit_config.processor.devices,\n            BaseTrainer=MAPTrainer,\n            JittedTrainer=JittedMAPTrainer,\n            MultiDeviceTrainer=MultiDeviceMAPTrainer,\n            disable_jit=fit_config.processor.disable_jit,\n        )\n\n        def _fit(i):", "completion": "    def _fit(i):\n        # Set random seed for reproducibility\n        np.random.seed(fit_config.random_seed + i)\n\n        # Initialize joint state\n        joint_state = JointState.from_joint(self.joint)\n\n        # Initialize posterior state\n        posterior_state = DeepEnsemblePosteriorStateRepository(\n            joint_state=joint_state,\n            posterior_approximator=self.posterior_approximator,\n            fit_config=fit_config,\n            random_seed=fit_config.random_seed + i,\n        )\n\n        # Load data\n        train_data = train_data_loader.load_data()\n\n        # Fit the model\n        trainer = trainer_cls(\n            joint=self.joint,\n            posterior_state=posterior_state,\n            train_data=train_data,\n            val_data"}
{"prompt": "\n    VanillaDataManager,\n    VanillaDataManagerConfig,\n)\nfrom nerfstudio.engine.callbacks import TrainingCallback, TrainingCallbackAttributes\nfrom nerfstudio.models.base_model import Model, ModelConfig\nfrom nerfstudio.utils import profiler\n\n\ndef module_wrapper(ddp_or_model: Union[DDP, Model]) -> Model:\n    \"\"\"\n    If DDP, then return the.module. Otherwise, return the model.\n    \"\"\"\n    if isinstance(ddp_or_model, DDP):\n        return cast(Model, ddp_or_model.module)\n    return ddp_or_model\n\n\nclass Pipeline(nn.Module):\n    \"\"\"The intent of this class is to provide a higher level interface for the Model\n    that will be easy to use for our Trainer class.\n\n    This class will contain high level functions for the model like getting the loss\n    dictionaries and visualization code. It should have ways to get the next iterations\n    training loss, evaluation loss, and generate whole images for visualization. Each model\n    class should be 1:1 with a pipeline that can act as a standardized interface and hide\n    differences in how each model takes in and outputs data.\n\n    This class's function is to hide the data manager and model classes from the trainer,\n    worrying about:\n    1) Fetching data with the data manager\n    2) Feeding the model the data and fetching the loss\n    Hopefully this provides a higher level interface for the trainer to use, and\n    simplifying the model classes, which each may have different forward() methods\n    and so on.\n\n    Args:\n        config: configuration to instantiate pipeline\n        device: location to place model and data\n        test_mode:\n            'train': loads train/eval datasets into memory\n            'test': loads train/test dataset into memory\n            'inference': does not load any dataset into memory\n        world_size: total number of machines available\n        local_rank: rank of current machine\n\n    Attributes:\n        datamanager: The data manager that will be used\n        model: The model that will be used\n    \"\"\"\n\n    # pylint: disable=abstract-method\n\n    datamanager: DataManager\n    _model: Model\n\n    @property\n    def model(self):\n        \"\"\"Returns the unwrapped model if in ddp\"\"\"\n        return module_wrapper(self._model)\n\n    @property\n    def device(self):\n        \"\"\"Returns the device that the model is on.\"\"\"\n        return self.model.device\n\n    @profiler.time_function\n    def get_train_loss_dict(self, step: int):\n        \"\"\"This function gets your training loss dict. This will be responsible for\n        getting the next batch of data from the DataManager and interfacing with the\n        Model class, feeding the data to the model's forward function.\n\n        Args:\n            step: current iteration step to update sampler if using DDP (distributed)\n        \"\"\"\n        if self.world_size > 1 and step:\n            assert self.datamanager.train_sampler is not None\n            self.datamanager.train_sampler.set_epoch(step)\n        ray_bundle, batch = self.datamanager.next_train(step)\n        model_outputs = self.model(ray_bundle, batch)\n        metrics_dict = self.model.get_metrics_dict(model_outputs, batch)\n        loss_dict = self.model.get_loss_dict(model_outputs, batch, metrics_dict)\n\n        return model_outputs, loss_dict, metrics_dict\n\n    @profiler.time_function\n    def get_eval_loss_dict(self, step: int):\n        \"\"\"This function gets your evaluation loss dict. It needs to get the data\n        from the DataManager and feed it to the model's forward function\n\n        Args:\n            step: current iteration step\n        \"\"\"\n        self.eval()\n        if self.world_size > 1:\n            assert self.datamanager.eval_sampler is not None\n            self.datamanager.eval_sampler.set_epoch(step)\n        ray_bundle, batch = self.datamanager.next_eval(step)\n        model_outputs = self.model(ray_bundle, batch)", "completion": "metrics_dict = self.model.get_metrics_dict(model_outputs, batch)\n        loss_dict = self.model.get_loss_dict(model_outputs, batch, metrics_dict)\n\n        return model_outputs, loss_dict, metrics_dict"}
{"prompt": "\n\nfrom..base_env_manager import EnvState\nfrom..subprocess_env_manager import AsyncSubprocessEnvManager, SyncSubprocessEnvManager\n\n\nclass TestSubprocessEnvManager:\n\n    @pytest.mark.unittest\n    def test_naive(self, setup_async_manager_cfg, setup_model_type):\n        env_fn = setup_async_manager_cfg.pop('env_fn')\n        env_manager = AsyncSubprocessEnvManager(env_fn, setup_async_manager_cfg)\n        model = setup_model_type()\n\n        env_manager.seed([314 for _ in range(env_manager.env_num)])\n        env_manager.launch(reset_param={i: {'stat':'stat_test'} for i in range(env_manager.env_num)})\n        assert all([s == 314 for s in env_manager._seed])\n        assert all([s =='stat_test'] for s in env_manager._stat)\n        # Test basic\n        name = env_manager._name\n        for i in range(env_manager.env_num):\n            assert name[i] == 'name{}'.format(i)\n        assert len(name) == env_manager.env_num\n        assert all([isinstance(n, str) for n in name])\n        name = env_manager.name\n        assert len(name) == env_manager.env_num\n        assert all([isinstance(n, str) for n in name])\n        assert env_manager._max_retry == 5\n        assert env_manager._reset_timeout == 10\n        # Test arribute\n        with pytest.raises(AttributeError):\n            data = env_manager.xxx\n        env_manager._env_ref.user_defined()\n        with pytest.raises(RuntimeError):\n            env_manager.user_defined()\n        # Test step\n        env_count = [0 for _ in range(env_manager.env_num)]\n        data_count = 0\n        start_time = time.time()\n        while not env_manager.done:\n            obs = env_manager.ready_obs\n            print('obs', obs.keys(), env_manager._env_states)\n            action = model.forward(obs)\n            assert 1 <= len(action) <= len(obs)\n            print('act', action.keys())\n            timestep = env_manager.step(action)\n            data_count += len(timestep)\n            assert len(timestep) >= 1\n            print('timestep', timestep.keys(), timestep, len(timestep))\n            for k, t in timestep.items():\n                if t.done:\n                    print('env{} finish episode{}'.format(k, env_count[k]))\n                    env_count[k] += 1\n        assert all([c == setup_async_manager_cfg.episode_num for c in env_count])\n        assert data_count == sum(env_manager._data_count)\n        assert all([env_manager._env_states[env_id] == EnvState.DONE for env_id in range(env_manager.env_num)])\n        end_time = time.time()\n        print('total step time: {}'.format(end_time - start_time))\n\n        # Test close\n        env_manager.close()\n        assert env_manager._closed\n        with pytest.raises(AssertionError):\n            env_manager.reset([])\n        with pytest.raises(AssertionError):\n            env_manager.step([])\n\n    @pytest.mark.unittest\n    def test_error(self, setup_sync_manager_cfg):\n        env_fn = setup_sync_manager_cfg.pop('env_fn')\n        env_manager = SyncSubprocessEnvManager(env_fn, setup_sync_manager_cfg)\n        # Test reset error\n        with pytest.raises(AssertionError):\n            env_manager.reset(reset_param={i: {'stat':'stat_test'} for i in range(env_manager.env_num)})\n        with pytest.raises(RuntimeError):", "completion": "            env_manager.launch(reset_param={i: {'stat':'stat_test'} for i in range(env_manager.env_num)})"}
{"prompt": " banner_messages = viewer_utils.setup_viewer(\n                config.viewer, log_filename=viewer_log_path, datapath=config.pipeline.datamanager.dataparser.data\n            )\n        self._check_viewer_warnings()\n        # set up writers/profilers if enabled\n        writer_log_path = self.base_dir / config.logging.relative_log_dir\n        writer.setup_event_writer(config.is_wandb_enabled(), config.is_tensorboard_enabled(), log_dir=writer_log_path)\n        writer.setup_local_writer(config.logging, max_iter=config.max_num_iterations, banner_messages=banner_messages)\n        writer.put_config(name=\"config\", config_dict=dataclasses.asdict(config), step=0)\n        profiler.setup_profiler(config.logging)\n\n    def setup(self, test_mode: Literal[\"test\", \"val\", \"inference\"] = \"val\"):\n        \"\"\"Setup the Trainer by calling other setup functions.\n\n        Args:\n            test_mode:\n                'val': loads train/val datasets into memory\n                'test': loads train/test datasets into memory\n                'inference': does not load any dataset into memory\n        \"\"\"\n        self.pipeline = self.config.pipeline.setup(\n            device=self.device, test_mode=test_mode, world_size=self.world_size, local_rank=self.local_rank\n        )\n        self.optimizers = self.setup_optimizers()\n\n        self._load_checkpoint()\n\n        self.callbacks = self.pipeline.get_training_callbacks(\n            TrainingCallbackAttributes(\n                optimizers=self.optimizers,  # type: ignore\n                grad_scaler=self.grad_scaler,  # type: ignore\n                pipeline=self.pipeline,  # type: ignore\n            )\n        )\n\n    def setup_optimizers(self) -> Optimizers:\n        \"\"\"Helper to set up the optimizers\n\n        Returns:\n            The optimizers object given the trainer config.\n        \"\"\"\n        optimizer_config = self.config.optimizers.copy()\n        camera_optimizer_config = self.config.pipeline.datamanager.camera_optimizer\n        param_groups = self.pipeline.get_param_groups()\n        if camera_optimizer_config.mode!= \"off\":\n            assert camera_optimizer_config.param_group not in optimizer_config\n            optimizer_config[camera_optimizer_config.param_group] = {\n                \"optimizer\": camera_optimizer_config.optimizer,\n                \"scheduler\": camera_optimizer_config.scheduler,\n            }\n        return Optimizers(optimizer_config, param_groups)\n\n    def train(self) -> None:\n        \"\"\"Train the model.\"\"\"\n        assert self.pipeline.datamanager.train_dataset is not None, \"Missing DatsetInputs\"\n\n        self.pipeline.datamanager.train_dataparser_outputs.save_dataparser_transform(\n            self.base_dir / \"dataparser_transforms.json\"\n        )\n\n        self._init_viewer_state()\n        with TimeWriter(writer, EventName.TOTAL_TRAIN_TIME):\n            num_iterations = self.config.max_num_iterations\n            step = 0\n            for step in range(self._start_step, self._start_step + num_iterations):\n                with TimeWriter(writer, EventName.ITER_TRAIN_TIME, step=step) as train_t:\n\n                    self.pipeline.train()\n\n                    # training callbacks before the training iteration\n                    for callback in self.callbacks:\n                        callback.run_callback_at_location(\n                            step, location=TrainingCallbackLocation.BEFORE_TRAIN_ITERATION\n                        )\n\n                    # time the forward pass\n                    loss, loss_dict, metrics_dict = self.train_iteration(step)\n\n                    # training callbacks after the training iteration\n                    for callback in self.callbacks:", "completion": "```\n                        callback.run_callback_at_location(\n                            step, location=TrainingCallbackLocation.AFTER_TRAIN_ITERATION\n                        )\n\n                    # time the backward pass\n                    with TimeWriter(writer, EventName.BACKWARD_TIME, step=step):\n                        self.pipeline.backward(loss)\n\n                    # update the parameters\n                    with TimeWriter(writer, EventName.OPTIMIZER_STEP_TIME, step=step):\n                        self.optimizers.step()\n\n                    # training callbacks after the optimizer step\n                    for callback in self.callbacks:\n                        callback.run_callback_at_location(\n                            step, location=TrainingCallbackLocation.AFTER_OPTIMIZER_STEP\n                        )\n\n                    # log training metrics\n                    self._log_training_metrics(loss, loss_dict, metrics_dict, step)\n\n                    # run"}
{"prompt": "fn.priority_key in td.keys()\n\n        sum([item for _, item in loss.items()]).backward()\n        assert torch.nn.utils.clip_grad.clip_grad_norm_(actor.parameters(), 1.0) > 0.0\n\n        # Check param update effect on targets\n        target_value = loss_fn.target_value_network_params.clone()\n        for p in loss_fn.parameters():\n            p.data += torch.randn_like(p)\n        target_value2 = loss_fn.target_value_network_params.clone()\n        if loss_fn.delay_value:\n            assert_allclose_td(target_value, target_value2)\n        else:\n            assert not (target_value == target_value2).any()\n\n        # check that policy is updated after parameter update\n        parameters = [p.clone() for p in actor.parameters()]\n        for p in loss_fn.parameters():\n            p.data += torch.randn_like(p)\n        assert all((p1!= p2).all() for p1, p2 in zip(parameters, actor.parameters()))\n\n    @pytest.mark.parametrize(\"n\", range(4))\n    @pytest.mark.parametrize(\"delay_value\", (False, True))\n    @pytest.mark.parametrize(\"device\", get_available_devices())\n    @pytest.mark.parametrize(\n        \"action_spec_type\", (\"nd_bounded\", \"one_hot\", \"categorical\")\n    )\n    def test_dqn_batcher(self, n, delay_value, device, action_spec_type, gamma=0.9):\n        torch.manual_seed(self.seed)\n        actor = self._create_mock_actor(\n            action_spec_type=action_spec_type, device=device\n        )\n\n        td = self._create_seq_mock_data_dqn(\n            action_spec_type=action_spec_type, device=device\n        )\n        loss_fn = DQNLoss(\n            actor, gamma=gamma, loss_function=\"l2\", delay_value=delay_value\n        )\n\n        ms = MultiStep(gamma=gamma, n_steps_max=n).to(device)\n        ms_td = ms(td.clone())\n\n        with _check_td_steady(ms_td):\n            loss_ms = loss_fn(ms_td)\n        assert loss_fn.priority_key in ms_td.keys()\n\n        with torch.no_grad():\n            loss = loss_fn(td)\n        if n == 0:\n            assert_allclose_td(td, ms_td.select(*list(td.keys())))\n            _loss = sum([item for _, item in loss.items()])\n            _loss_ms = sum([item for _, item in loss_ms.items()])\n            assert (\n                abs(_loss - _loss_ms) < 1e-3\n            ), f\"found abs(loss-loss_ms) = {abs(loss - loss_ms):4.5f} for n=0\"\n        else:\n            with pytest.raises(AssertionError):\n                assert_allclose_td(loss, loss_ms)\n        sum([item for _, item in loss_ms.items()]).backward()\n        assert torch.nn.utils.clip_grad.clip_grad_norm_(actor.parameters(), 1.0) > 0.0\n\n        # Check param update effect on targets\n        target_value = loss_fn.target_value_network_params.clone()\n        for p in loss_fn.parameters():\n            p.data += torch.randn_like(p)\n        target_value2 = loss_fn.target_value_network_params.clone()\n        if loss_fn.delay_value:\n            assert_allclose_td(target_value, target_value2)\n        else:\n            assert not (target_value == target_value2).any()\n\n        # check that policy is updated after parameter update\n        parameters = [p.clone() for p in actor.parameters()]", "completion": "```python\nfor p in loss_fn.parameters():\n    p.data += torch.randn_like(p)\nassert all((p1!= p2).all() for p1, p2 in zip(parameters, actor.parameters()))\n```"}
{"prompt": ".FloatTensor) -> torch.FloatTensor:\n        \"\"\"\n        Overview:\n            forward computation graph of the mixer network for Q_star in WQMIX.\n        Arguments:\n            - agent_qs (:obj:`torch.FloatTensor`): the independent q_value of each agent\n            - states (:obj:`torch.FloatTensor`): the emdedding vector of global state\n        Returns:\n            - q_tot (:obj:`torch.FloatTensor`): the total mixed q_value\n        Shapes:\n            - agent_qs (:obj:`torch.FloatTensor`): :math:`(T,B, N)`, where T is timestep,\n              B is batch size, A is agent_num, N is obs_shape\n            - states (:obj:`torch.FloatTensor`): :math:`(T, B, M)`, where M is global_obs_shape\n            - q_tot (:obj:`torch.FloatTensor`): :math:`(T, B, )`\n        \"\"\"\n        # in below annotations about the shape of the variables, T is timestep,\n        # B is batch_size A is agent_num, N is obs_shape\uff0c for example,\n        # in 3s5z, we can set T=10, B=32, A=8, N=216\n        bs = agent_qs.shape[:-1]  # (T*B, A)\n        states = states.reshape(-1, self.state_dim)  # T*B, N),\n        agent_qs = agent_qs.reshape(-1, self.agent_num)  # (T, B, A) -> (T*B, A)\n        inputs = torch.cat([states, agent_qs], dim=1)  # (T*B, N) (T*B, A)-> (T*B, N+A)\n        advs = self.net(inputs)  # (T*B, 1)\n        vs = self.V(states)  # (T*B, 1)\n        y = advs + vs\n        q_tot = y.view(*bs)  # (T*B, 1) -> (T, B)\n\n        return q_tot\n\n\n@MODEL_REGISTRY.register('wqmix')\nclass WQMix(nn.Module):\n    \"\"\"\n    Overview:\n        WQMIX network, which is same as Qmix network\n    Interface:\n        __init__, forward, _setup_global_encoder\n    \"\"\"\n\n    def __init__(\n            self,\n            agent_num: int,\n            obs_shape: int,\n            global_obs_shape: int,\n            action_shape: int,\n            hidden_size_list: list,\n            lstm_type: str = 'gru',\n            dueling: bool = False\n    ) -> None:\n        \"\"\"\n        Overview:\n            initialize Qmix network\n        Arguments:\n            - agent_num (:obj:`int`): the number of agent\n            - obs_shape (:obj:`int`): the dimension of each agent's observation state\n            - global_obs_shape (:obj:`int`): the dimension of global observation state\n            - action_shape (:obj:`int`): the dimension of action shape\n            - hidden_size_list (:obj:`list`): the list of hidden size\n            - lstm_type (:obj:`str`): use lstm or gru, default to gru\n            - dueling (:obj:`bool`): use dueling head or not, default to False.\n        \"\"\"\n        super(WQMix, self).__init__()\n        self._act = nn.ReLU()\n        self._q_network = DRQN(obs_shape, action_shape, hidden_size_list, lstm_type=lstm_type, dueling=dueling)\n        self._q_network_star = DRQN(obs_shape, action_shape, hidden_size_list, lstm_type=lstm_type, dueling=dueling)\n        embedding_size = hidden_size_list[-1]", "completion": "self._embedding = nn.Linear(global_obs_shape, embedding_size)"}
{"prompt": " CtxVar(\n                            outputs.contrastive_loss, LIFECYCLE.BATCH)\n                    ctx.y_true = CtxVar(labels, LIFECYCLE.BATCH)\n                    ctx.y_pred = CtxVar(outputs.logits.argmax(dim=-1),\n                                        LIFECYCLE.BATCH)\n\n            elif self.task in {'squad', 'newsqa'}:\n                outputs = ctx.model(\n                    input_ids=token_ids.to(ctx.device),\n                    token_type_ids=token_type_ids.to(ctx.device),\n                    attention_mask=attention_mask.to(ctx.device),\n                    start_positions=start_positions.to(ctx.device),\n                    end_positions=end_positions.to(ctx.device),\n                    contrast_monitor=ctx.contrast_monitor,\n                    in_contrast_prepare=self._in_contrast_prepare,\n                    example_indices=example_indices,\n                )\n                if not self._in_contrast_prepare:\n                    for i, example_idx in enumerate(example_indices):\n                        encoded_input = ctx.get('{}_encoded'.format(\n                            ctx.cur_split))[example_idx.item()]\n                        unique_id = int(encoded_input.unique_id)\n                        start_logits = \\\n                            outputs.logits[0][i].detach().cpu().tolist()\n                        end_logits = \\\n                            outputs.logits[1][i].detach().cpu().tolist()\n                        if ctx.cur_split!= 'train':\n                            if self.task =='squad':\n                                ctx.squad_results.append(\n                                    SquadResult(unique_id, start_logits,\n                                                end_logits))\n                            elif self.task == 'newsqa':\n                                ctx.newsqa_results.append(\n                                    NewsQAResult(unique_id, start_logits,\n                                                 end_logits))\n\n                    ctx.batch_size = CtxVar(len(token_ids), LIFECYCLE.BATCH)\n                    ctx.loss_batch = CtxVar(outputs.loss, LIFECYCLE.BATCH)\n                    if self.use_contrastive_loss:\n                        ctx.regular_loss_batch = CtxVar(\n                            outputs.regular_loss, LIFECYCLE.BATCH)\n                        ctx.contrastive_loss_batch = CtxVar(\n                            outputs.contrastive_loss, LIFECYCLE.BATCH)\n                    ctx.y_true = CtxVar(\n                        torch.cat([start_positions, end_positions]),\n                        LIFECYCLE.BATCH)\n                    ctx.y_pred = CtxVar(\n                        torch.cat(\n                            [out.argmax(dim=-1) for out in outputs.logits]),\n                        LIFECYCLE.BATCH)\n\n            elif self.task in {'cnndm','msqg'}:\n                if ctx.cur_split!= 'test':\n                    outputs = ctx.model(\n                        input_ids=token_ids.to(ctx.device),\n                        token_type_ids=token_type_ids.to(ctx.device),\n                        attention_mask=attention_mask.to(ctx.device),\n                        labels=labels.to(ctx.device),\n                        contrast_monitor=ctx.contrast_monitor,\n                        in_contrast_prepare=self._in_contrast_prepare,\n                        example_indices=example_indices,\n                    )\n                    if not self._in_contrast_prepare:\n                        ctx.batch_size = CtxVar(len(labels), LIFECYCLE.BATCH)\n                        ctx.loss_batch = CtxVar(outputs.loss, LIFECYCLE.BATCH)\n                        if self.use_contrastive_loss:\n                            ctx.regular_loss_batch = CtxVar(\n                                outputs.regular_loss, LIFECYCLE.BATCH)", "completion": "ctx.contrastive_loss_batch = CtxVar(\n                            outputs.contrastive_loss, LIFECYCLE.BATCH)\n                        ctx.y_true = CtxVar(labels, LIFECYCLE.BATCH)\n                        ctx.y_pred = CtxVar(outputs.logits.argmax(dim=-1),\n                                            LIFECYCLE.BATCH)"}
{"prompt": "uncated_ids = self.tokenizer(prompt, padding=\"longest\", return_tensors=\"pt\").input_ids\n\n            if untruncated_ids.shape[-1] >= text_input_ids.shape[-1] and not torch.equal(\n                text_input_ids, untruncated_ids\n            ):\n                removed_text = self.tokenizer.batch_decode(\n                    untruncated_ids[:, self.tokenizer.model_max_length - 1 : -1]\n                )\n                logger.warning(\n                    \"The following part of your input was truncated because CLIP can only handle sequences up to\"\n                    f\" {self.tokenizer.model_max_length} tokens: {removed_text}\"\n                )\n\n            if hasattr(self.text_encoder.config, \"use_attention_mask\") and self.text_encoder.config.use_attention_mask:\n                attention_mask = text_inputs.attention_mask.to(device)\n            else:\n                attention_mask = None\n\n            prompt_embeds = self.text_encoder(\n                text_input_ids.to(device),\n                attention_mask=attention_mask,\n            )\n            prompt_embeds = prompt_embeds[0]\n\n        prompt_embeds = prompt_embeds.to(dtype=self.text_encoder.dtype, device=device)\n\n        bs_embed, seq_len, _ = prompt_embeds.shape\n        # duplicate text embeddings for each generation per prompt, using mps friendly method\n        prompt_embeds = prompt_embeds.repeat(1, num_images_per_prompt, 1)\n        prompt_embeds = prompt_embeds.view(bs_embed * num_images_per_prompt, seq_len, -1)\n\n        # get unconditional embeddings for classifier free guidance\n        if do_classifier_free_guidance and negative_prompt_embeds is None:\n            uncond_tokens: List[str]\n            if negative_prompt is None:\n                uncond_tokens = [\"\"] * batch_size\n            elif type(prompt) is not type(negative_prompt):\n                raise TypeError(\n                    f\"`negative_prompt` should be the same type to `prompt`, but got {type(negative_prompt)}!=\"\n                    f\" {type(prompt)}.\"\n                )\n            elif isinstance(negative_prompt, str):\n                uncond_tokens = [negative_prompt]\n            elif batch_size!= len(negative_prompt):\n                raise ValueError(\n                    f\"`negative_prompt`: {negative_prompt} has batch size {len(negative_prompt)}, but `prompt`:\"\n                    f\" {prompt} has batch size {batch_size}. Please make sure that passed `negative_prompt` matches\"\n                    \" the batch size of `prompt`.\"\n                )\n            else:\n                uncond_tokens = negative_prompt\n\n            max_length = prompt_embeds.shape[1]\n            uncond_input = self.tokenizer(\n                uncond_tokens,\n                padding=\"max_length\",\n                max_length=max_length,\n                truncation=True,\n                return_tensors=\"pt\",\n            )\n\n            if hasattr(self.text_encoder.config, \"use_attention_mask\") and self.text_encoder.config.use_attention_mask:\n                attention_mask = uncond_input.attention_mask.to(device)\n            else:\n                attention_mask = None\n\n            negative_prompt_embeds = self.text_encoder(\n                uncond_input.input_ids.to(device),\n                attention_mask=attention_mask,\n            )\n            negative_prompt_embeds = negative_prompt_embeds[0]\n\n        if do_classifier_free_guidance:\n            # duplicate unconditional embeddings for each generation per prompt, using mps friendly method\n            seq_len = negative_prompt_embeds.shape[1]", "completion": "negative_prompt_embeds = negative_prompt_embeds.repeat(1, num_images_per_prompt, 1)\n            negative_prompt_embeds = negative_prompt_embeds.view(bs_embed * num_images_per_prompt, seq_len, -1)"}
{"prompt": "DiscreteTensorSpec\n            )\n            action_spec = action_spec_cls(*batch_size, 7)\n        if reward_spec is None:\n            reward_spec = UnboundedContinuousTensorSpec(shape=(1,))\n\n        if input_spec is None:\n            cls._out_key = \"observation_orig\"\n            input_spec = CompositeSpec(\n                **{\n                    cls._out_key: observation_spec[\"observation\"],\n                    \"action\": action_spec,\n                }\n            )\n        cls._reward_spec = reward_spec\n        cls._observation_spec = observation_spec\n        cls._input_spec = input_spec\n        cls.from_pixels = from_pixels\n        cls.categorical_action_encoding = categorical_action_encoding\n        return super().__new__(*args, **kwargs)\n\n    def _get_in_obs(self, obs):\n        return obs\n\n    def _get_out_obs(self, obs):\n        return obs\n\n    def _reset(self, tensordict: TensorDictBase = None) -> TensorDictBase:\n        self.counter += 1\n        state = torch.zeros(self.size) + self.counter\n        if tensordict is None:\n            tensordict = TensorDict({}, self.batch_size, device=self.device)\n        tensordict = tensordict.select().set(self.out_key, self._get_out_obs(state))\n        tensordict = tensordict.set(self._out_key, self._get_out_obs(state))\n        tensordict.set(\"done\", torch.zeros(*tensordict.shape, 1, dtype=torch.bool))\n        return tensordict\n\n    def _step(\n        self,\n        tensordict: TensorDictBase,\n    ) -> TensorDictBase:\n        tensordict = tensordict.to(self.device)\n        a = tensordict.get(\"action\")\n\n        if not self.categorical_action_encoding:\n            assert (a.sum(-1) == 1).all()\n\n        obs = self._get_in_obs(tensordict.get(self._out_key)) + a / self.maxstep\n        tensordict = tensordict.select()  # empty tensordict\n\n        tensordict.set(self.out_key, self._get_out_obs(obs))\n        tensordict.set(self._out_key, self._get_out_obs(obs))\n\n        done = torch.isclose(obs, torch.ones_like(obs) * (self.counter + 1))\n        reward = done.any(-1).unsqueeze(-1)\n        # set done to False\n        done = torch.zeros_like(done).all(-1).unsqueeze(-1)\n        tensordict.set(\"reward\", reward.to(torch.get_default_dtype()))\n        tensordict.set(\"done\", done)\n        return tensordict\n\n\nclass ContinuousActionVecMockEnv(_MockEnv):\n    @classmethod\n    def __new__(\n        cls,\n        *args,\n        observation_spec=None,\n        action_spec=None,\n        input_spec=None,\n        reward_spec=None,\n        from_pixels=False,\n        **kwargs,\n    ):\n        batch_size = kwargs.setdefault(\"batch_size\", torch.Size([]))\n        size = cls.size = 7\n        if observation_spec is None:\n            cls.out_key = \"observation\"\n            observation_spec = CompositeSpec(\n                observation=UnboundedContinuousTensorSpec(\n                    shape=torch.Size([*batch_size, size])\n                ),\n                observation_orig=UnboundedContinuousTensorSpec(\n                    shape=torch.Size([*batch_size, size])\n                ),\n                shape=batch_size,\n            )\n        if action_spec is None:", "completion": "            action_spec = BoundedContinuousTensorSpec(shape=(7,), minimum=-1, maximum=1)"}
{"prompt": "izier as vz\nfrom vizier._src.algorithms.optimizers import eagle_strategy\nfrom vizier._src.algorithms.optimizers import random_vectorized_optimizer as rvo\nfrom vizier._src.algorithms.optimizers import vectorized_base as vb\nfrom vizier._src.algorithms.testing import comparator_runner\nfrom vizier.pyvizier import converters\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\n\n\ndef randomize_array(converter: converters.TrialToArrayConverter) -> np.ndarray:\n  \"\"\"Generate a random array of features to be used as score_fn shift.\"\"\"\n  features_arrays = []\n  for spec in converter.output_specs:\n    if spec.type == converters.NumpyArraySpecType.ONEHOT_EMBEDDING:\n      dim = spec.num_dimensions - spec.num_oovs\n      features_arrays.append(\n          np.eye(spec.num_dimensions)[np.random.randint(0, dim)])\n    elif spec.type == converters.NumpyArraySpecType.CONTINUOUS:\n      features_arrays.append(np.random.uniform(0.4, 0.6, size=(1,)))\n    else:\n      raise ValueError(f'The type {spec.type} is not supported!')\n  return np.hstack(features_arrays)\n\n\ndef create_continuous_problem(\n    n_features: int,\n    problem: Optional[vz.ProblemStatement] = None) -> vz.ProblemStatement:\n  if not problem:\n    problem = vz.ProblemStatement()\n  root = problem.search_space.select_root()\n  for i in range(n_features):\n    root.add_float_param('x%d' % i, -5.0, 5.0)\n  return problem\n\n\ndef create_categorical_problem(\n    n_features: int,\n    categorical_dim: int = 6,\n    problem: Optional[vz.ProblemStatement] = None) -> vz.ProblemStatement:\n  if not problem:\n    problem = vz.ProblemStatement()\n  root = problem.search_space.select_root()\n  for i in range(n_features):\n    root.add_categorical_param(\n        'c%d' % i, feasible_values=[str(i) for i in range(categorical_dim)])\n  return problem\n\n\ndef create_mix_problem(n_features: int,\n                       categorical_dim: int = 8) -> vz.ProblemStatement:\n  problem = create_continuous_problem(n_features // 2)\n  return create_categorical_problem(n_features // 2, categorical_dim, problem)\n\n\n# TODO: Change to bbob functions when they can support batching.\ndef sphere(x: np.ndarray) -> np.ndarray:\n  return -np.sum(np.square(x), axis=-1)\n\n\ndef rastrigin_d10(x: np.ndarray) -> np.ndarray:\n  return 10 * np.sum(\n      np.cos(2 * np.pi * x), axis=-1) - np.sum(\n          np.square(x), axis=-1)\n\n\nclass EagleOptimizerConvegenceTest(parameterized.TestCase):\n  \"\"\"Test optimizing an acquisition functions using vectorized Eagle Strategy.\n  \"\"\"\n\n  @absltest.skip(\"Test takes too long externally.\")\n  @parameterized.product(\n      create_problem_fn=[\n          create_continuous_problem,\n          create_categorical_problem,\n          create_mix_problem,\n      ],\n      n_features=[10, 20],\n      score_fn=[sphere, rastrigin_d10],\n  )\n  def test_converges(self, create_problem_fn, n_features, score_fn):\n    logging.info('Starting a new convergence test (n_features: %s)', n_features)\n    evaluations = 20_000\n    problem = create_problem_fn(n_features)", "completion": "    optimizer = eagle_strategy.EagleStrategy(\n        problem,\n        score_fn=score_fn,\n        num_evaluations=evaluations,\n        randomize_fn=randomize_array,\n        batch_size=100,\n        num_candidates=100,\n        num_elites=10,\n        num_offsprings=100,\n        num_mutations=100,\n        mutation_stddev=0.1,\n        num_optimization_steps=10,\n        num_optimization_steps_per_iteration=1,\n        num_optimization_steps_per_iteration_in_final_stage=2,\n        max_num_iterations=100,\n        max_num_iterations_in_final_stage=100,\n        stop_threshold=0.01,\n        num_restarts=1,\n        num_restarts_in_final_stage=1"}
