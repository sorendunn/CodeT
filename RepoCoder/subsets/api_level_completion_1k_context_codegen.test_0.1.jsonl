{"prompt": ")\n            td_out.set(\"entropy\", entropy.mean().detach())  # for logging\n            td_out.set(\"loss_entropy\", -self.entropy_coef * entropy.mean())\n        if self.critic_coef:\n            loss_critic = self.loss_critic(tensordict)\n            td_out.set(\"loss_critic\", loss_critic.mean())\n        td_out.set(\"ESS\", ess.mean() / batch)\n        return td_out\n\n\nclass KLPENPPOLoss(PPOLoss):\n    \"\"\"KL Penalty PPO loss.\n\n    The KL penalty loss has the following formula:\n        loss = loss - beta * KL(old_policy, new_policy)\n    The \"beta\" parameter is adapted on-the-fly to match a target KL divergence between the new and old policy, thus\n    favouring a certain level of distancing between the two while still preventing them to be too much apart.\n\n    Args:\n        actor (SafeProbabilisticSequential): policy operator.\n        critic (ValueOperator): value operator.\n        advantage_key (str): the input tensordict key where the advantage is expected to be written.\n            default: \"advantage\"\n        dtarg (scalar): target KL divergence.\n        beta (scalar): initial KL divergence multiplier.\n            default: 1.0\n        increment (scalar): how much beta should be incremented if KL > dtarg. Valid range: increment >= 1.0\n            default: 2.0\n        decrement (scalar): how much beta should be decremented if KL < dtarg. Valid range: decrement <= 1.0\n            default: 0.5\n        entropy_bonus (bool): if True, an entropy bonus will be added to the loss to favour exploratory policies.\n        samples_mc_entropy (int): if the distribution retrieved from the policy operator does not have a closed form\n            formula for the entropy, a Monte-Carlo estimate will be used. samples_mc_entropy will control how many\n            samples will be used to compute this estimate.\n            default: 1\n        entropy_coef (scalar): entropy multiplier when computing the total loss.\n            default: 0.01\n        critic_coef (scalar): critic loss multiplier when computing the total loss.\n            default: 1.0\n        gamma (scalar): a discount factor for return computation.\n        loss_critic_type (str): loss function for the value discrepancy. Can be one of \"l1\", \"l2\" or \"smooth_l1\".\n        normalize_advantage (bool): if True, the advantage will be normalized before being used.\n            Defaults to True.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        actor: SafeProbabilisticSequential,\n        critic: SafeModule,\n        advantage_key=\"advantage\",\n        dtarg: float = 0.01,\n        beta: float = 1.0,\n        increment: float = 2,\n        decrement: float = 0.5,\n        samples_mc_kl: int = 1,\n        entropy_bonus: bool = True,\n        samples_mc_entropy: int = 1,\n        entropy_coef: float = 0.01,\n        critic_coef: float = 1.0,\n        gamma: float = 0.99,\n        loss_critic_type: str = \"smooth_l1\",\n        normalize_advantage: bool = True,\n        **kwargs,\n    ):\n        super(KLPENPPOLoss, self).__init__(\n            actor,\n            critic,\n            advantage_key,\n            entropy_bonus=entropy_bonus,\n            samples_mc_entropy=samples_mc_entropy,\n            entropy_coef=entropy_coef,\n            critic_coef=critic_coef,\n            gamma=gamma,\n            loss_critic_type=loss_critic_type,\n            normalize_advantage=normalize_advantage,\n            **kwargs,\n        )\n\n        self.dtarg = dtarg\n        self._beta_init = beta", "metadata": {"task_id": "pytorch_rl/109", "ground_truth": "        self.register_buffer(\"beta\", torch.tensor(beta))", "fpath_tuple": ["pytorch_rl", "torchrl", "objectives", "ppo.py"], "context_start_lineno": 260, "line_no": 341}}
{"prompt": "            camera_path_files = os.listdir(camera_path_dir)\n            all_path_dict = {}\n            for i in camera_path_files:\n                if i[-4:] == \"json\":\n                    all_path_dict[i[:-5]] = load_from_json(Path(os.path.join(camera_path_dir, i)))\n            self.vis[\"renderingState/all_camera_paths\"].write(all_path_dict)\n            self.vis[\"populate_paths_payload\"].delete()\n\n    def _check_webrtc_offer(self):\n        \"\"\"Check if there is a webrtc offer to respond to.\"\"\"\n        data = self.vis[\"webrtc/offer\"].read()\n        if data:\n            if self.webrtc_thread and self.webrtc_thread.is_alive():\n                # kill the previous thread if the webpage refreshes\n                self.kill_webrtc_signal = True\n                return\n\n            def loop_in_thread(loop):\n                asyncio.set_event_loop(loop)\n                loop.run_until_complete(self.send_webrtc_answer(data))\n\n            loop = asyncio.get_event_loop()\n            self.webrtc_thread = threading.Thread(target=loop_in_thread, args=(loop,))\n            self.webrtc_thread.daemon = True\n            self.webrtc_thread.start()\n            # remove the offer from the state tree\n            self.vis[\"webrtc/offer\"].delete()\n\n    def _update_render_aabb(self, graph):\n        \"\"\"\n        update the render aabb box for the viewer:\n\n        :param graph:\n        :return:\n        \"\"\"\n\n        crop_enabled = self.vis[\"renderingState/crop_enabled\"].read()\n        if crop_enabled!= self.prev_crop_enabled:\n            self.camera_moving = True\n            self.prev_crop_enabled = crop_enabled\n            self.prev_crop_bg_color = None\n            self.prev_crop_scale = None\n            self.prev_crop_center = None\n\n        if crop_enabled:\n            crop_scale = self.vis[\"renderingState/crop_scale\"].read()\n            crop_center = self.vis[\"renderingState/crop_center\"].read()\n            crop_bg_color = self.vis[\"renderingState/crop_bg_color\"].read()\n\n            if crop_bg_color!= self.prev_crop_bg_color:\n                self.camera_moving = True\n                self.prev_crop_bg_color = crop_bg_color\n\n            if crop_scale!= self.prev_crop_scale or crop_center!= self.prev_crop_center:\n                self.camera_moving = True\n                self.prev_crop_scale = crop_scale\n                self.prev_crop_center = crop_center\n\n                crop_scale = torch.tensor(crop_scale)\n                crop_center = torch.tensor(crop_center)\n\n                box_min = crop_center - crop_scale / 2.0\n                box_max = crop_center + crop_scale / 2.0\n\n                if isinstance(graph.render_aabb, SceneBox):\n                    graph.render_aabb.aabb[0] = box_min\n                    graph.render_aabb.aabb[1] = box_max\n                else:\n                    graph.render_aabb = SceneBox(aabb=torch.stack([box_min, box_max], dim=0))\n\n                # maybe should update only if true change?\n                json_ = graph.render_aabb.to_json()\n                self.vis[\"sceneState/sceneBox\"].write(json_)\n        else:\n            graph.render_aabb = None\n\n    def update_scene(self, trainer, step: int, graph: Model, num_rays_per_batch: int) -> None:\n        \"\"\"updates the scene based on the graph weights\n\n        Args:\n            step: iteration step of training\n            graph: the current checkpoint of the model\n        \"\"\"\n        has_temporal_distortion = getattr(graph, \"temporal_distortion\", None) is not None", "metadata": {"task_id": "nerfstudio-project_nerfstudio/28", "ground_truth": "        self.vis[\"model/has_temporal_distortion\"].write(str(has_temporal_distortion).lower())", "fpath_tuple": ["nerfstudio-project_nerfstudio", "nerfstudio", "viewer", "server", "viewer_utils.py"], "context_start_lineno": 397, "line_no": 481}}
{"prompt": "\"  # ensure determinism for the device-dependent torch.Generator\n        components = self.get_dummy_components()\n        sd_pipe = StableDiffusionImg2ImgPipeline(**components)\n        sd_pipe = sd_pipe.to(device)\n        sd_pipe.set_progress_bar_config(disable=None)\n\n        inputs = self.get_dummy_inputs(device)\n        negative_prompt = \"french fries\"\n        output = sd_pipe(**inputs, negative_prompt=negative_prompt)\n        image = output.images\n        image_slice = image[0, -3:, -3:, -1]\n\n        assert image.shape == (1, 32, 32, 3)\n        expected_slice = np.array([0.4065, 0.3783, 0.4050, 0.5266, 0.4781, 0.4252, 0.4203, 0.4692, 0.4365])\n\n        assert np.abs(image_slice.flatten() - expected_slice).max() < 1e-3\n\n    def test_stable_diffusion_img2img_multiple_init_images(self):\n        device = \"cpu\"  # ensure determinism for the device-dependent torch.Generator\n        components = self.get_dummy_components()\n        sd_pipe = StableDiffusionImg2ImgPipeline(**components)\n        sd_pipe = sd_pipe.to(device)\n        sd_pipe.set_progress_bar_config(disable=None)\n\n        inputs = self.get_dummy_inputs(device)\n        inputs[\"prompt\"] = [inputs[\"prompt\"]] * 2\n        inputs[\"image\"] = inputs[\"image\"].repeat(2, 1, 1, 1)\n        image = sd_pipe(**inputs).images\n        image_slice = image[-1, -3:, -3:, -1]\n\n        assert image.shape == (2, 32, 32, 3)\n        expected_slice = np.array([0.5144, 0.4447, 0.4735, 0.6676, 0.5526, 0.5454, 0.645, 0.5149, 0.4689])\n\n        assert np.abs(image_slice.flatten() - expected_slice).max() < 1e-3\n\n    def test_stable_diffusion_img2img_k_lms(self):\n        device = \"cpu\"  # ensure determinism for the device-dependent torch.Generator\n        components = self.get_dummy_components()\n        components[\"scheduler\"] = LMSDiscreteScheduler(\n            beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\"\n        )\n        sd_pipe = StableDiffusionImg2ImgPipeline(**components)\n        sd_pipe = sd_pipe.to(device)\n        sd_pipe.set_progress_bar_config(disable=None)\n\n        inputs = self.get_dummy_inputs(device)\n        image = sd_pipe(**inputs).images\n        image_slice = image[0, -3:, -3:, -1]\n\n        assert image.shape == (1, 32, 32, 3)\n        expected_slice = np.array([0.4367, 0.4986, 0.4372, 0.6706, 0.5665, 0.444, 0.5864, 0.6019, 0.5203])\n\n        assert np.abs(image_slice.flatten() - expected_slice).max() < 1e-3\n\n    def test_stable_diffusion_img2img_num_images_per_prompt(self):\n        device = \"cpu\"  # ensure determinism for the device-dependent torch.Generator\n        components = self.get_dummy_components()\n        sd_pipe = StableDiffusionImg2ImgPipeline(**components)\n        sd_pipe = sd_pipe.to(device)\n        sd_pipe.set_progress_bar_config(disable=None)\n\n        # test num_images_per_prompt=1 (default)", "metadata": {"task_id": "huggingface_diffusers/51", "ground_truth": "        inputs = self.get_dummy_inputs(device)", "fpath_tuple": ["huggingface_diffusers", "tests", "pipelines", "stable_diffusion", "test_stable_diffusion_img2img.py"], "context_start_lineno": 125, "line_no": 187}}
{"prompt": "import time\nimport signal\nimport pytest\nimport torch\nimport numpy as np\n\nfrom..base_env_manager import BaseEnvManager, EnvState\n\n\n@pytest.mark.unittest\nclass TestBaseEnvManager:\n\n    def test_naive(self, setup_base_manager_cfg):\n        env_fn = setup_base_manager_cfg.pop('env_fn')\n        env_manager = BaseEnvManager(env_fn, setup_base_manager_cfg)\n        env_manager.seed([314 for _ in range(env_manager.env_num)])\n        assert env_manager._closed\n        obs = env_manager.launch(reset_param={i: {'stat':'stat_test'} for i in range(env_manager.env_num)})\n        assert all([env_manager._env_states[env_id] == EnvState.RUN for env_id in range(env_manager.env_num)])\n        # Test basic\n        name = env_manager._name\n        assert len(name) == env_manager.env_num\n        assert all([isinstance(n, str) for n in name])\n        name = env_manager.name\n        assert len(name) == env_manager.env_num\n        assert all([isinstance(n, str) for n in name])\n        assert env_manager._max_retry == 5\n        assert env_manager._reset_timeout == 10\n        assert all([s == 314 for s in env_manager._seed])\n        assert all([s =='stat_test'] for s in env_manager._stat)\n        # Test arribute\n        with pytest.raises(AttributeError):\n            _ = env_manager.xxx\n        with pytest.raises(RuntimeError):\n            env_manager.user_defined()\n        # Test step\n        count = 1\n        start_time = time.time()\n        while not env_manager.done:\n            env_id = env_manager.ready_obs.keys()\n            action = {i: np.random.randn(4) for i in env_id}", "metadata": {"task_id": "opendilab_ACE/118", "ground_truth": "            timestep = env_manager.step(action)", "fpath_tuple": ["opendilab_ACE", "ding", "envs", "env_manager", "tests", "test_base_env_manager.py"], "context_start_lineno": 0, "line_no": 41}}
{"prompt": "rics_str = \"\"\n        for batch, outputs in zip(val_data_loader, val_outputs_loader):\n            val_losses_and_metrics_current_batch = self.val_step(\n                state, batch, outputs, fun, rng, val_dataset_size, metrics,\n            )\n            val_losses_and_metrics_epoch_all_steps.append(\n                val_losses_and_metrics_current_batch\n            )\n        # compute validation losses and metrics for the current epoch\n        val_losses_and_metrics_current_epoch = self.val_epoch_end(\n            val_losses_and_metrics_epoch_all_steps, state\n        )\n        # logging\n        if verbose:\n            val_epoch_metrics_str = \" | \".join(\n                [\n                    f\"{m}: {round(float(v), 5)}\"\n                    for m, v in val_losses_and_metrics_current_epoch.items()\n                ]\n            )\n        return val_losses_and_metrics_current_epoch, val_epoch_metrics_str\n\n    def val_step(\n        self,\n        state: CalibState,\n        batch: Batch,\n        outputs: Array,\n        fun: Callable,\n        rng: PRNGKeyArray,\n        n_data: int,\n        metrics: Optional[\n            Tuple[Callable[[jnp.ndarray, jnp.ndarray, Array], Array],...]\n        ] = None,\n    ) -> Dict[str, jnp.ndarray]:\n        val_loss, aux = self.val_loss_step(state, batch, outputs, fun, rng, n_data)\n        val_metrics = self.val_metrics_step(aux, batch, metrics)\n        return {\"val_loss\": val_loss, **val_metrics}\n\n    @abc.abstractmethod\n    def val_loss_step(\n        self,\n        state: CalibState,\n        batch: Batch,\n        outputs: Array,\n        fun: Callable,\n        rng: PRNGKeyArray,\n        n_data: int,\n    ) -> Tuple[jnp.ndarray, Dict[str, jnp.ndarray]]:\n        pass\n\n    def val_metrics_step(\n        self,\n        aux: Dict[str, jnp.ndarray],\n        batch: Batch,\n        metrics: Optional[\n            Tuple[Callable[[jnp.ndarray, jnp.ndarray, Array], Array],...]\n        ] = None,\n    ) -> Dict[str, jnp.ndarray]:\n        if metrics is not None:\n            val_metrics = self.compute_metrics(\n                self.predict_fn(aux[\"outputs\"]),\n                self.uncertainty_fn(aux[\"outputs\"]),\n                batch[1],\n                metrics,\n            )\n            return {f\"val_{m}\": v for m, v in val_metrics.items()}\n        else:\n            return {}\n\n    def training_epoch_end(\n        self, training_losses_and_metrics_current_epoch: List[Dict[str, jnp.ndarray]]\n    ) -> Dict[str, float]:\n        return self._get_mean_losses_and_metrics(\n            training_losses_and_metrics_current_epoch\n        )\n\n    def val_epoch_end(\n        self,\n        val_losses_and_metrics_current_epoch: List[Dict[str, jnp.ndarray]],\n        state: CalibState,\n    ) -> Dict[str, float]:\n        val_losses_and_metrics_current_epoch = self._get_mean_losses_and_metrics(\n            val_losses_and_metrics_current_epoch\n        )\n        # early stopping\n        improved = self.early_stopping_update(val_losses_and_metrics_current_epoch)\n        if improved and self.save_checkpoint_dir:", "metadata": {"task_id": "awslabs_fortuna/163", "ground_truth": "            self.save_checkpoint(state, self.save_checkpoint_dir, force_save=True)", "fpath_tuple": ["awslabs_fortuna", "fortuna", "calibration", "calibrator.py"], "context_start_lineno": 315, "line_no": 402}}
{"prompt": " key = random.split(rng)\n            epistemic_variances = self.epistemic_variance(\n                inputs_loader=inputs_loader,\n                n_posterior_samples=n_posterior_samples,\n                rng=key,\n                distribute=distribute,\n            )\n        return aleatoric_variances + epistemic_variances\n\n    def std(\n        self,\n        inputs_loader: InputsLoader,\n        n_posterior_samples: int = 30,\n        variances: Optional[jnp.ndarray] = None,\n        rng: Optional[PRNGKeyArray] = None,\n        distribute: bool = True,\n    ) -> jnp.ndarray:\n        r\"\"\"\n        Estimate the predictive standard deviation of the target variable, that is\n\n       .. math::\n            \\text{Var}_{Y|x, D}[Y],\n\n        where:\n         - :math:`x` is an observed input variable;\n         - :math:`Y` is a random target variable;\n         - :math:`\\mathcal{D}` is the observed training data set.\n\n        Parameters\n        ----------\n        inputs_loader : InputsLoader\n            A loader of input data points.\n        n_posterior_samples : int\n            Number of samples to draw from the posterior distribution for each input.\n        variances: Optional[jnp.ndarray]\n            An estimate of the predictive variance.\n        rng : Optional[PRNGKeyArray]\n            A random number generator. If not passed, this will be taken from the attributes of this class.\n        distribute: bool\n            Whether to distribute computation over multiple devices, if available.\n\n        Returns\n        -------\n        jnp.ndarray\n            An estimate of the predictive standard deviation for each input.\n        \"\"\"\n        if variances is None:\n            variances = self.variance(\n                inputs_loader=inputs_loader,\n                n_posterior_samples=n_posterior_samples,\n                rng=rng,\n                distribute=distribute,\n            )\n        return jnp.sqrt(variances)\n\n    @staticmethod\n    def _unshard_ensemble_arrays(arr: Array) -> Array:\n        arr = arr.swapaxes(1, 2)\n        arr = arr.reshape((arr.shape[0] * arr.shape[1],) + arr.shape[2:])\n        return arr.swapaxes(0, 1)\n\n    def _loop_fun_through_inputs_loader(\n        self,\n        fun: Callable,\n        inputs_loader: InputsLoader,\n        n_posterior_samples: int,\n        rng: PRNGKeyArray,\n        distribute: bool = True,\n        **kwargs\n    ) -> Array:\n        if distribute and jax.local_device_count() <= 1:\n            distribute = False\n\n        def fun2(_inputs):\n            return fun(_inputs, n_posterior_samples, rng, **kwargs)\n\n        if distribute:\n            inputs_loader = DeviceDimensionAugmentedInputsLoader(inputs_loader)\n            fun2 = pmap(fun2)\n            return jnp.concatenate(\n                [\n                    self.likelihood._unshard_array(fun2(inputs))\n                    for inputs in inputs_loader\n                ],\n                0,\n            )\n        fun2 = jit(fun2)\n        return jnp.concatenate([fun2(inputs) for inputs in inputs_loader], 0)\n\n    def _loop_fun_through_data_loader(\n        self,\n        fun: Callable,\n        data_loader: DataLoader,\n        n_posterior_samples: int,\n        rng: PRNGKeyArray,\n        distribute: bool = True,\n        **kwargs\n    ) -> Array:\n        if distribute and jax.local_device_count() <= 1:\n            distribute = False\n\n        def fun2(_batch):\n            return fun(_batch, n_posterior_samples, rng, **kwargs)\n\n        if distribute:", "metadata": {"task_id": "awslabs_fortuna/101", "ground_truth": "            data_loader = DeviceDimensionAugmentedDataLoader(data_loader)", "fpath_tuple": ["awslabs_fortuna", "fortuna", "prob_model", "predictive", "base.py"], "context_start_lineno": 760, "line_no": 865}}
{"prompt": "from __future__ import annotations\n\nimport logging\nimport os\nimport pathlib\nfrom typing import List, Optional\n\nimport numpy as np\nfrom jax._src.prng import PRNGKeyArray\n\nfrom fortuna.data.loader import DataLoader\nfrom fortuna.prob_model.fit_config import FitConfig\nfrom fortuna.prob_model.joint.base import Joint\nfrom fortuna.prob_model.joint.state import JointState\nfrom fortuna.prob_model.posterior.base import Posterior\nfrom fortuna.prob_model.posterior.deep_ensemble import DEEP_ENSEMBLE_NAME\nfrom fortuna.prob_model.posterior.deep_ensemble.deep_ensemble_approximator import \\\n    DeepEnsemblePosteriorApproximator\nfrom fortuna.prob_model.posterior.deep_ensemble.deep_ensemble_repositories import \\\n    DeepEnsemblePosteriorStateRepository\nfrom fortuna.prob_model.posterior.map.map_posterior import MAPState\nfrom fortuna.prob_model.posterior.map.map_trainer import (\n    JittedMAPTrainer, MAPTrainer, MultiDeviceMAPTrainer)\nfrom fortuna.typing import Path, Status\nfrom fortuna.utils.device import select_trainer_given_devices\n\nlogger = logging.getLogger(__name__)\n\n\nclass DeepEnsemblePosterior(Posterior):\n    def __init__(\n        self, joint: Joint, posterior_approximator: DeepEnsemblePosteriorApproximator,\n    ):\n        \"\"\"\n        Deep ensemble approximate posterior class.\n\n        Parameters\n        ----------\n        joint: Joint\n            Joint distribution.\n        posterior_approximator: DeepEnsemble\n            Deep ensemble posterior approximator.\n        \"\"\"\n        super().__init__(joint=joint, posterior_approximator=posterior_approximator)\n\n    def __str__(self):\n        return DEEP_ENSEMBLE_NAME\n\n    def fit(\n        self,\n        train_data_loader: DataLoader,\n        val_data_loader: Optional[DataLoader] = None,\n        fit_config: FitConfig = FitConfig(),\n        **kwargs,\n    ) -> List[Status]:\n        if (\n            fit_config.checkpointer.dump_state is True\n            and not fit_config.checkpointer.save_checkpoint_dir\n        ):\n            raise ValueError(\n                \"`save_checkpoint_dir` must be passed when `dump_state` is set to True.\"\n            )\n\n        trainer_cls = select_trainer_given_devices(\n            devices=fit_config.processor.devices,\n            BaseTrainer=MAPTrainer,\n            JittedTrainer=JittedMAPTrainer,\n            MultiDeviceTrainer=MultiDeviceMAPTrainer,\n            disable_jit=fit_config.processor.disable_jit,\n        )\n\n        def _fit(i):", "metadata": {"task_id": "awslabs_fortuna/57", "ground_truth": "            init_prob_model_state, n_train_data, n_val_data = self._init(\n                train_data_loader, val_data_loader\n            )", "fpath_tuple": ["awslabs_fortuna", "fortuna", "prob_model", "posterior", "deep_ensemble", "deep_ensemble_posterior.py"], "context_start_lineno": 0, "line_no": 72}}
{"prompt": "\n    VanillaDataManager,\n    VanillaDataManagerConfig,\n)\nfrom nerfstudio.engine.callbacks import TrainingCallback, TrainingCallbackAttributes\nfrom nerfstudio.models.base_model import Model, ModelConfig\nfrom nerfstudio.utils import profiler\n\n\ndef module_wrapper(ddp_or_model: Union[DDP, Model]) -> Model:\n    \"\"\"\n    If DDP, then return the.module. Otherwise, return the model.\n    \"\"\"\n    if isinstance(ddp_or_model, DDP):\n        return cast(Model, ddp_or_model.module)\n    return ddp_or_model\n\n\nclass Pipeline(nn.Module):\n    \"\"\"The intent of this class is to provide a higher level interface for the Model\n    that will be easy to use for our Trainer class.\n\n    This class will contain high level functions for the model like getting the loss\n    dictionaries and visualization code. It should have ways to get the next iterations\n    training loss, evaluation loss, and generate whole images for visualization. Each model\n    class should be 1:1 with a pipeline that can act as a standardized interface and hide\n    differences in how each model takes in and outputs data.\n\n    This class's function is to hide the data manager and model classes from the trainer,\n    worrying about:\n    1) Fetching data with the data manager\n    2) Feeding the model the data and fetching the loss\n    Hopefully this provides a higher level interface for the trainer to use, and\n    simplifying the model classes, which each may have different forward() methods\n    and so on.\n\n    Args:\n        config: configuration to instantiate pipeline\n        device: location to place model and data\n        test_mode:\n            'train': loads train/eval datasets into memory\n            'test': loads train/test dataset into memory\n            'inference': does not load any dataset into memory\n        world_size: total number of machines available\n        local_rank: rank of current machine\n\n    Attributes:\n        datamanager: The data manager that will be used\n        model: The model that will be used\n    \"\"\"\n\n    # pylint: disable=abstract-method\n\n    datamanager: DataManager\n    _model: Model\n\n    @property\n    def model(self):\n        \"\"\"Returns the unwrapped model if in ddp\"\"\"\n        return module_wrapper(self._model)\n\n    @property\n    def device(self):\n        \"\"\"Returns the device that the model is on.\"\"\"\n        return self.model.device\n\n    @profiler.time_function\n    def get_train_loss_dict(self, step: int):\n        \"\"\"This function gets your training loss dict. This will be responsible for\n        getting the next batch of data from the DataManager and interfacing with the\n        Model class, feeding the data to the model's forward function.\n\n        Args:\n            step: current iteration step to update sampler if using DDP (distributed)\n        \"\"\"\n        if self.world_size > 1 and step:\n            assert self.datamanager.train_sampler is not None\n            self.datamanager.train_sampler.set_epoch(step)\n        ray_bundle, batch = self.datamanager.next_train(step)\n        model_outputs = self.model(ray_bundle, batch)\n        metrics_dict = self.model.get_metrics_dict(model_outputs, batch)\n        loss_dict = self.model.get_loss_dict(model_outputs, batch, metrics_dict)\n\n        return model_outputs, loss_dict, metrics_dict\n\n    @profiler.time_function\n    def get_eval_loss_dict(self, step: int):\n        \"\"\"This function gets your evaluation loss dict. It needs to get the data\n        from the DataManager and feed it to the model's forward function\n\n        Args:\n            step: current iteration step\n        \"\"\"\n        self.eval()\n        if self.world_size > 1:\n            assert self.datamanager.eval_sampler is not None\n            self.datamanager.eval_sampler.set_epoch(step)\n        ray_bundle, batch = self.datamanager.next_eval(step)\n        model_outputs = self.model(ray_bundle, batch)", "metadata": {"task_id": "nerfstudio-project_nerfstudio/85", "ground_truth": "        metrics_dict = self.model.get_metrics_dict(model_outputs, batch)", "fpath_tuple": ["nerfstudio-project_nerfstudio", "nerfstudio", "pipelines", "base_pipeline.py"], "context_start_lineno": 41, "line_no": 140}}
{"prompt": "\n\nfrom..base_env_manager import EnvState\nfrom..subprocess_env_manager import AsyncSubprocessEnvManager, SyncSubprocessEnvManager\n\n\nclass TestSubprocessEnvManager:\n\n    @pytest.mark.unittest\n    def test_naive(self, setup_async_manager_cfg, setup_model_type):\n        env_fn = setup_async_manager_cfg.pop('env_fn')\n        env_manager = AsyncSubprocessEnvManager(env_fn, setup_async_manager_cfg)\n        model = setup_model_type()\n\n        env_manager.seed([314 for _ in range(env_manager.env_num)])\n        env_manager.launch(reset_param={i: {'stat':'stat_test'} for i in range(env_manager.env_num)})\n        assert all([s == 314 for s in env_manager._seed])\n        assert all([s =='stat_test'] for s in env_manager._stat)\n        # Test basic\n        name = env_manager._name\n        for i in range(env_manager.env_num):\n            assert name[i] == 'name{}'.format(i)\n        assert len(name) == env_manager.env_num\n        assert all([isinstance(n, str) for n in name])\n        name = env_manager.name\n        assert len(name) == env_manager.env_num\n        assert all([isinstance(n, str) for n in name])\n        assert env_manager._max_retry == 5\n        assert env_manager._reset_timeout == 10\n        # Test arribute\n        with pytest.raises(AttributeError):\n            data = env_manager.xxx\n        env_manager._env_ref.user_defined()\n        with pytest.raises(RuntimeError):\n            env_manager.user_defined()\n        # Test step\n        env_count = [0 for _ in range(env_manager.env_num)]\n        data_count = 0\n        start_time = time.time()\n        while not env_manager.done:\n            obs = env_manager.ready_obs\n            print('obs', obs.keys(), env_manager._env_states)\n            action = model.forward(obs)\n            assert 1 <= len(action) <= len(obs)\n            print('act', action.keys())\n            timestep = env_manager.step(action)\n            data_count += len(timestep)\n            assert len(timestep) >= 1\n            print('timestep', timestep.keys(), timestep, len(timestep))\n            for k, t in timestep.items():\n                if t.done:\n                    print('env{} finish episode{}'.format(k, env_count[k]))\n                    env_count[k] += 1\n        assert all([c == setup_async_manager_cfg.episode_num for c in env_count])\n        assert data_count == sum(env_manager._data_count)\n        assert all([env_manager._env_states[env_id] == EnvState.DONE for env_id in range(env_manager.env_num)])\n        end_time = time.time()\n        print('total step time: {}'.format(end_time - start_time))\n\n        # Test close\n        env_manager.close()\n        assert env_manager._closed\n        with pytest.raises(AssertionError):\n            env_manager.reset([])\n        with pytest.raises(AssertionError):\n            env_manager.step([])\n\n    @pytest.mark.unittest\n    def test_error(self, setup_sync_manager_cfg):\n        env_fn = setup_sync_manager_cfg.pop('env_fn')\n        env_manager = SyncSubprocessEnvManager(env_fn, setup_sync_manager_cfg)\n        # Test reset error\n        with pytest.raises(AssertionError):\n            env_manager.reset(reset_param={i: {'stat':'stat_test'} for i in range(env_manager.env_num)})\n        with pytest.raises(RuntimeError):", "metadata": {"task_id": "opendilab_ACE/108", "ground_truth": "            obs = env_manager.launch(reset_param={i: {'stat': 'error'} for i in range(env_manager.env_num)})", "fpath_tuple": ["opendilab_ACE", "ding", "envs", "env_manager", "tests", "test_subprocess_env_manager.py"], "context_start_lineno": 4, "line_no": 79}}
{"prompt": " banner_messages = viewer_utils.setup_viewer(\n                config.viewer, log_filename=viewer_log_path, datapath=config.pipeline.datamanager.dataparser.data\n            )\n        self._check_viewer_warnings()\n        # set up writers/profilers if enabled\n        writer_log_path = self.base_dir / config.logging.relative_log_dir\n        writer.setup_event_writer(config.is_wandb_enabled(), config.is_tensorboard_enabled(), log_dir=writer_log_path)\n        writer.setup_local_writer(config.logging, max_iter=config.max_num_iterations, banner_messages=banner_messages)\n        writer.put_config(name=\"config\", config_dict=dataclasses.asdict(config), step=0)\n        profiler.setup_profiler(config.logging)\n\n    def setup(self, test_mode: Literal[\"test\", \"val\", \"inference\"] = \"val\"):\n        \"\"\"Setup the Trainer by calling other setup functions.\n\n        Args:\n            test_mode:\n                'val': loads train/val datasets into memory\n                'test': loads train/test datasets into memory\n                'inference': does not load any dataset into memory\n        \"\"\"\n        self.pipeline = self.config.pipeline.setup(\n            device=self.device, test_mode=test_mode, world_size=self.world_size, local_rank=self.local_rank\n        )\n        self.optimizers = self.setup_optimizers()\n\n        self._load_checkpoint()\n\n        self.callbacks = self.pipeline.get_training_callbacks(\n            TrainingCallbackAttributes(\n                optimizers=self.optimizers,  # type: ignore\n                grad_scaler=self.grad_scaler,  # type: ignore\n                pipeline=self.pipeline,  # type: ignore\n            )\n        )\n\n    def setup_optimizers(self) -> Optimizers:\n        \"\"\"Helper to set up the optimizers\n\n        Returns:\n            The optimizers object given the trainer config.\n        \"\"\"\n        optimizer_config = self.config.optimizers.copy()\n        camera_optimizer_config = self.config.pipeline.datamanager.camera_optimizer\n        param_groups = self.pipeline.get_param_groups()\n        if camera_optimizer_config.mode!= \"off\":\n            assert camera_optimizer_config.param_group not in optimizer_config\n            optimizer_config[camera_optimizer_config.param_group] = {\n                \"optimizer\": camera_optimizer_config.optimizer,\n                \"scheduler\": camera_optimizer_config.scheduler,\n            }\n        return Optimizers(optimizer_config, param_groups)\n\n    def train(self) -> None:\n        \"\"\"Train the model.\"\"\"\n        assert self.pipeline.datamanager.train_dataset is not None, \"Missing DatsetInputs\"\n\n        self.pipeline.datamanager.train_dataparser_outputs.save_dataparser_transform(\n            self.base_dir / \"dataparser_transforms.json\"\n        )\n\n        self._init_viewer_state()\n        with TimeWriter(writer, EventName.TOTAL_TRAIN_TIME):\n            num_iterations = self.config.max_num_iterations\n            step = 0\n            for step in range(self._start_step, self._start_step + num_iterations):\n                with TimeWriter(writer, EventName.ITER_TRAIN_TIME, step=step) as train_t:\n\n                    self.pipeline.train()\n\n                    # training callbacks before the training iteration\n                    for callback in self.callbacks:\n                        callback.run_callback_at_location(\n                            step, location=TrainingCallbackLocation.BEFORE_TRAIN_ITERATION\n                        )\n\n                    # time the forward pass\n                    loss, loss_dict, metrics_dict = self.train_iteration(step)\n\n                    # training callbacks after the training iteration\n                    for callback in self.callbacks:", "metadata": {"task_id": "nerfstudio-project_nerfstudio/9", "ground_truth": "                        callback.run_callback_at_location(step, location=TrainingCallbackLocation.AFTER_TRAIN_ITERATION)", "fpath_tuple": ["nerfstudio-project_nerfstudio", "nerfstudio", "engine", "trainer.py"], "context_start_lineno": 124, "line_no": 204}}
{"prompt": "fn.priority_key in td.keys()\n\n        sum([item for _, item in loss.items()]).backward()\n        assert torch.nn.utils.clip_grad.clip_grad_norm_(actor.parameters(), 1.0) > 0.0\n\n        # Check param update effect on targets\n        target_value = loss_fn.target_value_network_params.clone()\n        for p in loss_fn.parameters():\n            p.data += torch.randn_like(p)\n        target_value2 = loss_fn.target_value_network_params.clone()\n        if loss_fn.delay_value:\n            assert_allclose_td(target_value, target_value2)\n        else:\n            assert not (target_value == target_value2).any()\n\n        # check that policy is updated after parameter update\n        parameters = [p.clone() for p in actor.parameters()]\n        for p in loss_fn.parameters():\n            p.data += torch.randn_like(p)\n        assert all((p1!= p2).all() for p1, p2 in zip(parameters, actor.parameters()))\n\n    @pytest.mark.parametrize(\"n\", range(4))\n    @pytest.mark.parametrize(\"delay_value\", (False, True))\n    @pytest.mark.parametrize(\"device\", get_available_devices())\n    @pytest.mark.parametrize(\n        \"action_spec_type\", (\"nd_bounded\", \"one_hot\", \"categorical\")\n    )\n    def test_dqn_batcher(self, n, delay_value, device, action_spec_type, gamma=0.9):\n        torch.manual_seed(self.seed)\n        actor = self._create_mock_actor(\n            action_spec_type=action_spec_type, device=device\n        )\n\n        td = self._create_seq_mock_data_dqn(\n            action_spec_type=action_spec_type, device=device\n        )\n        loss_fn = DQNLoss(\n            actor, gamma=gamma, loss_function=\"l2\", delay_value=delay_value\n        )\n\n        ms = MultiStep(gamma=gamma, n_steps_max=n).to(device)\n        ms_td = ms(td.clone())\n\n        with _check_td_steady(ms_td):\n            loss_ms = loss_fn(ms_td)\n        assert loss_fn.priority_key in ms_td.keys()\n\n        with torch.no_grad():\n            loss = loss_fn(td)\n        if n == 0:\n            assert_allclose_td(td, ms_td.select(*list(td.keys())))\n            _loss = sum([item for _, item in loss.items()])\n            _loss_ms = sum([item for _, item in loss_ms.items()])\n            assert (\n                abs(_loss - _loss_ms) < 1e-3\n            ), f\"found abs(loss-loss_ms) = {abs(loss - loss_ms):4.5f} for n=0\"\n        else:\n            with pytest.raises(AssertionError):\n                assert_allclose_td(loss, loss_ms)\n        sum([item for _, item in loss_ms.items()]).backward()\n        assert torch.nn.utils.clip_grad.clip_grad_norm_(actor.parameters(), 1.0) > 0.0\n\n        # Check param update effect on targets\n        target_value = loss_fn.target_value_network_params.clone()\n        for p in loss_fn.parameters():\n            p.data += torch.randn_like(p)\n        target_value2 = loss_fn.target_value_network_params.clone()\n        if loss_fn.delay_value:\n            assert_allclose_td(target_value, target_value2)\n        else:\n            assert not (target_value == target_value2).any()\n\n        # check that policy is updated after parameter update\n        parameters = [p.clone() for p in actor.parameters()]", "metadata": {"task_id": "pytorch_rl/185", "ground_truth": "        for p in loss_fn.parameters():", "fpath_tuple": ["pytorch_rl", "test", "test_cost.py"], "context_start_lineno": 301, "line_no": 375}}
{"prompt": ".FloatTensor) -> torch.FloatTensor:\n        \"\"\"\n        Overview:\n            forward computation graph of the mixer network for Q_star in WQMIX.\n        Arguments:\n            - agent_qs (:obj:`torch.FloatTensor`): the independent q_value of each agent\n            - states (:obj:`torch.FloatTensor`): the emdedding vector of global state\n        Returns:\n            - q_tot (:obj:`torch.FloatTensor`): the total mixed q_value\n        Shapes:\n            - agent_qs (:obj:`torch.FloatTensor`): :math:`(T,B, N)`, where T is timestep,\n              B is batch size, A is agent_num, N is obs_shape\n            - states (:obj:`torch.FloatTensor`): :math:`(T, B, M)`, where M is global_obs_shape\n            - q_tot (:obj:`torch.FloatTensor`): :math:`(T, B, )`\n        \"\"\"\n        # in below annotations about the shape of the variables, T is timestep,\n        # B is batch_size A is agent_num, N is obs_shape\uff0c for example,\n        # in 3s5z, we can set T=10, B=32, A=8, N=216\n        bs = agent_qs.shape[:-1]  # (T*B, A)\n        states = states.reshape(-1, self.state_dim)  # T*B, N),\n        agent_qs = agent_qs.reshape(-1, self.agent_num)  # (T, B, A) -> (T*B, A)\n        inputs = torch.cat([states, agent_qs], dim=1)  # (T*B, N) (T*B, A)-> (T*B, N+A)\n        advs = self.net(inputs)  # (T*B, 1)\n        vs = self.V(states)  # (T*B, 1)\n        y = advs + vs\n        q_tot = y.view(*bs)  # (T*B, 1) -> (T, B)\n\n        return q_tot\n\n\n@MODEL_REGISTRY.register('wqmix')\nclass WQMix(nn.Module):\n    \"\"\"\n    Overview:\n        WQMIX network, which is same as Qmix network\n    Interface:\n        __init__, forward, _setup_global_encoder\n    \"\"\"\n\n    def __init__(\n            self,\n            agent_num: int,\n            obs_shape: int,\n            global_obs_shape: int,\n            action_shape: int,\n            hidden_size_list: list,\n            lstm_type: str = 'gru',\n            dueling: bool = False\n    ) -> None:\n        \"\"\"\n        Overview:\n            initialize Qmix network\n        Arguments:\n            - agent_num (:obj:`int`): the number of agent\n            - obs_shape (:obj:`int`): the dimension of each agent's observation state\n            - global_obs_shape (:obj:`int`): the dimension of global observation state\n            - action_shape (:obj:`int`): the dimension of action shape\n            - hidden_size_list (:obj:`list`): the list of hidden size\n            - lstm_type (:obj:`str`): use lstm or gru, default to gru\n            - dueling (:obj:`bool`): use dueling head or not, default to False.\n        \"\"\"\n        super(WQMix, self).__init__()\n        self._act = nn.ReLU()\n        self._q_network = DRQN(obs_shape, action_shape, hidden_size_list, lstm_type=lstm_type, dueling=dueling)\n        self._q_network_star = DRQN(obs_shape, action_shape, hidden_size_list, lstm_type=lstm_type, dueling=dueling)\n        embedding_size = hidden_size_list[-1]", "metadata": {"task_id": "opendilab_ACE/116", "ground_truth": "        self._mixer = Mixer(agent_num, global_obs_shape, mixing_embed_dim=embedding_size)", "fpath_tuple": ["opendilab_ACE", "ding", "model", "template", "wqmix.py"], "context_start_lineno": 45, "line_no": 111}}
{"prompt": " CtxVar(\n                            outputs.contrastive_loss, LIFECYCLE.BATCH)\n                    ctx.y_true = CtxVar(labels, LIFECYCLE.BATCH)\n                    ctx.y_pred = CtxVar(outputs.logits.argmax(dim=-1),\n                                        LIFECYCLE.BATCH)\n\n            elif self.task in {'squad', 'newsqa'}:\n                outputs = ctx.model(\n                    input_ids=token_ids.to(ctx.device),\n                    token_type_ids=token_type_ids.to(ctx.device),\n                    attention_mask=attention_mask.to(ctx.device),\n                    start_positions=start_positions.to(ctx.device),\n                    end_positions=end_positions.to(ctx.device),\n                    contrast_monitor=ctx.contrast_monitor,\n                    in_contrast_prepare=self._in_contrast_prepare,\n                    example_indices=example_indices,\n                )\n                if not self._in_contrast_prepare:\n                    for i, example_idx in enumerate(example_indices):\n                        encoded_input = ctx.get('{}_encoded'.format(\n                            ctx.cur_split))[example_idx.item()]\n                        unique_id = int(encoded_input.unique_id)\n                        start_logits = \\\n                            outputs.logits[0][i].detach().cpu().tolist()\n                        end_logits = \\\n                            outputs.logits[1][i].detach().cpu().tolist()\n                        if ctx.cur_split!= 'train':\n                            if self.task =='squad':\n                                ctx.squad_results.append(\n                                    SquadResult(unique_id, start_logits,\n                                                end_logits))\n                            elif self.task == 'newsqa':\n                                ctx.newsqa_results.append(\n                                    NewsQAResult(unique_id, start_logits,\n                                                 end_logits))\n\n                    ctx.batch_size = CtxVar(len(token_ids), LIFECYCLE.BATCH)\n                    ctx.loss_batch = CtxVar(outputs.loss, LIFECYCLE.BATCH)\n                    if self.use_contrastive_loss:\n                        ctx.regular_loss_batch = CtxVar(\n                            outputs.regular_loss, LIFECYCLE.BATCH)\n                        ctx.contrastive_loss_batch = CtxVar(\n                            outputs.contrastive_loss, LIFECYCLE.BATCH)\n                    ctx.y_true = CtxVar(\n                        torch.cat([start_positions, end_positions]),\n                        LIFECYCLE.BATCH)\n                    ctx.y_pred = CtxVar(\n                        torch.cat(\n                            [out.argmax(dim=-1) for out in outputs.logits]),\n                        LIFECYCLE.BATCH)\n\n            elif self.task in {'cnndm','msqg'}:\n                if ctx.cur_split!= 'test':\n                    outputs = ctx.model(\n                        input_ids=token_ids.to(ctx.device),\n                        token_type_ids=token_type_ids.to(ctx.device),\n                        attention_mask=attention_mask.to(ctx.device),\n                        labels=labels.to(ctx.device),\n                        contrast_monitor=ctx.contrast_monitor,\n                        in_contrast_prepare=self._in_contrast_prepare,\n                        example_indices=example_indices,\n                    )\n                    if not self._in_contrast_prepare:\n                        ctx.batch_size = CtxVar(len(labels), LIFECYCLE.BATCH)\n                        ctx.loss_batch = CtxVar(outputs.loss, LIFECYCLE.BATCH)\n                        if self.use_contrastive_loss:\n                            ctx.regular_loss_batch = CtxVar(\n                                outputs.regular_loss, LIFECYCLE.BATCH)", "metadata": {"task_id": "alibaba_FederatedScope/116", "ground_truth": "                            ctx.contrastive_loss_batch = CtxVar(\n                                outputs.contrastive_loss, LIFECYCLE.BATCH)", "fpath_tuple": ["alibaba_FederatedScope", "federatedscope", "nlp", "hetero_tasks", "trainer", "trainer.py"], "context_start_lineno": 393, "line_no": 461}}
{"prompt": "uncated_ids = self.tokenizer(prompt, padding=\"longest\", return_tensors=\"pt\").input_ids\n\n            if untruncated_ids.shape[-1] >= text_input_ids.shape[-1] and not torch.equal(\n                text_input_ids, untruncated_ids\n            ):\n                removed_text = self.tokenizer.batch_decode(\n                    untruncated_ids[:, self.tokenizer.model_max_length - 1 : -1]\n                )\n                logger.warning(\n                    \"The following part of your input was truncated because CLIP can only handle sequences up to\"\n                    f\" {self.tokenizer.model_max_length} tokens: {removed_text}\"\n                )\n\n            if hasattr(self.text_encoder.config, \"use_attention_mask\") and self.text_encoder.config.use_attention_mask:\n                attention_mask = text_inputs.attention_mask.to(device)\n            else:\n                attention_mask = None\n\n            prompt_embeds = self.text_encoder(\n                text_input_ids.to(device),\n                attention_mask=attention_mask,\n            )\n            prompt_embeds = prompt_embeds[0]\n\n        prompt_embeds = prompt_embeds.to(dtype=self.text_encoder.dtype, device=device)\n\n        bs_embed, seq_len, _ = prompt_embeds.shape\n        # duplicate text embeddings for each generation per prompt, using mps friendly method\n        prompt_embeds = prompt_embeds.repeat(1, num_images_per_prompt, 1)\n        prompt_embeds = prompt_embeds.view(bs_embed * num_images_per_prompt, seq_len, -1)\n\n        # get unconditional embeddings for classifier free guidance\n        if do_classifier_free_guidance and negative_prompt_embeds is None:\n            uncond_tokens: List[str]\n            if negative_prompt is None:\n                uncond_tokens = [\"\"] * batch_size\n            elif type(prompt) is not type(negative_prompt):\n                raise TypeError(\n                    f\"`negative_prompt` should be the same type to `prompt`, but got {type(negative_prompt)}!=\"\n                    f\" {type(prompt)}.\"\n                )\n            elif isinstance(negative_prompt, str):\n                uncond_tokens = [negative_prompt]\n            elif batch_size!= len(negative_prompt):\n                raise ValueError(\n                    f\"`negative_prompt`: {negative_prompt} has batch size {len(negative_prompt)}, but `prompt`:\"\n                    f\" {prompt} has batch size {batch_size}. Please make sure that passed `negative_prompt` matches\"\n                    \" the batch size of `prompt`.\"\n                )\n            else:\n                uncond_tokens = negative_prompt\n\n            max_length = prompt_embeds.shape[1]\n            uncond_input = self.tokenizer(\n                uncond_tokens,\n                padding=\"max_length\",\n                max_length=max_length,\n                truncation=True,\n                return_tensors=\"pt\",\n            )\n\n            if hasattr(self.text_encoder.config, \"use_attention_mask\") and self.text_encoder.config.use_attention_mask:\n                attention_mask = uncond_input.attention_mask.to(device)\n            else:\n                attention_mask = None\n\n            negative_prompt_embeds = self.text_encoder(\n                uncond_input.input_ids.to(device),\n                attention_mask=attention_mask,\n            )\n            negative_prompt_embeds = negative_prompt_embeds[0]\n\n        if do_classifier_free_guidance:\n            # duplicate unconditional embeddings for each generation per prompt, using mps friendly method\n            seq_len = negative_prompt_embeds.shape[1]", "metadata": {"task_id": "huggingface_diffusers/178", "ground_truth": "            negative_prompt_embeds = negative_prompt_embeds.to(dtype=self.text_encoder.dtype, device=device)", "fpath_tuple": ["huggingface_diffusers", "src", "diffusers", "pipelines", "stable_diffusion", "pipeline_stable_diffusion_inpaint_legacy.py"], "context_start_lineno": 268, "line_no": 344}}
{"prompt": "DiscreteTensorSpec\n            )\n            action_spec = action_spec_cls(*batch_size, 7)\n        if reward_spec is None:\n            reward_spec = UnboundedContinuousTensorSpec(shape=(1,))\n\n        if input_spec is None:\n            cls._out_key = \"observation_orig\"\n            input_spec = CompositeSpec(\n                **{\n                    cls._out_key: observation_spec[\"observation\"],\n                    \"action\": action_spec,\n                }\n            )\n        cls._reward_spec = reward_spec\n        cls._observation_spec = observation_spec\n        cls._input_spec = input_spec\n        cls.from_pixels = from_pixels\n        cls.categorical_action_encoding = categorical_action_encoding\n        return super().__new__(*args, **kwargs)\n\n    def _get_in_obs(self, obs):\n        return obs\n\n    def _get_out_obs(self, obs):\n        return obs\n\n    def _reset(self, tensordict: TensorDictBase = None) -> TensorDictBase:\n        self.counter += 1\n        state = torch.zeros(self.size) + self.counter\n        if tensordict is None:\n            tensordict = TensorDict({}, self.batch_size, device=self.device)\n        tensordict = tensordict.select().set(self.out_key, self._get_out_obs(state))\n        tensordict = tensordict.set(self._out_key, self._get_out_obs(state))\n        tensordict.set(\"done\", torch.zeros(*tensordict.shape, 1, dtype=torch.bool))\n        return tensordict\n\n    def _step(\n        self,\n        tensordict: TensorDictBase,\n    ) -> TensorDictBase:\n        tensordict = tensordict.to(self.device)\n        a = tensordict.get(\"action\")\n\n        if not self.categorical_action_encoding:\n            assert (a.sum(-1) == 1).all()\n\n        obs = self._get_in_obs(tensordict.get(self._out_key)) + a / self.maxstep\n        tensordict = tensordict.select()  # empty tensordict\n\n        tensordict.set(self.out_key, self._get_out_obs(obs))\n        tensordict.set(self._out_key, self._get_out_obs(obs))\n\n        done = torch.isclose(obs, torch.ones_like(obs) * (self.counter + 1))\n        reward = done.any(-1).unsqueeze(-1)\n        # set done to False\n        done = torch.zeros_like(done).all(-1).unsqueeze(-1)\n        tensordict.set(\"reward\", reward.to(torch.get_default_dtype()))\n        tensordict.set(\"done\", done)\n        return tensordict\n\n\nclass ContinuousActionVecMockEnv(_MockEnv):\n    @classmethod\n    def __new__(\n        cls,\n        *args,\n        observation_spec=None,\n        action_spec=None,\n        input_spec=None,\n        reward_spec=None,\n        from_pixels=False,\n        **kwargs,\n    ):\n        batch_size = kwargs.setdefault(\"batch_size\", torch.Size([]))\n        size = cls.size = 7\n        if observation_spec is None:\n            cls.out_key = \"observation\"\n            observation_spec = CompositeSpec(\n                observation=UnboundedContinuousTensorSpec(\n                    shape=torch.Size([*batch_size, size])\n                ),\n                observation_orig=UnboundedContinuousTensorSpec(\n                    shape=torch.Size([*batch_size, size])\n                ),\n                shape=batch_size,\n            )\n        if action_spec is None:", "metadata": {"task_id": "pytorch_rl/9", "ground_truth": "            action_spec = BoundedTensorSpec(\n                -1,\n                1,\n                (\n                    *batch_size,\n                    7,\n                ),\n            )", "fpath_tuple": ["pytorch_rl", "test", "mocking_classes.py"], "context_start_lineno": 360, "line_no": 448}}
{"prompt": "izier as vz\nfrom vizier._src.algorithms.optimizers import eagle_strategy\nfrom vizier._src.algorithms.optimizers import random_vectorized_optimizer as rvo\nfrom vizier._src.algorithms.optimizers import vectorized_base as vb\nfrom vizier._src.algorithms.testing import comparator_runner\nfrom vizier.pyvizier import converters\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\n\n\ndef randomize_array(converter: converters.TrialToArrayConverter) -> np.ndarray:\n  \"\"\"Generate a random array of features to be used as score_fn shift.\"\"\"\n  features_arrays = []\n  for spec in converter.output_specs:\n    if spec.type == converters.NumpyArraySpecType.ONEHOT_EMBEDDING:\n      dim = spec.num_dimensions - spec.num_oovs\n      features_arrays.append(\n          np.eye(spec.num_dimensions)[np.random.randint(0, dim)])\n    elif spec.type == converters.NumpyArraySpecType.CONTINUOUS:\n      features_arrays.append(np.random.uniform(0.4, 0.6, size=(1,)))\n    else:\n      raise ValueError(f'The type {spec.type} is not supported!')\n  return np.hstack(features_arrays)\n\n\ndef create_continuous_problem(\n    n_features: int,\n    problem: Optional[vz.ProblemStatement] = None) -> vz.ProblemStatement:\n  if not problem:\n    problem = vz.ProblemStatement()\n  root = problem.search_space.select_root()\n  for i in range(n_features):\n    root.add_float_param('x%d' % i, -5.0, 5.0)\n  return problem\n\n\ndef create_categorical_problem(\n    n_features: int,\n    categorical_dim: int = 6,\n    problem: Optional[vz.ProblemStatement] = None) -> vz.ProblemStatement:\n  if not problem:\n    problem = vz.ProblemStatement()\n  root = problem.search_space.select_root()\n  for i in range(n_features):\n    root.add_categorical_param(\n        'c%d' % i, feasible_values=[str(i) for i in range(categorical_dim)])\n  return problem\n\n\ndef create_mix_problem(n_features: int,\n                       categorical_dim: int = 8) -> vz.ProblemStatement:\n  problem = create_continuous_problem(n_features // 2)\n  return create_categorical_problem(n_features // 2, categorical_dim, problem)\n\n\n# TODO: Change to bbob functions when they can support batching.\ndef sphere(x: np.ndarray) -> np.ndarray:\n  return -np.sum(np.square(x), axis=-1)\n\n\ndef rastrigin_d10(x: np.ndarray) -> np.ndarray:\n  return 10 * np.sum(\n      np.cos(2 * np.pi * x), axis=-1) - np.sum(\n          np.square(x), axis=-1)\n\n\nclass EagleOptimizerConvegenceTest(parameterized.TestCase):\n  \"\"\"Test optimizing an acquisition functions using vectorized Eagle Strategy.\n  \"\"\"\n\n  @absltest.skip(\"Test takes too long externally.\")\n  @parameterized.product(\n      create_problem_fn=[\n          create_continuous_problem,\n          create_categorical_problem,\n          create_mix_problem,\n      ],\n      n_features=[10, 20],\n      score_fn=[sphere, rastrigin_d10],\n  )\n  def test_converges(self, create_problem_fn, n_features, score_fn):\n    logging.info('Starting a new convergence test (n_features: %s)', n_features)\n    evaluations = 20_000\n    problem = create_problem_fn(n_features)", "metadata": {"task_id": "google_vizier/64", "ground_truth": "    converter = converters.TrialToArrayConverter.from_study_config(problem)", "fpath_tuple": ["google_vizier", "vizier", "_src", "algorithms", "optimizers", "eagle_optimizer_convergence_test.py"], "context_start_lineno": 22, "line_no": 107}}
{"prompt": "self, device=\"cpu\", seed=0):\n        # TODO: use tensor inputs instead of PIL, this is here just to leave the old expected_slices untouched\n        image = floats_tensor((1, 3, 32, 32), rng=random.Random(seed)).to(device)\n        image = image.cpu().permute(0, 2, 3, 1)[0]\n        init_image = Image.fromarray(np.uint8(image)).convert(\"RGB\").resize((64, 64))\n        mask_image = Image.fromarray(np.uint8(image + 4)).convert(\"RGB\").resize((64, 64))\n        example_image = Image.fromarray(np.uint8(image)).convert(\"RGB\").resize((32, 32))\n\n        if str(device).startswith(\"mps\"):\n            generator = torch.manual_seed(seed)\n        else:\n            generator = torch.Generator(device=device).manual_seed(seed)\n        inputs = {\n            \"example_image\": example_image,\n            \"image\": init_image,\n            \"mask_image\": mask_image,\n            \"generator\": generator,\n            \"num_inference_steps\": 2,\n            \"guidance_scale\": 6.0,\n            \"output_type\": \"numpy\",\n        }\n        return inputs\n\n    def test_paint_by_example_inpaint(self):\n        components = self.get_dummy_components()\n\n        # make sure here that pndm scheduler skips prk\n        pipe = PaintByExamplePipeline(**components)\n        pipe = pipe.to(\"cpu\")\n        pipe.set_progress_bar_config(disable=None)\n\n        inputs = self.get_dummy_inputs()\n        output = pipe(**inputs)\n        image = output.images\n\n        image_slice = image[0, -3:, -3:, -1]\n\n        assert image.shape == (1, 64, 64, 3)\n        expected_slice = np.array([0.4701, 0.5555, 0.3994, 0.5107, 0.5691, 0.4517, 0.5125, 0.4769, 0.4539])\n\n        assert np.abs(image_slice.flatten() - expected_slice).max() < 1e-2\n\n    def test_paint_by_example_image_tensor(self):\n        device = \"cpu\"\n        inputs = self.get_dummy_inputs()\n        inputs.pop(\"mask_image\")\n        image = self.convert_to_pt(inputs.pop(\"image\"))\n        mask_image = image.clamp(0, 1) / 2\n\n        # make sure here that pndm scheduler skips prk\n        pipe = PaintByExamplePipeline(**self.get_dummy_components())\n        pipe = pipe.to(device)\n        pipe.set_progress_bar_config(disable=None)\n\n        output = pipe(image=image, mask_image=mask_image[:, 0], **inputs)\n        out_1 = output.images\n\n        image = image.cpu().permute(0, 2, 3, 1)[0]\n        mask_image = mask_image.cpu().permute(0, 2, 3, 1)[0]\n\n        image = Image.fromarray(np.uint8(image)).convert(\"RGB\")\n        mask_image = Image.fromarray(np.uint8(mask_image)).convert(\"RGB\")\n\n        output = pipe(**self.get_dummy_inputs())\n        out_2 = output.images\n\n        assert out_1.shape == (1, 64, 64, 3)\n        assert np.abs(out_1.flatten() - out_2.flatten()).max() < 5e-2\n\n    def test_paint_by_example_inpaint_with_num_images_per_prompt(self):\n        device = \"cpu\"\n        pipe = PaintByExamplePipeline(**self.get_dummy_components())\n        pipe = pipe.to(device)\n        pipe.set_progress_bar_config(disable=None)", "metadata": {"task_id": "huggingface_diffusers/65", "ground_truth": "        inputs = self.get_dummy_inputs()", "fpath_tuple": ["huggingface_diffusers", "tests", "pipelines", "paint_by_example", "test_paint_by_example.py"], "context_start_lineno": 96, "line_no": 171}}
{"prompt": "\n            raise ValueError(f\"`prompt` has to be of type `str` or `list` but is {type(prompt)}\")\n\n        if strength < 0 or strength > 1:\n            raise ValueError(f\"The value of strength should in [0.0, 1.0] but is {strength}\")\n\n        if (callback_steps is None) or (\n            callback_steps is not None and (not isinstance(callback_steps, int) or callback_steps <= 0)\n        ):\n            raise ValueError(\n                f\"`callback_steps` has to be a positive integer but is {callback_steps} of type\"\n                f\" {type(callback_steps)}.\"\n            )\n\n        if prompt is not None and prompt_embeds is not None:\n            raise ValueError(\n                f\"Cannot forward both `prompt`: {prompt} and `prompt_embeds`: {prompt_embeds}. Please make sure to\"\n                \" only forward one of the two.\"\n            )\n        elif prompt is None and prompt_embeds is None:\n            raise ValueError(\n                \"Provide either `prompt` or `prompt_embeds`. Cannot leave both `prompt` and `prompt_embeds` undefined.\"\n            )\n        elif prompt is not None and (not isinstance(prompt, str) and not isinstance(prompt, list)):\n            raise ValueError(f\"`prompt` has to be of type `str` or `list` but is {type(prompt)}\")\n\n        if negative_prompt is not None and negative_prompt_embeds is not None:\n            raise ValueError(\n                f\"Cannot forward both `negative_prompt`: {negative_prompt} and `negative_prompt_embeds`:\"\n                f\" {negative_prompt_embeds}. Please make sure to only forward one of the two.\"\n            )\n\n        if prompt_embeds is not None and negative_prompt_embeds is not None:\n            if prompt_embeds.shape!= negative_prompt_embeds.shape:\n                raise ValueError(\n                    \"`prompt_embeds` and `negative_prompt_embeds` must have the same shape when passed directly, but\"\n                    f\" got: `prompt_embeds` {prompt_embeds.shape}!= `negative_prompt_embeds`\"\n                    f\" {negative_prompt_embeds.shape}.\"\n                )\n\n    # Copied from diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline.prepare_extra_step_kwargs\n    def prepare_extra_step_kwargs(self, generator, eta):\n        # prepare extra kwargs for the scheduler step, since not all schedulers have the same signature\n        # eta (\u03b7) is only used with the DDIMScheduler, it will be ignored for other schedulers.\n        # eta corresponds to \u03b7 in DDIM paper: https://arxiv.org/abs/2010.02502\n        # and should be between [0, 1]\n\n        accepts_eta = \"eta\" in set(inspect.signature(self.scheduler.step).parameters.keys())\n        extra_step_kwargs = {}\n        if accepts_eta:\n            extra_step_kwargs[\"eta\"] = eta\n\n        # check if the scheduler accepts generator\n        accepts_generator = \"generator\" in set(inspect.signature(self.scheduler.step).parameters.keys())\n        if accepts_generator:\n            extra_step_kwargs[\"generator\"] = generator\n        return extra_step_kwargs\n\n    # Copied from diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline.run_safety_checker\n    def run_safety_checker(self, image, device, dtype):\n        if self.safety_checker is not None:\n            safety_checker_input = self.feature_extractor(self.numpy_to_pil(image), return_tensors=\"pt\").to(device)\n            image, has_nsfw_concept = self.safety_checker(", "metadata": {"task_id": "huggingface_diffusers/61", "ground_truth": "                images=image, clip_input=safety_checker_input.pixel_values.to(dtype)", "fpath_tuple": ["huggingface_diffusers", "src", "diffusers", "pipelines", "stable_diffusion", "pipeline_cycle_diffusion.py"], "context_start_lineno": 405, "line_no": 468}}
{"prompt": "components)\n        ```\n\n        Returns:\n            A dictionary containing all the modules needed to initialize the pipeline.\n        \"\"\"\n        expected_modules, optional_parameters = self._get_signature_keys(self)\n        components = {\n            k: getattr(self, k) for k in self.config.keys() if not k.startswith(\"_\") and k not in optional_parameters\n        }\n\n        if set(components.keys())!= expected_modules:\n            raise ValueError(\n                f\"{self} has been incorrectly initialized or {self.__class__} is incorrectly implemented. Expected\"\n                f\" {expected_modules} to be defined, but {components} are defined.\"\n            )\n\n        return components\n\n    @staticmethod\n    def numpy_to_pil(images):\n        \"\"\"\n        Convert a numpy image or a batch of images to a PIL image.\n        \"\"\"\n        if images.ndim == 3:\n            images = images[None,...]\n        images = (images * 255).round().astype(\"uint8\")\n        if images.shape[-1] == 1:\n            # special case for grayscale (single channel) images\n            pil_images = [Image.fromarray(image.squeeze(), mode=\"L\") for image in images]\n        else:\n            pil_images = [Image.fromarray(image) for image in images]\n\n        return pil_images\n\n    def progress_bar(self, iterable=None, total=None):\n        if not hasattr(self, \"_progress_bar_config\"):\n            self._progress_bar_config = {}\n        elif not isinstance(self._progress_bar_config, dict):\n            raise ValueError(\n                f\"`self._progress_bar_config` should be of type `dict`, but is {type(self._progress_bar_config)}.\"\n            )\n\n        if iterable is not None:\n            return tqdm(iterable, **self._progress_bar_config)\n        elif total is not None:\n            return tqdm(total=total, **self._progress_bar_config)\n        else:\n            raise ValueError(\"Either `total` or `iterable` has to be defined.\")\n\n    def set_progress_bar_config(self, **kwargs):\n        self._progress_bar_config = kwargs\n\n    def enable_xformers_memory_efficient_attention(self, attention_op: Optional[Callable] = None):\n        r\"\"\"\n        Enable memory efficient attention as implemented in xformers.\n\n        When this option is enabled, you should observe lower GPU memory usage and a potential speed up at inference\n        time. Speed up at training time is not guaranteed.\n\n        Warning: When Memory Efficient Attention and Sliced attention are both enabled, the Memory Efficient Attention\n        is used.\n\n        Parameters:\n            attention_op (`Callable`, *optional*):\n                Override the default `None` operator for use as `op` argument to the\n                [`memory_efficient_attention()`](https://facebookresearch.github.io/xformers/components/ops.html#xformers.ops.memory_efficient_attention)\n                function of xFormers.\n\n        Examples:\n\n        ```py\n        >>> import torch\n        >>> from diffusers import DiffusionPipeline\n        >>> from xformers.ops import MemoryEfficientAttentionFlashAttentionOp\n\n        >>> pipe = DiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-2-1\", torch_dtype=torch.float16)\n        >>> pipe = pipe.to(\"cuda\")\n        >>> pipe.enable_xformers_memory_efficient_attention(attention_op=MemoryEfficientAttentionFlashAttentionOp)\n        >>> # Workaround for not accepting attention shape using VAE for Flash Attention\n        >>> pipe.vae.enable_xformers_memory_efficient_attention(attention_op=None)\n        ```\n        \"\"\"\n        self.set_use_memory_efficient_attention_xformers(True, attention_op)\n\n    def disable_xformers_memory_efficient_attention(self):\n        r\"\"\"\n        Disable memory efficient attention as implemented in xformers.\n        \"\"\"", "metadata": {"task_id": "huggingface_diffusers/191", "ground_truth": "        self.set_use_memory_efficient_attention_xformers(False)", "fpath_tuple": ["huggingface_diffusers", "src", "diffusers", "pipelines", "pipeline_utils.py"], "context_start_lineno": 786, "line_no": 875}}
{"prompt": "((1,) + self.shape_inputs)\n                ),\n                lik_log_var=self.reg_lik.model_manager.likelihood_log_variance_model.init(\n                    self.rng, jnp.zeros((1,) + self.shape_inputs)\n                ),\n            )\n        )\n\n        for batch_data in self.reg_data_arr:\n            batched_log_joint_prob1 = self.reg_lik._batched_log_joint_prob(\n                params, batch_data, n_data=batch_data[1].shape[0]\n            )\n            batched_log_joint_prob2 = self.reg_lik._batched_log_joint_prob(\n                params, batch_data, n_data=2 * batch_data[1].shape[0]\n            )\n            assert jnp.allclose(batched_log_joint_prob2, 2 * batched_log_joint_prob1)\n            assert jnp.array([batched_log_joint_prob1]).shape == (1,)\n\n            _, aux = self.reg_lik._batched_log_joint_prob(\n                params,\n                batch_data,\n                n_data=batch_data[1].shape[0],\n                return_aux=[\"outputs\"],\n            )\n            assert aux[\"outputs\"].shape == (self.n_inputs, 2 * self.output_dim)\n\n    def test_lik_log_joint_prob(self):\n        params = FrozenDict(\n            dict(\n                model=self.reg_lik.model_manager.model.init(\n                    self.rng, jnp.zeros((1,) + self.shape_inputs)\n                ),\n                lik_log_var=self.reg_lik.model_manager.likelihood_log_variance_model.init(\n                    self.rng, jnp.zeros((1,) + self.shape_inputs)\n                ),\n            )\n        )\n\n        log_probs = self.reg_lik.log_prob(params, self.reg_data_arr)\n        assert log_probs.shape == (self.n_inputs,)\n\n        log_probs = self.reg_lik.log_prob(params, self.reg_data_gen_fun)\n        assert log_probs.shape == (self.n_batches * self.batch_size,)\n\n    def test_sample(self):\n        params = FrozenDict(\n            dict(\n                model=self.reg_lik.model_manager.model.init(\n                    self.rng, jnp.zeros((1,) + self.shape_inputs)\n                ),\n                lik_log_var=self.reg_lik.model_manager.likelihood_log_variance_model.init(\n                    self.rng, jnp.zeros((1,) + self.shape_inputs)\n                ),\n            )\n        )\n\n        samples = self.reg_lik.sample(10, params, self.reg_inputs_arr)\n        assert samples.shape == (10, self.n_inputs, self.output_dim)\n\n        params = FrozenDict(\n            dict(\n                model=self.class_lik.model_manager.model.init(\n                    self.rng, jnp.zeros((1,) + self.shape_inputs)\n                ),\n            )\n        )\n\n        samples = self.class_lik.sample(10, params, self.class_inputs_arr)\n        assert samples.shape == (10, self.n_inputs)\n\n    def test_reg_stats(self):\n        params = FrozenDict(\n            dict(\n                model=self.reg_lik.model_manager.model.init(\n                    self.rng, jnp.zeros((1,) + self.shape_inputs)\n                ),\n                lik_log_var=self.reg_lik.model_manager.likelihood_log_variance_model.init(\n                    self.rng, jnp.zeros((1,) + self.shape_inputs)\n                ),\n            )\n        )", "metadata": {"task_id": "awslabs_fortuna/47", "ground_truth": "        assert self.reg_lik.mean(params, self.reg_inputs_arr).shape == (", "fpath_tuple": ["awslabs_fortuna", "tests", "fortuna", "prob_model", "test_likelihood.py"], "context_start_lineno": 104, "line_no": 186}}
{"prompt": "stopping_update(validation_metrics)\n\n    def test_early_stopping_update_ok_min(self):\n        validation_metrics_step1 = {\"metric1\": 1, \"metric2\": 2}\n        validation_metrics_step2 = {\"metric1\": 0.8, \"metric2\": 2}\n        validation_metrics_step3 = {\"metric1\": 0.6, \"metric2\": 2}\n        validation_metrics_step4 = {\"metric1\": 1.1, \"metric2\": 2}\n        validation_metrics_step5 = {\"metric1\": 1, \"metric2\": 2}\n\n        trainer = FakeTrainerWithEarlyStopping(\n            early_stopping_monitor=\"metric1\",\n            early_stopping_min_delta=0,\n            early_stopping_patience=1,\n            early_stopping_mode=\"min\",\n        )\n        improved = trainer.early_stopping_update(validation_metrics_step1)\n        self.assertTrue(improved)\n        improved = trainer.early_stopping_update(validation_metrics_step2)\n        self.assertTrue(improved)\n        improved = trainer.early_stopping_update(validation_metrics_step3)\n        self.assertTrue(improved)\n        improved = trainer.early_stopping_update(validation_metrics_step4)\n        self.assertFalse(improved)\n        self.assertFalse(trainer._early_stopping.should_stop)\n        improved = trainer.early_stopping_update(validation_metrics_step5)\n        self.assertFalse(improved)\n        self.assertTrue(trainer._early_stopping.should_stop)\n\n        trainer = FakeTrainerWithEarlyStopping(\n            early_stopping_monitor=\"metric1\",\n            early_stopping_min_delta=0,\n            early_stopping_patience=2,\n            early_stopping_mode=\"min\",\n        )\n        improved = trainer.early_stopping_update(validation_metrics_step1)\n        self.assertTrue(improved)\n        improved = trainer.early_stopping_update(validation_metrics_step2)\n        self.assertTrue(improved)\n        improved = trainer.early_stopping_update(validation_metrics_step3)\n        self.assertTrue(improved)\n        improved = trainer.early_stopping_update(validation_metrics_step4)\n        self.assertFalse(improved)\n        self.assertFalse(trainer._early_stopping.should_stop)\n        improved = trainer.early_stopping_update(validation_metrics_step5)\n        self.assertFalse(improved)\n        self.assertFalse(trainer._early_stopping.should_stop)\n        improved = trainer.early_stopping_update(validation_metrics_step5)\n        self.assertFalse(improved)\n        self.assertTrue(trainer._early_stopping.should_stop)\n\n    def test_early_stopping_update_ok_max(self):\n        validation_metrics_step1 = {\"metric1\": 1, \"metric2\": 2}\n        validation_metrics_step2 = {\"metric1\": 1.6, \"metric2\": 2}\n        validation_metrics_step3 = {\"metric1\": 1.8, \"metric2\": 2}\n        validation_metrics_step4 = {\"metric1\": 0.1, \"metric2\": 2}\n        validation_metrics_step5 = {\"metric1\": 0.2, \"metric2\": 2}\n\n        trainer = FakeTrainerWithEarlyStopping(\n            early_stopping_monitor=\"metric1\",\n            early_stopping_min_delta=0,\n            early_stopping_patience=2,\n            early_stopping_mode=\"max\",\n        )\n        improved = trainer.early_stopping_update(validation_metrics_step1)\n        self.assertTrue(improved)", "metadata": {"task_id": "awslabs_fortuna/76", "ground_truth": "        improved = trainer.early_stopping_update(validation_metrics_step2)", "fpath_tuple": ["awslabs_fortuna", "tests", "fortuna", "test_mixin.py"], "context_start_lineno": 186, "line_no": 251}}
{"prompt": "_or_epoch == 'batch':\n                    num_sample = self._cfg.train.local_update_steps * \\\n                                 self._cfg.dataloader.batch_size\n                else:\n                    num_sample = self._cfg.train.local_update_steps * \\\n                                 len(self.data['train'])\n                join_in_info['num_sample'] = num_sample\n                if self._cfg.trainer.type == 'nodefullbatch_trainer':\n                    join_in_info['num_sample'] = self.data['data'].x.shape[0]\n            elif requirement.lower() == 'client_resource':\n                assert self.comm_bandwidth is not None and self.comp_speed \\\n                       is not None, \"The requirement join_in_info \" \\\n                                    \"'client_resource' does not exist.\"\n                join_in_info['client_resource'] = self.model_size / \\\n                    self.comm_bandwidth + self.comp_speed\n            else:\n                raise ValueError(\n                    'Fail to get the join in information with type {}'.format(\n                        requirement))\n        self.comm_manager.send(\n            Message(msg_type='join_in_info',\n                    sender=self.ID,\n                    receiver=[self.server_id],\n                    state=self.state,\n                    timestamp=timestamp,\n                    content=join_in_info))\n\n    def callback_funcs_for_address(self, message: Message):\n        \"\"\"\n        The handling function for receiving other clients' IP addresses, \\\n        which is used for constructing a complex topology\n\n        Arguments:\n            message: The received message\n        \"\"\"\n        content = message.content\n        for neighbor_id, address in content.items():\n            if int(neighbor_id)!= self.ID:\n                self.comm_manager.add_neighbors(neighbor_id, address)\n\n    def callback_funcs_for_evaluate(self, message: Message):\n        \"\"\"\n        The handling function for receiving the request of evaluating\n\n        Arguments:\n            message: The received message\n        \"\"\"\n        sender, timestamp = message.sender, message.timestamp\n        self.state = message.state\n        if message.content is not None:\n            self.trainer.update(message.content,\n                                strict=self._cfg.federate.share_local_model)\n        if self.early_stopper.early_stopped and self._cfg.federate.method in [\n                \"local\", \"global\"\n        ]:\n            metrics = list(self.best_results.values())[0]\n        else:\n            metrics = {}\n            if self._cfg.finetune.before_eval:\n                self.trainer.finetune()\n            for split in self._cfg.eval.split:\n                # TODO: The time cost of evaluation is not considered here\n                eval_metrics = self.trainer.evaluate(\n                    target_data_split_name=split)\n\n                if self._cfg.federate.mode == 'distributed':\n                    logger.info(\n                        self._monitor.format_eval_res(eval_metrics,\n                                                      rnd=self.state,\n                                                      role='Client #{}'.format(\n                                                          self.ID),\n                                                      return_raw=True))\n\n                metrics.update(**eval_metrics)\n\n            formatted_eval_res = self._monitor.format_eval_res(\n                metrics,\n                rnd=self.state,\n                role='Client #{}'.format(self.ID),\n                forms=['raw'],\n                return_raw=True)\n            self._monitor.update_best_result(self.best_results,\n                                             formatted_eval_res['Results_raw'],\n                                             results_type=f\"client #{self.ID}\")\n            self.history_results = merge_dict_of_results(\n                self.history_results, formatted_eval_res['Results_raw'])\n            self.early_stopper.track_and_check(self.history_results[\n                self._cfg.eval.best_res_update_round_wise_key])", "metadata": {"task_id": "alibaba_FederatedScope/34", "ground_truth": "        self.comm_manager.send(\n            Message(msg_type='metrics',\n                    sender=self.ID,\n                    receiver=[sender],\n                    state=self.state,\n                    timestamp=timestamp,\n                    content=metrics))", "fpath_tuple": ["alibaba_FederatedScope", "federatedscope", "core", "workers", "client.py"], "context_start_lineno": 407, "line_no": 496}}
{"prompt": "env.decode_obs(rollout)\n        assert \"reco_observation\" in rollout.keys()\n        # second pass\n        tensordict = mb_env.decode_obs(mb_env.reset(), compute_latents=True)\n        assert \"reco_observation\" in tensordict.keys()\n\n    @pytest.mark.parametrize(\"imagination_horizon\", [3, 5])\n    @pytest.mark.parametrize(\"discount_loss\", [True, False])\n    def test_dreamer_actor(self, device, imagination_horizon, discount_loss):\n        tensordict = self._create_actor_data(2, 3, 10, 5).to(device)\n        mb_env = self._create_mb_env(10, 5).to(device)\n        actor_model = self._create_actor_model(10, 5).to(device)\n        value_model = self._create_value_model(10, 5).to(device)\n        loss_module = DreamerActorLoss(\n            actor_model,\n            value_model,\n            mb_env,\n            imagination_horizon=imagination_horizon,\n            discount_loss=discount_loss,\n        )\n        loss_td, fake_data = loss_module(tensordict)\n        assert not fake_data.requires_grad\n        assert fake_data.shape == torch.Size([tensordict.numel(), imagination_horizon])\n        if discount_loss:\n            assert loss_module.discount_loss\n\n        assert loss_td.get(\"loss_actor\") is not None\n        loss = loss_td.get(\"loss_actor\")\n        loss.backward()\n        grad_is_zero = True\n        for name, param in loss_module.named_parameters():\n            if param.grad is not None:\n                valid_gradients = not (\n                    torch.isnan(param.grad).any() or torch.isinf(param.grad).any()\n                )\n                grad_is_zero = (\n                    grad_is_zero and torch.sum(torch.pow((param.grad), 2)) == 0\n                )\n                if not valid_gradients:\n                    raise ValueError(f\"Invalid gradients for {name}\")\n        if grad_is_zero:\n            raise ValueError(\"Gradients are zero\")\n        loss_module.zero_grad()\n\n    @pytest.mark.parametrize(\"discount_loss\", [True, False])\n    def test_dreamer_value(self, device, discount_loss):\n        tensordict = self._create_value_data(2, 3, 10, 5).to(device)\n        value_model = self._create_value_model(10, 5).to(device)\n        loss_module = DreamerValueLoss(value_model, discount_loss=discount_loss)\n        loss_td, fake_data = loss_module(tensordict)\n        assert loss_td.get(\"loss_value\") is not None\n        assert not fake_data.requires_grad\n        loss = loss_td.get(\"loss_value\")\n        loss.backward()\n        grad_is_zero = True\n        for name, param in loss_module.named_parameters():\n            if param.grad is not None:\n                valid_gradients = not (\n                    torch.isnan(param.grad).any() or torch.isinf(param.grad).any()\n                )\n                grad_is_zero = (\n                    grad_is_zero and torch.sum(torch.pow((param.grad), 2)) == 0\n                )\n                if not valid_gradients:\n                    raise ValueError(f\"Invalid gradients for {name}\")\n        if grad_is_zero:\n            raise ValueError(\"Gradients are zero\")\n        loss_module.zero_grad()\n\n\ndef test_hold_out():\n    net = torch.nn.Linear(3, 4)\n    x = torch.randn(1, 3)\n    x_rg = torch.randn(1, 3, requires_grad=True)\n    y = net(x)\n    assert y.requires_grad", "metadata": {"task_id": "pytorch_rl/32", "ground_truth": "    with hold_out_net(net):", "fpath_tuple": ["pytorch_rl", "test", "test_cost.py"], "context_start_lineno": 2722, "line_no": 2798}}
{"prompt": "# coding=utf-8\n# Copyright 2022 HuggingFace Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport gc\nimport unittest\n\nimport numpy as np\nimport torch\n\nfrom diffusers import DanceDiffusionPipeline, IPNDMScheduler, UNet1DModel\nfrom diffusers.utils import slow, torch_device\nfrom diffusers.utils.testing_utils import require_torch_gpu\n\nfrom...test_pipelines_common import PipelineTesterMixin\n\n\ntorch.backends.cuda.matmul.allow_tf32 = False\n\n\nclass DanceDiffusionPipelineFastTests(PipelineTesterMixin, unittest.TestCase):\n    pipeline_class = DanceDiffusionPipeline\n    test_attention_slicing = False\n    test_cpu_offload = False\n\n    def get_dummy_components(self):\n        torch.manual_seed(0)\n        unet = UNet1DModel(\n            block_out_channels=(32, 32, 64),\n            extra_in_channels=16,\n            sample_size=512,\n            sample_rate=16_000,\n            in_channels=2,\n            out_channels=2,\n            flip_sin_to_cos=True,\n            use_timestep_embedding=False,\n            time_embedding_type=\"fourier\",\n            mid_block_type=\"UNetMidBlock1D\",\n            down_block_types=(\"DownBlock1DNoSkip\", \"DownBlock1D\", \"AttnDownBlock1D\"),\n            up_block_types=(\"AttnUpBlock1D\", \"UpBlock1D\", \"UpBlock1DNoSkip\"),\n        )\n        scheduler = IPNDMScheduler()\n\n        components = {\n            \"unet\": unet,\n            \"scheduler\": scheduler,\n        }\n        return components\n\n    def get_dummy_inputs(self, device, seed=0):\n        if str(device).startswith(\"mps\"):\n            generator = torch.manual_seed(seed)\n        else:\n            generator = torch.Generator(device=device).manual_seed(seed)\n        inputs = {\n            \"batch_size\": 1,\n            \"generator\": generator,\n            \"num_inference_steps\": 4,\n        }\n        return inputs\n\n    def test_dance_diffusion(self):\n        device = \"cpu\"  # ensure determinism for the device-dependent torch.Generator\n        components = self.get_dummy_components()\n        pipe = DanceDiffusionPipeline(**components)\n        pipe = pipe.to(device)\n        pipe.set_progress_bar_config(disable=None)\n\n        inputs = self.get_dummy_inputs(device)\n        output = pipe(**inputs)\n        audio = output.audios\n\n        audio_slice = audio[0, -3:, -3:]\n\n        assert audio.shape == (1, 2, components[\"unet\"].sample_size)\n        expected_slice = np.array([-0.7265, 1.0000, -0.8388, 0.1175, 0.9498, -1.0000])\n        assert np.abs(audio_slice.flatten() - expected_slice).max() < 1e-2\n\n\n@slow\n@require_torch_gpu\nclass PipelineIntegrationTests(unittest.TestCase):\n    def tearDown(self):\n        # clean up the VRAM after each test", "metadata": {"task_id": "huggingface_diffusers/54", "ground_truth": "        super().tearDown()", "fpath_tuple": ["huggingface_diffusers", "tests", "pipelines", "dance_diffusion", "test_dance_diffusion.py"], "context_start_lineno": 0, "line_no": 95}}
{"prompt": "mentedError(f'Client {self.ID} do not have y.')\n\n        en_norm_feat = message.content\n        sender = message.sender\n        self.msg_buffer['encrypted_norm_feat'][sender] = en_norm_feat\n\n        if len(self.msg_buffer['encrypted_norm_feat'].keys()) == \\\n                len([each for each in self.comm_manager.neighbors if each!=\n                    self.server_id]):\n            threshold = worker._cfg.feat_engr.selec_threshold\n            merged_feat, merged_y = merge_splits_feat(worker.data)\n\n            # Filter local feature\n            if merged_feat is not None:\n                feat_corrcoef = []\n                for i in range(merged_feat.shape[1]):\n                    feat_corrcoef.append(\n                        np.sum(\n                            (merged_feat[:, i] - np.mean(merged_feat[:, i])) *\n                            (merged_y - np.mean(merged_y)) /\n                            merged_feat.shape[0] /\n                            (np.std(merged_feat[:, i]) * np.std(merged_y))))\n                filtered_col = (np.array(feat_corrcoef) <\n                                threshold).nonzero()[0]\n                logger.info(f'The eliminated feature of Client {self.ID} is'\n                            f' {filtered_col}')\n                # Filter feature\n                for split in ['train_data', 'val_data', 'test_data']:\n                    if hasattr(worker.data, split):\n                        split_data = getattr(worker.data, split)\n                        if split_data is not None and 'x' in split_data:\n                            split_data['x'] = \\\n                                np.delete(split_data['x'], filtered_col,\n                                          axis=1)\n            self.comm_manager.send(\n                Message(msg_type='feat_dim',\n                        sender=self.ID,\n                        receiver=[self.server_id],\n                        content=(split_data['x'].shape[1], filtered_col)))\n\n            norm_y = (merged_y - np.mean(merged_y)) / np.std(merged_y)\n            # Calculate correlation coefficient\n            for sender, en_norm_feat in \\\n                    self.msg_buffer['encrypted_norm_feat'].items():\n                en_feat_corrcoef = []\n                for i in range(np.array(en_norm_feat).shape[1]):\n                    en_feat_corrcoef.append(\n                        np.sum(np.array(en_norm_feat)[:, i] * norm_y))\n\n                # Send to server for decryption\n                logger.info(f'Client {self.ID} send en_feat_corrcoef to'\n                            f' {self.server_id}.')\n                self.comm_manager.send(\n                    Message(msg_type='en_feat_corrcoef',\n                            sender=self.ID,\n                            receiver=[self.server_id],\n                            content=(sender, en_feat_corrcoef)))\n\n    def callbacks_funcs_for_feat_corrcoef(self, message: Message):\n        feat_corrcoef = message.content\n\n        threshold = worker._cfg.feat_engr.selec_threshold\n        filtered_col = (np.array(feat_corrcoef) < threshold).nonzero()[0]\n        logger.info(f'The eliminated feature of Client {self.ID} is'\n                    f' {filtered_col}')\n\n        # Filter feature\n        for split in ['train_data', 'val_data', 'test_data']:\n            if hasattr(worker.data, split):\n                split_data = getattr(worker.data, split)\n                if split_data is not None and 'x' in split_data:\n                    split_data['x'] = \\\n                        np.delete(split_data['x'], filtered_col, axis=1)\n\n        self.comm_manager.send(", "metadata": {"task_id": "alibaba_FederatedScope/149", "ground_truth": "            Message(msg_type='feat_dim',\n                    sender=self.ID,\n                    receiver=[self.server_id],\n                    content=(split_data['x'].shape[1], filtered_col)))", "fpath_tuple": ["alibaba_FederatedScope", "federatedscope", "core", "feature", "vfl", "selection", "correlation_filter.py"], "context_start_lineno": 156, "line_no": 231}}
{"prompt": "            inputs_loader,\n            mutable,\n            calib_params,\n            calib_mutable,\n            distribute,\n            **kwargs\n        )\n\n        samples = self.prob_output_layer.sample(\n            n_target_samples, outputs, rng=rng, **kwargs\n        )\n        if len(return_aux) > 0:\n            return samples, dict(outputs=outputs)\n        return samples\n\n    def _batched_sample(\n        self,\n        n_target_samples: int,\n        params: Params,\n        inputs: Array,\n        mutable: Optional[Mutable] = None,\n        calib_params: Optional[CalibParams] = None,\n        calib_mutable: Optional[CalibMutable] = None,\n        return_aux: Optional[List[str]] = None,\n        rng: Optional[PRNGKeyArray] = None,\n        **kwargs\n    ) -> Union[jnp.ndarray, Tuple[jnp.ndarray, dict]]:\n        if return_aux is None:\n            return_aux = []\n        supported_aux = [\"outputs\"]\n        unsupported_aux = [s for s in return_aux if s not in supported_aux]\n        if sum(unsupported_aux) > 0:\n            raise Exception(\n                \"\"\"The auxiliary objects {} are unknown. Please make sure that all elements of `return_aux` \n                            belong to the following list: {}\"\"\".format(\n                    unsupported_aux, supported_aux\n                )\n            )\n\n        outputs = self._get_batched_calibrated_outputs(\n            params, inputs, mutable, calib_params, calib_mutable, **kwargs\n        )\n\n        samples = self.prob_output_layer.sample(\n            n_target_samples, outputs, rng=rng, **kwargs\n        )\n        if len(return_aux) > 0:\n            return samples, dict(outputs=outputs)\n        return samples\n\n    def get_calibrated_outputs(\n        self,\n        params: Params,\n        inputs_loader: InputsLoader,\n        mutable: Optional[Mutable] = None,\n        calib_params: Optional[CalibParams] = None,\n        calib_mutable: Optional[CalibMutable] = None,\n        distribute: bool = True,\n        **kwargs\n    ) -> jnp.ndarray:\n        \"\"\"\n        Compute the outputs and their calibrated version.\n\n        Parameters\n        ----------\n        params : Params\n            The random parameters of the probabilistic model.\n        inputs_loader : InputsLoader\n            A loader of input data points.\n        mutable : Optional[Mutable]\n            The mutable objects used to evaluate the models.\n        calib_params : Optional[CalibParams]\n            The calibration parameters of the probabilistic model.\n        calib_mutable : Optional[CalibMutable]\n            The calibration mutable objects used to evaluate the calibrators.\n        distribute: bool\n            Whether to distribute computation over multiple devices, if available.\n\n        Returns\n        -------\n        jnp.ndarray\n            The calibrated outputs.\n        \"\"\"\n        outputs = self.get_outputs(params, inputs_loader, mutable, distribute, **kwargs)\n\n        if self.output_calib_manager is not None:\n            outputs = self.output_calib_manager.apply(\n                params=calib_params[\"output_calibrator\"]\n                if calib_params is not None\n                else None,\n                mutable=calib_mutable[\"output_calibrator\"]\n                if calib_mutable is not None\n                else None,\n                outputs=outputs,\n                **kwargs\n            )\n        return outputs\n\n    def _get_batched_calibrated_outputs(\n        self,\n        params: Params,\n        inputs: Array,\n        mutable: Optional[Mutable] = None,\n        calib_params: Optional[CalibParams] = None,\n        calib_mutable: Optional[CalibMutable] = None,\n        **kwargs\n    ) -> jnp.ndarray:", "metadata": {"task_id": "awslabs_fortuna/7", "ground_truth": "        outputs = self.model_manager.apply(params, inputs, mutable, **kwargs)", "fpath_tuple": ["awslabs_fortuna", "fortuna", "prob_model", "likelihood", "base.py"], "context_start_lineno": 299, "line_no": 406}}
{"prompt": "import time\nimport signal\nimport pytest\nimport torch\nimport numpy as np\n\nfrom..base_env_manager import BaseEnvManager, EnvState\n\n\n@pytest.mark.unittest\nclass TestBaseEnvManager:\n\n    def test_naive(self, setup_base_manager_cfg):\n        env_fn = setup_base_manager_cfg.pop('env_fn')\n        env_manager = BaseEnvManager(env_fn, setup_base_manager_cfg)\n        env_manager.seed([314 for _ in range(env_manager.env_num)])\n        assert env_manager._closed\n        obs = env_manager.launch(reset_param={i: {'stat':'stat_test'} for i in range(env_manager.env_num)})\n        assert all([env_manager._env_states[env_id] == EnvState.RUN for env_id in range(env_manager.env_num)])\n        # Test basic\n        name = env_manager._name\n        assert len(name) == env_manager.env_num\n        assert all([isinstance(n, str) for n in name])\n        name = env_manager.name\n        assert len(name) == env_manager.env_num\n        assert all([isinstance(n, str) for n in name])\n        assert env_manager._max_retry == 5\n        assert env_manager._reset_timeout == 10\n        assert all([s == 314 for s in env_manager._seed])\n        assert all([s =='stat_test'] for s in env_manager._stat)\n        # Test arribute\n        with pytest.raises(AttributeError):\n            _ = env_manager.xxx\n        with pytest.raises(RuntimeError):\n            env_manager.user_defined()\n        # Test step\n        count = 1\n        start_time = time.time()\n        while not env_manager.done:\n            env_id = env_manager.ready_obs.keys()\n            action = {i: np.random.randn(4) for i in env_id}\n            timestep = env_manager.step(action)\n            assert len(timestep) == len(env_id)\n            print('Count {}'.format(count))\n            print([v.info for v in timestep.values()])\n            print([v.done for v in timestep.values()])\n            count += 1\n        end_time = time.time()\n        print('total step time: {}'.format(end_time - start_time))\n        assert all([env_manager._env_states[env_id] == EnvState.DONE for env_id in range(env_manager.env_num)])\n        assert all([c == setup_base_manager_cfg.episode_num for c in env_manager._env_episode_count.values()])\n        # Test close\n        env_manager.close()\n        assert env_manager._closed\n        assert all([not env_manager._envs[env_id]._launched for env_id in range(env_manager.env_num)])\n        assert all([env_manager._env_states[env_id] == EnvState.VOID for env_id in range(env_manager.env_num)])\n        with pytest.raises(AssertionError):", "metadata": {"task_id": "opendilab_ACE/66", "ground_truth": "            env_manager.reset([])", "fpath_tuple": ["opendilab_ACE", "ding", "envs", "env_manager", "tests", "test_base_env_manager.py"], "context_start_lineno": 0, "line_no": 57}}
{"prompt": "new__(\n        cls,\n        *args,\n        observation_spec=None,\n        action_spec=None,\n        input_spec=None,\n        reward_spec=None,\n        from_pixels=False,\n        categorical_action_encoding=False,\n        **kwargs,\n    ):\n        batch_size = kwargs.setdefault(\"batch_size\", torch.Size([]))\n        size = cls.size = 7\n        if observation_spec is None:\n            cls.out_key = \"observation\"\n            observation_spec = CompositeSpec(\n                observation=UnboundedContinuousTensorSpec(\n                    shape=torch.Size([*batch_size, size])\n                ),\n                observation_orig=UnboundedContinuousTensorSpec(\n                    shape=torch.Size([*batch_size, size])\n                ),\n                shape=batch_size,\n            )\n        if action_spec is None:\n            action_spec_cls = (\n                DiscreteTensorSpec\n                if categorical_action_encoding\n                else OneHotDiscreteTensorSpec\n            )\n            action_spec = action_spec_cls(*batch_size, 7)\n        if reward_spec is None:\n            reward_spec = UnboundedContinuousTensorSpec(shape=(1,))\n\n        if input_spec is None:\n            cls._out_key = \"observation_orig\"\n            input_spec = CompositeSpec(\n                **{\n                    cls._out_key: observation_spec[\"observation\"],\n                    \"action\": action_spec,\n                }\n            )\n        cls._reward_spec = reward_spec\n        cls._observation_spec = observation_spec\n        cls._input_spec = input_spec\n        cls.from_pixels = from_pixels\n        cls.categorical_action_encoding = categorical_action_encoding\n        return super().__new__(*args, **kwargs)\n\n    def _get_in_obs(self, obs):\n        return obs\n\n    def _get_out_obs(self, obs):\n        return obs\n\n    def _reset(self, tensordict: TensorDictBase = None) -> TensorDictBase:\n        self.counter += 1\n        state = torch.zeros(self.size) + self.counter\n        if tensordict is None:\n            tensordict = TensorDict({}, self.batch_size, device=self.device)\n        tensordict = tensordict.select().set(self.out_key, self._get_out_obs(state))\n        tensordict = tensordict.set(self._out_key, self._get_out_obs(state))\n        tensordict.set(\"done\", torch.zeros(*tensordict.shape, 1, dtype=torch.bool))\n        return tensordict\n\n    def _step(\n        self,\n        tensordict: TensorDictBase,\n    ) -> TensorDictBase:\n        tensordict = tensordict.to(self.device)\n        a = tensordict.get(\"action\")\n\n        if not self.categorical_action_encoding:\n            assert (a.sum(-1) == 1).all()\n\n        obs = self._get_in_obs(tensordict.get(self._out_key)) + a / self.maxstep\n        tensordict = tensordict.select()  # empty tensordict\n\n        tensordict.set(self.out_key, self._get_out_obs(obs))\n        tensordict.set(self._out_key, self._get_out_obs(obs))\n\n        done = torch.isclose(obs, torch.ones_like(obs) * (self.counter + 1))\n        reward = done.any(-1).unsqueeze(-1)\n        # set done to False\n        done = torch.zeros_like(done).all(-1).unsqueeze(-1)\n        tensordict.set(\"reward\", reward.to(torch.get_default_dtype()))", "metadata": {"task_id": "pytorch_rl/130", "ground_truth": "        tensordict.set(\"done\", done)", "fpath_tuple": ["pytorch_rl", "test", "mocking_classes.py"], "context_start_lineno": 332, "line_no": 418}}
{"prompt": "            assert 1 <= len(action) <= len(obs)\n            print('act', action.keys())\n            timestep = env_manager.step(action)\n            data_count += len(timestep)\n            assert len(timestep) >= 1\n            print('timestep', timestep.keys(), timestep, len(timestep))\n            for k, t in timestep.items():\n                if t.done:\n                    print('env{} finish episode{}'.format(k, env_count[k]))\n                    env_count[k] += 1\n        assert all([c == setup_async_manager_cfg.episode_num for c in env_count])\n        assert data_count == sum(env_manager._data_count)\n        assert all([env_manager._env_states[env_id] == EnvState.DONE for env_id in range(env_manager.env_num)])\n        end_time = time.time()\n        print('total step time: {}'.format(end_time - start_time))\n\n        # Test close\n        env_manager.close()\n        assert env_manager._closed\n        with pytest.raises(AssertionError):\n            env_manager.reset([])\n        with pytest.raises(AssertionError):\n            env_manager.step([])\n\n    @pytest.mark.unittest\n    def test_error(self, setup_sync_manager_cfg):\n        env_fn = setup_sync_manager_cfg.pop('env_fn')\n        env_manager = SyncSubprocessEnvManager(env_fn, setup_sync_manager_cfg)\n        # Test reset error\n        with pytest.raises(AssertionError):\n            env_manager.reset(reset_param={i: {'stat':'stat_test'} for i in range(env_manager.env_num)})\n        with pytest.raises(RuntimeError):\n            obs = env_manager.launch(reset_param={i: {'stat': 'error'} for i in range(env_manager.env_num)})\n        assert env_manager._closed\n        time.sleep(0.5)  # necessary time interval\n        obs = env_manager.launch(reset_param={i: {'stat':'stat_test'} for i in range(env_manager.env_num)})\n        assert not env_manager._closed\n\n        timestep = env_manager.step({i: np.random.randn(4) for i in range(env_manager.env_num)})\n        assert len(timestep) == env_manager.env_num\n\n        # Test step catched error\n        action = {i: np.random.randn(4) for i in range(env_manager.env_num)}\n        action[0] = 'catched_error'\n        assert not env_manager._closed\n        timestep = env_manager.step(action)\n        assert not env_manager._closed\n\n        assert timestep[0].info['abnormal']\n        assert all(['abnormal' not in timestep[i].info for i in range(1, env_manager.env_num)])\n        assert env_manager._env_states[0] == EnvState.ERROR\n        assert len(env_manager.ready_obs) == 3\n        # wait for reset\n        env_manager.reset({0: {'stat':'stat_test'}})\n        while not len(env_manager.ready_obs) == env_manager.env_num:\n            time.sleep(0.1)\n        assert env_manager._env_states[0] == EnvState.RUN\n        assert len(env_manager.ready_obs) == 4\n        timestep = env_manager.step({i: np.random.randn(4) for i in range(env_manager.env_num)})\n\n        # Test step error\n        action[0] = 'error'\n        with pytest.raises(Exception):\n            timestep = env_manager.step(action)\n        assert env_manager._closed\n\n        env_manager.close()\n        with pytest.raises(AssertionError):  # Assert env manager is not closed", "metadata": {"task_id": "opendilab_ACE/36", "ground_truth": "            env_manager.reset([])", "fpath_tuple": ["opendilab_ACE", "ding", "envs", "env_manager", "tests", "test_subprocess_env_manager.py"], "context_start_lineno": 47, "line_no": 115}}
{"prompt": ": Optional[str] = None,\n      trial_ids: Optional[Iterable[int]] = None,\n      min_trial_id: Optional[int] = None,\n      max_trial_id: Optional[int] = None,\n      status_matches: Optional[vz.TrialStatus] = None,\n      include_intermediate_measurements: bool = True) -> List[vz.Trial]:\n    self._check_study_guid(study_guid)\n    min_trial_id = min_trial_id or 1\n    max_trial_id = max_trial_id or (len(self._trials))\n    trials = [\n        t for t in self._trials[min_trial_id - 1:max_trial_id]\n        if (status_matches is None or t.status == status_matches)\n    ]\n    if trial_ids is not None:\n      trial_ids = set(trial_ids)\n      trials = [t for t in trials if t.id in trial_ids]\n    return trials\n\n  def CheckCancelled(self, note: Optional[str] = None) -> None:\n    pass\n\n  def TimeRemaining(self) -> datetime.timedelta:\n    return datetime.timedelta(seconds=100.0)\n\n  def _UpdateMetadata(self, delta: vz.MetadataDelta) -> None:\n    \"\"\"Assign metadata to trials.\"\"\"\n    for ns in delta.on_study.namespaces():\n      self.study_config.metadata.abs_ns(ns).update(delta.on_study.abs_ns(ns))\n\n    for tid, metadatum in delta.on_trials.items():\n      if not tid > 0:\n        raise ValueError(f'Bad Trial Id: {tid}')\n      for ns in metadatum.namespaces():\n        self._trials[tid - 1].metadata.abs_ns(ns).update(metadatum.abs_ns(ns))\n\n  # TODO: Return `count` trials for multi-objectives, when\n  # `count` exceeds the size of the Pareto frontier.\n  def GetBestTrials(self, *, count: Optional[int] = None) -> List[vz.Trial]:\n    \"\"\"Returns optimal trials.\n\n    Single-objective study:\n      * If `count` is unset, returns all tied top trials.\n      * If `count` is set, returns top `count` trials, breaking ties\n           arbitrarily.\n\n    Multi-objective study:\n      * If `count` is unset, returns all Pareto optimal trials.\n      * If `count` is set, returns up to `count` of Pareto optimal trials that\n          are arbitrarily selected.\n\n    Args:\n      count: If unset, returns Pareto optimal trials only. If set, returns the\n        top \"count\" trials.\n\n    Returns:\n      Best trials.\n    \"\"\"\n    if not self.study_config.metric_information.of_type(\n        vz.MetricType.OBJECTIVE):\n      raise ValueError('Requires at least one objective metric.')\n    if self.study_config.metric_information.of_type(vz.MetricType.SAFETY):\n      raise ValueError('Cannot work with safe metrics.')\n\n    converter = converters.TrialToArrayConverter.from_study_config(\n        self.study_config,\n        flip_sign_for_minimization_metrics=True,\n        dtype=np.float32)\n\n    if len(self.study_config.metric_information) == 1:\n      # Single metric: Sort and take top N.\n      count = count or 1  # Defaults to 1.\n      labels = converter.to_labels(self._trials).squeeze()\n      sorted_idx = np.argsort(-labels)  # np.argsort sorts in ascending order.\n      return list(np.asarray(self._trials)[sorted_idx[:count]])\n    else:\n      algorithm = multimetric.FastParetoOptimalAlgorithm()\n      is_optimal = algorithm.is_pareto_optimal(", "metadata": {"task_id": "google_vizier/59", "ground_truth": "          points=converter.to_labels(self._trials))", "fpath_tuple": ["google_vizier", "vizier", "_src", "pythia", "local_policy_supporters.py"], "context_start_lineno": 79, "line_no": 156}}
{"prompt": "Model(self.class_output_dim),\n                posterior_approximator=MAPPosteriorApproximator(),\n                output_calibrator=ClassificationTemperatureScaler(),\n            )\n            status = prob_class_map.train(\n                train_data_loader=self.class_train_data_loader,\n                calib_data_loader=self.class_val_data_loader,\n                val_data_loader=self.class_val_data_loader,\n                map_fit_config=self.class_fit_config_nodir_nodump,\n                fit_config=self.class_fit_config_dir_dump(tmp_dir),\n                calib_config=self.class_calib_config_nodir_nodump,\n            )\n            status = prob_class.train(\n                train_data_loader=self.class_train_data_loader,\n                calib_data_loader=self.class_val_data_loader,\n                val_data_loader=self.class_val_data_loader,\n                map_fit_config=self.class_fit_config_nodir_nodump,\n                fit_config=self.class_fit_config_restore(tmp_dir),\n                calib_config=self.class_calib_config_nodir_nodump,\n            )\n\n            # load state\n            prob_class.load_state(checkpoint_path=tmp_dir)\n\n            # save state\n            prob_class.save_state(checkpoint_path=tmp_dir)\n\n    def test_dryrun_reg_swag(self):\n        with tempfile.TemporaryDirectory() as tmp_dir:\n            prob_reg = ProbRegressor(\n                model=MyModel(self.reg_output_dim),\n                likelihood_log_variance_model=MyModel(self.reg_output_dim),\n                posterior_approximator=SWAGPosteriorApproximator(rank=2),\n                output_calibrator=RegressionTemperatureScaler(),\n            )\n            # no save dir, no dump\n            status = prob_reg.train(\n                train_data_loader=self.reg_train_data_loader,\n                calib_data_loader=self.reg_val_data_loader,\n                val_data_loader=self.reg_val_data_loader,\n                map_fit_config=self.reg_fit_config_nodir_nodump,\n                fit_config=self.reg_fit_config_nodir_nodump,\n                calib_config=self.reg_calib_config_nodir_nodump,\n            )\n            sample = prob_reg.posterior.sample()\n\n            # no save dir but dump\n            with self.assertRaises(ValueError):\n                status = prob_reg.train(\n                    train_data_loader=self.reg_train_data_loader,\n                    calib_data_loader=self.reg_val_data_loader,\n                    val_data_loader=self.reg_val_data_loader,\n                    map_fit_config=self.reg_fit_config_nodir_nodump,\n                    fit_config=self.reg_fit_config_nodir_dump,\n                    calib_config=self.reg_calib_config_nodir_nodump,\n                )\n\n            # save dir, no dump\n            status = prob_reg.train(\n                train_data_loader=self.reg_train_data_loader,\n                calib_data_loader=self.reg_val_data_loader,\n                val_data_loader=self.reg_val_data_loader,\n                map_fit_config=self.reg_fit_config_nodir_nodump,\n                fit_config=self.reg_fit_config_dir_nodump(tmp_dir),\n                calib_config=self.reg_calib_config_nodir_nodump,\n            )\n            sample = prob_reg.posterior.sample()\n            prob_reg.posterior.load_state(tmp_dir)\n\n            # save dir and dump", "metadata": {"task_id": "awslabs_fortuna/51", "ground_truth": "            status = prob_reg.train(\n                train_data_loader=self.reg_train_data_loader,\n                calib_data_loader=self.reg_val_data_loader,\n                val_data_loader=self.reg_val_data_loader,\n                map_fit_config=self.reg_fit_config_nodir_nodump,\n                fit_config=self.reg_fit_config_dir_nodump(tmp_dir),\n                calib_config=self.reg_calib_config_nodir_nodump,\n            )", "fpath_tuple": ["awslabs_fortuna", "tests", "fortuna", "prob_model", "test_train.py"], "context_start_lineno": 737, "line_no": 807}}
{"prompt": "to_proto())\n\n  @absltest.skip('???')\n  def testTrialToDict(self):\n    py_study_config = vz.StudyConfig(\n        metric_information=[\n            vz.MetricInformation(\n                name='objective', goal=vz.ObjectiveMetricGoal.MAXIMIZE\n            )\n        ]\n    )\n    root = py_study_config.search_space.root\n    root.add_float_param('learning_rate', 0.01, 3.0)\n    root.add_int_param('units', 10, 1000, scale_type=vz.ScaleType.LOG)\n    root.add_discrete_param('batch_size', [8, 16, 32])\n    root.add_discrete_param(\n        'floating_point_param', [8., 16., 32.], auto_cast=False)\n    root.add_categorical_param('activation', ['tanh','relu'])\n    root.add_bool_param('synchronous')\n\n    trial_proto = study_pb2.Trial(id=str(1))\n    trial_proto.parameters.add(\n        parameter_id='activation', value=struct_pb2.Value(string_value='relu'))\n    trial_proto.parameters.add(\n        parameter_id='synchronus', value=struct_pb2.Value(string_value='true'))\n    trial_proto.parameters.add(\n        parameter_id='batch_size', value=struct_pb2.Value(number_value=32))\n    trial_proto.parameters.add(\n        parameter_id='floating_point_param',\n        value=struct_pb2.Value(number_value=32))\n    trial_proto.parameters.add(\n        parameter_id='learning_rate', value=struct_pb2.Value(number_value=0.5))\n    trial_proto.parameters.add(\n        parameter_id='units', value=struct_pb2.Value(number_value=50))\n\n    parameters = py_study_config.trial_parameters(trial_proto)\n    expected = {\n        'learning_rate': 0.5,\n        'units': 50,\n        'activation':'relu',\n        'batch_size': 32,\n        'floating_point_param': 32.,\n       'synchronous': True\n    }\n    self.assertEqual(expected, parameters)\n    self.assertIsInstance(parameters['batch_size'], int)\n    self.assertIsInstance(parameters['floating_point_param'], float)\n\n  def testPyTrialToDict(self):\n    py_study_config = vz.StudyConfig(\n        metric_information=[\n            vz.MetricInformation(\n                name='objective', goal=vz.ObjectiveMetricGoal.MAXIMIZE\n            )\n        ]\n    )\n    root = py_study_config.search_space.root\n    root.add_float_param('learning_rate', 0.01, 3.0)\n    root.add_int_param('units', 10, 1000, scale_type=vz.ScaleType.LOG)\n    root.add_discrete_param('batch_size', [8, 16, 32])\n    root.add_discrete_param(\n        'floating_point_param', [8., 16., 32.], auto_cast=False)\n    root.add_categorical_param('activation', ['tanh','relu'])\n    root.add_bool_param('synchronous')\n\n    pytrial = vz.Trial(id=1)\n    pytrial.parameters = {\n        'activation': vz.ParameterValue(value='relu'),\n       'synchronous': vz.ParameterValue(value=True),\n        'batch_size': vz.ParameterValue(value=32),\n        'floating_point_param': vz.ParameterValue(value=32.0),\n        'learning_rate': vz.ParameterValue(value=0.5),\n        'units': vz.ParameterValue(value=50),\n    }", "metadata": {"task_id": "google_vizier/119", "ground_truth": "    parameters = py_study_config._pytrial_parameters(pytrial)", "fpath_tuple": ["google_vizier", "vizier", "_src", "pyvizier", "oss", "study_config_test.py"], "context_start_lineno": 298, "line_no": 372}}
{"prompt": "state_dict\n        ReplayBufferTrainer.load_state_dict = ReplayBufferTrainer_load_state_dict\n        TensorDict.state_dict = TensorDict_state_dict\n        TensorDict.load_state_dict = TensorDict_load_state_dict\n\n\nclass TestOptimizer:\n    @staticmethod\n    def _setup():\n        torch.manual_seed(0)\n        x = torch.randn(5, 10)\n        model1 = nn.Linear(10, 20)\n        model2 = nn.Linear(10, 20)\n        td = TensorDict(\n            {\n                \"loss_1\": model1(x).sum(),\n                \"loss_2\": model2(x).sum(),\n            },\n            batch_size=[],\n        )\n        model1_params = list(model1.parameters())\n        model2_params = list(model2.parameters())\n        all_params = model1_params + model2_params\n        return model1_params, model2_params, all_params, td\n\n    def test_optimizer_set_as_argument(self):\n        _, _, all_params, td = self._setup()\n\n        optimizer = torch.optim.SGD(all_params, lr=1e-3)\n        trainer = mocking_trainer(optimizer=optimizer)\n\n        params_before = [torch.clone(p) for p in all_params]\n        td_out = trainer._optimizer_hook(td)\n        params_after = all_params\n\n        assert \"grad_norm_0\" in td_out.keys()\n        assert all(\n            not torch.equal(p_before, p_after)\n            for p_before, p_after in zip(params_before, params_after)\n        )\n\n    def test_optimizer_set_as_hook(self):\n        _, _, all_params, td = self._setup()\n\n        optimizer = torch.optim.SGD(all_params, lr=1e-3)\n        trainer = mocking_trainer(optimizer=None)\n        hook = OptimizerHook(optimizer)\n        hook.register(trainer)\n\n        params_before = [torch.clone(p) for p in all_params]\n        td_out = trainer._optimizer_hook(td)\n        params_after = all_params\n\n        assert \"grad_norm_0\" in td_out.keys()\n        assert all(\n            not torch.equal(p_before, p_after)\n            for p_before, p_after in zip(params_before, params_after)\n        )\n\n    def test_optimizer_no_optimizer(self):\n        _, _, all_params, td = self._setup()\n\n        trainer = mocking_trainer(optimizer=None)\n\n        params_before = [torch.clone(p) for p in all_params]\n        td_out = trainer._optimizer_hook(td)\n        params_after = all_params\n\n        assert not [key for key in td_out.keys() if key.startswith(\"grad_norm_\")]\n        assert all(\n            torch.equal(p_before, p_after)\n            for p_before, p_after in zip(params_before, params_after)\n        )\n\n    def test_optimizer_hook_loss_components_empty(self):\n        model = nn.Linear(10, 20)\n        optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n        with pytest.raises(ValueError, match=\"loss_components list cannot be empty\"):\n            OptimizerHook(optimizer, loss_components=[])\n\n    def test_optimizer_hook_loss_components_partial(self):\n        model1_params, model2_params, all_params, td = self._setup()\n\n        optimizer = torch.optim.SGD(all_params, lr=1e-3)\n        trainer = mocking_trainer(optimizer=None)\n        hook = OptimizerHook(optimizer, loss_components=[\"loss_1\"])", "metadata": {"task_id": "pytorch_rl/6", "ground_truth": "        hook.register(trainer)", "fpath_tuple": ["pytorch_rl", "test", "test_trainer.py"], "context_start_lineno": 468, "line_no": 554}}
{"prompt": " import JittedMixin, MultiDeviceMixin\nfrom fortuna.typing import Array, Status\nfrom fortuna.utils.device import select_trainer_given_devices\n\n\nclass JittedADVITrainer(JittedMixin, ADVITrainer):\n    pass\n\n\nclass MultiDeviceADVITrainer(MultiDeviceMixin, ADVITrainer):\n    pass\n\n\nclass ADVIPosterior(Posterior):\n    def __init__(\n        self, joint: Joint, posterior_approximator: ADVIPosteriorApproximator,\n    ):\n        \"\"\"\n        Automatic Differentiation Variational Inference (ADVI) approximate posterior class.\n\n        Parameters\n        ----------\n        joint: Joint\n            A joint distribution object.\n        posterior_approximator: ADVI\n            An ADVI posterior approximator.\n        \"\"\"\n        super().__init__(joint=joint, posterior_approximator=posterior_approximator)\n\n    def __str__(self):\n        return ADVI_NAME\n\n    def fit(\n        self,\n        train_data_loader: DataLoader,\n        val_data_loader: Optional[DataLoader] = None,\n        fit_config: FitConfig = FitConfig(),\n        **kwargs,\n    ) -> Status:\n        if (\n            fit_config.checkpointer.dump_state is True\n            and not fit_config.checkpointer.save_checkpoint_dir\n        ):\n            raise ValueError(\n                \"`save_checkpoint_dir` must be passed when `dump_state` is set to True.\"\n            )\n\n        init_prob_model_state, n_train_data, n_val_data = self._init(\n            train_data_loader, val_data_loader\n        )\n\n        rav, self.unravel = ravel_pytree(init_prob_model_state.params)\n        size_rav = len(rav)\n        self.base = DiagGaussian(\n            mean=jnp.zeros(size_rav),\n            std=self.posterior_approximator.std_base * jnp.ones(size_rav),\n        )\n        self.architecture = ADVIArchitecture(\n            size_rav, std_init_params=self.posterior_approximator.std_init_params\n        )\n\n        trainer_cls = select_trainer_given_devices(\n            devices=fit_config.processor.devices,\n            BaseTrainer=ADVITrainer,\n            JittedTrainer=JittedADVITrainer,\n            MultiDeviceTrainer=MultiDeviceADVITrainer,\n            disable_jit=fit_config.processor.disable_jit,\n        )\n\n        trainer = trainer_cls(\n            predict_fn=self.joint.likelihood.prob_output_layer.predict,\n            save_checkpoint_dir=fit_config.checkpointer.save_checkpoint_dir,\n            save_every_n_steps=fit_config.checkpointer.save_every_n_steps,\n            keep_top_n_checkpoints=fit_config.checkpointer.keep_top_n_checkpoints,\n            disable_training_metrics_computation=fit_config.monitor.disable_training_metrics_computation,\n            eval_every_n_epochs=fit_config.monitor.eval_every_n_epochs,\n            early_stopping_monitor=fit_config.monitor.early_stopping_monitor,\n            early_stopping_min_delta=fit_config.monitor.early_stopping_min_delta,\n            early_stopping_patience=fit_config.monitor.early_stopping_patience,\n            base=self.base,\n            architecture=self.architecture,\n        )\n\n        state = None\n        if fit_config.checkpointer.restore_checkpoint_path:\n            state = self.restore_checkpoint(\n                restore_checkpoint_path=fit_config.checkpointer.restore_checkpoint_path,\n                optimizer=fit_config.optimizer.method,\n            )\n\n        if type(state)!= ADVIState:", "metadata": {"task_id": "awslabs_fortuna/169", "ground_truth": "            state = ADVIState.init(\n                FrozenDict(\n                    zip(\n                        (\"mean\", \"logvar\"),\n                        trainer.init_params(\n                            self.rng.get(),\n                            mean=ravel_pytree(\n                                getattr(state, \"params\", init_prob_model_state.params)\n                            )[0],\n                        ),\n                    )\n                ),\n                getattr(state, \"mutable\", init_prob_model_state.mutable),\n                fit_config.optimizer.method,\n                getattr(state, \"calib_params\", init_prob_model_state.calib_params),\n                getattr(state, \"calib_mutable\", init_prob_model_state.calib_mutable),\n            )", "fpath_tuple": ["awslabs_fortuna", "fortuna", "prob_model", "posterior", "normalizing_flow", "advi", "advi_posterior.py"], "context_start_lineno": 27, "line_no": 118}}
{"prompt": "`PNDMScheduler`].\n        safety_checker ([`StableDiffusionSafetyChecker`]):\n            Classification module that estimates whether generated images could be considered offensive or harmful.\n            Please, refer to the [model card](https://huggingface.co/runwayml/stable-diffusion-v1-5) for details.\n        feature_extractor ([`CLIPFeatureExtractor`]):\n            Model that extracts features from generated images to be used as inputs for the `safety_checker`.\n    \"\"\"\n    # TODO: feature_extractor is required to encode initial images (if they are in PIL format),\n    # we should give a descriptive message if the pipeline doesn't have one.\n    _optional_components = [\"safety_checker\"]\n\n    def __init__(\n        self,\n        vae: AutoencoderKL,\n        image_encoder: PaintByExampleImageEncoder,\n        unet: UNet2DConditionModel,\n        scheduler: Union[DDIMScheduler, PNDMScheduler, LMSDiscreteScheduler],\n        safety_checker: StableDiffusionSafetyChecker,\n        feature_extractor: CLIPFeatureExtractor,\n        requires_safety_checker: bool = False,\n    ):\n        super().__init__()\n\n        self.register_modules(\n            vae=vae,\n            image_encoder=image_encoder,\n            unet=unet,\n            scheduler=scheduler,\n            safety_checker=safety_checker,\n            feature_extractor=feature_extractor,\n        )\n        self.vae_scale_factor = 2 ** (len(self.vae.config.block_out_channels) - 1)\n        self.register_to_config(requires_safety_checker=requires_safety_checker)\n\n    def enable_sequential_cpu_offload(self, gpu_id=0):\n        r\"\"\"\n        Offloads all models to CPU using accelerate, significantly reducing memory usage. When called, unet,\n        text_encoder, vae and safety checker have their state dicts saved to CPU and then are moved to a\n        `torch.device('meta') and loaded to GPU only when their specific submodule has its `forward` method called.\n        \"\"\"\n        if is_accelerate_available():\n            from accelerate import cpu_offload\n        else:\n            raise ImportError(\"Please install accelerate via `pip install accelerate`\")\n\n        device = torch.device(f\"cuda:{gpu_id}\")\n\n        for cpu_offloaded_model in [self.unet, self.vae, self.image_encoder]:\n            cpu_offload(cpu_offloaded_model, execution_device=device)\n\n        if self.safety_checker is not None:\n            cpu_offload(self.safety_checker, execution_device=device, offload_buffers=True)\n\n    @property\n    # Copied from diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline._execution_device\n    def _execution_device(self):\n        r\"\"\"\n        Returns the device on which the pipeline's models will be executed. After calling\n        `pipeline.enable_sequential_cpu_offload()` the execution device can only be inferred from Accelerate's module\n        hooks.\n        \"\"\"\n        if self.device!= torch.device(\"meta\") or not hasattr(self.unet, \"_hf_hook\"):\n            return self.device\n        for module in self.unet.modules():\n            if (\n                hasattr(module, \"_hf_hook\")\n                and hasattr(module._hf_hook, \"execution_device\")\n                and module._hf_hook.execution_device is not None\n            ):\n                return torch.device(module._hf_hook.execution_device)\n        return self.device\n\n    # Copied from diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline.run_safety_checker\n    def run_safety_checker(self, image, device, dtype):\n        if self.safety_checker is not None:", "metadata": {"task_id": "huggingface_diffusers/13", "ground_truth": "            safety_checker_input = self.feature_extractor(self.numpy_to_pil(image), return_tensors=\"pt\").to(device)", "fpath_tuple": ["huggingface_diffusers", "src", "diffusers", "pipelines", "paint_by_example", "pipeline_paint_by_example.py"], "context_start_lineno": 156, "line_no": 231}}
{"prompt": "import pytest\nimport torch\n\nfrom ding.torch_utils.distribution import Pd, CategoricalPd, CategoricalPdPytorch\n\n\n@pytest.mark.unittest\nclass TestProbDistribution:\n\n    def test_Pd(self):\n        pd = Pd()\n        with pytest.raises(NotImplementedError):\n            pd.neglogp(torch.randn(5, ))\n        with pytest.raises(NotImplementedError):\n            pd.noise_mode()\n        with pytest.raises(NotImplementedError):\n            pd.mode()\n        with pytest.raises(NotImplementedError):", "metadata": {"task_id": "opendilab_ACE/154", "ground_truth": "            pd.sample()", "fpath_tuple": ["opendilab_ACE", "ding", "torch_utils", "tests", "test_distribution.py"], "context_start_lineno": 0, "line_no": 18}}
{"prompt": "\n    ) -> None:\n        self.init_resolution = config.init_resolution\n        self.upsampling_iters = config.upsampling_iters\n        self.num_den_components = config.num_den_components\n        self.num_color_components = config.num_color_components\n        self.appearance_dim = config.appearance_dim\n        self.upsampling_steps = (\n            np.round(\n                np.exp(\n                    np.linspace(\n                        np.log(config.init_resolution),\n                        np.log(config.final_resolution),\n                        len(config.upsampling_iters) + 1,\n                    )\n                )\n            )\n           .astype(\"int\")\n           .tolist()[1:]\n        )\n        super().__init__(config=config, **kwargs)\n\n    def get_training_callbacks(\n        self, training_callback_attributes: TrainingCallbackAttributes\n    ) -> List[TrainingCallback]:\n\n        # the callback that we want to run every X iterations after the training iteration\n        def reinitialize_optimizer(\n            self, training_callback_attributes: TrainingCallbackAttributes, step: int  # pylint: disable=unused-argument\n        ):\n            index = self.upsampling_iters.index(step)\n            resolution = self.upsampling_steps[index]\n\n            # upsample the position and direction grids\n            self.field.density_encoding.upsample_grid(resolution)\n            self.field.color_encoding.upsample_grid(resolution)\n\n            # reinitialize the encodings optimizer\n            optimizers_config = training_callback_attributes.optimizers.config\n            enc = training_callback_attributes.pipeline.get_param_groups()[\"encodings\"]\n            lr_init = optimizers_config[\"encodings\"][\"optimizer\"].lr\n\n            training_callback_attributes.optimizers.optimizers[\"encodings\"] = optimizers_config[\"encodings\"][\n                \"optimizer\"\n            ].setup(params=enc)\n            if optimizers_config[\"encodings\"][\"scheduler\"]:\n                training_callback_attributes.optimizers.schedulers[\"encodings\"] = optimizers_config[\"encodings\"][\n                    \"scheduler\"\n                ].setup(optimizer=training_callback_attributes.optimizers.optimizers[\"encodings\"], lr_init=lr_init)\n\n        callbacks = [\n            TrainingCallback(\n                where_to_run=[TrainingCallbackLocation.AFTER_TRAIN_ITERATION],\n                iters=self.upsampling_iters,\n                func=reinitialize_optimizer,\n                args=[self, training_callback_attributes],\n            )\n        ]\n        return callbacks\n\n    def update_to_step(self, step: int) -> None:\n        if step < self.upsampling_iters[0]:\n            return\n\n        new_iters = list(self.upsampling_iters) + [step + 1]\n        new_iters.sort()\n\n        index = new_iters.index(step + 1)\n        new_grid_resolution = self.upsampling_steps[index - 1]\n\n        self.field.density_encoding.upsample_grid(new_grid_resolution)  # type: ignore\n        self.field.color_encoding.upsample_grid(new_grid_resolution)  # type: ignore\n\n    def populate_modules(self):\n        \"\"\"Set the fields and modules\"\"\"\n        super().populate_modules()\n\n        # setting up fields\n        density_encoding = TensorVMEncoding(\n            resolution=self.init_resolution,\n            num_components=self.num_den_components,\n        )\n        color_encoding = TensorVMEncoding(\n            resolution=self.init_resolution,\n            num_components=self.num_color_components,\n        )\n\n        feature_encoding = NeRFEncoding(in_dim=self.appearance_dim, num_frequencies=2, min_freq_exp=0, max_freq_exp=2)", "metadata": {"task_id": "nerfstudio-project_nerfstudio/126", "ground_truth": "        direction_encoding = NeRFEncoding(in_dim=3, num_frequencies=2, min_freq_exp=0, max_freq_exp=2)", "fpath_tuple": ["nerfstudio-project_nerfstudio", "nerfstudio", "models", "tensorf.py"], "context_start_lineno": 86, "line_no": 174}}
{"prompt": "connection) -> None:\n        \"\"\"\n        Overview:\n            Init dataloader with input parameters. Will run as a thread through ``self.get_data_thread``.\n        Arguments:\n            - p (:obj:`tm.multiprocessing.connection`): Parent connection.\n            - c (:obj:`tm.multiprocessing.connection`): Child connection.\n        \"\"\"\n        c.close()  # Close unused c, only use p\n        while not self.end_flag:\n            if not p.poll(timeout=0.2):\n                time.sleep(0.01)\n                continue\n            try:\n                cmd = p.recv()\n            except EOFError:\n                break\n            if cmd == 'get_data':\n                # Main worker asks for data.\n                data = self.data_source(self.batch_size)\n                # ``data`` can be callable, e.g. a function to read data from file, therefore we can divide\n                # this job to pieces, assign to every slave worker and accomplish jobs asynchronously.\n                # But if we get a list of dicts, which means the data has already been processed and\n                # can be used directly, we can put it directly in async_train_queue and wait it\n                # to be accessed by a user, e.g. learner.\n                if isinstance(data[0], dict):\n                    data = self.collate_fn(data)\n                    self.async_train_queue.put(data)\n                    p.send('pass')\n                else:\n                    p.send(data)\n        p.close()\n\n    def _async_loop(self, p: tm.multiprocessing.connection, c: tm.multiprocessing.connection) -> None:\n        \"\"\"\n        Overview:\n            Main worker process. Run through ``self.async_process``.\n            Firstly, get data from ``self.get_data_thread``.\n            If multiple workers, put data in ``self.job_queue`` for further multiprocessing operation;\n            If only one worker, process data and put directly into ``self.async_train_queue``.\n        Arguments:\n            - p (:obj:`tm.multiprocessing.connection`): Parent connection.\n            - c (:obj:`tm.multiprocessing.connection`): Child connection.\n        \"\"\"\n        p.close()  # Close unused p, only use c\n        while not self.end_flag:\n            if self.num_workers > 1:\n                # Multiple workers: Put jobs (chunked data) into job_queue\n                if self.job_queue.full():\n                    time.sleep(0.001)\n                else:\n                    # Get data from ``_get_data`` thread.\n                    c.send('get_data')\n                    data = c.recv()\n                    if isinstance(data, str) and data == 'pass':\n                        continue\n                    # Get data to be processed, chunk it into pieces and put them into job_queue.\n                    chunk_num = self.batch_size // self.chunk_size\n                    with self.batch_id.get_lock():\n                        for i in range(chunk_num):\n                            start, end = i * self.chunk_size, (i + 1) * self.chunk_size\n                            self.job_queue.put({'batch_id': self.batch_id.value, 'job': data[start:end]})\n                        self.batch_id.value = (self.batch_id.value + 1) % self.queue_maxsize  # Increment batch_id\n                    time.sleep(0.001)\n            else:\n                # Only one worker: Process data and directly put it into async_train_queue\n                if self.async_train_queue.full():\n                    time.sleep(0.001)\n                else:\n                    c.send('get_data')\n                    data = c.recv()\n                    if isinstance(data, str) and data == 'pass':\n                        continue\n                    data = [fn() for fn in data]  # Implement functions in list ``data``.\n                    data = self.collate_fn(data)\n                    self.async_train_queue.put(data)", "metadata": {"task_id": "opendilab_ACE/29", "ground_truth": "        c.close()", "fpath_tuple": ["opendilab_ACE", "ding", "utils", "data", "dataloader.py"], "context_start_lineno": 144, "line_no": 220}}
{"prompt": "# Preprocess data for training and store relevant data.\n    X_train, Y_train, self._X_inf, self._Y_inf = self._preprocess(X, Y)\n    self._num_vars = X_train.shape[1]\n\n    # Create matrix with all covariates based on order.\n    X_train = self._order_effects(X_train)\n    (nSamps, nCoeffs) = X_train.shape\n\n    # Check if X_train contains columns w/ zeros and find corresponding indices.\n    check_zero = np.all(X_train == np.zeros((nSamps, 1)), axis=0)\n    idx_nnzero = np.where(check_zero == False)[0]  # pylint:disable=singleton-comparison,g-explicit-bool-comparison\n\n    # Remove columns of zeros in X_train.\n    if np.any(check_zero):\n      X_train = X_train[:, idx_nnzero]\n\n    # Run Gibbs sampler.\n    bhs = _BayesianHorseshoeLinearRegression()\n\n    counter = 0\n    while counter < self._num_gibbs_retries:\n      # re-run if there is an error during sampling\n      counter += 1\n      try:\n        alphaGibbs, a0, _, _, _ = bhs.regress(X_train, Y_train)\n        # run until alpha matrix does not contain any NaNs\n        if not np.isnan(alphaGibbs).any():\n          break\n      except np.linalg.LinAlgError:\n        logging.error('Numerical error during Gibbs sampling. Trying again.')\n        continue\n\n    if counter >= self._num_gibbs_retries:\n      raise ValueError('Gibbs sampling failed for %d tries.' %\n                       self._num_gibbs_retries)\n\n    # append zeros back - note alpha(1,:) is linear intercept\n    alpha_pad = np.zeros(nCoeffs)\n    alpha_pad[idx_nnzero] = alphaGibbs[:, -1]\n    self._alpha = np.append(a0, alpha_pad)\n\n  @property\n  def alpha(self) -> np.ndarray:\n    if self._alpha is None:\n      raise ValueError('You first need to call `regress()` on the data.')\n    return self._alpha\n\n  @property\n  def num_vars(self) -> int:\n    if self._num_vars is None:\n      raise ValueError('You first need to call `regress()` on the data.')\n    return self._num_vars\n\n  def surrogate_model(self, x: np.ndarray) -> FloatType:\n    \"\"\"Surrogate model.\n\n    Args:\n      x: Should only contain one row.\n\n    Returns:\n      Surrogate objective.\n    \"\"\"\n\n    # Generate x_all (all basis vectors) based on model order.\n    x_all = np.append(1, self._order_effects(x))\n\n    # check if previous x led to Inf output (if so, barrier=Inf)\n    barrier = 0.\n    if self._X_inf.shape[0]!= 0 and np.equal(x, self._X_inf).all(axis=1).any():\n      barrier = np.inf\n\n    return np.dot(x_all, self._alpha) + barrier\n\n  def _order_effects(self, X: np.ndarray) -> np.ndarray:\n    \"\"\"Function computes data matrix for all coupling.\"\"\"\n\n    # Find number of variables\n    n_samp, n_vars = X.shape\n\n    # Generate matrix to store results\n    x_allpairs = X\n\n    for ord_i in range(2, self._order + 1):\n\n      # generate all combinations of indices (without diagonals)\n      offdProd = np.array(\n          list(itertools.combinations(np.arange(n_vars), ord_i)))\n\n      # generate products of input variables\n      x_comb = np.zeros((n_samp, offdProd.shape[0], ord_i))", "metadata": {"task_id": "google_vizier/65", "ground_truth": "      for j in range(ord_i):", "fpath_tuple": ["google_vizier", "vizier", "_src", "algorithms", "designers", "bocs.py"], "context_start_lineno": 247, "line_no": 337}}
{"prompt": "(self):\n        self.evaluator.compute(\n            model_or_pipeline=self.pipe,\n            data=self.data,\n            input_column=\"text\",\n            label_column=\"label\",\n            label_mapping=self.label_mapping,\n        )\n\n    def test_model_init(self):\n        self.evaluator.compute(\n            model_or_pipeline=self.default_model,\n            tokenizer=self.default_tokenizer,\n            data=self.data,\n            input_column=\"text\",\n            label_column=\"label\",\n            label_mapping=self.label_mapping,\n        )\n\n    def test_model_str_init(self):\n        self.evaluator.compute(\n            model_or_pipeline=self.default_ckpt,\n            data=self.data,\n            input_column=\"text\",\n            label_column=\"label\",\n            label_mapping=self.label_mapping,\n        )\n\n\nclass TestTextClassificationEvaluator(TestCase):\n    def setUp(self):\n        self.data = Dataset.from_dict({\"label\": [1, 0], \"text\": [\"great movie\", \"horrible movie\"]})\n        self.default_model = \"lvwerra/distilbert-imdb\"\n        self.input_column = \"text\"\n        self.label_column = \"label\"\n        self.pipe = DummyTextClassificationPipeline()\n        self.perf_pipe = DummyTextClassificationPipeline(sleep_time=0.1)\n        self.evaluator = evaluator(\"text-classification\")\n        self.label_mapping = {\"NEGATIVE\": 0.0, \"POSITIVE\": 1.0}\n\n    def test_pipe_init(self):\n        results = self.evaluator.compute(\n            model_or_pipeline=self.pipe,\n            data=self.data,\n            input_column=\"text\",\n            label_column=\"label\",\n            label_mapping=self.label_mapping,\n        )\n        self.assertEqual(results[\"accuracy\"], 1.0)\n\n    @slow\n    def test_model_init(self):\n        results = self.evaluator.compute(\n            model_or_pipeline=self.default_model,\n            data=self.data,\n            metric=\"accuracy\",\n            input_column=self.input_column,\n            label_column=self.label_column,\n            label_mapping=self.label_mapping,\n        )\n\n        model = AutoModelForSequenceClassification.from_pretrained(self.default_model)\n        tokenizer = AutoTokenizer.from_pretrained(self.default_model)\n\n        self.assertEqual(results[\"accuracy\"], 1.0)\n        results = self.evaluator.compute(\n            model_or_pipeline=model,\n            data=self.data,\n            metric=\"accuracy\",\n            tokenizer=tokenizer,\n            label_mapping=self.label_mapping,\n        )\n        self.assertEqual(results[\"accuracy\"], 1.0)\n\n    def test_class_init(self):\n        evaluator = TextClassificationEvaluator()\n        self.assertEqual(evaluator.task, \"text-classification\")\n        self.assertIsNone(evaluator.default_metric_name)\n\n        results = evaluator.compute(\n            model_or_pipeline=self.pipe,\n            data=self.data,\n            metric=\"f1\",\n            label_mapping=self.label_mapping,\n        )\n        self.assertEqual(results[\"f1\"], 1.0)\n\n    @slow\n    def test_default_pipe_init(self):\n        results = self.evaluator.compute(\n            data=self.data,\n            label_mapping=self.label_mapping,\n        )\n        self.assertEqual(results[\"accuracy\"], 1.0)\n\n    def test_data_loading(self):\n\n        # Test passing in dataset by name with split", "metadata": {"task_id": "huggingface_evaluate/96", "ground_truth": "        data = self.evaluator.load_data(\"evaluate/imdb-ci\", split=\"test[:1]\")", "fpath_tuple": ["huggingface_evaluate", "tests", "test_evaluator.py"], "context_start_lineno": 201, "line_no": 299}}
{"prompt": " unsqueeze to get a batch dimension and then squeeze later\n        if not self.shape:\n            cameras = self.reshape((1,))\n            assert torch.all(\n                torch.tensor(camera_indices == 0) if isinstance(camera_indices, int) else camera_indices == 0\n            ), \"Can only index into single camera with no batch dimensions if index is zero\"\n        else:\n            cameras = self\n\n        # If the camera indices are an int, then we need to make sure that the camera batch is 1D\n        if isinstance(camera_indices, int):\n            assert (\n                len(cameras.shape) == 1\n            ), \"camera_indices must be a tensor if cameras are batched with more than 1 batch dimension\"\n            camera_indices = torch.tensor([camera_indices], device=cameras.device)\n\n        assert camera_indices.shape[-1] == len(\n            cameras.shape\n        ), \"camera_indices must have shape (num_rays:..., num_cameras_batch_dims)\"\n\n        # If keep_shape is True, then we need to make sure that the camera indices in question\n        # are all the same height and width and can actually be batched while maintaining the image\n        # shape\n        if keep_shape is True:\n            assert torch.all(cameras.height[camera_indices] == cameras.height[camera_indices[0]]) and torch.all(\n                cameras.width[camera_indices] == cameras.width[camera_indices[0]]\n            ), \"Can only keep shape if all cameras have the same height and width\"\n\n        # If the cameras don't all have same height / width, if coords is not none, we will need to generate\n        # a flat list of coords for each camera and then concatenate otherwise our rays will be jagged.\n        # Camera indices, camera_opt, and distortion will also need to be broadcasted accordingly which is non-trivial\n        if cameras.is_jagged and coords is None and (keep_shape is None or keep_shape is False):\n            index_dim = camera_indices.shape[-1]\n            camera_indices = camera_indices.reshape(-1, index_dim)\n            _coords = [cameras.get_image_coords(index=tuple(index)).reshape(-1, 2) for index in camera_indices]\n            camera_indices = torch.cat(\n                [index.unsqueeze(0).repeat(coords.shape[0], 1) for index, coords in zip(camera_indices, _coords)],\n            )\n            coords = torch.cat(_coords, dim=0)\n            assert coords.shape[0] == camera_indices.shape[0]\n            # Need to get the coords of each indexed camera and flatten all coordinate maps and concatenate them\n\n        # The case where we aren't jagged && keep_shape (since otherwise coords is already set) and coords\n        # is None. In this case we append (h, w) to the num_rays dimensions for all tensors. In this case,\n        # each image in camera_indices has to have the same shape since otherwise we would have error'd when\n        # we checked keep_shape is valid or we aren't jagged.\n        if coords is None:\n            index_dim = camera_indices.shape[-1]\n            index = camera_indices.reshape(-1, index_dim)[0]\n            coords: torch.Tensor = cameras.get_image_coords(index=tuple(index))  # (h, w, 2)\n            coords = coords.reshape(coords.shape[:2] + (1,) * len(camera_indices.shape[:-1]) + (2,))  # (h, w, 1..., 2)\n            coords = coords.expand(coords.shape[:2] + camera_indices.shape[:-1] + (2,))  # (h, w, num_rays, 2)\n            camera_opt_to_camera = (  # (h, w, num_rays, 3, 4) or None", "metadata": {"task_id": "nerfstudio-project_nerfstudio/118", "ground_truth": "                camera_opt_to_camera.broadcast_to(coords.shape[:-1] + (3, 4))", "fpath_tuple": ["nerfstudio-project_nerfstudio", "nerfstudio", "cameras", "cameras.py"], "context_start_lineno": 376, "line_no": 429}}
{"prompt": "self.class_lik.rng = rng\n\n        self.reg_data_arr = DataLoader.from_array_data(\n            make_array_random_data(\n                n_data=self.n_inputs,\n                shape_inputs=self.shape_inputs,\n                output_dim=self.output_dim,\n                output_type=\"continuous\",\n            )\n        )\n        self.reg_inputs_arr = InputsLoader.from_data_loader(self.reg_data_arr)\n\n        self.reg_data_gen_fun = DataLoader.from_callable_iterable(\n            make_generator_fun_random_data(\n                n_batches=self.n_batches,\n                batch_size=self.batch_size,\n                shape_inputs=self.shape_inputs,\n                output_dim=self.output_dim,\n                output_type=\"continuous\",\n            )\n        )\n        self.reg_inputs_gen_fun = InputsLoader.from_data_loader(self.reg_data_gen_fun)\n\n        self.class_data_arr = DataLoader.from_array_data(\n            make_array_random_data(\n                n_data=self.n_inputs,\n                shape_inputs=self.shape_inputs,\n                output_dim=self.output_dim,\n                output_type=\"discrete\",\n            )\n        )\n        self.class_inputs_arr = InputsLoader.from_data_loader(self.class_data_arr)\n\n        self.class_data_gen_fun = DataLoader.from_callable_iterable(\n            make_generator_fun_random_data(\n                n_batches=self.n_batches,\n                batch_size=self.batch_size,\n                shape_inputs=self.shape_inputs,\n                output_dim=self.output_dim,\n                output_type=\"discrete\",\n            )\n        )\n        self.class_inputs_gen_fun = InputsLoader.from_data_loader(\n            self.class_data_gen_fun\n        )\n\n    def test_lik_batched_log_joint_prob(self):\n        params = FrozenDict(\n            dict(\n                model=self.reg_lik.model_manager.model.init(\n                    self.rng, jnp.zeros((1,) + self.shape_inputs)\n                ),\n                lik_log_var=self.reg_lik.model_manager.likelihood_log_variance_model.init(\n                    self.rng, jnp.zeros((1,) + self.shape_inputs)\n                ),\n            )\n        )\n\n        for batch_data in self.reg_data_arr:\n            batched_log_joint_prob1 = self.reg_lik._batched_log_joint_prob(\n                params, batch_data, n_data=batch_data[1].shape[0]\n            )\n            batched_log_joint_prob2 = self.reg_lik._batched_log_joint_prob(\n                params, batch_data, n_data=2 * batch_data[1].shape[0]\n            )\n            assert jnp.allclose(batched_log_joint_prob2, 2 * batched_log_joint_prob1)\n            assert jnp.array([batched_log_joint_prob1]).shape == (1,)\n\n            _, aux = self.reg_lik._batched_log_joint_prob(\n                params,\n                batch_data,\n                n_data=batch_data[1].shape[0],\n                return_aux=[\"outputs\"],\n            )\n            assert aux[\"outputs\"].shape == (self.n_inputs, 2 * self.output_dim)\n\n    def test_lik_log_joint_prob(self):\n        params = FrozenDict(\n            dict(\n                model=self.reg_lik.model_manager.model.init(\n                    self.rng, jnp.zeros((1,) + self.shape_inputs)\n                ),", "metadata": {"task_id": "awslabs_fortuna/40", "ground_truth": "                lik_log_var=self.reg_lik.model_manager.likelihood_log_variance_model.init(\n                    self.rng, jnp.zeros((1,) + self.shape_inputs)\n                ),", "fpath_tuple": ["awslabs_fortuna", "tests", "fortuna", "prob_model", "test_likelihood.py"], "context_start_lineno": 54, "line_no": 136}}
{"prompt": " Env must be created in worker, which is a trick of avoiding env pickle errors.\n    # A more robust version is used by default. But this one is also preserved.\n    @staticmethod\n    def worker_fn(\n            p: connection.Connection, c: connection.Connection, env_fn_wrapper: 'CloudPickleWrapper',\n            obs_buffer: ShmBuffer, method_name_list: list\n    ) -> None:  # noqa\n        \"\"\"\n        Overview:\n            Subprocess's target function to run.\n        \"\"\"\n        torch.set_num_threads(1)\n        env_fn = env_fn_wrapper.data\n        env = env_fn()\n        p.close()\n        try:\n            while True:\n                try:\n                    cmd, args, kwargs = c.recv()\n                except EOFError:  # for the case when the pipe has been closed\n                    c.close()\n                    break\n                try:\n                    if cmd == 'getattr':\n                        ret = getattr(env, args[0])\n                    elif cmd in method_name_list:\n                        if cmd =='step':\n                            timestep = env.step(*args, **kwargs)\n                            if is_abnormal_timestep(timestep):\n                                ret = timestep\n                            else:\n                                if obs_buffer is not None:\n                                    obs_buffer.fill(timestep.obs)\n                                    timestep = timestep._replace(obs=None)\n                                ret = timestep\n                        elif cmd =='reset':\n                            ret = env.reset(*args, **kwargs)  # obs\n                            if obs_buffer is not None:\n                                obs_buffer.fill(ret)\n                                ret = None\n                        elif args is None and kwargs is None:\n                            ret = getattr(env, cmd)()\n                        else:\n                            ret = getattr(env, cmd)(*args, **kwargs)\n                    else:\n                        raise KeyError(\"not support env cmd: {}\".format(cmd))\n                    c.send(ret)\n                except Exception as e:\n                    # when there are some errors in env, worker_fn will send the errors to env manager\n                    # directly send error to another process will lose the stack trace, so we create a new Exception\n                    c.send(\n                        e.__class__(\n                            '\\nEnv Process Exception:\\n' + ''.join(traceback.format_tb(e.__traceback__)) + repr(e)\n                        )\n                    )\n                if cmd == 'close':\n                    c.close()\n                    break\n        except KeyboardInterrupt:\n            c.close()\n\n    @staticmethod\n    def worker_fn_robust(\n            parent,\n            child,\n            env_fn_wrapper,\n            obs_buffer,\n            method_name_list,\n            reset_timeout=60,\n            step_timeout=60,\n            max_retry=1\n    ) -> None:\n        \"\"\"\n        Overview:\n            A more robust version of subprocess's target function to run. Used by default.\n        \"\"\"\n        torch.set_num_threads(1)\n        env_fn = env_fn_wrapper.data\n        env = env_fn()\n        parent.close()\n\n        @retry_wrapper(max_retry=max_retry)\n        @timeout_wrapper(timeout=step_timeout)\n        def step_fn(*args, **kwargs):\n            timestep = env.step(*args, **kwargs)\n            if is_abnormal_timestep(timestep):\n                ret = timestep\n            else:\n                if obs_buffer is not None:\n                    obs_buffer.fill(timestep.obs)\n                    timestep = timestep._replace(obs=None)\n                ret = timestep\n            return ret\n\n        # self._reset method has add retry_wrapper decorator\n        @timeout_wrapper(timeout=reset_timeout)\n        def reset_fn(*args, **kwargs):\n            try:\n                ret = env.reset(*args, **kwargs)\n                if obs_buffer is not None:\n                    obs_buffer.fill(ret)\n                    ret = None\n                return ret\n            except Exception as e:", "metadata": {"task_id": "opendilab_ACE/163", "ground_truth": "                env.close()", "fpath_tuple": ["opendilab_ACE", "ding", "envs", "env_manager", "subprocess_env_manager.py"], "context_start_lineno": 480, "line_no": 584}}
{"prompt": ")\\s+import\\s+[^#\\r\\n]*(?:#\\s+From:\\s+)?([^\\r\\n]*)\",\n                line,\n                flags=re.MULTILINE,\n            )\n            if match is None:\n                continue\n        if match.group(1):\n            # The import starts with a '.', we will download the relevant file\n            if any(imp[1] == match.group(2) for imp in imports):\n                # We already have this import\n                continue\n            if match.group(3):\n                # The import has a comment with 'From:', we'll retrieve it from the given url\n                url_path = match.group(3)\n                url_path, sub_directory = convert_github_url(url_path)\n                imports.append((\"external\", match.group(2), url_path, sub_directory))\n            elif match.group(2):\n                # The import should be at the same place as the file\n                imports.append((\"internal\", match.group(2), match.group(2), None))\n        else:\n            if match.group(3):\n                # The import has a comment with `From: git+https:...`, asks user to pip install from git.\n                url_path = match.group(3)\n                imports.append((\"library\", match.group(2), url_path, None))\n            else:\n                imports.append((\"library\", match.group(2), match.group(2), None))\n\n    return imports\n\n\ndef _download_additional_modules(\n    name: str, base_path: str, imports: Tuple[str, str, str, str], download_config: Optional[DownloadConfig]\n) -> List[Tuple[str, str]]:\n    \"\"\"\n    Download additional module for a module <name>.py at URL (or local path) <base_path>/<name>.py\n    The imports must have been parsed first using ``get_imports``.\n\n    If some modules need to be installed with pip, an error is raised showing how to install them.\n    This function return the list of downloaded modules as tuples (import_name, module_file_path).\n\n    The downloaded modules can then be moved into an importable directory with ``_copy_script_and_other_resources_in_importable_dir``.\n    \"\"\"\n    local_imports = []\n    library_imports = []\n    download_config = download_config.copy()\n    if download_config.download_desc is None:\n        download_config.download_desc = \"Downloading extra modules\"\n    for import_type, import_name, import_path, sub_directory in imports:\n        if import_type == \"library\":\n            library_imports.append((import_name, import_path))  # Import from a library\n            continue\n\n        if import_name == name:\n            raise ValueError(\n                f\"Error in the {name} script, importing relative {import_name} module \"\n                f\"but {import_name} is the name of the script. \"\n                f\"Please change relative import {import_name} to another name and add a '# From: URL_OR_PATH' \"\n                f\"comment pointing to the original relative import file path.\"\n            )\n        if import_type == \"internal\":\n            url_or_filename = url_or_path_join(base_path, import_path + \".py\")\n        elif import_type == \"external\":\n            url_or_filename = import_path\n        else:\n            raise ValueError(\"Wrong import_type\")\n\n        local_import_path = cached_path(\n            url_or_filename,\n            download_config=download_config,\n        )\n        if sub_directory is not None:\n            local_import_path = os.path.join(local_import_path, sub_directory)\n        local_imports.append((import_name, local_import_path))\n\n    # Check library imports\n    needs_to_be_installed = set()\n    for library_import_name, library_import_path in library_imports:\n        try:\n            lib = importlib.import_module(library_import_name)  # noqa F841\n        except ImportError:", "metadata": {"task_id": "huggingface_evaluate/89", "ground_truth": "            needs_to_be_installed.add((library_import_name, library_import_path))", "fpath_tuple": ["huggingface_evaluate", "src", "evaluate", "loading.py"], "context_start_lineno": 182, "line_no": 262}}
{"prompt": "sample = self.dummy_sample_deter * scheduler.init_noise_sigma\n        sample = sample.to(torch_device)\n\n        for i, t in enumerate(scheduler.timesteps):\n            sample = scheduler.scale_model_input(sample, t)\n\n            model_output = model(sample, t)\n\n            output = scheduler.step(model_output, t, sample, generator=generator)\n            sample = output.prev_sample\n\n        result_sum = torch.sum(torch.abs(sample))\n        result_mean = torch.mean(torch.abs(sample))\n\n        assert abs(result_sum.item() - 0.0002) < 1e-2\n        assert abs(result_mean.item() - 2.2676e-06) < 1e-3\n\n    def test_full_loop_device(self):\n        scheduler_class = self.scheduler_classes[0]\n        scheduler_config = self.get_scheduler_config()\n        scheduler = scheduler_class(**scheduler_config)\n\n        scheduler.set_timesteps(self.num_inference_steps, device=torch_device)\n\n        generator = torch.manual_seed(0)\n\n        model = self.dummy_model()\n        sample = self.dummy_sample_deter * scheduler.init_noise_sigma\n        sample = sample.to(torch_device)\n\n        for t in scheduler.timesteps:\n            sample = scheduler.scale_model_input(sample, t)\n\n            model_output = model(sample, t)\n\n            output = scheduler.step(model_output, t, sample, generator=generator)\n            sample = output.prev_sample\n\n        result_sum = torch.sum(torch.abs(sample))\n        result_mean = torch.mean(torch.abs(sample))\n\n        assert abs(result_sum.item() - 10.0807) < 1e-2\n        assert abs(result_mean.item() - 0.0131) < 1e-3\n\n\nclass EulerAncestralDiscreteSchedulerTest(SchedulerCommonTest):\n    scheduler_classes = (EulerAncestralDiscreteScheduler,)\n    num_inference_steps = 10\n\n    def get_scheduler_config(self, **kwargs):\n        config = {\n            \"num_train_timesteps\": 1100,\n            \"beta_start\": 0.0001,\n            \"beta_end\": 0.02,\n            \"beta_schedule\": \"linear\",\n        }\n\n        config.update(**kwargs)\n        return config\n\n    def test_timesteps(self):\n        for timesteps in [10, 50, 100, 1000]:\n            self.check_over_configs(num_train_timesteps=timesteps)\n\n    def test_betas(self):\n        for beta_start, beta_end in zip([0.00001, 0.0001, 0.001], [0.0002, 0.002, 0.02]):\n            self.check_over_configs(beta_start=beta_start, beta_end=beta_end)\n\n    def test_schedules(self):\n        for schedule in [\"linear\", \"scaled_linear\"]:\n            self.check_over_configs(beta_schedule=schedule)\n\n    def test_prediction_type(self):\n        for prediction_type in [\"epsilon\", \"v_prediction\"]:\n            self.check_over_configs(prediction_type=prediction_type)\n\n    def test_full_loop_no_noise(self):\n        scheduler_class = self.scheduler_classes[0]\n        scheduler_config = self.get_scheduler_config()\n        scheduler = scheduler_class(**scheduler_config)\n\n        scheduler.set_timesteps(self.num_inference_steps)\n\n        generator = torch.manual_seed(0)\n\n        model = self.dummy_model()\n        sample = self.dummy_sample_deter * scheduler.init_noise_sigma", "metadata": {"task_id": "huggingface_diffusers/189", "ground_truth": "        sample = sample.to(torch_device)", "fpath_tuple": ["huggingface_diffusers", "tests", "test_scheduler.py"], "context_start_lineno": 1825, "line_no": 1912}}
{"prompt": "_metric_with_cache_dir(self):\n        preds, refs = DummyMetric.predictions_and_references()\n        expected_results = DummyMetric.expected_results()\n\n        with tempfile.TemporaryDirectory() as tmp_dir:\n            metric = DummyMetric(experiment_id=\"test_dummy_metric\", cache_dir=tmp_dir)\n            self.assertDictEqual(expected_results, metric.compute(predictions=preds, references=refs))\n            del metric\n\n    def test_concurrent_metrics(self):\n        preds, refs = DummyMetric.predictions_and_references()\n        other_preds, other_refs = DummyMetric.other_predictions_and_references()\n        expected_results = DummyMetric.expected_results()\n        other_expected_results = DummyMetric.other_expected_results()\n\n        metric = DummyMetric(experiment_id=\"test_concurrent_metrics\")\n        other_metric = DummyMetric(\n            experiment_id=\"test_concurrent_metrics\",\n        )\n\n        self.assertDictEqual(expected_results, metric.compute(predictions=preds, references=refs))\n        self.assertDictEqual(\n            other_expected_results, other_metric.compute(predictions=other_preds, references=other_refs)\n        )\n        del metric, other_metric\n\n        metric = DummyMetric(\n            experiment_id=\"test_concurrent_metrics\",\n        )\n        other_metric = DummyMetric(\n            experiment_id=\"test_concurrent_metrics\",\n        )\n        metric.add_batch(predictions=preds, references=refs)\n        other_metric.add_batch(predictions=other_preds, references=other_refs)\n        self.assertDictEqual(expected_results, metric.compute())\n        self.assertDictEqual(other_expected_results, other_metric.compute())\n\n        for pred, ref, other_pred, other_ref in zip(preds, refs, other_preds, other_refs):\n            metric.add(prediction=pred, reference=ref)\n            other_metric.add(prediction=other_pred, reference=other_ref)\n        self.assertDictEqual(expected_results, metric.compute())\n        self.assertDictEqual(other_expected_results, other_metric.compute())\n        del metric, other_metric\n\n        # With keep_in_memory\n        metric = DummyMetric(experiment_id=\"test_concurrent_metrics\", keep_in_memory=True)\n        other_metric = DummyMetric(experiment_id=\"test_concurrent_metrics\", keep_in_memory=True)\n\n        self.assertDictEqual(expected_results, metric.compute(predictions=preds, references=refs))\n        self.assertDictEqual(\n            other_expected_results, other_metric.compute(predictions=other_preds, references=other_refs)\n        )\n\n        metric = DummyMetric(experiment_id=\"test_concurrent_metrics\", keep_in_memory=True)\n        other_metric = DummyMetric(experiment_id=\"test_concurrent_metrics\", keep_in_memory=True)\n        metric.add_batch(predictions=preds, references=refs)\n        other_metric.add_batch(predictions=other_preds, references=other_refs)\n        self.assertDictEqual(expected_results, metric.compute())\n        self.assertDictEqual(other_expected_results, other_metric.compute())\n\n        for pred, ref, other_pred, other_ref in zip(preds, refs, other_preds, other_refs):\n            metric.add(prediction=pred, reference=ref)\n            other_metric.add(prediction=other_pred, reference=other_ref)", "metadata": {"task_id": "huggingface_evaluate/178", "ground_truth": "        self.assertDictEqual(expected_results, metric.compute())", "fpath_tuple": ["huggingface_evaluate", "tests", "test_metric.py"], "context_start_lineno": 199, "line_no": 262}}
{"prompt": "onents)\n        pipe.to(torch_device)\n        pipe.set_progress_bar_config(disable=None)\n\n        # Warmup pass when using mps (see #372)\n        if torch_device == \"mps\":\n            _ = pipe(**self.get_dummy_inputs(torch_device))\n\n        output = pipe(**self.get_dummy_inputs(torch_device))[0]\n        output_tuple = pipe(**self.get_dummy_inputs(torch_device), return_dict=False)[0]\n\n        max_diff = np.abs(output - output_tuple).max()\n        self.assertLess(max_diff, 1e-4)\n\n    def test_num_inference_steps_consistent(self):\n        components = self.get_dummy_components()\n        pipe = self.pipeline_class(**components)\n        pipe.to(torch_device)\n        pipe.set_progress_bar_config(disable=None)\n\n        # Warmup pass when using mps (see #372)\n        if torch_device == \"mps\":\n            _ = pipe(**self.get_dummy_inputs(torch_device))\n\n        outputs = []\n        times = []\n        for num_steps in [9, 6, 3]:\n            inputs = self.get_dummy_inputs(torch_device)\n\n            for arg in self.num_inference_steps_args:\n                inputs[arg] = num_steps\n\n            start_time = time.time()\n            output = pipe(**inputs)[0]\n            inference_time = time.time() - start_time\n\n            outputs.append(output)\n            times.append(inference_time)\n\n        # check that all outputs have the same shape\n        self.assertTrue(all(outputs[0].shape == output.shape for output in outputs))\n        # check that the inference time increases with the number of inference steps\n        self.assertTrue(all(times[i] < times[i - 1] for i in range(1, len(times))))\n\n    def test_components_function(self):\n        init_components = self.get_dummy_components()\n        pipe = self.pipeline_class(**init_components)\n\n        self.assertTrue(hasattr(pipe, \"components\"))\n        self.assertTrue(set(pipe.components.keys()) == set(init_components.keys()))\n\n    @unittest.skipIf(torch_device!= \"cuda\", reason=\"float16 requires CUDA\")\n    def test_float16_inference(self):\n        components = self.get_dummy_components()\n        pipe = self.pipeline_class(**components)\n        pipe.to(torch_device)\n        pipe.set_progress_bar_config(disable=None)\n\n        for name, module in components.items():\n            if hasattr(module, \"half\"):\n                components[name] = module.half()\n        pipe_fp16 = self.pipeline_class(**components)\n        pipe_fp16.to(torch_device)\n        pipe_fp16.set_progress_bar_config(disable=None)\n\n        output = pipe(**self.get_dummy_inputs(torch_device))[0]\n        output_fp16 = pipe_fp16(**self.get_dummy_inputs(torch_device))[0]\n\n        max_diff = np.abs(output - output_fp16).max()\n        self.assertLess(max_diff, 1e-2, \"The outputs of the fp16 and fp32 pipelines are too different.\")\n\n    @unittest.skipIf(torch_device!= \"cuda\", reason=\"float16 requires CUDA\")\n    def test_save_load_float16(self):\n        components = self.get_dummy_components()\n        for name, module in components.items():\n            if hasattr(module, \"half\"):\n                components[name] = module.to(torch_device).half()\n        pipe = self.pipeline_class(**components)", "metadata": {"task_id": "huggingface_diffusers/198", "ground_truth": "        pipe.to(torch_device)", "fpath_tuple": ["huggingface_diffusers", "tests", "test_pipelines_common.py"], "context_start_lineno": 281, "line_no": 359}}
{"prompt": " split=\"validation[:1]\")\n        self.evaluator.prepare_data(\n            data=data,\n            input_column=\"tokens\",\n            label_column=\"ner_tags\",\n            join_by=\" \",\n        )\n\n        # Test passing in dataset by name without data_split and inferring the optimal split\n        data = self.evaluator.load_data(\"evaluate/conll2003-ci\")\n        self.evaluator.prepare_data(\n            data=data,\n            input_column=\"tokens\",\n            label_column=\"ner_tags\",\n            join_by=\" \",\n        )\n\n        # Test that it chooses the correct one (e.g. conll2003 has train, validation, test but should select test)\n        self.assertEqual(data.split, \"test\")\n\n        # Test that the data point returned is correct; this maps to the first example in the dataset\n        self.assertEqual(data[0][\"id\"], \"0\")\n\n    def test_wrong_task(self):\n        self.assertRaises(KeyError, evaluator, \"bad_task\")\n\n    def test_words_to_offsets(self):\n        task_evaluator = evaluator(\"token-classification\")\n\n        words = [\"This\", \"is\", \"a\", \"test\", \".\"]\n        join_by = \" \"\n\n        offsets = task_evaluator.words_to_offsets(words, join_by)\n\n        self.assertListEqual([(0, 3), (5, 6), (8, 8), (10, 13), (15, 15)], offsets)\n\n        words = [\"\u65e5\", \"\u672c\", \"\u8a9e\", \"\u306f\u306a\u305b\u308b\u306e?\"]\n        join_by = \"\"\n\n        offsets = task_evaluator.words_to_offsets(words, join_by)\n\n        self.assertListEqual([(0, 0), (1, 1), (2, 2), (3, 8)], offsets)\n\n    def test_predictions_processor(self):\n        task_evaluator = evaluator(\"token-classification\")\n        join_by = \" \"\n        words = [[\"New\", \"York\", \"a\", \"nice\", \"City\", \".\"]]\n\n        # aligned start and words\n        predictions = [\n            [\n                {\"start\": 0, \"entity\": \"B-LOC\"},\n                {\"start\": 2, \"entity\": \"I-LOC\"},\n                {\"start\": 4, \"entity\": \"I-LOC\"},\n                {\"start\": 9, \"entity\": \"O\"},\n                {\"start\": 11, \"entity\": \"O\"},\n                {\"start\": 16, \"entity\": \"B-LOC\"},\n                {\"start\": 21, \"entity\": \"O\"},\n            ]\n        ]\n        predictions = task_evaluator.predictions_processor(predictions, words, join_by)\n        self.assertListEqual(predictions[\"predictions\"][0], [\"B-LOC\", \"I-LOC\", \"O\", \"O\", \"B-LOC\", \"O\"])\n\n        # non-aligned start and words\n        predictions = [\n            [\n                {\"start\": 0, \"entity\": \"B-LOC\"},\n                {\"start\": 2, \"entity\": \"I-LOC\"},\n                {\"start\": 9, \"entity\": \"O\"},\n                {\"start\": 11, \"entity\": \"O\"},\n                {\"start\": 16, \"entity\": \"B-LOC\"},\n                {\"start\": 21, \"entity\": \"O\"},\n            ]\n        ]\n        predictions = task_evaluator.predictions_processor(predictions, words, join_by)\n        self.assertListEqual(predictions[\"predictions\"][0], [\"B-LOC\", \"O\", \"O\", \"O\", \"B-LOC\", \"O\"])\n\n        # non-aligned start and words\n        predictions = [\n            [\n                {\"start\": 0, \"entity\": \"B-LOC\"},\n                {\"start\": 6, \"entity\": \"I-LOC\"},\n                {\"start\": 9, \"entity\": \"O\"},\n                {\"start\": 11, \"entity\": \"O\"},\n                {\"start\": 16, \"entity\": \"B-LOC\"},\n                {\"start\": 21, \"entity\": \"O\"},\n            ]\n        ]", "metadata": {"task_id": "huggingface_evaluate/135", "ground_truth": "        predictions = task_evaluator.predictions_processor(predictions, words, join_by)", "fpath_tuple": ["huggingface_evaluate", "tests", "test_evaluator.py"], "context_start_lineno": 748, "line_no": 836}}
{"prompt": "u\"], 0)\n\n    def test_class_init(self):\n        evaluator = Text2TextGenerationEvaluator()\n        self.assertEqual(evaluator.task, \"text2text-generation\")\n        self.assertIsNone(evaluator.default_metric_name)\n\n        results = evaluator.compute(\n            model_or_pipeline=self.pipe,\n            data=self.data,\n            metric=\"bleu\",\n        )\n        self.assertEqual(results[\"bleu\"], 0)\n\n    @slow\n    def test_default_pipe_init(self):\n        results = self.evaluator.compute(data=self.data)\n        self.assertEqual(results[\"bleu\"], 0)\n\n    def test_overwrite_default_metric(self):\n        rouge = load(\"rouge\")\n        results = self.evaluator.compute(\n            model_or_pipeline=self.pipe,\n            data=self.data,\n            metric=rouge,\n        )\n        self.assertEqual(results[\"rouge1\"], 1.0)\n        results = self.evaluator.compute(\n            model_or_pipeline=self.pipe,\n            data=self.data,\n            metric=\"rouge\",\n        )\n        self.assertEqual(results[\"rouge1\"], 1.0)\n\n    def test_summarization(self):\n        pipe = DummyText2TextGenerationPipeline(task=\"summarization\", prefix=\"summary\")\n        e = evaluator(\"summarization\")\n\n        results = e.compute(\n            model_or_pipeline=pipe,\n            data=self.data,\n        )\n        self.assertEqual(results[\"rouge1\"], 1.0)\n\n    def test_translation(self):\n        pipe = DummyText2TextGenerationPipeline(task=\"translation\", prefix=\"translation\")\n        e = evaluator(\"translation\")\n\n        results = e.compute(\n            model_or_pipeline=pipe,\n            data=self.data,\n        )\n        self.assertEqual(results[\"bleu\"], 0)\n\n\nclass TestAutomaticSpeechRecognitionEvaluator(TestCase):\n    def setUp(self):\n        self.data = Dataset.from_dict(\n            {\n                \"path\": [\n                    # Examples copied from default speech model of\n                    # `automic-speech-recognition` pipeline:\n                    # https://huggingface.co/facebook/wav2vec2-base-960h\n                    # https://github.com/huggingface/transformers/blob/main/src/transformers/pipelines/__init__.py#L161\n                    \"https://cdn-media.huggingface.co/speech_samples/sample1.flac\",\n                    \"https://cdn-media.huggingface.co/speech_samples/sample2.flac\",\n                ],\n                \"sentence\": [\"Ipsum Lorem\"] * 2,\n            }\n        )\n        self.pipe = DummyAutomaticSpeechRecognitionPipeline()\n        self.evaluator = evaluator(\"automatic-speech-recognition\")\n\n    def test_pipe_init(self):\n        print(self.evaluator)\n        results = self.evaluator.compute(\n            model_or_pipeline=self.pipe,\n            data=self.data,\n        )\n        print(results)\n        self.assertEqual(results[\"wer\"], 1.0)\n\n    def test_class_init(self):\n        evaluator = AutomaticSpeechRecognitionEvaluator()\n        self.assertEqual(evaluator.task, \"automatic-speech-recognition\")\n        self.assertIsNone(evaluator.default_metric_name)\n\n        results = evaluator.compute(\n            model_or_pipeline=self.pipe,\n            data=self.data,\n            metric=\"wer\",\n        )\n        self.assertEqual(results[\"wer\"], 1.0)\n\n    @slow\n    def test_default_pipe_init(self):", "metadata": {"task_id": "huggingface_evaluate/104", "ground_truth": "        results = self.evaluator.compute(data=self.data)", "fpath_tuple": ["huggingface_evaluate", "tests", "test_evaluator.py"], "context_start_lineno": 917, "line_no": 1013}}
{"prompt": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\nimport argparse\nimport os\nimport tempfile\nfrom argparse import Namespace\nfrom collections import OrderedDict\nfrom os import path, walk\nfrom time import sleep\n\nimport pytest\nimport torch\nfrom torch import nn\n\ntry:\n    from tensorboard.backend.event_processing import event_accumulator\n    from torchrl.record.loggers import TensorboardLogger\n\n    _has_tb = True\nexcept ImportError:\n    _has_tb = False\n\nfrom tensordict import TensorDict\nfrom torchrl.data import (\n    LazyMemmapStorage,\n    LazyTensorStorage,\n    ListStorage,\n    TensorDictPrioritizedReplayBuffer,\n    TensorDictReplayBuffer,\n)\nfrom torchrl.envs.libs.gym import _has_gym\nfrom torchrl.trainers import Recorder, Trainer\nfrom torchrl.trainers.helpers import transformed_env_constructor\nfrom torchrl.trainers.trainers import (\n    _has_tqdm,\n    _has_ts,\n    BatchSubSampler,\n    CountFramesLog,\n    LogReward,\n    mask_batch,\n    OptimizerHook,\n    ReplayBufferTrainer,\n    RewardNormalizer,\n    SelectKeys,\n    UpdateWeights,\n)\n\n\ndef _fun_checker(fun, checker):\n    def new_fun(*args, **kwargs):\n        checker[0] = True\n        return fun(*args, **kwargs)\n\n    return new_fun, fun\n\n\nclass MockingOptim:\n    param_groups = [{\"params\": []}]\n\n\nclass MockingCollector:\n    called_update_policy_weights_ = False\n\n    def set_seed(self, seed, **kwargs):\n        return seed\n\n    def update_policy_weights_(self):\n        self.called_update_policy_weights_ = True\n\n    def shutdown(self):\n        pass\n\n    def state_dict(self):\n        return {}\n\n    def load_state_dict(self, state_dict):\n        pass\n\n\nclass MockingLossModule(nn.Module):\n    pass\n\n\n_mocking_optim = MockingOptim()\n\n\ndef mocking_trainer(file=None, optimizer=_mocking_optim) -> Trainer:\n    trainer = Trainer(\n        MockingCollector(),\n        *[\n            None,\n        ]\n        * 2,\n        loss_module=MockingLossModule(),\n        optimizer=optimizer,\n        save_trainer_file=file,\n    )\n    trainer._pbar_str = OrderedDict()\n    return trainer\n\n\nclass TestSelectKeys:\n    def test_selectkeys(self):\n        trainer = mocking_trainer()\n        key1 = \"first key\"\n        key2 = \"second key\"\n        td = TensorDict(\n            {\n                key1: torch.randn(3),\n                key2: torch.randn(3),\n            },\n            [],\n        )\n        trainer.register_op(\"batch_process\", SelectKeys([key1]))\n        td_out = trainer._process_batch_hook(td)\n        assert key1 in td_out.keys()\n        assert key2 not in td_out.keys()\n\n    def test_selectkeys_statedict(self):\n        if not _has_ts:\n            os.environ[\"CKPT_BACKEND\"] = \"torch\"\n        trainer = mocking_trainer()\n        key1 = \"first key\"\n        key2 = \"second key\"\n        td = TensorDict(\n            {\n                key1: torch.randn(3),\n                key2: torch.randn(3),\n            },\n            [],\n        )\n        hook = SelectKeys([key1])", "metadata": {"task_id": "pytorch_rl/36", "ground_truth": "        hook.register(trainer)", "fpath_tuple": ["pytorch_rl", "test", "test_trainer.py"], "context_start_lineno": 0, "line_no": 134}}
{"prompt": "Array,\n    ) -> Tuple[CalibState, List[Array], List[Array], PRNGKeyArray]:\n        return state, targets, outputs, rng\n\n    def on_train_end(self, state: CalibState) -> CalibState:\n        self.save_checkpoint(\n            state,\n            save_checkpoint_dir=self.save_checkpoint_dir,\n            keep=self.keep_top_n_checkpoints,\n            force_save=True,\n        )\n        return state\n\n    def on_val_start(self, state: CalibState) -> CalibState:\n        return state\n\n    def compute_metrics(\n        self,\n        preds: Array,\n        uncertainties: Array,\n        targets: Array,\n        metrics: Optional[\n            Tuple[Callable[[jnp.ndarray, jnp.ndarray, Array], Array],...]\n        ],\n    ) -> Dict[str, Array]:\n        metrics_vals = {}\n        for metric in metrics:\n            metrics_vals[metric.__name__] = metric(preds, uncertainties, targets)\n        return metrics_vals\n\n\nclass JittedMixin:\n    @partial(jax.jit, static_argnums=(0, 4))\n    def training_step(\n        self,\n        state: CalibState,\n        targets: Array,\n        outputs: Array,\n        fun: Callable,\n        rng: PRNGKeyArray,\n    ) -> Tuple[CalibState, Dict[str, Any]]:\n        return super().training_step(state, targets, outputs, fun, rng)\n\n    @partial(jax.jit, static_argnums=(0, 4))\n    def val_loss_step(\n        self,\n        state: CalibState,\n        targets: Array,\n        outputs: Array,\n        fun: Callable,\n        rng: PRNGKeyArray,\n    ) -> Dict[str, jnp.ndarray]:\n        return super().val_loss_step(state, targets, outputs, fun, rng)\n\n\nclass MultiDeviceMixin:\n    all_reduce_mean = jax.pmap(lambda x: lax.pmean(x, \"x\"), \"x\")\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.multi_device = True\n\n    @staticmethod\n    def _add_device_dim_to_array(arr: Array) -> Array:\n        n_devices = jax.local_device_count()\n        if arr.shape[0] % n_devices!= 0:\n            raise ValueError(\n                f\"The number of data points of all outputs and targets must be a multiple of {n_devices}, \"\n                f\"that is the number of available devices. However, {arr.shape[0]} were found.\"\n            )\n        return arr.reshape((n_devices, -1) + arr.shape[1:]) if arr is not None else arr\n\n    @staticmethod\n    def sync_mutable(state: CalibState) -> CalibState:\n        return (\n            state.replace(mutable=MultiDeviceMixin.all_reduce_mean(state.mutable))\n            if state.mutable[\"output_calibrator\"] is not None\n            else state\n        )\n\n    @staticmethod\n    def sync_gradients_and_loss(\n        grads: jnp.ndarray, loss: jnp.ndarray\n    ) -> Tuple[jnp.ndarray, jnp.ndarray]:\n        grad = lax.pmean(grads, axis_name=\"batch\")\n        loss = lax.pmean(loss, axis_name=\"batch\")\n        return grad, loss\n\n    def save_checkpoint(\n        self,\n        state: CalibState,\n        save_checkpoint_dir: Path,\n        keep: int = 1,\n        force_save: bool = False,\n        prefix: str = \"checkpoint_\",\n    ) -> None:\n        state = self.sync_mutable(state)\n        state = jax.device_get(tree_map(lambda x: x[0], state))", "metadata": {"task_id": "awslabs_fortuna/141", "ground_truth": "        return super(MultiDeviceMixin, self).save_checkpoint(\n            state, save_checkpoint_dir, keep, force_save, prefix\n        )", "fpath_tuple": ["awslabs_fortuna", "fortuna", "calib_model", "calib_model_calibrator.py"], "context_start_lineno": 443, "line_no": 541}}
{"prompt": "_kwargs = self.prepare_extra_step_kwargs(generator, eta)\n\n        safety_momentum = None\n\n        num_warmup_steps = len(timesteps) - num_inference_steps * self.scheduler.order\n        with self.progress_bar(total=num_inference_steps) as progress_bar:\n            for i, t in enumerate(timesteps):\n                # expand the latents if we are doing classifier free guidance\n                latent_model_input = (\n                    torch.cat([latents] * (3 if enable_safety_guidance else 2))\n                    if do_classifier_free_guidance\n                    else latents\n                )\n                latent_model_input = self.scheduler.scale_model_input(latent_model_input, t)\n\n                # predict the noise residual\n                noise_pred = self.unet(latent_model_input, t, encoder_hidden_states=prompt_embeds).sample\n\n                # perform guidance\n                if do_classifier_free_guidance:\n                    noise_pred_out = noise_pred.chunk((3 if enable_safety_guidance else 2))\n                    noise_pred_uncond, noise_pred_text = noise_pred_out[0], noise_pred_out[1]\n\n                    # default classifier free guidance\n                    noise_guidance = noise_pred_text - noise_pred_uncond\n\n                    # Perform SLD guidance\n                    if enable_safety_guidance:\n                        if safety_momentum is None:\n                            safety_momentum = torch.zeros_like(noise_guidance)\n                        noise_pred_safety_concept = noise_pred_out[2]\n\n                        # Equation 6\n                        scale = torch.clamp(\n                            torch.abs((noise_pred_text - noise_pred_safety_concept)) * sld_guidance_scale, max=1.0\n                        )\n\n                        # Equation 6\n                        safety_concept_scale = torch.where(\n                            (noise_pred_text - noise_pred_safety_concept) >= sld_threshold,\n                            torch.zeros_like(scale),\n                            scale,\n                        )\n\n                        # Equation 4\n                        noise_guidance_safety = torch.mul(\n                            (noise_pred_safety_concept - noise_pred_uncond), safety_concept_scale\n                        )\n\n                        # Equation 7\n                        noise_guidance_safety = noise_guidance_safety + sld_momentum_scale * safety_momentum\n\n                        # Equation 8\n                        safety_momentum = sld_mom_beta * safety_momentum + (1 - sld_mom_beta) * noise_guidance_safety\n\n                        if i >= sld_warmup_steps:  # Warmup\n                            # Equation 3\n                            noise_guidance = noise_guidance - noise_guidance_safety\n\n                    noise_pred = noise_pred_uncond + guidance_scale * noise_guidance\n\n                    # compute the previous noisy sample x_t -> x_t-1\n                latents = self.scheduler.step(noise_pred, t, latents, **extra_step_kwargs).prev_sample\n\n                # call the callback, if provided\n                if i == len(timesteps) - 1 or ((i + 1) > num_warmup_steps and (i + 1) % self.scheduler.order == 0):\n                    progress_bar.update()\n                    if callback is not None and i % callback_steps == 0:\n                        callback(i, t, latents)\n\n        # 8. Post-processing\n        image = self.decode_latents(latents)\n\n        # 9. Run safety checker\n        image, has_nsfw_concept, flagged_images = self.run_safety_checker(\n            image, device, prompt_embeds.dtype, enable_safety_guidance\n        )\n\n        # 10. Convert to PIL\n        if output_type == \"pil\":\n            image = self.numpy_to_pil(image)\n            if flagged_images is not None:", "metadata": {"task_id": "huggingface_diffusers/88", "ground_truth": "                flagged_images = self.numpy_to_pil(flagged_images)", "fpath_tuple": ["huggingface_diffusers", "src", "diffusers", "pipelines", "stable_diffusion_safe", "pipeline_stable_diffusion_safe.py"], "context_start_lineno": 639, "line_no": 721}}
{"prompt": "\n        Arguments:\n            - player (:obj:`ActivePlayer`): The active player to be launched a job.\n            - eval_flag (:obj:`bool`): Whether this is an evaluation job.\n        Returns:\n            - job_info (:obj:`dict`): Job info. Should include keys ['lauch_player'].\n        \"\"\"\n        raise NotImplementedError\n\n    def judge_snapshot(self, player_id: str, force: bool = False) -> bool:\n        \"\"\"\n        Overview:\n            Judge whether a player is trained enough for snapshot. If yes, call player's ``snapshot``, create a\n            historical player(prepare the checkpoint and add it to the shared payoff), then mutate it, and return True.\n            Otherwise, return False.\n        Arguments:\n            - player_id (:obj:`ActivePlayer`): The active player's id.\n        Returns:\n            - snapshot_or_not (:obj:`dict`): Whether the active player is snapshotted.\n        \"\"\"\n        with self._active_players_lock:\n            idx = self.active_players_ids.index(player_id)\n            player = self.active_players[idx]\n            if force or player.is_trained_enough():\n                # Snapshot\n                hp = player.snapshot(self.metric_env)\n                self.save_checkpoint(player.checkpoint_path, hp.checkpoint_path)\n                self.historical_players.append(hp)\n                self.payoff.add_player(hp)\n                # Mutate\n                self._mutate_player(player)\n                return True\n            else:\n                return False\n\n    @abstractmethod\n    def _mutate_player(self, player: ActivePlayer) -> None:\n        \"\"\"\n        Overview:\n            Players have the probability to mutate, e.g. Reset network parameters.\n            Called by ``self.judge_snapshot``.\n        Arguments:\n            - player (:obj:`ActivePlayer`): The active player that may mutate.\n        \"\"\"\n        raise NotImplementedError\n\n    def update_active_player(self, player_info: dict) -> None:\n        \"\"\"\n        Overview:\n            Update an active player's info.\n        Arguments:\n            - player_info (:obj:`dict`): Info dict of the player which is to be updated.\n        ArgumentsKeys:\n            - necessary: `player_id`, `train_iteration`\n        \"\"\"\n        try:\n            idx = self.active_players_ids.index(player_info['player_id'])\n            player = self.active_players[idx]\n            return self._update_player(player, player_info)\n        except ValueError as e:\n            print(e)\n\n    @abstractmethod\n    def _update_player(self, player: ActivePlayer, player_info: dict) -> None:\n        \"\"\"\n        Overview:\n            Update an active player. Called by ``self.update_active_player``.\n        Arguments:\n            - player (:obj:`ActivePlayer`): The active player that will be updated.\n            - player_info (:obj:`dict`): Info dict of the active player which is to be updated.\n        \"\"\"\n        raise NotImplementedError\n\n    def finish_job(self, job_info: dict) -> None:\n        \"\"\"\n        Overview:\n            Finish current job. Update shared payoff to record the game results.\n        Arguments:\n            - job_info (:obj:`dict`): A dict containing job result information.\n        \"\"\"\n        # TODO(nyz) more fine-grained job info\n        self.payoff.update(job_info)\n        if 'eval_flag' in job_info and job_info['eval_flag']:\n            home_id, away_id = job_info['player_id']\n            home_player, away_player = self.get_player_by_id(home_id), self.get_player_by_id(away_id)\n            job_info_result = job_info['result']\n            if isinstance(job_info_result[0], list):\n                job_info_result = sum(job_info_result, [])", "metadata": {"task_id": "opendilab_ACE/94", "ground_truth": "            home_player.rating, away_player.rating = self.metric_env.rate_1vs1(\n                home_player.rating, away_player.rating, result=job_info_result\n            )", "fpath_tuple": ["opendilab_ACE", "ding", "league", "base_league.py"], "context_start_lineno": 160, "line_no": 248}}
{"prompt": "input_converter(\n      pc: vz.ParameterConfig) -> converters.DefaultModelInputConverter:\n    return converters.DefaultModelInputConverter(\n        pc, scale=True, max_discrete_indices=0, onehot_embed=True)\n\n  return [create_input_converter(pc) for pc in search_space.parameters]\n\n\ndef _create_metric_converter(\n    mc: vz.MetricInformation) -> converters.DefaultModelOutputConverter:\n  # TODO: Do something other than raising an error\n  return converters.DefaultModelOutputConverter(\n      mc,\n      flip_sign_for_minimization_metrics=True,\n      shift_safe_metrics=True,\n      raise_errors_for_missing_metrics=True)\n\n\nclass PopulationConverter(templates.PopulationConverter):\n  \"\"\"Population converter.\"\"\"\n\n  def __init__(self,\n               search_space: vz.SearchSpace,\n               metrics: Collection[vz.MetricInformation],\n               *,\n               metadata_ns: str = 'population'):\n    self._objective_metrics, self._safe_metrics = _filter_and_split(metrics)\n    self._num_objective_metrics = len(self._objective_metrics)\n    self._num_safe_metrics = len(self._safe_metrics)\n    self._metrics = self._objective_metrics + self._safe_metrics\n    self._metadata_ns = metadata_ns\n\n    self._trial_converter = converters.DefaultTrialConverter(\n        _create_parameter_converters(search_space),\n        [_create_metric_converter(mc) for mc in self._metrics])\n    self._empty_feature_dict = converters.DictOf2DArrays(\n        self._trial_converter.to_features([]))\n\n  def to_suggestions(  # pytype: disable=signature-mismatch  # overriding-parameter-type-checks\n      self, offsprings: Offspring) -> Collection[vz.TrialSuggestion]:\n    parameters_list = self._trial_converter.to_parameters(\n        self._empty_feature_dict.dict_like(offsprings.xs))\n    suggestions = [vz.TrialSuggestion(p) for p in parameters_list]\n    for idx, t in enumerate(suggestions):\n      t.metadata.ns(self._metadata_ns).update(offsprings[idx:idx + 1].dump())\n    return suggestions\n\n  def _empty_offsprings(self) -> Offspring:\n    return Offspring(self._empty_feature_dict.asarray(), np.zeros([0]),\n                     np.zeros([0]))\n\n  def to_population(self, completed: Sequence[vz.CompletedTrial]) -> Population:\n    \"\"\"Converts trials into population. Accepts an empty list.\"\"\"\n    offsprings = self._empty_offsprings()  # create empty\n    # Each Trial should contain its genes as metadata. (Note that\n    # genes-to-trial mapping is many-to-one). We try to load the genes.\n    for t in completed:\n      metadata = t.metadata.ns(self._metadata_ns)\n      try:\n        offsprings += Offspring.load(metadata)\n      except serializable.DecodeError:\n        # Upon failure, arbitrarily choose one set of genes that map to the\n        # current trial.\n        offsprings += Offspring(\n            converters.DictOf2DArrays(self._trial_converter.to_features(\n                [t])).asarray(), np.zeros([1]), np.zeros([1]))\n\n    ys = self._trial_converter.to_labels_array(completed)\n    return Population(offsprings.xs, ys[:, :self._num_objective_metrics],\n                      ys[:, self._num_objective_metrics:],\n                      np.zeros([ys.shape[0]]), offsprings.generations,\n                      offsprings.ids,", "metadata": {"task_id": "google_vizier/140", "ground_truth": "                      np.asarray([t.id for t in completed], dtype=np.int32))", "fpath_tuple": ["google_vizier", "vizier", "_src", "algorithms", "evolution", "numpy_populations.py"], "context_start_lineno": 246, "line_no": 318}}
{"prompt": " FedSage_Plus\nfrom federatedscope.gfl.fedsageplus.utils import GraphMender, HideGraph\nfrom federatedscope.gfl.fedsageplus.trainer import LocalGenTrainer, \\\n    FedGenTrainer\n\nlogger = logging.getLogger(__name__)\n\n\nclass FedSagePlusServer(Server):\n    def __init__(self,\n                 ID=-1,\n                 state=0,\n                 config=None,\n                 data=None,\n                 model=None,\n                 client_num=5,\n                 total_round_num=10,\n                 device='cpu',\n                 strategy=None,\n                 **kwargs):\n        r\"\"\"\n        FedSage+ consists of three of training stages.\n        Stage1: 0, local pre-train for generator.\n        Stage2: -> 2 * fedgen_epoch, federated training for generator.\n        Stage3: -> 2 * fedgen_epoch + total_round_num: federated training\n        for GraphSAGE Classifier\n        \"\"\"\n        super(FedSagePlusServer,\n              self).__init__(ID, state, config, data, model, client_num,\n                             total_round_num, device, strategy, **kwargs)\n\n        assert self.model_num == 1, \"Not supported multi-model for \" \\\n                                    \"FedSagePlusServer\"\n\n        # If state < fedgen_epoch and state % 2 == 0:\n        #     Server receive [model, embedding, label]\n        # If state < fedgen_epoch and state % 2 == 1:\n        #     Server receive [gradient]\n        self.fedgen_epoch = 2 * self._cfg.fedsageplus.fedgen_epoch\n        self.total_round_num = total_round_num + self.fedgen_epoch\n        self.grad_cnt = 0\n\n    def _register_default_handlers(self):\n        self.register_handlers('join_in', self.callback_funcs_for_join_in)\n        self.register_handlers('join_in_info', self.callback_funcs_for_join_in)\n        self.register_handlers('clf_para', self.callback_funcs_model_para)\n        self.register_handlers('gen_para', self.callback_funcs_model_para)\n        self.register_handlers('gradient', self.callback_funcs_gradient)\n        self.register_handlers('metrics', self.callback_funcs_for_metrics)\n\n    def callback_funcs_for_join_in(self, message: Message):\n        if 'info' in message.msg_type:\n            sender, info = message.sender, message.content\n            for key in self._cfg.federate.join_in_info:\n                assert key in info\n            self.join_in_info[sender] = info\n            logger.info('Server: Client #{:d} has joined in!'.format(sender))\n        else:\n            self.join_in_client_num += 1\n            sender, address = message.sender, message.content\n            if int(sender) == -1:  # assign number to client\n                sender = self.join_in_client_num\n                self.comm_manager.add_neighbors(neighbor_id=sender,\n                                                address=address)\n                self.comm_manager.send(\n                    Message(msg_type='assign_client_id',\n                            sender=self.ID,\n                            receiver=[sender],\n                            state=self.state,\n                            content=str(sender)))\n            else:\n                self.comm_manager.add_neighbors(neighbor_id=sender,\n                                                address=address)\n\n            if len(self._cfg.federate.join_in_info)!= 0:\n                self.comm_manager.send(\n                    Message(msg_type='ask_for_join_in_info',\n                            sender=self.ID,\n                            receiver=[sender],\n                            state=self.state,\n                            content=self._cfg.federate.join_in_info.copy()))", "metadata": {"task_id": "alibaba_FederatedScope/98", "ground_truth": "        if self.check_client_join_in():", "fpath_tuple": ["alibaba_FederatedScope", "federatedscope", "gfl", "fedsageplus", "worker.py"], "context_start_lineno": 13, "line_no": 95}}
{"prompt": "\"\"\"\nTest the ray classes.\n\"\"\"\n\nimport pytest\nimport torch\n\nfrom nerfstudio.cameras.rays import Frustums\n\n\ndef test_frustum_get_position():\n    \"\"\"Test position calculation\"\"\"\n\n    origin = torch.Tensor([0, 1, 2])[None,...]\n    direction = torch.Tensor([0, 1, 0])[None,...]\n    frustum_start = torch.Tensor([2])[None,...]\n    frustum_end = torch.Tensor([3])[None,...]\n\n    target_position = torch.Tensor([0, 3.5, 2])[None,...]\n\n    frustum = Frustums(\n        origins=origin,\n        directions=direction,\n        starts=frustum_start,\n        ends=frustum_end,\n        pixel_area=torch.ones((1, 1)),\n    )\n\n    positions = frustum.get_positions()\n    assert positions == pytest.approx(target_position, abs=1e-6)\n\n\ndef test_frustum_get_gaussian_blob():\n    \"\"\"Test gaussian blob calculation\"\"\"\n\n    frustum = Frustums(\n        origins=torch.ones((5, 3)),\n        directions=torch.ones((5, 3)),\n        starts=torch.ones((5, 1)),\n        ends=torch.ones((5, 1)),\n        pixel_area=torch.ones((5, 1)),\n    )", "metadata": {"task_id": "nerfstudio-project_nerfstudio/55", "ground_truth": "    gaussian_blob = frustum.get_gaussian_blob()", "fpath_tuple": ["nerfstudio-project_nerfstudio", "tests", "cameras", "test_rays.py"], "context_start_lineno": 0, "line_no": 43}}
{"prompt": "iment_id=\"test_dummy_metric_pickle\")\n\n            with open(tmp_file, \"wb\") as f:\n                pickle.dump(metric, f)\n            del metric\n\n            with open(tmp_file, \"rb\") as f:\n                metric = pickle.load(f)\n            self.assertDictEqual(expected_results, metric.compute(predictions=preds, references=refs))\n            del metric\n\n    def test_input_numpy(self):\n        import numpy as np\n\n        preds, refs = DummyMetric.predictions_and_references()\n        expected_results = DummyMetric.expected_results()\n        preds, refs = np.array(preds), np.array(refs)\n\n        metric = DummyMetric(experiment_id=\"test_input_numpy\")\n        self.assertDictEqual(expected_results, metric.compute(predictions=preds, references=refs))\n        del metric\n\n        metric = DummyMetric(experiment_id=\"test_input_numpy\")\n        metric.add_batch(predictions=preds, references=refs)\n        self.assertDictEqual(expected_results, metric.compute())\n        del metric\n\n        metric = DummyMetric(experiment_id=\"test_input_numpy\")\n        for pred, ref in zip(preds, refs):\n            metric.add(prediction=pred, reference=ref)\n        self.assertDictEqual(expected_results, metric.compute())\n        del metric\n\n    @require_torch\n    def test_input_torch(self):\n        import torch\n\n        preds, refs = DummyMetric.predictions_and_references()\n        expected_results = DummyMetric.expected_results()\n        preds, refs = torch.tensor(preds), torch.tensor(refs)\n\n        metric = DummyMetric(experiment_id=\"test_input_torch\")\n        self.assertDictEqual(expected_results, metric.compute(predictions=preds, references=refs))\n        del metric\n\n        metric = DummyMetric(experiment_id=\"test_input_torch\")\n        metric.add_batch(predictions=preds, references=refs)\n        self.assertDictEqual(expected_results, metric.compute())\n        del metric\n\n        metric = DummyMetric(experiment_id=\"test_input_torch\")\n        for pred, ref in zip(preds, refs):\n            metric.add(prediction=pred, reference=ref)\n        self.assertDictEqual(expected_results, metric.compute())\n        del metric\n\n    @require_tf\n    def test_input_tf(self):\n        import tensorflow as tf\n\n        preds, refs = DummyMetric.predictions_and_references()\n        expected_results = DummyMetric.expected_results()\n        preds, refs = tf.constant(preds), tf.constant(refs)\n\n        metric = DummyMetric(experiment_id=\"test_input_tf\")\n        self.assertDictEqual(expected_results, metric.compute(predictions=preds, references=refs))\n        del metric\n\n        metric = DummyMetric(experiment_id=\"test_input_tf\")\n        metric.add_batch(predictions=preds, references=refs)\n        self.assertDictEqual(expected_results, metric.compute())\n        del metric\n\n        metric = DummyMetric(experiment_id=\"test_input_tf\")\n        for pred, ref in zip(preds, refs):\n            metric.add(prediction=pred, reference=ref)\n        self.assertDictEqual(expected_results, metric.compute())\n        del metric\n\n    def test_string_casting(self):\n        metric = DummyMetric(experiment_id=\"test_string_casting\")\n        metric.info.features = Features({\"predictions\": Value(\"string\"), \"references\": Value(\"string\")})", "metadata": {"task_id": "huggingface_evaluate/175", "ground_truth": "        metric.compute(predictions=[\"a\"], references=[\"a\"])", "fpath_tuple": ["huggingface_evaluate", "tests", "test_metric.py"], "context_start_lineno": 423, "line_no": 505}}
{"prompt": "_seed(0)\n        unet = UNet2DConditionModel(\n            block_out_channels=(32, 64),\n            layers_per_block=2,\n            sample_size=32,\n            in_channels=9,\n            out_channels=4,\n            down_block_types=(\"DownBlock2D\", \"CrossAttnDownBlock2D\"),\n            up_block_types=(\"CrossAttnUpBlock2D\", \"UpBlock2D\"),\n            cross_attention_dim=32,\n            # SD2-specific config below\n            attention_head_dim=(2, 4, 8, 8),\n            use_linear_projection=True,\n        )\n        scheduler = PNDMScheduler(skip_prk_steps=True)\n        torch.manual_seed(0)\n        vae = AutoencoderKL(\n            block_out_channels=[32, 64],\n            in_channels=3,\n            out_channels=3,\n            down_block_types=[\"DownEncoderBlock2D\", \"DownEncoderBlock2D\"],\n            up_block_types=[\"UpDecoderBlock2D\", \"UpDecoderBlock2D\"],\n            latent_channels=4,\n            sample_size=128,\n        )\n        torch.manual_seed(0)\n        text_encoder_config = CLIPTextConfig(\n            bos_token_id=0,\n            eos_token_id=2,\n            hidden_size=32,\n            intermediate_size=37,\n            layer_norm_eps=1e-05,\n            num_attention_heads=4,\n            num_hidden_layers=5,\n            pad_token_id=1,\n            vocab_size=1000,\n            # SD2-specific config below\n            hidden_act=\"gelu\",\n            projection_dim=512,\n        )\n        text_encoder = CLIPTextModel(text_encoder_config)\n        tokenizer = CLIPTokenizer.from_pretrained(\"hf-internal-testing/tiny-random-clip\")\n\n        components = {\n            \"unet\": unet,\n            \"scheduler\": scheduler,\n            \"vae\": vae,\n            \"text_encoder\": text_encoder,\n            \"tokenizer\": tokenizer,\n            \"safety_checker\": None,\n            \"feature_extractor\": None,\n        }\n        return components\n\n    def get_dummy_inputs(self, device, seed=0):\n        # TODO: use tensor inputs instead of PIL, this is here just to leave the old expected_slices untouched\n        image = floats_tensor((1, 3, 32, 32), rng=random.Random(seed)).to(device)\n        image = image.cpu().permute(0, 2, 3, 1)[0]\n        init_image = Image.fromarray(np.uint8(image)).convert(\"RGB\").resize((64, 64))\n        mask_image = Image.fromarray(np.uint8(image + 4)).convert(\"RGB\").resize((64, 64))\n        if str(device).startswith(\"mps\"):\n            generator = torch.manual_seed(seed)\n        else:\n            generator = torch.Generator(device=device).manual_seed(seed)\n        inputs = {\n            \"prompt\": \"A painting of a squirrel eating a burger\",\n            \"image\": init_image,\n            \"mask_image\": mask_image,\n            \"generator\": generator,\n            \"num_inference_steps\": 2,\n            \"guidance_scale\": 6.0,\n            \"output_type\": \"numpy\",\n        }\n        return inputs\n\n    def test_stable_diffusion_inpaint(self):\n        device = \"cpu\"  # ensure determinism for the device-dependent torch.Generator\n        components = self.get_dummy_components()\n        sd_pipe = StableDiffusionInpaintPipeline(**components)\n        sd_pipe = sd_pipe.to(device)\n        sd_pipe.set_progress_bar_config(disable=None)", "metadata": {"task_id": "huggingface_diffusers/161", "ground_truth": "        inputs = self.get_dummy_inputs(device)", "fpath_tuple": ["huggingface_diffusers", "tests", "pipelines", "stable_diffusion_2", "test_stable_diffusion_inpaint.py"], "context_start_lineno": 38, "line_no": 120}}
{"prompt": "from federatedscope.core.configs.config import CN\nfrom federatedscope.core.configs.yacs_config import Argument\nfrom federatedscope.register import register_config\n\n\ndef extend_fl_algo_cfg(cfg):\n    # ---------------------------------------------------------------------- #\n    # fedopt related options, a general fl algorithm\n    # ---------------------------------------------------------------------- #\n    cfg.fedopt = CN()\n\n    cfg.fedopt.use = False\n\n    cfg.fedopt.optimizer = CN(new_allowed=True)\n    cfg.fedopt.optimizer.type = Argument(\n        'SGD', description=\"optimizer type for FedOPT\")\n    cfg.fedopt.optimizer.lr = Argument(\n        0.01, description=\"learning rate for FedOPT optimizer\")\n\n    # ---------------------------------------------------------------------- #\n    # fedprox related options, a general fl algorithm\n    # ---------------------------------------------------------------------- #", "metadata": {"task_id": "alibaba_FederatedScope/130", "ground_truth": "    cfg.fedprox = CN()", "fpath_tuple": ["alibaba_FederatedScope", "federatedscope", "core", "configs", "cfg_fl_algo.py"], "context_start_lineno": 0, "line_no": 22}}
{"prompt": "import evaluate\nfrom evaluate.utils import launch_gradio_widget\n", "metadata": {"task_id": "huggingface_evaluate/0", "ground_truth": "module = evaluate.load(\"recall\")", "fpath_tuple": ["huggingface_evaluate", "metrics", "recall", "app.py"], "context_start_lineno": 0, "line_no": 4}}
{"prompt": "        if isinstance(reward_spec, CompositeSpec):\n            # If reward_spec is a CompositeSpec, all in_keys should be keys of reward_spec\n            if not all(k in reward_spec.keys() for k in self.in_keys):\n                raise KeyError(\"Not all in_keys are present in \u00b4reward_spec\u00b4\")\n\n            # Define episode specs for all out_keys\n            for out_key in self.out_keys:\n                episode_spec = UnboundedContinuousTensorSpec(\n                    shape=reward_spec.shape,\n                    device=reward_spec.device,\n                    dtype=reward_spec.dtype,\n                )\n                episode_specs.update({out_key: episode_spec})\n\n        else:\n            # If reward_spec is not a CompositeSpec, the only in_key should be \u00b4reward\u00b4\n            if not set(self.in_keys) == {\"reward\"}:\n                raise KeyError(\n                    \"reward_spec is not a CompositeSpec class, in_keys should only include \u00b4reward\u00b4\"\n                )\n\n            # Define episode spec\n            episode_spec = UnboundedContinuousTensorSpec(\n                device=reward_spec.device,\n                dtype=reward_spec.dtype,\n                shape=reward_spec.shape,\n            )\n            episode_specs.update({\"episode_reward\": episode_spec})\n\n        # Update observation_spec with episode_specs\n        if not isinstance(observation_spec, CompositeSpec):\n            observation_spec = CompositeSpec(\n                observation=observation_spec, shape=self.parent.batch_size\n            )\n        observation_spec.update(episode_specs)\n        return observation_spec\n\n\nclass StepCounter(Transform):\n    \"\"\"Counts the steps from a reset and sets the done state to True after a certain number of steps.\n\n    Args:\n        max_steps (:obj:`int`, optional): a positive integer that indicates the maximum number of steps to take before\n        setting the done state to True. If set to None (the default value), the environment will run indefinitely until\n        the done state is manually set by the user or by the environment itself. However, the step count will still be\n        incremented on each call to step() into the `step_count` attribute.\n    \"\"\"\n\n    invertible = False\n\n    def __init__(self, max_steps: Optional[int] = None):\n        if max_steps is not None and max_steps < 1:\n            raise ValueError(\"max_steps should have a value greater or equal to one.\")\n        self.max_steps = max_steps\n        super().__init__([])\n\n    def reset(self, tensordict: TensorDictBase) -> TensorDictBase:\n        _reset = tensordict.get(\n            \"_reset\",\n            default=torch.ones(\n                tensordict.batch_size, dtype=torch.bool, device=tensordict.device\n            ),\n        )\n        step_count = tensordict.get(\n            \"step_count\",\n            torch.zeros(\n                tensordict.batch_size,\n                dtype=torch.int64,\n                device=tensordict.device,\n            ),\n        )\n        step_count[_reset] = 0\n        tensordict.set(\n            \"step_count\",\n            step_count,\n        )\n        return tensordict\n\n    def _step(self, tensordict: TensorDictBase) -> TensorDictBase:\n        step_count = tensordict.get(\n            \"step_count\",\n            torch.zeros(\n                tensordict.batch_size,\n                dtype=torch.int64,\n                device=tensordict.device,\n            ),\n        )\n        next_step_count = step_count + 1\n        tensordict.set(\"step_count\", next_step_count)\n        if self.max_steps is not None:\n            done = tensordict.get(\"done\")\n            done = done | (next_step_count >= self.max_steps).unsqueeze(-1)", "metadata": {"task_id": "pytorch_rl/87", "ground_truth": "            tensordict.set(\"done\", done)", "fpath_tuple": ["pytorch_rl", "torchrl", "envs", "transforms", "transforms.py"], "context_start_lineno": 2633, "line_no": 2725}}
{"prompt": "ior(\n        hidden_dim=cfg.rssm_hidden_dim, state_dim=cfg.state_dim\n    )\n    reward_module = MLP(\n        out_features=1, depth=2, num_cells=cfg.mlp_num_units, activation_class=nn.ELU\n    )\n\n    world_model = _dreamer_make_world_model(\n        obs_encoder, obs_decoder, rssm_prior, rssm_posterior, reward_module\n    ).to(device)\n    with torch.no_grad(), set_exploration_mode(\"random\"):\n        tensordict = proof_environment.rollout(4)\n        tensordict = tensordict.to_tensordict().to(device)\n        tensordict = tensordict.to(device)\n        world_model(tensordict)\n\n    model_based_env = _dreamer_make_mbenv(\n        reward_module,\n        rssm_prior,\n        obs_decoder,\n        proof_environment,\n        use_decoder_in_env,\n        cfg.state_dim,\n        cfg.rssm_hidden_dim,\n    )\n    model_based_env = model_based_env.to(device)\n\n    actor_simulator, actor_realworld = _dreamer_make_actors(\n        obs_encoder,\n        rssm_prior,\n        rssm_posterior,\n        cfg.mlp_num_units,\n        action_key,\n        proof_environment,\n    )\n    actor_simulator = actor_simulator.to(device)\n\n    value_model = _dreamer_make_value_model(cfg.mlp_num_units, value_key)\n    value_model = value_model.to(device)\n    with torch.no_grad(), set_exploration_mode(\"random\"):\n        tensordict = model_based_env.rollout(4)\n        tensordict = tensordict.to(device)\n        tensordict = actor_simulator(tensordict)\n        value_model(tensordict)\n\n    actor_realworld = actor_realworld.to(device)\n    if proof_env_is_none:\n        proof_environment.close()\n        torch.cuda.empty_cache()\n        del proof_environment\n\n    del tensordict\n    return world_model, model_based_env, actor_simulator, value_model, actor_realworld\n\n\ndef _dreamer_make_world_model(\n    obs_encoder, obs_decoder, rssm_prior, rssm_posterior, reward_module\n):\n    # World Model and reward model\n    rssm_rollout = RSSMRollout(\n        SafeModule(\n            rssm_prior,\n            in_keys=[\"state\", \"belief\", \"action\"],\n            out_keys=[\n                (\"next\", \"prior_mean\"),\n                (\"next\", \"prior_std\"),\n                \"_\",\n                (\"next\", \"belief\"),\n            ],\n        ),\n        SafeModule(\n            rssm_posterior,\n            in_keys=[(\"next\", \"belief\"), (\"next\", \"encoded_latents\")],\n            out_keys=[\n                (\"next\", \"posterior_mean\"),\n                (\"next\", \"posterior_std\"),\n                (\"next\", \"state\"),\n            ],\n        ),\n    )\n\n    transition_model = SafeSequential(\n        SafeModule(\n            obs_encoder,\n            in_keys=[(\"next\", \"pixels\")],\n            out_keys=[(\"next\", \"encoded_latents\")],\n        ),\n        rssm_rollout,\n        SafeModule(\n            obs_decoder,\n            in_keys=[(\"next\", \"state\"), (\"next\", \"belief\")],\n            out_keys=[(\"next\", \"reco_pixels\")],\n        ),\n    )\n    reward_model = SafeModule(\n        reward_module,\n        in_keys=[(\"next\", \"state\"), (\"next\", \"belief\")],\n        out_keys=[\"reward\"],\n    )", "metadata": {"task_id": "pytorch_rl/66", "ground_truth": "    world_model = WorldModelWrapper(\n        transition_model,\n        reward_model,\n    )", "fpath_tuple": ["pytorch_rl", "torchrl", "trainers", "helpers", "models.py"], "context_start_lineno": 1511, "line_no": 1610}}
{"prompt": " Union\n\n# Lint as: python3\nfrom datasets import Dataset\n\n\ntry:\n    TRANSFORMERS_AVAILABLE = True\nexcept ImportError:\n    TRANSFORMERS_AVAILABLE = False\n\nfrom typing_extensions import Literal\n\nfrom..module import EvaluationModule\nfrom..utils.file_utils import add_end_docstrings, add_start_docstrings\nfrom..utils.logging import get_logger\nfrom.base import EVALUATOR_COMPUTE_RETURN_DOCSTRING, EVALUTOR_COMPUTE_START_DOCSTRING, Evaluator\nfrom.utils import DatasetColumn\n\n\nlogger = get_logger(__name__)\n\n\nTASK_DOCUMENTATION = r\"\"\"\n    Examples:\n    ```python\n    >>> from evaluate import evaluator\n    >>> from datasets import load_dataset\n    >>> task_evaluator = evaluator(\"question-answering\")\n    >>> data = load_dataset(\"squad\", split=\"validation[:2]\")\n    >>> results = task_evaluator.compute(\n    >>>     model_or_pipeline=\"sshleifer/tiny-distilbert-base-cased-distilled-squad\",\n    >>>     data=data,\n    >>>     metric=\"squad\",\n    >>> )\n    ```\n\n    <Tip>\n\n    Datasets where the answer may be missing in the context are supported, for example SQuAD v2 dataset. In this case, it is safer to pass `squad_v2_format=True` to\n    the compute() call.\n\n    </Tip>\n\n    ```python\n    >>> from evaluate import evaluator\n    >>> from datasets import load_dataset\n    >>> task_evaluator = evaluator(\"question-answering\")\n    >>> data = load_dataset(\"squad_v2\", split=\"validation[:2]\")\n    >>> results = task_evaluator.compute(\n    >>>     model_or_pipeline=\"mrm8488/bert-tiny-finetuned-squadv2\",\n    >>>     data=data,\n    >>>     metric=\"squad_v2\",\n    >>>     squad_v2_format=True,\n    >>> )\n    ```\n\"\"\"\n\n\nclass QuestionAnsweringEvaluator(Evaluator):\n    \"\"\"\n    Question answering evaluator. This evaluator handles\n    [**extractive** question answering](https://huggingface.co/docs/transformers/task_summary#extractive-question-answering),\n    where the answer to the question is extracted from a context.\n\n    This question answering evaluator can currently be loaded from [`evaluator`] using the default task name\n    `question-answering`.\n\n    Methods in this class assume a data format compatible with the\n    [`~transformers.QuestionAnsweringPipeline`].\n    \"\"\"\n\n    PIPELINE_KWARGS = {}\n\n    def __init__(self, task=\"question-answering\", default_metric_name=None):\n        super().__init__(task, default_metric_name=default_metric_name)\n\n    def prepare_data(\n        self, data: Dataset, question_column: str, context_column: str, id_column: str, label_column: str\n    ):\n        \"\"\"Prepare data.\"\"\"\n        if data is None:\n            raise ValueError(\n                \"Please specify a valid `data` object - either a `str` with a name or a `Dataset` object.\"\n            )\n        self.check_required_columns(\n            data,\n            {\n                \"question_column\": question_column,\n                \"context_column\": context_column,\n                \"id_column\": id_column,\n                \"label_column\": label_column,\n            },\n        )\n\n        metric_inputs = dict()\n        metric_inputs[\"references\"] = [\n            {\"id\": element[id_column], \"answers\": element[label_column]} for element in data\n        ]\n\n        return metric_inputs, {\n            \"question\": DatasetColumn(data, question_column),", "metadata": {"task_id": "huggingface_evaluate/140", "ground_truth": "            \"context\": DatasetColumn(data, context_column),", "fpath_tuple": ["huggingface_evaluate", "src", "evaluate", "evaluator", "question_answering.py"], "context_start_lineno": 14, "line_no": 116}}
{"prompt": "from federatedscope.core.configs.config import CN\nfrom federatedscope.core.configs.yacs_config import Argument\nfrom federatedscope.register import register_config\n\n\ndef extend_fl_algo_cfg(cfg):\n    # ---------------------------------------------------------------------- #\n    # fedopt related options, a general fl algorithm\n    # ---------------------------------------------------------------------- #\n    cfg.fedopt = CN()\n\n    cfg.fedopt.use = False\n\n    cfg.fedopt.optimizer = CN(new_allowed=True)\n    cfg.fedopt.optimizer.type = Argument(\n        'SGD', description=\"optimizer type for FedOPT\")\n    cfg.fedopt.optimizer.lr = Argument(\n        0.01, description=\"learning rate for FedOPT optimizer\")\n\n    # ---------------------------------------------------------------------- #\n    # fedprox related options, a general fl algorithm\n    # ---------------------------------------------------------------------- #\n    cfg.fedprox = CN()\n\n    cfg.fedprox.use = False\n    cfg.fedprox.mu = 0.\n\n    # ---------------------------------------------------------------------- #\n    # Personalization related options, pFL\n    # ---------------------------------------------------------------------- #\n    cfg.personalization = CN()\n\n    # client-distinct param names, e.g., ['pre', 'post']\n    cfg.personalization.local_param = []\n    cfg.personalization.share_non_trainable_para = False\n    cfg.personalization.local_update_steps = -1\n    # @regular_weight:\n    # The smaller the regular_weight is, the stronger emphasising on\n    # personalized model\n    # For Ditto, the default value=0.1, the search space is [0.05, 0.1, 0.2,\n    # 1, 2]\n    # For pFedMe, the default value=15\n    cfg.personalization.regular_weight = 0.1\n\n    # @lr:\n    # 1) For pFedME, the personalized learning rate to calculate theta\n    # approximately using K steps\n    # 2) 0.0 indicates use the value according to optimizer.lr in case of\n    # users have not specify a valid lr\n    cfg.personalization.lr = 0.0\n\n    cfg.personalization.K = 5  # the local approximation steps for pFedMe\n    cfg.personalization.beta = 1.0  # the average moving parameter for pFedMe\n\n    # ---------------------------------------------------------------------- #\n    # FedSage+ related options, gfl\n    # ---------------------------------------------------------------------- #\n    cfg.fedsageplus = CN()\n\n    # Number of nodes generated by the generator\n    cfg.fedsageplus.num_pred = 5\n    # Hidden layer dimension of generator\n    cfg.fedsageplus.gen_hidden = 128\n    # Hide graph portion\n    cfg.fedsageplus.hide_portion = 0.5\n    # Federated training round for generator\n    cfg.fedsageplus.fedgen_epoch = 200\n    # Local pre-train round for generator\n    cfg.fedsageplus.loc_epoch = 1\n    # Coefficient for criterion number of missing node\n    cfg.fedsageplus.a = 1.0\n    # Coefficient for criterion feature\n    cfg.fedsageplus.b = 1.0\n    # Coefficient for criterion classification\n    cfg.fedsageplus.c = 1.0\n\n    # ---------------------------------------------------------------------- #\n    # GCFL+ related options, gfl\n    # ---------------------------------------------------------------------- #\n    cfg.gcflplus = CN()\n\n    # Bound for mean_norm\n    cfg.gcflplus.EPS_1 = 0.05\n    # Bound for max_norm\n    cfg.gcflplus.EPS_2 = 0.1\n    # Length of the gradient sequence\n    cfg.gcflplus.seq_length = 5\n    # Whether standardized dtw_distances\n    cfg.gcflplus.standardize = False\n\n    # ---------------------------------------------------------------------- #\n    # FLIT+ related options, gfl\n    # ---------------------------------------------------------------------- #", "metadata": {"task_id": "alibaba_FederatedScope/182", "ground_truth": "    cfg.flitplus = CN()", "fpath_tuple": ["alibaba_FederatedScope", "federatedscope", "core", "configs", "cfg_fl_algo.py"], "context_start_lineno": 0, "line_no": 93}}
{"prompt": "# Copyright 2022 The Nerfstudio Team. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Fields for nerf-w\"\"\"\n\nfrom typing import Dict, Optional, Tuple\n\nimport torch\nfrom torch import nn\nfrom torchtyping import TensorType\n\nfrom nerfstudio.cameras.rays import RaySamples\nfrom nerfstudio.field_components.embedding import Embedding\nfrom nerfstudio.field_components.encodings import Encoding, Identity\nfrom nerfstudio.field_components.field_heads import (\n    DensityFieldHead,\n    FieldHeadNames,\n    RGBFieldHead,\n    TransientDensityFieldHead,\n    TransientRGBFieldHead,\n    UncertaintyFieldHead,\n)\nfrom nerfstudio.field_components.mlp import MLP\nfrom nerfstudio.fields.base_field import Field\n\n\nclass VanillaNerfWField(Field):\n    \"\"\"The NeRF-W field which has appearance and transient conditioning.\n\n    Args:\n        num_images: How many images exist in the dataset.\n        position_encoding: Position encoder.\n        direction_encoding: Direction encoder.\n        base_mlp_num_layers: Number of layers for base MLP.\n        base_mlp_layer_width: Width of base MLP layers.\n        head_mlp_num_layers: Number of layer for output head MLP.\n        head_mlp_layer_width: Width of output head MLP layers.\n        appearance_embedding_dim:: Dimension of appearance embedding.\n        transient_embedding_dim:: Dimension of transient embedding.\n        skip_connections: Where to add skip connection in base MLP.\n    \"\"\"\n\n    def __init__(\n        self,\n        num_images: int,\n        position_encoding: Encoding = Identity(in_dim=3),\n        direction_encoding: Encoding = Identity(in_dim=3),\n        base_mlp_num_layers: int = 8,\n        base_mlp_layer_width: int = 256,\n        head_mlp_num_layers: int = 2,\n        head_mlp_layer_width: int = 128,\n        appearance_embedding_dim: int = 48,\n        transient_embedding_dim: int = 16,\n        skip_connections: Tuple[int] = (4,),\n    ) -> None:\n        super().__init__()\n        self.num_images = num_images\n        self.position_encoding = position_encoding\n        self.direction_encoding = direction_encoding\n        self.base_mlp_num_layers = base_mlp_num_layers\n        self.base_mlp_layer_width = base_mlp_layer_width\n        self.head_mlp_num_layers = head_mlp_num_layers\n        self.head_mlp_layer_width = head_mlp_layer_width\n        self.appearance_embedding_dim = appearance_embedding_dim\n        self.transient_embedding_dim = transient_embedding_dim\n\n        self.embedding_appearance = Embedding(self.num_images, self.appearance_embedding_dim)", "metadata": {"task_id": "nerfstudio-project_nerfstudio/193", "ground_truth": "        self.embedding_transient = Embedding(self.num_images, self.transient_embedding_dim)", "fpath_tuple": ["nerfstudio-project_nerfstudio", "nerfstudio", "fields", "nerfw_field.py"], "context_start_lineno": 0, "line_no": 78}}
{"prompt": " is not None:\n            assert len(_policy) == 2, \"1v1 serial evaluator needs 2 policy, but found {}\".format(len(_policy))\n            self._policy = _policy\n        for p in self._policy:\n            p.reset()\n\n    def reset(self, _policy: Optional[List[namedtuple]] = None, _env: Optional[BaseEnvManager] = None) -> None:\n        \"\"\"\n        Overview:\n            Reset evaluator's policy and environment. Use new policy and environment to collect data.\n            If _env is None, reset the old environment.\n            If _env is not None, replace the old environment in the evaluator with the new passed in \\\n                environment and launch.\n            If _policy is None, reset the old policy.\n            If _policy is not None, replace the old policy in the evaluator with the new passed in policy.\n        Arguments:\n            - policy (:obj:`Optional[List[namedtuple]]`): the api namedtuple of eval_mode policy\n            - env (:obj:`Optional[BaseEnvManager]`): instance of the subclass of vectorized \\\n                env_manager(BaseEnvManager)\n        \"\"\"\n        if _env is not None:\n            self.reset_env(_env)\n        if _policy is not None:\n            self.reset_policy(_policy)\n        self._max_eval_reward = float(\"-inf\")\n        self._last_eval_iter = 0\n        self._end_flag = False\n\n    def close(self) -> None:\n        \"\"\"\n        Overview:\n            Close the evaluator. If end_flag is False, close the environment, flush the tb_logger\\\n                and close the tb_logger.\n        \"\"\"\n        if self._end_flag:\n            return\n        self._end_flag = True\n        self._env.close()\n        self._tb_logger.flush()\n        self._tb_logger.close()\n\n    def __del__(self):\n        \"\"\"\n        Overview:\n            Execute the close command and close the evaluator. __del__ is automatically called \\\n                to destroy the evaluator instance when the evaluator finishes its work\n        \"\"\"\n        self.close()\n\n    def should_eval(self, train_iter: int) -> bool:\n        \"\"\"\n        Overview:\n            Determine whether you need to start the evaluation mode, if the number of training has reached\\\n                the maximum number of times to start the evaluator, return True\n        \"\"\"\n        if (train_iter - self._last_eval_iter) < self._cfg.eval_freq and train_iter!= 0:\n            return False\n        self._last_eval_iter = train_iter\n        return True\n\n    def eval(\n            self,\n            save_ckpt_fn: Callable = None,\n            train_iter: int = -1,\n            envstep: int = -1,\n            n_episode: Optional[int] = None\n    ) -> Tuple[bool, float, list]:\n        '''\n        Overview:\n            Evaluate policy and store the best policy based on whether it reaches the highest historical reward.\n        Arguments:\n            - save_ckpt_fn (:obj:`Callable`): Saving ckpt function, which will be triggered by getting the best reward.\n            - train_iter (:obj:`int`): Current training iteration.\n            - envstep (:obj:`int`): Current env interaction step.\n            - n_episode (:obj:`int`): Number of evaluation episodes.\n        Returns:\n            - stop_flag (:obj:`bool`): Whether this training program can be ended.\n            - eval_reward (:obj:`float`): Current eval_reward.\n            - return_info (:obj:`list`): Environment information of each finished episode\n        '''\n        if n_episode is None:\n            n_episode = self._default_n_episode\n        assert n_episode is not None, \"please indicate eval n_episode\"\n        envstep_count = 0\n        info = {}\n        return_info = [[] for _ in range(2)]\n        eval_monitor = VectorEvalMonitor(self._env.env_num, n_episode)", "metadata": {"task_id": "opendilab_ACE/42", "ground_truth": "        self._env.reset()", "fpath_tuple": ["opendilab_ACE", "ding", "worker", "collector", "battle_interaction_serial_evaluator.py"], "context_start_lineno": 109, "line_no": 196}}
{"prompt": " prompt_embeds.shape[0]\n\n        if prompt_embeds is None:\n            text_inputs = self.tokenizer(\n                prompt,\n                padding=\"max_length\",\n                max_length=self.tokenizer.model_max_length,\n                truncation=True,\n                return_tensors=\"pt\",\n            )\n            text_input_ids = text_inputs.input_ids\n            untruncated_ids = self.tokenizer(prompt, padding=\"longest\", return_tensors=\"pt\").input_ids\n\n            if untruncated_ids.shape[-1] >= text_input_ids.shape[-1] and not torch.equal(\n                text_input_ids, untruncated_ids\n            ):\n                removed_text = self.tokenizer.batch_decode(\n                    untruncated_ids[:, self.tokenizer.model_max_length - 1 : -1]\n                )\n                logger.warning(\n                    \"The following part of your input was truncated because CLIP can only handle sequences up to\"\n                    f\" {self.tokenizer.model_max_length} tokens: {removed_text}\"\n                )\n\n            if hasattr(self.text_encoder.config, \"use_attention_mask\") and self.text_encoder.config.use_attention_mask:\n                attention_mask = text_inputs.attention_mask.to(device)\n            else:\n                attention_mask = None\n\n            prompt_embeds = self.text_encoder(\n                text_input_ids.to(device),\n                attention_mask=attention_mask,\n            )\n            prompt_embeds = prompt_embeds[0]\n\n        prompt_embeds = prompt_embeds.to(dtype=self.text_encoder.dtype, device=device)\n\n        bs_embed, seq_len, _ = prompt_embeds.shape\n        # duplicate text embeddings for each generation per prompt, using mps friendly method\n        prompt_embeds = prompt_embeds.repeat(1, num_images_per_prompt, 1)\n        prompt_embeds = prompt_embeds.view(bs_embed * num_images_per_prompt, seq_len, -1)\n\n        # get unconditional embeddings for classifier free guidance\n        if do_classifier_free_guidance and negative_prompt_embeds is None:\n            uncond_tokens: List[str]\n            if negative_prompt is None:\n                uncond_tokens = [\"\"] * batch_size\n            elif type(prompt) is not type(negative_prompt):\n                raise TypeError(\n                    f\"`negative_prompt` should be the same type to `prompt`, but got {type(negative_prompt)}!=\"\n                    f\" {type(prompt)}.\"\n                )\n            elif isinstance(negative_prompt, str):\n                uncond_tokens = [negative_prompt]\n            elif batch_size!= len(negative_prompt):\n                raise ValueError(\n                    f\"`negative_prompt`: {negative_prompt} has batch size {len(negative_prompt)}, but `prompt`:\"\n                    f\" {prompt} has batch size {batch_size}. Please make sure that passed `negative_prompt` matches\"\n                    \" the batch size of `prompt`.\"\n                )\n            else:\n                uncond_tokens = negative_prompt\n\n            max_length = prompt_embeds.shape[1]\n            uncond_input = self.tokenizer(\n                uncond_tokens,\n                padding=\"max_length\",\n                max_length=max_length,\n                truncation=True,\n                return_tensors=\"pt\",\n            )\n\n            if hasattr(self.text_encoder.config, \"use_attention_mask\") and self.text_encoder.config.use_attention_mask:\n                attention_mask = uncond_input.attention_mask.to(device)\n            else:\n                attention_mask = None\n\n            negative_prompt_embeds = self.text_encoder(", "metadata": {"task_id": "huggingface_diffusers/142", "ground_truth": "                uncond_input.input_ids.to(device),", "fpath_tuple": ["huggingface_diffusers", "src", "diffusers", "pipelines", "stable_diffusion", "pipeline_stable_diffusion_upscale.py"], "context_start_lineno": 193, "line_no": 271}}
{"prompt": "def dummy_text_encoder(self):\n        torch.manual_seed(0)\n        config = CLIPTextConfig(\n            bos_token_id=0,\n            eos_token_id=2,\n            hidden_size=self.text_embedder_hidden_size,\n            intermediate_size=37,\n            layer_norm_eps=1e-05,\n            num_attention_heads=4,\n            num_hidden_layers=5,\n            pad_token_id=1,\n            vocab_size=1000,\n        )\n        return CLIPTextModel(config)\n\n    @property\n    def dummy_transformer(self):\n        torch.manual_seed(0)\n\n        height = 12\n        width = 12\n\n        model_kwargs = {\n            \"attention_bias\": True,\n            \"cross_attention_dim\": 32,\n            \"attention_head_dim\": height * width,\n            \"num_attention_heads\": 1,\n            \"num_vector_embeds\": self.num_embed,\n            \"num_embeds_ada_norm\": self.num_embeds_ada_norm,\n            \"norm_num_groups\": 32,\n            \"sample_size\": width,\n            \"activation_fn\": \"geglu-approximate\",\n        }\n\n        model = Transformer2DModel(**model_kwargs)\n        return model\n\n    def test_vq_diffusion(self):\n        device = \"cpu\"\n\n        vqvae = self.dummy_vqvae\n        text_encoder = self.dummy_text_encoder\n        tokenizer = self.dummy_tokenizer\n        transformer = self.dummy_transformer\n        scheduler = VQDiffusionScheduler(self.num_embed)\n        learned_classifier_free_sampling_embeddings = LearnedClassifierFreeSamplingEmbeddings(learnable=False)\n\n        pipe = VQDiffusionPipeline(\n            vqvae=vqvae,\n            text_encoder=text_encoder,\n            tokenizer=tokenizer,\n            transformer=transformer,\n            scheduler=scheduler,\n            learned_classifier_free_sampling_embeddings=learned_classifier_free_sampling_embeddings,\n        )\n        pipe = pipe.to(device)\n        pipe.set_progress_bar_config(disable=None)\n\n        prompt = \"teddy bear playing in the pool\"\n\n        generator = torch.Generator(device=device).manual_seed(0)\n        output = pipe([prompt], generator=generator, num_inference_steps=2, output_type=\"np\")\n        image = output.images\n\n        generator = torch.Generator(device=device).manual_seed(0)\n        image_from_tuple = pipe(\n            [prompt], generator=generator, output_type=\"np\", return_dict=False, num_inference_steps=2\n        )[0]\n\n        image_slice = image[0, -3:, -3:, -1]\n        image_from_tuple_slice = image_from_tuple[0, -3:, -3:, -1]\n\n        assert image.shape == (1, 24, 24, 3)\n\n        expected_slice = np.array([0.6583, 0.6410, 0.5325, 0.5635, 0.5563, 0.4234, 0.6008, 0.5491, 0.4880])\n\n        assert np.abs(image_slice.flatten() - expected_slice).max() < 1e-2\n        assert np.abs(image_from_tuple_slice.flatten() - expected_slice).max() < 1e-2\n\n    def test_vq_diffusion_classifier_free_sampling(self):\n        device = \"cpu\"\n\n        vqvae = self.dummy_vqvae\n        text_encoder = self.dummy_text_encoder\n        tokenizer = self.dummy_tokenizer\n        transformer = self.dummy_transformer\n        scheduler = VQDiffusionScheduler(self.num_embed)", "metadata": {"task_id": "huggingface_diffusers/93", "ground_truth": "        learned_classifier_free_sampling_embeddings = LearnedClassifierFreeSamplingEmbeddings(\n            learnable=True, hidden_size=self.text_embedder_hidden_size, length=tokenizer.model_max_length\n        )", "fpath_tuple": ["huggingface_diffusers", "tests", "pipelines", "vq_diffusion", "test_vq_diffusion.py"], "context_start_lineno": 71, "line_no": 158}}
{"prompt": "), (32, 5)])\n    @pytest.mark.parametrize(\"n_workers\", [1, 2])\n    def test_parallel_env_reset_flag(self, batch_size, n_workers, max_steps=3):\n        torch.manual_seed(1)\n        env = ParallelEnv(\n            n_workers, lambda: CountingEnv(max_steps=max_steps, batch_size=batch_size)\n        )\n        env.set_seed(1)\n        action = env.action_spec.rand()\n        action[:] = 1\n        for i in range(max_steps):\n            td = env.step(\n                TensorDict(\n                    {\"action\": action}, batch_size=env.batch_size, device=env.device\n                )\n            )\n            assert (td[\"done\"] == 0).all()\n            assert (td[\"next\"][\"observation\"] == i + 1).all()\n\n        td = env.step(\n            TensorDict({\"action\": action}, batch_size=env.batch_size, device=env.device)\n        )\n        assert (td[\"done\"] == 1).all()\n        assert (td[\"next\"][\"observation\"] == max_steps + 1).all()\n\n        _reset = torch.randint(low=0, high=2, size=env.batch_size, dtype=torch.bool)\n        while not _reset.any():\n            _reset = torch.randint(low=0, high=2, size=env.batch_size, dtype=torch.bool)\n\n        td_reset = env.reset(\n            TensorDict({\"_reset\": _reset}, batch_size=env.batch_size, device=env.device)\n        )\n        env.close()\n\n        assert (td_reset[\"done\"][_reset] == 0).all()\n        assert (td_reset[\"observation\"][_reset] == 0).all()\n        assert (td_reset[\"done\"][~_reset] == 1).all()\n        assert (td_reset[\"observation\"][~_reset] == max_steps + 1).all()\n\n\n@pytest.mark.parametrize(\"batch_size\", [(), (2,), (32, 5)])\ndef test_env_base_reset_flag(batch_size, max_steps=3):\n    env = CountingEnv(max_steps=max_steps, batch_size=batch_size)\n    env.set_seed(1)\n\n    action = env.action_spec.rand()\n    action[:] = 1\n\n    for i in range(max_steps):\n        td = env.step(\n            TensorDict({\"action\": action}, batch_size=env.batch_size, device=env.device)\n        )\n        assert (td[\"done\"] == 0).all()\n        assert (td[\"next\"][\"observation\"] == i + 1).all()\n\n    td = env.step(\n        TensorDict({\"action\": action}, batch_size=env.batch_size, device=env.device)\n    )\n    assert (td[\"done\"] == 1).all()\n    assert (td[\"next\"][\"observation\"] == max_steps + 1).all()\n\n    _reset = torch.randint(low=0, high=2, size=env.batch_size, dtype=torch.bool)\n    td_reset = env.reset(\n        TensorDict({\"_reset\": _reset}, batch_size=env.batch_size, device=env.device)\n    )\n\n    assert (td_reset[\"done\"][_reset] == 0).all()\n    assert (td_reset[\"observation\"][_reset] == 0).all()\n    assert (td_reset[\"done\"][~_reset] == 1).all()\n    assert (td_reset[\"observation\"][~_reset] == max_steps + 1).all()\n\n\n@pytest.mark.skipif(not _has_gym, reason=\"no gym\")\ndef test_seed():\n    torch.manual_seed(0)\n    env1 = GymEnv(PENDULUM_VERSIONED)", "metadata": {"task_id": "pytorch_rl/154", "ground_truth": "    env1.set_seed(0)", "fpath_tuple": ["pytorch_rl", "test", "test_env.py"], "context_start_lineno": 903, "line_no": 979}}
{"prompt": "import abc\nfrom typing import Any, List, Optional, Tuple, Union\n\nimport jax.numpy as jnp\nfrom jax._src.prng import PRNGKeyArray\n\nfrom fortuna.output_calibrator.output_calib_manager.base import \\\n    OutputCalibManager\nfrom fortuna.prob_output_layer.base import ProbOutputLayer\nfrom fortuna.typing import Array, CalibMutable, CalibParams\nfrom fortuna.utils.random import WithRNG\n\n\nclass Predictive(WithRNG, abc.ABC):\n    def __init__(\n        self,\n        output_calib_manager: OutputCalibManager,\n        prob_output_layer: ProbOutputLayer,\n    ):\n        r\"\"\"\n        Abstract predictive distribution. It characterizes the distribution of the target variable given the\n        calibrated outputs. It can be see as :math:`p(y|\\omega)`, where :math:`y` is a target variable and\n        :math:`\\omega` a calibrated output.\n        \"\"\"\n        self.output_calib_manager = output_calib_manager\n        self.prob_output_layer = prob_output_layer\n        self.state = None\n\n    def log_prob(\n        self, outputs: Array, targets: Array, calibrated: bool = True, **kwargs\n    ) -> jnp.ndarray:\n        \"\"\"\n        Evaluate the log-probability density function (a.k.a. log-pdf) given the outputs and target data.\n\n        Parameters\n        ----------\n        outputs : Array\n            Calibrated outputs.\n        targets : Array\n            Target data points.\n        calibrated : bool\n            Whether the outputs should be calibrated when computing this method. If `calibrated` is set to True, the\n            model must have been calibrated beforehand.\n\n        Returns\n        -------\n        jnp.ndarray\n            An evaluation of the log-pdf for each data point.\n        \"\"\"\n        if calibrated:\n            self._check_calibrated()\n            state = self.state.get()\n            outputs = self.output_calib_manager.apply(\n                params=state.params[\"output_calibrator\"],\n                outputs=outputs,\n                mutable=state.mutable[\"output_calibrator\"],\n            )\n        return self.prob_output_layer.log_prob(outputs, targets, **kwargs)\n\n    def sample(\n        self,\n        n_target_samples: int,\n        outputs: Array,\n        rng: Optional[PRNGKeyArray] = None,\n        calibrated: bool = True,\n        **kwargs\n    ) -> jnp.ndarray:\n        \"\"\"\n        Sample target variables for each outputs.\n\n        Parameters\n        ----------\n        n_target_samples: int\n            The number of target samples to draw for each of the outputs.\n        outputs : Array\n            Calibrated outputs.\n        rng : Optional[PRNGKeyArray]\n            A random number generator. If not passed, this will be taken from the attributes of this class.\n        calibrated : bool\n            Whether the outputs should be calibrated when computing this method. If `calibrated` is set to True, the\n            model must have been calibrated beforehand.\n\n        Returns\n        -------\n        jnp.ndarray\n            Samples of the target variable for each output.\n        \"\"\"\n        if calibrated:\n            self._check_calibrated()\n            state = self.state.get()\n            outputs = self.output_calib_manager.apply(\n                params=state.params[\"output_calibrator\"],\n                outputs=outputs,\n                mutable=state.mutable[\"output_calibrator\"],\n            )", "metadata": {"task_id": "awslabs_fortuna/66", "ground_truth": "        return self.prob_output_layer.sample(n_target_samples, outputs, rng, **kwargs)", "fpath_tuple": ["awslabs_fortuna", "fortuna", "calib_model", "predictive", "base.py"], "context_start_lineno": 0, "line_no": 95}}
{"prompt": ")])\n        end_time = time.time()\n        print('total step time: {}'.format(end_time - start_time))\n\n        # Test close\n        env_manager.close()\n        assert env_manager._closed\n        with pytest.raises(AssertionError):\n            env_manager.reset([])\n        with pytest.raises(AssertionError):\n            env_manager.step([])\n\n    @pytest.mark.unittest\n    def test_error(self, setup_sync_manager_cfg):\n        env_fn = setup_sync_manager_cfg.pop('env_fn')\n        env_manager = SyncSubprocessEnvManager(env_fn, setup_sync_manager_cfg)\n        # Test reset error\n        with pytest.raises(AssertionError):\n            env_manager.reset(reset_param={i: {'stat':'stat_test'} for i in range(env_manager.env_num)})\n        with pytest.raises(RuntimeError):\n            obs = env_manager.launch(reset_param={i: {'stat': 'error'} for i in range(env_manager.env_num)})\n        assert env_manager._closed\n        time.sleep(0.5)  # necessary time interval\n        obs = env_manager.launch(reset_param={i: {'stat':'stat_test'} for i in range(env_manager.env_num)})\n        assert not env_manager._closed\n\n        timestep = env_manager.step({i: np.random.randn(4) for i in range(env_manager.env_num)})\n        assert len(timestep) == env_manager.env_num\n\n        # Test step catched error\n        action = {i: np.random.randn(4) for i in range(env_manager.env_num)}\n        action[0] = 'catched_error'\n        assert not env_manager._closed\n        timestep = env_manager.step(action)\n        assert not env_manager._closed\n\n        assert timestep[0].info['abnormal']\n        assert all(['abnormal' not in timestep[i].info for i in range(1, env_manager.env_num)])\n        assert env_manager._env_states[0] == EnvState.ERROR\n        assert len(env_manager.ready_obs) == 3\n        # wait for reset\n        env_manager.reset({0: {'stat':'stat_test'}})\n        while not len(env_manager.ready_obs) == env_manager.env_num:\n            time.sleep(0.1)\n        assert env_manager._env_states[0] == EnvState.RUN\n        assert len(env_manager.ready_obs) == 4\n        timestep = env_manager.step({i: np.random.randn(4) for i in range(env_manager.env_num)})\n\n        # Test step error\n        action[0] = 'error'\n        with pytest.raises(Exception):\n            timestep = env_manager.step(action)\n        assert env_manager._closed\n\n        env_manager.close()\n        with pytest.raises(AssertionError):  # Assert env manager is not closed\n            env_manager.reset([])\n        with pytest.raises(AssertionError):  # Assert env manager is not closed\n            env_manager.step([])\n\n    @pytest.mark.tmp  # gitlab ci and local test pass, github always fail\n    def test_block(self, setup_async_manager_cfg, setup_watchdog, setup_model_type):\n        env_fn = setup_async_manager_cfg.pop('env_fn')\n        env_manager = AsyncSubprocessEnvManager(env_fn, setup_async_manager_cfg)\n        watchdog = setup_watchdog(60)\n        model = setup_model_type()\n        # Test reset timeout\n        watchdog.start()\n        with pytest.raises(RuntimeError):\n            reset_param = {i: {'stat': 'block'} for i in range(env_manager.env_num)}", "metadata": {"task_id": "opendilab_ACE/183", "ground_truth": "            obs = env_manager.launch(reset_param=reset_param)", "fpath_tuple": ["opendilab_ACE", "ding", "envs", "env_manager", "tests", "test_subprocess_env_manager.py"], "context_start_lineno": 59, "line_no": 129}}
{"prompt": "metrics=(accuracy,))\n        )\n        self.reg_fit_config_nodir_nodump = FitConfig(\n            optimizer=FitOptimizer(n_epochs=3), monitor=FitMonitor(metrics=(rmse,))\n        )\n        self.calib_config_dir_nodump = lambda directory, metric: CalibConfig(\n            optimizer=CalibOptimizer(n_epochs=3),\n            monitor=CalibMonitor(metrics=(metric,)),\n            checkpointer=CalibCheckpointer(save_checkpoint_dir=directory),\n        )\n        self.calib_config_dir_dump = lambda directory, metric: CalibConfig(\n            optimizer=CalibOptimizer(n_epochs=3),\n            monitor=CalibMonitor(metrics=(metric,)),\n            checkpointer=CalibCheckpointer(\n                save_checkpoint_dir=directory, dump_state=True\n            ),\n        )\n        self.calib_config_restore = lambda directory, metric: CalibConfig(\n            optimizer=CalibOptimizer(n_epochs=3),\n            monitor=CalibMonitor(metrics=(metric,)),\n            checkpointer=CalibCheckpointer(restore_checkpoint_path=directory),\n        )\n\n    def test_dryrun_reg_map(self):\n        with tempfile.TemporaryDirectory() as tmp_dir:\n            prob_reg = ProbRegressor(\n                model=MLP(self.reg_output_dim),\n                likelihood_log_variance_model=MyModel(self.reg_output_dim),\n                posterior_approximator=MAPPosteriorApproximator(),\n                output_calibrator=RegressionTemperatureScaler(),\n            )\n            # no save dir, no dump\n            train_status = prob_reg.train(\n                train_data_loader=self.reg_train_data_loader,\n                calib_data_loader=self.reg_val_data_loader,\n                fit_config=self.reg_fit_config_nodir_nodump,\n            )\n\n            state = prob_reg.posterior.state.get()\n            outputs = prob_reg.model_manager.apply(\n                params=state.params,\n                inputs=self.reg_val_data_loader.to_array_inputs(),\n                mutable=state.mutable,\n            )\n            targets = self.reg_val_data_loader.to_array_targets()\n\n            # calibrate from initialized state, save checkpoint\n            calib_model = CalibRegressor()\n            calib_status = calib_model.calibrate(\n                calib_outputs=outputs,\n                calib_targets=targets,\n                val_outputs=outputs,\n                val_targets=targets,\n                calib_config=self.calib_config_dir_nodump(tmp_dir, standard_error),\n            )\n\n            # calibrate from restored checkpoint\n            calib_status = calib_model.calibrate(\n                calib_outputs=outputs,\n                calib_targets=targets,\n                val_outputs=outputs,\n                val_targets=targets,\n                calib_config=self.calib_config_restore(tmp_dir, standard_error),\n            )\n\n            # calibrate from restored checkpoint, save checkpoint and dump\n            calib_status = calib_model.calibrate(\n                calib_outputs=outputs,\n                calib_targets=targets,\n                val_outputs=outputs,\n                val_targets=targets,\n                calib_config=self.calib_config_dir_dump(tmp_dir, standard_error),\n            )\n\n            # load state\n            calib_model.load_state(checkpoint_path=tmp_dir)\n\n            # save state\n            calib_model.save_state(checkpoint_path=tmp_dir)\n\n    def test_dryrun_class_map(self):\n        with tempfile.TemporaryDirectory() as tmp_dir:", "metadata": {"task_id": "awslabs_fortuna/192", "ground_truth": "            prob_class = ProbClassifier(\n                model=MLP(self.class_output_dim),\n                posterior_approximator=MAPPosteriorApproximator(),\n                output_calibrator=RegressionTemperatureScaler(),\n            )", "fpath_tuple": ["awslabs_fortuna", "tests", "fortuna", "calib_model", "test_calibrate.py"], "context_start_lineno": 100, "line_no": 182}}
{"prompt": " for f in files)\n\n    def test_returned_cached_folder(self):\n        prompt = \"hello\"\n        pipe = StableDiffusionPipeline.from_pretrained(\n            \"hf-internal-testing/tiny-stable-diffusion-torch\", safety_checker=None\n        )\n        _, local_path = StableDiffusionPipeline.from_pretrained(\n            \"hf-internal-testing/tiny-stable-diffusion-torch\", safety_checker=None, return_cached_folder=True\n        )\n        pipe_2 = StableDiffusionPipeline.from_pretrained(local_path)\n\n        pipe = pipe.to(torch_device)\n        pipe_2 = pipe_2.to(torch_device)\n\n        generator = torch.manual_seed(0)\n        out = pipe(prompt, num_inference_steps=2, generator=generator, output_type=\"numpy\").images\n\n        generator = torch.manual_seed(0)\n        out_2 = pipe_2(prompt, num_inference_steps=2, generator=generator, output_type=\"numpy\").images\n\n        assert np.max(np.abs(out - out_2)) < 1e-3\n\n    def test_download_safetensors(self):\n        with tempfile.TemporaryDirectory() as tmpdirname:\n            # pipeline has Flax weights\n            _ = DiffusionPipeline.from_pretrained(\n                \"hf-internal-testing/tiny-stable-diffusion-pipe-safetensors\",\n                safety_checker=None,\n                cache_dir=tmpdirname,\n            )\n\n            all_root_files = [t[-1] for t in os.walk(os.path.join(tmpdirname, os.listdir(tmpdirname)[0], \"snapshots\"))]\n            files = [item for sublist in all_root_files for item in sublist]\n\n            # None of the downloaded files should be a pytorch file even if we have some here:\n            # https://huggingface.co/hf-internal-testing/tiny-stable-diffusion-pipe/blob/main/unet/diffusion_flax_model.msgpack\n            assert not any(f.endswith(\".bin\") for f in files)\n\n    def test_download_no_safety_checker(self):\n        prompt = \"hello\"\n        pipe = StableDiffusionPipeline.from_pretrained(\n            \"hf-internal-testing/tiny-stable-diffusion-torch\", safety_checker=None\n        )\n        pipe = pipe.to(torch_device)\n        generator = torch.manual_seed(0)\n        out = pipe(prompt, num_inference_steps=2, generator=generator, output_type=\"numpy\").images\n\n        pipe_2 = StableDiffusionPipeline.from_pretrained(\"hf-internal-testing/tiny-stable-diffusion-torch\")\n        pipe_2 = pipe_2.to(torch_device)\n        generator = torch.manual_seed(0)\n        out_2 = pipe_2(prompt, num_inference_steps=2, generator=generator, output_type=\"numpy\").images\n\n        assert np.max(np.abs(out - out_2)) < 1e-3\n\n    def test_load_no_safety_checker_explicit_locally(self):\n        prompt = \"hello\"\n        pipe = StableDiffusionPipeline.from_pretrained(\n            \"hf-internal-testing/tiny-stable-diffusion-torch\", safety_checker=None\n        )\n        pipe = pipe.to(torch_device)\n        generator = torch.manual_seed(0)\n        out = pipe(prompt, num_inference_steps=2, generator=generator, output_type=\"numpy\").images\n\n        with tempfile.TemporaryDirectory() as tmpdirname:\n            pipe.save_pretrained(tmpdirname)\n            pipe_2 = StableDiffusionPipeline.from_pretrained(tmpdirname, safety_checker=None)", "metadata": {"task_id": "huggingface_diffusers/163", "ground_truth": "            pipe_2 = pipe_2.to(torch_device)", "fpath_tuple": ["huggingface_diffusers", "tests", "test_pipelines.py"], "context_start_lineno": 74, "line_no": 141}}
{"prompt": "if camera_indices is not None:\n            camera_indices = camera_indices[ray_indices]\n\n        zeros = torch.zeros_like(origins[:, :1])\n        ray_samples = RaySamples(\n            frustums=Frustums(\n                origins=origins,\n                directions=dirs,\n                starts=starts,\n                ends=ends,\n                pixel_area=zeros,\n            ),\n            camera_indices=camera_indices,\n        )\n        return ray_samples, ray_indices\n\n\nclass ProposalNetworkSampler(Sampler):\n    \"\"\"Sampler that uses a proposal network to generate samples.\"\"\"\n\n    def __init__(\n        self,\n        num_proposal_samples_per_ray: Tuple[int] = (64,),\n        num_nerf_samples_per_ray: int = 32,\n        num_proposal_network_iterations: int = 2,\n        single_jitter: bool = False,\n        update_sched: Callable = lambda x: 1,\n    ) -> None:\n        super().__init__()\n        self.num_proposal_samples_per_ray = num_proposal_samples_per_ray\n        self.num_nerf_samples_per_ray = num_nerf_samples_per_ray\n        self.num_proposal_network_iterations = num_proposal_network_iterations\n        self.update_sched = update_sched\n        if self.num_proposal_network_iterations < 1:\n            raise ValueError(\"num_proposal_network_iterations must be >= 1\")\n\n        # samplers\n        self.initial_sampler = UniformLinDispPiecewiseSampler(single_jitter=single_jitter)\n        self.pdf_sampler = PDFSampler(include_original=False, single_jitter=single_jitter)\n\n        self._anneal = 1.0\n        self._steps_since_update = 0\n        self._step = 0\n\n    def set_anneal(self, anneal: float) -> None:\n        \"\"\"Set the anneal value for the proposal network.\"\"\"\n        self._anneal = anneal\n\n    def step_cb(self, step):\n        \"\"\"Callback to register a training step has passed. This is used to keep track of the sampling schedule\"\"\"\n        self._step = step\n        self._steps_since_update += 1\n\n    def generate_ray_samples(\n        self,\n        ray_bundle: Optional[RayBundle] = None,\n        density_fns: Optional[List[Callable]] = None,\n    ) -> Tuple[RaySamples, List, List]:\n        assert ray_bundle is not None\n        assert density_fns is not None\n\n        weights_list = []\n        ray_samples_list = []\n\n        n = self.num_proposal_network_iterations\n        weights = None\n        ray_samples = None\n        updated = self._steps_since_update > self.update_sched(self._step) or self._step < 10\n        for i_level in range(n + 1):\n            is_prop = i_level < n\n            num_samples = self.num_proposal_samples_per_ray[i_level] if is_prop else self.num_nerf_samples_per_ray\n            if i_level == 0:\n                # Uniform sampling because we need to start with some samples\n                ray_samples = self.initial_sampler(ray_bundle, num_samples=num_samples)\n            else:\n                # PDF sampling based on the last samples and their weights\n                # Perform annealing to the weights. This will be a no-op if self._anneal is 1.0.\n                assert weights is not None\n                annealed_weights = torch.pow(weights, self._anneal)\n                ray_samples = self.pdf_sampler(ray_bundle, ray_samples, annealed_weights, num_samples=num_samples)\n            if is_prop:\n                if updated:\n                    # always update on the first step or the inf check in grad scaling crashes", "metadata": {"task_id": "nerfstudio-project_nerfstudio/6", "ground_truth": "                    density = density_fns[i_level](ray_samples.frustums.get_positions())", "fpath_tuple": ["nerfstudio-project_nerfstudio", "nerfstudio", "model_components", "ray_samplers.py"], "context_start_lineno": 491, "line_no": 574}}
{"prompt": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import annotations\n\n\"\"\"CMA-ES designer.\"\"\"\nimport json\nimport queue\nfrom typing import Optional, Sequence\n\nfrom evojax.algo import cma_jax\nimport jax.numpy as jnp\nimport numpy as np\n\nfrom vizier import algorithms as vza\nfrom vizier import pyvizier as vz\nfrom vizier.pyvizier import converters\nfrom vizier.utils import json_utils\n\n\nclass CMAESDesigner(vza.PartiallySerializableDesigner):\n  \"\"\"CMA-ES designer wrapping evo-jax.\n\n  NOTE: Since the base version of CMA-ES expects the entire population size to\n  be evaluated before an update, we must use temporary queues to hold partially\n  finished populations.\n  \"\"\"\n\n  def __init__(self, problem_statement: vz.ProblemStatement, **cma_kwargs):\n    \"\"\"Init.\n\n    Args:\n      problem_statement: Must use a flat DOUBLE-only search space.\n      **cma_kwargs: Keyword arguments for the CMA_ES_JAX class.\n    \"\"\"\n    self._problem_statement = problem_statement\n    self._metric_name = self._problem_statement.metric_information.item().name\n\n    self._search_space = self._problem_statement.search_space\n    if self._search_space.is_conditional:\n      raise ValueError(\n          f'This designer {self} does not support conditional search.')\n    for parameter_config in self._search_space.parameters:\n      if not parameter_config.type.is_continuous():\n        raise ValueError(\n            f'This designer {self} only supports continuous parameters.')\n    self._num_params = len(self._search_space.parameters)\n    if self._num_params < 2:\n      raise ValueError(\n          f'CMA-ES only supports search spaces with >=2 parameters. Current number of parameters: {self._num_params}'\n      )\n\n    self._converter = converters.TrialToArrayConverter.from_study_config(\n        self._problem_statement)\n    self._cma_es_jax = cma_jax.CMA_ES_JAX(\n        param_size=self._num_params, **cma_kwargs)\n    self._trial_population = queue.Queue(\n        maxsize=self._cma_es_jax.hyper_parameters.pop_size)\n\n  def update(self, trials: vza.CompletedTrials) -> None:\n    completed_trials = list(trials.completed)\n\n    # Keep inserting completed trials into population. If population is full,\n    # a CMA-ES update and queue clear are triggered.\n    while completed_trials:", "metadata": {"task_id": "google_vizier/128", "ground_truth": "      self._trial_population.put(completed_trials.pop())", "fpath_tuple": ["google_vizier", "vizier", "_src", "algorithms", "designers", "cmaes.py"], "context_start_lineno": 0, "line_no": 76}}
{"prompt": " `\"smooth_l1\"`.\n        alpha_init (float, optional): initial entropy multiplier.\n            Default is 1.0.\n        min_alpha (float, optional): min value of alpha.\n            Default is 0.1.\n        max_alpha (float, optional): max value of alpha.\n            Default is 10.0.\n        fixed_alpha (bool, optional): if True, alpha will be fixed to its\n            initial value. Otherwise, alpha will be optimized to\n            match the 'target_entropy' value.\n            Default is :obj:`False`.\n        target_entropy (float or str, optional): Target entropy for the\n            stochastic policy. Default is \"auto\", where target entropy is\n            computed as :obj:`-prod(n_actions)`.\n        delay_actor (bool, optional): Whether to separate the target actor\n            networks from the actor networks used for data collection.\n            Default is :obj:`False`.\n        delay_qvalue (bool, optional): Whether to separate the target Q value\n            networks from the Q value networks used for data collection.\n            Default is :obj:`False`.\n        delay_value (bool, optional): Whether to separate the target value\n            networks from the value networks used for data collection.\n            Default is :obj:`False`.\n    \"\"\"\n\n    def __init__(\n        self,\n        actor_network: ProbabilisticActor,\n        qvalue_network: SafeModule,\n        value_network: Optional[SafeModule] = None,\n        num_qvalue_nets: int = 2,\n        gamma: Number = 0.99,\n        priotity_key: str = \"td_error\",\n        loss_function: str = \"smooth_l1\",\n        alpha_init: float = 1.0,\n        min_alpha: float = 0.1,\n        max_alpha: float = 10.0,\n        fixed_alpha: bool = False,\n        target_entropy: Union[str, float] = \"auto\",\n        delay_actor: bool = False,\n        delay_qvalue: bool = False,\n        delay_value: bool = False,\n    ) -> None:\n        if not _has_functorch:\n            raise ImportError(\"Failed to import functorch.\") from FUNCTORCH_ERROR\n        super().__init__()\n\n        # Actor\n        self.delay_actor = delay_actor\n        self.convert_to_functional(\n            actor_network,\n            \"actor_network\",\n            create_target_params=self.delay_actor,\n            funs_to_decorate=[\"forward\", \"get_dist\"],\n        )\n\n        # Value\n        if value_network is not None:\n            self._version = 1\n            self.delay_value = delay_value\n            self.convert_to_functional(\n                value_network,\n                \"value_network\",\n                create_target_params=self.delay_value,\n                compare_against=list(actor_network.parameters()),\n            )\n        else:\n            self._version = 2\n\n        # Q value\n        self.delay_qvalue = delay_qvalue\n        self.num_qvalue_nets = num_qvalue_nets\n        if self._version == 1:\n            value_params = list(value_network.parameters())\n        else:\n            value_params = []\n        self.convert_to_functional(\n            qvalue_network,\n            \"qvalue_network\",\n            num_qvalue_nets,\n            create_target_params=self.delay_qvalue,\n            compare_against=list(actor_network.parameters()) + value_params,\n        )\n\n        self.register_buffer(\"gamma\", torch.tensor(gamma))\n        self.priority_key = priotity_key\n        self.loss_function = loss_function\n        try:\n            device = next(self.parameters()).device\n        except AttributeError:\n            device = torch.device(\"cpu\")\n        self.register_buffer(\"alpha_init\", torch.tensor(alpha_init, device=device))\n        self.register_buffer(\n            \"min_log_alpha\", torch.tensor(min_alpha, device=device).log()\n        )", "metadata": {"task_id": "pytorch_rl/101", "ground_truth": "        self.register_buffer(\n            \"max_log_alpha\", torch.tensor(max_alpha, device=device).log()\n        )", "fpath_tuple": ["pytorch_rl", "torchrl", "objectives", "sac.py"], "context_start_lineno": 54, "line_no": 149}}
{"prompt": "metric=\"word_count\",\n        )\n        self.assertIsInstance(results[\"unique_words\"], int)\n\n    @slow\n    def test_default_pipe_init(self):\n        results = self.evaluator.compute(data=self.data)\n        self.assertIsInstance(results[\"unique_words\"], int)\n\n    def test_overwrite_default_metric(self):\n        word_length = load(\"word_length\")\n        results = self.evaluator.compute(\n            model_or_pipeline=self.pipe,\n            data=self.data,\n            metric=word_length,\n        )\n        self.assertIsInstance(results[\"average_word_length\"], int)\n        results = self.evaluator.compute(\n            model_or_pipeline=self.pipe,\n            data=self.data,\n            metric=\"word_length\",\n        )\n        self.assertIsInstance(results[\"average_word_length\"], int)\n\n    def test_process_predictions_multiple_return_sequences(self):\n        processed_predictions = self.evaluator.predictions_processor(\n            [\n                [{\"generated_text\": \"A\"}, {\"generated_text\": \"B\"}],\n                [{\"generated_text\": \"C\"}, {\"generated_text\": \"D\"}],\n            ]\n        )\n        self.assertEqual(processed_predictions, {\"data\": [\"A\", \"B\", \"C\", \"D\"]})\n\n\nclass TestText2TextGenerationEvaluator(TestCase):\n    def setUp(self):\n        self.data = Dataset.from_dict(\n            {\n                \"text\": [\"Lorem ipsum\"] * 4,\n                \"label\": [\"Ipsum Lorem\"] * 4,\n            }\n        )\n        self.pipe = DummyText2TextGenerationPipeline()\n        self.evaluator = evaluator(\"text2text-generation\")\n\n    def test_pipe_init(self):\n        results = self.evaluator.compute(\n            model_or_pipeline=self.pipe,\n            data=self.data,\n        )\n        self.assertEqual(results[\"bleu\"], 0)\n\n    def test_class_init(self):\n        evaluator = Text2TextGenerationEvaluator()\n        self.assertEqual(evaluator.task, \"text2text-generation\")\n        self.assertIsNone(evaluator.default_metric_name)\n\n        results = evaluator.compute(\n            model_or_pipeline=self.pipe,\n            data=self.data,\n            metric=\"bleu\",\n        )\n        self.assertEqual(results[\"bleu\"], 0)\n\n    @slow\n    def test_default_pipe_init(self):\n        results = self.evaluator.compute(data=self.data)\n        self.assertEqual(results[\"bleu\"], 0)\n\n    def test_overwrite_default_metric(self):\n        rouge = load(\"rouge\")\n        results = self.evaluator.compute(\n            model_or_pipeline=self.pipe,\n            data=self.data,\n            metric=rouge,\n        )\n        self.assertEqual(results[\"rouge1\"], 1.0)\n        results = self.evaluator.compute(\n            model_or_pipeline=self.pipe,\n            data=self.data,\n            metric=\"rouge\",\n        )\n        self.assertEqual(results[\"rouge1\"], 1.0)\n\n    def test_summarization(self):\n        pipe = DummyText2TextGenerationPipeline(task=\"summarization\", prefix=\"summary\")\n        e = evaluator(\"summarization\")\n\n        results = e.compute(\n            model_or_pipeline=pipe,\n            data=self.data,\n        )\n        self.assertEqual(results[\"rouge1\"], 1.0)\n\n    def test_translation(self):\n        pipe = DummyText2TextGenerationPipeline(task=\"translation\", prefix=\"translation\")\n        e = evaluator(\"translation\")", "metadata": {"task_id": "huggingface_evaluate/147", "ground_truth": "        results = e.compute(\n            model_or_pipeline=pipe,\n            data=self.data,\n        )", "fpath_tuple": ["huggingface_evaluate", "tests", "test_evaluator.py"], "context_start_lineno": 867, "line_no": 965}}
{"prompt": "# Copyright 2022 The Nerfstudio Team. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\nImplementation of mip-NeRF.\n\"\"\"\nfrom __future__ import annotations\n\nfrom typing import Dict, List, Tuple\n\nimport torch\nfrom torch.nn import Parameter\nfrom torchmetrics import PeakSignalNoiseRatio\nfrom torchmetrics.functional import structural_similarity_index_measure\nfrom torchmetrics.image.lpip import LearnedPerceptualImagePatchSimilarity\n\nfrom nerfstudio.cameras.rays import RayBundle\nfrom nerfstudio.field_components.encodings import NeRFEncoding\nfrom nerfstudio.field_components.field_heads import FieldHeadNames\nfrom nerfstudio.fields.vanilla_nerf_field import NeRFField\nfrom nerfstudio.model_components.losses import MSELoss\nfrom nerfstudio.model_components.ray_samplers import PDFSampler, UniformSampler\nfrom nerfstudio.model_components.renderers import (\n    AccumulationRenderer,\n    DepthRenderer,\n    RGBRenderer,\n)\nfrom nerfstudio.models.base_model import Model, ModelConfig\nfrom nerfstudio.utils import colormaps, colors, misc\n\n\nclass MipNerfModel(Model):\n    \"\"\"mip-NeRF model\n\n    Args:\n        config: MipNerf configuration to instantiate model\n    \"\"\"\n\n    def __init__(\n        self,\n        config: ModelConfig,\n        **kwargs,\n    ) -> None:\n        self.field = None\n        super().__init__(config=config, **kwargs)\n\n    def populate_modules(self):\n        \"\"\"Set the fields and modules\"\"\"", "metadata": {"task_id": "nerfstudio-project_nerfstudio/133", "ground_truth": "        super().populate_modules()", "fpath_tuple": ["nerfstudio-project_nerfstudio", "nerfstudio", "models", "mipnerf.py"], "context_start_lineno": 0, "line_no": 59}}
{"prompt": "[\n                {\"start\": 0, \"entity\": \"B-LOC\"},\n                {\"start\": 9, \"entity\": \"O\"},\n                {\"start\": 11, \"entity\": \"O\"},\n                {\"start\": 16, \"entity\": \"B-LOC\"},\n                {\"start\": 21, \"entity\": \"O\"},\n            ]\n        ]\n        predictions = task_evaluator.predictions_processor(predictions, words, join_by)\n        self.assertListEqual(predictions[\"predictions\"][0], [\"B-LOC\", \"O\", \"O\", \"O\", \"B-LOC\", \"O\"])\n\n\nclass TestTextGenerationEvaluator(TestCase):\n    def setUp(self):\n        self.data = Dataset.from_dict({\"text\": [\"Lorem ipsum\"]})\n        self.pipe = DummyTextGenerationPipeline(num_return_sequences=4)\n        self.evaluator = evaluator(\"text-generation\")\n\n    def test_class_init(self):\n        evaluator = TextGenerationEvaluator()\n        self.assertEqual(evaluator.task, \"text-generation\")\n        self.assertIsNone(evaluator.default_metric_name)\n\n        results = evaluator.compute(\n            model_or_pipeline=self.pipe,\n            data=self.data,\n            metric=\"word_count\",\n        )\n        self.assertIsInstance(results[\"unique_words\"], int)\n\n    @slow\n    def test_default_pipe_init(self):\n        results = self.evaluator.compute(data=self.data)\n        self.assertIsInstance(results[\"unique_words\"], int)\n\n    def test_overwrite_default_metric(self):\n        word_length = load(\"word_length\")\n        results = self.evaluator.compute(\n            model_or_pipeline=self.pipe,\n            data=self.data,\n            metric=word_length,\n        )\n        self.assertIsInstance(results[\"average_word_length\"], int)\n        results = self.evaluator.compute(\n            model_or_pipeline=self.pipe,\n            data=self.data,\n            metric=\"word_length\",\n        )\n        self.assertIsInstance(results[\"average_word_length\"], int)\n\n    def test_process_predictions_multiple_return_sequences(self):\n        processed_predictions = self.evaluator.predictions_processor(\n            [\n                [{\"generated_text\": \"A\"}, {\"generated_text\": \"B\"}],\n                [{\"generated_text\": \"C\"}, {\"generated_text\": \"D\"}],\n            ]\n        )\n        self.assertEqual(processed_predictions, {\"data\": [\"A\", \"B\", \"C\", \"D\"]})\n\n\nclass TestText2TextGenerationEvaluator(TestCase):\n    def setUp(self):\n        self.data = Dataset.from_dict(\n            {\n                \"text\": [\"Lorem ipsum\"] * 4,\n                \"label\": [\"Ipsum Lorem\"] * 4,\n            }\n        )\n        self.pipe = DummyText2TextGenerationPipeline()\n        self.evaluator = evaluator(\"text2text-generation\")\n\n    def test_pipe_init(self):\n        results = self.evaluator.compute(\n            model_or_pipeline=self.pipe,\n            data=self.data,\n        )\n        self.assertEqual(results[\"bleu\"], 0)\n\n    def test_class_init(self):\n        evaluator = Text2TextGenerationEvaluator()\n        self.assertEqual(evaluator.task, \"text2text-generation\")\n        self.assertIsNone(evaluator.default_metric_name)\n\n        results = evaluator.compute(\n            model_or_pipeline=self.pipe,\n            data=self.data,\n            metric=\"bleu\",\n        )\n        self.assertEqual(results[\"bleu\"], 0)\n\n    @slow\n    def test_default_pipe_init(self):", "metadata": {"task_id": "huggingface_evaluate/158", "ground_truth": "        results = self.evaluator.compute(data=self.data)", "fpath_tuple": ["huggingface_evaluate", "tests", "test_evaluator.py"], "context_start_lineno": 841, "line_no": 933}}
{"prompt": "import evaluate\nfrom evaluate.utils import launch_gradio_widget\n", "metadata": {"task_id": "huggingface_evaluate/127", "ground_truth": "module = evaluate.load(\"meteor\")", "fpath_tuple": ["huggingface_evaluate", "metrics", "meteor", "app.py"], "context_start_lineno": 0, "line_no": 4}}
{"prompt": "_loader=self.class_train_data_loader,\n                calib_data_loader=self.class_val_data_loader,\n                val_data_loader=self.class_val_data_loader,\n                map_fit_config=self.class_fit_config_nodir_nodump,\n                fit_config=self.class_fit_config_dir_dump(tmp_dir),\n                calib_config=self.class_calib_config_nodir_nodump,\n            )\n            sample = prob_class.posterior.sample()\n            prob_class.posterior.load_state(tmp_dir)\n\n            # restore from laplace\n            status = prob_class.train(\n                train_data_loader=self.class_train_data_loader,\n                calib_data_loader=self.class_val_data_loader,\n                val_data_loader=self.class_val_data_loader,\n                map_fit_config=self.class_fit_config_nodir_nodump,\n                fit_config=self.class_fit_config_restore(tmp_dir),\n                calib_config=self.class_calib_config_nodir_nodump,\n            )\n\n            # restore from map\n            prob_class_map = ProbClassifier(\n                model=MyModel(self.class_output_dim),\n                posterior_approximator=MAPPosteriorApproximator(),\n                output_calibrator=ClassificationTemperatureScaler(),\n            )\n            status = prob_class_map.train(\n                train_data_loader=self.class_train_data_loader,\n                calib_data_loader=self.class_val_data_loader,\n                val_data_loader=self.class_val_data_loader,\n                map_fit_config=self.class_fit_config_nodir_nodump,\n                fit_config=self.class_fit_config_dir_dump(tmp_dir),\n                calib_config=self.class_calib_config_nodir_nodump,\n            )\n            status = prob_class.train(\n                train_data_loader=self.class_train_data_loader,\n                calib_data_loader=self.class_val_data_loader,\n                val_data_loader=self.class_val_data_loader,\n                map_fit_config=self.class_fit_config_nodir_nodump,\n                fit_config=self.class_fit_config_restore(tmp_dir),\n                calib_config=self.class_calib_config_nodir_nodump,\n            )\n\n            # load state\n            prob_class.load_state(checkpoint_path=tmp_dir)\n\n            # save state\n            prob_class.save_state(checkpoint_path=tmp_dir)\n\n    def test_dryrun_reg_swag(self):\n        with tempfile.TemporaryDirectory() as tmp_dir:\n            prob_reg = ProbRegressor(\n                model=MyModel(self.reg_output_dim),\n                likelihood_log_variance_model=MyModel(self.reg_output_dim),\n                posterior_approximator=SWAGPosteriorApproximator(rank=2),\n                output_calibrator=RegressionTemperatureScaler(),\n            )\n            # no save dir, no dump\n            status = prob_reg.train(\n                train_data_loader=self.reg_train_data_loader,\n                calib_data_loader=self.reg_val_data_loader,\n                val_data_loader=self.reg_val_data_loader,\n                map_fit_config=self.reg_fit_config_nodir_nodump,\n                fit_config=self.reg_fit_config_nodir_nodump,\n                calib_config=self.reg_calib_config_nodir_nodump,\n            )\n            sample = prob_reg.posterior.sample()\n\n            # no save dir but dump\n            with self.assertRaises(ValueError):", "metadata": {"task_id": "awslabs_fortuna/29", "ground_truth": "                status = prob_reg.train(\n                    train_data_loader=self.reg_train_data_loader,\n                    calib_data_loader=self.reg_val_data_loader,\n                    val_data_loader=self.reg_val_data_loader,\n                    map_fit_config=self.reg_fit_config_nodir_nodump,\n                    fit_config=self.reg_fit_config_nodir_dump,\n                    calib_config=self.reg_calib_config_nodir_nodump,\n                )", "fpath_tuple": ["awslabs_fortuna", "tests", "fortuna", "prob_model", "test_train.py"], "context_start_lineno": 715, "line_no": 785}}
{"prompt": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport argparse\n\nimport pytest\nimport torch\nfrom _utils_internal import get_available_devices\nfrom scipy.stats import ttest_1samp\nfrom tensordict.tensordict import TensorDict\nfrom torch import nn\nfrom torchrl.data import BoundedTensorSpec, CompositeSpec\nfrom torchrl.envs.transforms.transforms import gSDENoise\nfrom torchrl.envs.utils import set_exploration_mode\nfrom torchrl.modules import SafeModule, SafeSequential\nfrom torchrl.modules.distributions import TanhNormal\nfrom torchrl.modules.distributions.continuous import (\n    IndependentNormal,\n    NormalParamWrapper,\n)\nfrom torchrl.modules.models.exploration import LazygSDEModule\nfrom torchrl.modules.tensordict_module.actors import ProbabilisticActor\nfrom torchrl.modules.tensordict_module.exploration import (\n    _OrnsteinUhlenbeckProcess,\n    AdditiveGaussianWrapper,\n    OrnsteinUhlenbeckProcessWrapper,\n)\n\n\n@pytest.mark.parametrize(\"device\", get_available_devices())\ndef test_ou(device, seed=0):\n    torch.manual_seed(seed)\n    td = TensorDict({\"action\": torch.randn(3) / 10}, batch_size=[], device=device)\n    ou = _OrnsteinUhlenbeckProcess(10.0, mu=2.0, x0=-4, sigma=0.1, sigma_min=0.01)\n\n    tds = []\n    for i in range(2000):\n        td = ou.add_sample(td)\n        tds.append(td.clone())\n        td.set_(\"action\", torch.randn(3) / 10)\n        if i % 1000 == 0:\n            td.zero_()\n\n    tds = torch.stack(tds, 0)\n\n    tset, pval_acc = ttest_1samp(tds.get(\"action\")[950:1000, 0].cpu().numpy(), 2.0)\n    tset, pval_reg = ttest_1samp(tds.get(\"action\")[:50, 0].cpu().numpy(), 2.0)\n    assert pval_acc > 0.05\n    assert pval_reg < 0.1\n\n    tset, pval_acc = ttest_1samp(tds.get(\"action\")[1950:2000, 0].cpu().numpy(), 2.0)\n    tset, pval_reg = ttest_1samp(tds.get(\"action\")[1000:1050, 0].cpu().numpy(), 2.0)\n    assert pval_acc > 0.05\n    assert pval_reg < 0.1\n\n\n@pytest.mark.parametrize(\"device\", get_available_devices())\ndef test_ou_wrapper(device, d_obs=4, d_act=6, batch=32, n_steps=100, seed=0):\n    torch.manual_seed(seed)", "metadata": {"task_id": "pytorch_rl/172", "ground_truth": "    net = NormalParamWrapper(nn.Linear(d_obs, 2 * d_act)).to(device)", "fpath_tuple": ["pytorch_rl", "test", "test_exploration.py"], "context_start_lineno": 0, "line_no": 61}}
{"prompt": ".\n        distribute: bool\n            Whether to distribute computation over multiple devices, if available.\n\n        Returns\n        -------\n        jnp.ndarray\n            An estimate of the predictive mean for each input.\n        \"\"\"\n        return super().mean(inputs_loader, n_posterior_samples, rng, distribute)\n\n    def mode(\n        self,\n        inputs_loader: InputsLoader,\n        n_posterior_samples: int = 30,\n        means: Optional[jnp.ndarray] = None,\n        rng: Optional[PRNGKeyArray] = None,\n        distribute: bool = True,\n    ) -> jnp.ndarray:\n        if means is None:\n            means = self.mean(\n                inputs_loader=inputs_loader,\n                n_posterior_samples=n_posterior_samples,\n                rng=rng,\n                distribute=distribute,\n            )\n        return jnp.argmax(means, -1)\n\n    def aleatoric_variance(\n        self,\n        inputs_loader: InputsLoader,\n        n_posterior_samples: int = 30,\n        rng: Optional[PRNGKeyArray] = None,\n        distribute: bool = True,\n    ) -> jnp.ndarray:\n        r\"\"\"\n        Estimate the predictive aleatoric variance of the one-hot encoded target variable, that is\n\n       .. math::\n            \\text{Var}_{W|\\mathcal{D}}[\\mathbb{E}_{\\tilde{Y}|W, x}[\\tilde{Y}]],\n\n        where:\n         - :math:`x` is an observed input variable;\n         - :math:`\\tilde{Y}` is a one-hot encoded random target variable;\n         - :math:`\\mathcal{D}` is the observed training data set;\n         - :math:`W` denotes the random model parameters.\n\n        Parameters\n        ----------\n        inputs_loader : InputsLoader\n            A loader of input data points.\n        n_posterior_samples : int\n            Number of samples to draw from the posterior distribution for each input.\n        rng : Optional[PRNGKeyArray]\n            A random number generator. If not passed, this will be taken from the attributes of this class.\n        distribute: bool\n            Whether to distribute computation over multiple devices, if available.\n\n        Returns\n        -------\n        jnp.ndarray\n            An estimate of the predictive aleatoric variance for each input.\n        \"\"\"\n        return super().aleatoric_variance(\n            inputs_loader, n_posterior_samples, rng, distribute\n        )\n\n    def epistemic_variance(\n        self,\n        inputs_loader: InputsLoader,\n        n_posterior_samples: int = 30,\n        rng: Optional[PRNGKeyArray] = None,\n        distribute: bool = True,\n        **kwargs\n    ) -> jnp.ndarray:\n        r\"\"\"\n        Estimate the predictive epistemic variance of the one-hot encoded target variable, that is\n\n       .. math::\n            \\mathbb{E}_{W|D}[\\text{Var}_{\\tilde{Y}|W, x}[\\tilde{Y}]],\n\n        where:\n         - :math:`x` is an observed input variable;\n         - :math:`\\tilde{Y}` is a one-hot encoded random target variable;\n         - :math:`\\mathcal{D}` is the observed training data set;\n         - :math:`W` denotes the random model parameters.\n\n        Parameters\n        ----------\n        inputs_loader : InputsLoader\n            A loader of input data points.\n        n_posterior_samples : int\n            Number of samples to draw from the posterior distribution for each input.\n        rng : Optional[PRNGKeyArray]\n            A random number generator. If not passed, this will be taken from the attributes of this class.\n        distribute: bool\n            Whether to distribute computation over multiple devices, if available.\n\n        Returns\n        -------\n        jnp.ndarray\n            An estimate of the predictive epistemic variance for each input.\n        \"\"\"", "metadata": {"task_id": "awslabs_fortuna/146", "ground_truth": "        return super().epistemic_variance(\n            inputs_loader, n_posterior_samples, rng, distribute\n        )", "fpath_tuple": ["awslabs_fortuna", "fortuna", "prob_model", "predictive", "classification.py"], "context_start_lineno": 50, "line_no": 153}}
{"prompt": "from.registry import Registry\n\nPOLICY_REGISTRY = Registry()\nENV_REGISTRY = Registry()\nLEARNER_REGISTRY = Registry()", "metadata": {"task_id": "opendilab_ACE/37", "ground_truth": "COMM_LEARNER_REGISTRY = Registry()", "fpath_tuple": ["opendilab_ACE", "ding", "utils", "registry_factory.py"], "context_start_lineno": 0, "line_no": 5}}
{"prompt": "        td_select = td_select.flatten_keys(sep)\n        if td.batch_dims:\n            raise RuntimeError(\n                f\"VecNorm should be used with non-batched environments. \"\n                f\"Got batch_size={td.batch_size}\"\n            )\n        keys = list(td_select.keys())\n        for key in keys:\n            td_select.set(key + \"_ssq\", td_select.get(key).clone())\n            td_select.set(\n                key + \"_count\",\n                torch.zeros(\n                    *td.batch_size,\n                    1,\n                    device=td_select.device,\n                    dtype=torch.float,\n                ),\n            )\n            td_select.rename_key(key, key + \"_sum\")\n        td_select.exclude(*keys).zero_()\n        td_select = td_select.unflatten_keys(sep)\n        if memmap:\n            return td_select.memmap_()\n        return td_select.share_memory_()\n\n    def get_extra_state(self) -> OrderedDict:\n        return collections.OrderedDict({\"lock\": self.lock, \"td\": self._td})\n\n    def set_extra_state(self, state: OrderedDict) -> None:\n        lock = state[\"lock\"]\n        if lock is not None:\n            \"\"\"\n            since locks can't be serialized, we have use cases for stripping them\n            for example in ParallelEnv, in which case keep the lock we already have\n            to avoid an updated tensor dict being sent between processes to erase locks\n            \"\"\"\n            self.lock = lock\n        td = state[\"td\"]\n        if td is not None and not td.is_shared():\n            raise RuntimeError(\n                \"Only shared tensordicts can be set in VecNorm transforms\"\n            )\n        self._td = td\n\n    def __repr__(self) -> str:\n        return (\n            f\"{self.__class__.__name__}(decay={self.decay:4.4f},\"\n            f\"eps={self.eps:4.4f}, keys={self.in_keys})\"\n        )\n\n\nclass RewardSum(Transform):\n    \"\"\"Tracks episode cumulative rewards.\n\n    This transform accepts a list of tensordict reward keys (i.e. \u00b4in_keys\u00b4) and tracks their cumulative\n    value along each episode. When called, the transform creates a new tensordict key for each in_key named\n    \u00b4episode_{in_key}\u00b4 where  the cumulative values are written. All \u00b4in_keys\u00b4 should be part of the env\n    reward and be present in the env reward_spec.\n\n    If no in_keys are specified, this transform assumes \u00b4reward\u00b4 to be the input key. However, multiple rewards\n    (e.g. reward1 and reward2) can also be specified. If \u00b4in_keys\u00b4 are not present in the provided tensordict,\n    this transform hos no effect.\n    \"\"\"\n\n    def __init__(\n        self,\n        in_keys: Optional[Sequence[str]] = None,\n        out_keys: Optional[Sequence[str]] = None,\n    ):\n        \"\"\"Initialises the transform. Filters out non-reward input keys and defines output keys.\"\"\"\n        if in_keys is None:\n            in_keys = [\"reward\"]\n        out_keys = [f\"episode_{in_key}\" for in_key in in_keys]\n\n        super().__init__(in_keys=in_keys, out_keys=out_keys)\n\n    def reset(self, tensordict: TensorDictBase) -> TensorDictBase:\n        \"\"\"Resets episode rewards.\"\"\"\n        # Non-batched environments\n        if len(tensordict.batch_size) < 1 or tensordict.batch_size[0] == 1:\n            for in_key, out_key in zip(self.in_keys, self.out_keys):\n                if out_key in tensordict.keys():\n                    tensordict[out_key] = torch.zeros_like(tensordict[out_key])\n                elif in_key == \"reward\":", "metadata": {"task_id": "pytorch_rl/199", "ground_truth": "                    tensordict[out_key] = self.parent.reward_spec.zero()", "fpath_tuple": ["pytorch_rl", "torchrl", "envs", "transforms", "transforms.py"], "context_start_lineno": 2476, "line_no": 2560}}
{"prompt": "class, device, gradient_mode, advantage):\n        if pack_version.parse(torch.__version__) > pack_version.parse(\"1.14\"):\n            raise pytest.skip(\"make_functional_with_buffers needs to be changed\")\n        torch.manual_seed(self.seed)\n        td = self._create_seq_mock_data_ppo(device=device)\n\n        actor = self._create_mock_actor(device=device)\n        value = self._create_mock_value(device=device)\n        if advantage == \"gae\":\n            advantage = GAE(\n                gamma=0.9, lmbda=0.9, value_network=value, differentiable=gradient_mode\n            )\n        elif advantage == \"td\":\n            advantage = TDEstimate(\n                gamma=0.9, value_network=value, differentiable=gradient_mode\n            )\n        elif advantage == \"td_lambda\":\n            advantage = TDLambdaEstimate(\n                gamma=0.9, lmbda=0.9, value_network=value, differentiable=gradient_mode\n            )\n        else:\n            raise NotImplementedError\n\n        loss_fn = loss_class(actor, value, gamma=0.9, loss_critic_type=\"l2\")\n\n        floss_fn, params, buffers = make_functional_with_buffers(loss_fn)\n        # fill params with zero\n        for p in params:\n            p.data.zero_()\n        # assert len(list(floss_fn.parameters())) == 0\n        with pytest.raises(\n            KeyError, match=re.escape('key \"advantage\" not found in TensorDict with')\n        ):\n            _ = floss_fn(params, buffers, td)\n        advantage(td)\n        loss = floss_fn(params, buffers, td)\n\n        loss_critic = loss[\"loss_critic\"]\n        loss_objective = loss[\"loss_objective\"] + loss.get(\"loss_entropy\", 0.0)\n        loss_critic.backward(retain_graph=True)\n        # check that grads are independent and non null\n        named_parameters = loss_fn.named_parameters()\n        for (name, _), p in zip(named_parameters, params):\n            if p.grad is not None and p.grad.norm() > 0.0:\n                assert \"actor\" not in name\n                assert \"critic\" in name\n            if p.grad is None:\n                assert \"actor\" in name\n                assert \"critic\" not in name\n\n        for param in params:\n            param.grad = None\n        loss_objective.backward()\n        named_parameters = loss_fn.named_parameters()\n\n        for (name, other_p), p in zip(named_parameters, params):\n            assert other_p.shape == p.shape\n            assert other_p.dtype == p.dtype\n            assert other_p.device == p.device\n            if p.grad is not None and p.grad.norm() > 0.0:\n                assert \"actor\" in name\n                assert \"critic\" not in name\n            if p.grad is None:\n                assert \"actor\" not in name\n                assert \"critic\" in name\n        for param in params:\n            param.grad = None\n\n\nclass TestA2C:\n    seed = 0\n\n    def _create_mock_actor(self, batch=2, obs_dim=3, action_dim=4, device=\"cpu\"):\n        # Actor\n        action_spec = BoundedTensorSpec(\n            -torch.ones(action_dim), torch.ones(action_dim), (action_dim,)\n        )\n        net = NormalParamWrapper(nn.Linear(obs_dim, 2 * action_dim))\n        module = SafeModule(net, in_keys=[\"observation\"], out_keys=[\"loc\", \"scale\"])\n        actor = ProbabilisticActor(\n            module=module,\n            in_keys=[\"loc\", \"scale\"],\n            spec=action_spec,\n            distribution_class=TanhNormal,\n        )", "metadata": {"task_id": "pytorch_rl/127", "ground_truth": "        return actor.to(device)", "fpath_tuple": ["pytorch_rl", "test", "test_cost.py"], "context_start_lineno": 2038, "line_no": 2123}}
{"prompt": "set_verbosity_error()\n        transformers.utils.logging.set_verbosity_error()\n        diffusers.utils.logging.set_verbosity_error()\n\n    # If passed along, set the training seed now.\n    if args.seed is not None:\n        set_seed(args.seed)\n\n    # Generate class images if prior preservation is enabled.\n    if args.with_prior_preservation:\n        for i in range(len(class_data_dir)):\n            class_images_dir = Path(class_data_dir[i])\n            if not class_images_dir.exists():\n                class_images_dir.mkdir(parents=True)\n            cur_class_images = len(list(class_images_dir.iterdir()))\n\n            if cur_class_images < args.num_class_images:\n                torch_dtype = torch.float16 if accelerator.device.type == \"cuda\" else torch.float32\n                if args.prior_generation_precision == \"fp32\":\n                    torch_dtype = torch.float32\n                elif args.prior_generation_precision == \"fp16\":\n                    torch_dtype = torch.float16\n                elif args.prior_generation_precision == \"bf16\":\n                    torch_dtype = torch.bfloat16\n                pipeline = DiffusionPipeline.from_pretrained(\n                    args.pretrained_model_name_or_path,\n                    torch_dtype=torch_dtype,\n                    safety_checker=None,\n                    revision=args.revision,\n                )\n                pipeline.set_progress_bar_config(disable=True)\n\n                num_new_images = args.num_class_images - cur_class_images\n                logger.info(f\"Number of class images to sample: {num_new_images}.\")\n\n                sample_dataset = PromptDataset(class_prompt[i], num_new_images)\n                sample_dataloader = torch.utils.data.DataLoader(sample_dataset, batch_size=args.sample_batch_size)\n\n                sample_dataloader = accelerator.prepare(sample_dataloader)\n                pipeline.to(accelerator.device)\n\n                for example in tqdm(\n                    sample_dataloader, desc=\"Generating class images\", disable=not accelerator.is_local_main_process\n                ):\n                    images = pipeline(example[\"prompt\"]).images\n\n                    for i, image in enumerate(images):\n                        hash_image = hashlib.sha1(image.tobytes()).hexdigest()\n                        image_filename = (\n                            class_images_dir / f\"{example['index'][i] + cur_class_images}-{hash_image}.jpg\"\n                        )\n                        image.save(image_filename)\n\n                del pipeline\n                if torch.cuda.is_available():\n                    torch.cuda.empty_cache()\n\n    # Handle the repository creation\n    if accelerator.is_main_process:\n        if args.push_to_hub:\n            if args.hub_model_id is None:\n                repo_name = get_full_repo_name(Path(args.output_dir).name, token=args.hub_token)\n            else:\n                repo_name = args.hub_model_id\n            create_repo(repo_name, exist_ok=True, token=args.hub_token)\n            repo = Repository(args.output_dir, clone_from=repo_name, token=args.hub_token)\n\n            with open(os.path.join(args.output_dir, \".gitignore\"), \"w+\") as gitignore:\n                if \"step_*\" not in gitignore:\n                    gitignore.write(\"step_*\\n\")\n                if \"epoch_*\" not in gitignore:\n                    gitignore.write(\"epoch_*\\n\")\n        elif args.output_dir is not None:\n            os.makedirs(args.output_dir, exist_ok=True)\n\n    # Load the tokenizer\n    if args.tokenizer_name:", "metadata": {"task_id": "huggingface_diffusers/146", "ground_truth": "        tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name, revision=args.revision, use_fast=False)", "fpath_tuple": ["huggingface_diffusers", "examples", "research_projects", "multi_subject_dreambooth", "train_multi_subject_dreambooth.py"], "context_start_lineno": 514, "line_no": 591}}
{"prompt": " 16, 16, device=device)\n                for key in keys_total\n            },\n            batch,\n        )\n        squeeze(td)\n\n        expected_size = [*size, nchannels, 16, 16]\n        for key in keys_total.difference(keys):\n            assert td.get(key).shape[len(batch) :] == torch.Size(expected_size)\n\n        if expected_size[squeeze_dim] == 1:\n            del expected_size[squeeze_dim]\n        for key in keys:\n            assert td.get(key).shape[len(batch) :] == torch.Size(expected_size)\n\n    @pytest.mark.parametrize(\"squeeze_dim\", [1, -2])\n    @pytest.mark.parametrize(\"nchannels\", [1, 3])\n    @pytest.mark.parametrize(\"batch\", [[], [2], [2, 4]])\n    @pytest.mark.parametrize(\"size\", [[], [4]])\n    @pytest.mark.parametrize(\n        \"keys\", [[\"observation\", \"some_other_key\"], [\"observation_pixels\"]]\n    )\n    @pytest.mark.parametrize(\"device\", get_available_devices())\n    @pytest.mark.parametrize(\n        \"keys_inv\", [[], [\"action\", \"some_other_key\"], [\"observation_pixels\"]]\n    )\n    def test_squeeze_inv(\n        self, keys, keys_inv, size, nchannels, batch, device, squeeze_dim\n    ):\n        torch.manual_seed(0)\n        keys_total = set(keys + keys_inv)\n        squeeze = SqueezeTransform(squeeze_dim, in_keys=keys, in_keys_inv=keys_inv)\n        td = TensorDict(\n            {\n                key: torch.randn(*batch, *size, nchannels, 16, 16, device=device)\n                for key in keys_total\n            },\n            batch,\n        )\n        squeeze.inv(td)\n\n        expected_size = [*size, nchannels, 16, 16]\n        for key in keys_total.difference(keys_inv):\n            assert td.get(key).shape[len(batch) :] == torch.Size(expected_size)\n\n        if squeeze_dim < 0:\n            expected_size.insert(len(expected_size) + squeeze_dim + 1, 1)\n        else:\n            expected_size.insert(squeeze_dim, 1)\n        expected_size = torch.Size(expected_size)\n\n        for key in keys_inv:\n            assert td.get(key).shape[len(batch) :] == torch.Size(expected_size)\n\n    @pytest.mark.skipif(not _has_tv, reason=\"no torchvision\")\n    @pytest.mark.parametrize(\n        \"keys\",\n        [[(\"next\", \"observation\"), \"some_other_key\"], [(\"next\", \"observation_pixels\")]],\n    )\n    @pytest.mark.parametrize(\"device\", get_available_devices())\n    def test_grayscale(self, keys, device):\n        torch.manual_seed(0)\n        nchannels = 3\n        gs = GrayScale(in_keys=keys)\n        dont_touch = torch.randn(1, nchannels, 16, 16, device=device)\n        td = TensorDict(\n            {key: torch.randn(1, nchannels, 16, 16, device=device) for key in keys},\n            [1],\n            device=device,\n        )\n        td.set(\"dont touch\", dont_touch.clone())\n        gs(td)\n        for key in keys:\n            assert td.get(key).shape[-3] == 1\n        assert (td.get(\"dont touch\") == dont_touch).all()\n\n        if len(keys) == 1:\n            observation_spec = BoundedTensorSpec(-1, 1, (nchannels, 16, 16))", "metadata": {"task_id": "pytorch_rl/47", "ground_truth": "            observation_spec = gs.transform_observation_spec(observation_spec)", "fpath_tuple": ["pytorch_rl", "test", "test_transforms.py"], "context_start_lineno": 719, "line_no": 798}}
{"prompt": "(\n            torch.tensor([-100, -1, 0, 1, 9, 10, 100], dtype=torch.long)\n        )\n        assert (\n            projected == torch.tensor([0, 0, 0, 1, 9, 9, 9], dtype=torch.long)\n        ).all()\n\n        projected = action_spec.project(\n            torch.tensor([-100.0, -1.0, 0.0, 1.0, 9.0, 10.0, 100.0], dtype=torch.float)\n        )\n        assert (\n            projected == torch.tensor([0, 0, 0, 1, 9, 9, 9], dtype=torch.long)\n        ).all()\n\n    def test_bounded_rand(self):\n        spec = BoundedTensorSpec(-3, 3, torch.Size((1,)))\n        sample = torch.stack([spec.rand() for _ in range(100)])\n        assert (-3 <= sample).all() and (3 >= sample).all()\n\n    def test_ndbounded_shape(self):\n        spec = BoundedTensorSpec(-3, 3 * torch.ones(10, 5), shape=[10, 5])\n        sample = torch.stack([spec.rand() for _ in range(100)], 0)\n        assert (-3 <= sample).all() and (3 >= sample).all()\n        assert sample.shape == torch.Size([100, 10, 5])\n\n\nclass TestExpand:\n    @pytest.mark.parametrize(\n        \"shape1\",\n        [\n            None,\n            (4,),\n            (5, 4),\n        ],\n    )\n    @pytest.mark.parametrize(\"shape2\", [(), (10,)])\n    def test_binary(self, shape1, shape2):\n        spec = BinaryDiscreteTensorSpec(\n            n=4, shape=shape1, device=\"cpu\", dtype=torch.bool\n        )\n        if shape1 is not None:\n            shape2_real = (*shape2, *shape1)\n        else:\n            shape2_real = (*shape2, 4)\n\n        spec2 = spec.expand(shape2_real)\n        assert spec2 is not spec\n        assert spec2.dtype == spec.dtype\n        assert (spec2.zero() == spec.zero()).all()\n        assert spec2.rand().shape == spec2.shape\n        assert spec2.zero().shape == spec2.shape\n        spec2 = spec.expand(*shape2_real)\n        assert spec2 is not spec\n        assert spec2.dtype == spec.dtype\n        assert (spec2.zero() == spec.zero()).all()\n        assert spec2.rand().shape == spec2.shape\n        assert spec2.zero().shape == spec2.shape\n\n    @pytest.mark.parametrize(\"shape2\", [(), (5,)])\n    @pytest.mark.parametrize(\n        \"shape1,mini,maxi\",\n        [\n            [(10,), -torch.ones([]), torch.ones([])],\n            [None, -torch.ones([10]), torch.ones([])],\n            [None, -torch.ones([]), torch.ones([10])],\n            [(10,), -torch.ones([]), torch.ones([10])],\n            [(10,), -torch.ones([10]), torch.ones([])],\n            [(10,), -torch.ones([10]), torch.ones([10])],\n        ],\n    )\n    def test_bounded(self, shape1, shape2, mini, maxi):\n        spec = BoundedTensorSpec(\n            mini, maxi, shape=shape1, device=\"cpu\", dtype=torch.bool\n        )\n        shape1 = spec.shape\n        assert shape1 == torch.Size([10])\n        shape2_real = (*shape2, *shape1)\n\n        spec2 = spec.expand(shape2_real)\n        assert spec2 is not spec\n        assert spec2.dtype == spec.dtype\n        assert (spec2.zero() == spec.zero()).all()", "metadata": {"task_id": "pytorch_rl/100", "ground_truth": "        assert spec2.rand().shape == spec2.shape", "fpath_tuple": ["pytorch_rl", "test", "test_specs.py"], "context_start_lineno": 1092, "line_no": 1174}}
{"prompt": "]{4}-[0-9]{2}-[0-9]{2}_[0-9]{6}\")\n        timestamp_match = timestamp_reg.findall(str(self.log_filename.parents[0]))\n        self.vis[\"renderingState/export_path\"].write(timestamp_match[-1])\n\n        # clear the current scene\n        self.vis[\"sceneState/sceneBox\"].delete()\n        self.vis[\"sceneState/cameras\"].delete()\n\n        # draw the training cameras and images\n        image_indices = self._pick_drawn_image_idxs(len(dataset))\n        for idx in image_indices:\n            image = dataset[idx][\"image\"]\n            bgr = image[..., [2, 1, 0]]\n            camera_json = dataset.cameras.to_json(camera_idx=idx, image=bgr, max_size=100)\n            self.vis[f\"sceneState/cameras/{idx:06d}\"].write(camera_json)\n\n        # draw the scene box (i.e., the bounding box)\n        json_ = dataset.scene_box.to_json()\n        self.vis[\"sceneState/sceneBox\"].write(json_)\n\n        # set the initial state whether to train or not\n        self.vis[\"renderingState/isTraining\"].write(start_train)\n\n        max_scene_box = torch.max(dataset.scene_box.aabb[1] - dataset.scene_box.aabb[0]).item()\n        self.vis[\"renderingState/max_box_size\"].write(max_scene_box)\n\n        # self.vis[\"renderingState/render_time\"].write(str(0))\n\n        # set the properties of the camera\n        # self.vis[\"renderingState/camera\"].write(json_)\n\n        # set the main camera intrinsics to one from the dataset\n        # K = camera.get_intrinsics_matrix()\n        # set_persp_intrinsics_matrix(self.vis, K.double().numpy())\n\n    def _check_camera_path_payload(self, trainer, step: int):\n        \"\"\"Check to see if the camera path export button was pressed.\"\"\"\n        # check if we should interrupt from a button press?\n        camera_path_payload = self.vis[\"camera_path_payload\"].read()\n        if camera_path_payload:\n            # save a model checkpoint\n            trainer.save_checkpoint(step)\n            # write to json file in datapath directory\n            camera_path_filename = camera_path_payload[\"camera_path_filename\"] + \".json\"\n            camera_path = camera_path_payload[\"camera_path\"]\n            camera_paths_directory = os.path.join(self.datapath, \"camera_paths\")\n            if not os.path.exists(camera_paths_directory):\n                os.mkdir(camera_paths_directory)\n\n            write_to_json(Path(os.path.join(camera_paths_directory, camera_path_filename)), camera_path)\n            self.vis[\"camera_path_payload\"].delete()\n\n    def _check_populate_paths_payload(self, trainer, step: int):\n        populate_paths_payload = self.vis[\"populate_paths_payload\"].read()\n        if populate_paths_payload:\n            # save a model checkpoint\n            trainer.save_checkpoint(step)\n            # get all camera paths\n            camera_path_dir = os.path.join(self.datapath, \"camera_paths\")\n            camera_path_files = os.listdir(camera_path_dir)\n            all_path_dict = {}\n            for i in camera_path_files:\n                if i[-4:] == \"json\":\n                    all_path_dict[i[:-5]] = load_from_json(Path(os.path.join(camera_path_dir, i)))\n            self.vis[\"renderingState/all_camera_paths\"].write(all_path_dict)", "metadata": {"task_id": "nerfstudio-project_nerfstudio/150", "ground_truth": "            self.vis[\"populate_paths_payload\"].delete()", "fpath_tuple": ["nerfstudio-project_nerfstudio", "nerfstudio", "viewer", "server", "viewer_utils.py"], "context_start_lineno": 338, "line_no": 403}}
{"prompt": "):\n    for k, v in cn_node.items():\n        if isinstance(v, Argument) and k not in help_info_dict:\n            help_info_dict[prefix + k] = v.description\n        elif isinstance(v, CN):\n            set_help_info(v,\n                          help_info_dict,\n                          prefix=f\"{k}.\" if prefix == \"\" else f\"{prefix}{k}.\")\n\n\nclass CN(CfgNode):\n    \"\"\"\n    An extended configuration system based on [yacs]( \\\n    https://github.com/rbgirshick/yacs). \\\n    The two-level tree structure consists of several internal dict-like \\\n    containers to allow simple key-value access and management.\n    \"\"\"\n    def __init__(self, init_dict=None, key_list=None, new_allowed=False):\n        init_dict = super().__init__(init_dict, key_list, new_allowed)\n        self.__cfg_check_funcs__ = list()  # to check the config values\n        # validity\n        self.__help_info__ = dict()  # build the help dict\n\n        self.is_ready_for_run = False  # whether this CfgNode has checked its\n        # validity, completeness and clean some un-useful info\n\n        if init_dict:\n            for k, v in init_dict.items():\n                if isinstance(v, Argument):\n                    self.__help_info__[k] = v.description\n                elif isinstance(v, CN) and \"help_info\" in v:\n                    for name, des in v.__help_info__.items():\n                        self.__help_info__[name] = des\n\n    def __getattr__(self, name):\n        if name in self:\n            return self[name]\n        else:\n            raise AttributeError(name)\n\n    def __delattr__(self, name):\n        if name in self:\n            del self[name]\n        else:\n            raise AttributeError(name)\n\n    def clear_aux_info(self):\n        \"\"\"\n        Clears all the auxiliary information of the CN object.\n        \"\"\"\n        if hasattr(self, \"__cfg_check_funcs__\"):\n            delattr(self, \"__cfg_check_funcs__\")\n        if hasattr(self, \"__help_info__\"):\n            delattr(self, \"__help_info__\")\n        if hasattr(self, \"is_ready_for_run\"):\n            delattr(self, \"is_ready_for_run\")\n        for v in self.values():\n            if isinstance(v, CN):\n                v.clear_aux_info()\n\n    def print_help(self, arg_name=\"\"):\n        \"\"\"\n        print help info for a specific given ``arg_name`` or \\\n        for all arguments if not given ``arg_name``\n\n        Args:\n            arg_name: name of specific args\n        \"\"\"\n        if arg_name!= \"\" and arg_name in self.__help_info__:\n            print(f\"  --{arg_name} \\t {self.__help_info__[arg_name]}\")\n        else:\n            for k, v in self.__help_info__.items():\n                print(f\"  --{k} \\t {v}\")\n\n    def register_cfg_check_fun(self, cfg_check_fun):\n        \"\"\"\n        Register a function that checks the configuration node.\n\n        Args:\n            cfg_check_fun: function for validation the correctness of cfg.\n        \"\"\"\n        self.__cfg_check_funcs__.append(cfg_check_fun)\n\n    def merge_from_file(self, cfg_filename, check_cfg=True):\n        \"\"\"\n        load configs from a yaml file, another cfg instance or a list \\\n        stores the keys and values.\n\n        Args:\n            cfg_filename: file name of yaml file\n            check_cfg: whether enable ``assert_cfg()``\n        \"\"\"\n        cfg_check_funcs = copy.copy(self.__cfg_check_funcs__)\n        with open(cfg_filename, \"r\") as f:", "metadata": {"task_id": "alibaba_FederatedScope/93", "ground_truth": "            cfg = self.load_cfg(f)", "fpath_tuple": ["alibaba_FederatedScope", "federatedscope", "core", "configs", "config.py"], "context_start_lineno": 13, "line_no": 107}}
{"prompt": "Optional[namedtuple]`): the api namedtuple of eval_mode policy\n            - env (:obj:`Optional[BaseEnvManager]`): instance of the subclass of vectorized \\\n                env_manager(BaseEnvManager)\n        \"\"\"\n        if _env is not None:\n            self.reset_env(_env)\n        if _policy is not None:\n            self.reset_policy(_policy)\n        self._max_eval_reward = float(\"-inf\")\n        self._last_eval_iter = 0\n        self._end_flag = False\n\n    def close(self) -> None:\n        \"\"\"\n        Overview:\n            Close the evaluator. If end_flag is False, close the environment, flush the tb_logger\\\n                and close the tb_logger.\n        \"\"\"\n        if self._end_flag:\n            return\n        self._end_flag = True\n        self._env.close()\n        self._tb_logger.flush()\n        self._tb_logger.close()\n\n    def __del__(self):\n        \"\"\"\n        Overview:\n            Execute the close command and close the evaluator. __del__ is automatically called \\\n                to destroy the evaluator instance when the evaluator finishes its work\n        \"\"\"\n        self.close()\n\n    def should_eval(self, train_iter: int) -> bool:\n        \"\"\"\n        Overview:\n            Determine whether you need to start the evaluation mode, if the number of training has reached\\\n                the maximum number of times to start the evaluator, return True\n        \"\"\"\n        if train_iter == self._last_eval_iter:\n            return False\n        if (train_iter - self._last_eval_iter) < self._cfg.eval_freq and train_iter!= 0:\n            return False\n        self._last_eval_iter = train_iter\n        return True\n\n    def eval(\n            self,\n            save_ckpt_fn: Callable = None,\n            train_iter: int = -1,\n            envstep: int = -1,\n            n_episode: Optional[int] = None\n    ) -> Tuple[bool, float]:\n        '''\n        Overview:\n            Evaluate policy and store the best policy based on whether it reaches the highest historical reward.\n        Arguments:\n            - save_ckpt_fn (:obj:`Callable`): Saving ckpt function, which will be triggered by getting the best reward.\n            - train_iter (:obj:`int`): Current training iteration.\n            - envstep (:obj:`int`): Current env interaction step.\n            - n_episode (:obj:`int`): Number of evaluation episodes.\n        Returns:\n            - stop_flag (:obj:`bool`): Whether this training program can be ended.\n            - eval_reward (:obj:`float`): Current eval_reward.\n        '''\n        if n_episode is None:\n            n_episode = self._default_n_episode\n        assert n_episode is not None, \"please indicate eval n_episode\"\n        envstep_count = 0\n        info = {}\n        eval_monitor = VectorEvalMonitor(self._env.env_num, n_episode)\n        self._env.reset()\n        self._policy.reset()\n\n        with self._timer:\n            while not eval_monitor.is_finished():\n                obs = self._env.ready_obs\n                obs = to_tensor(obs, dtype=torch.float32)\n                policy_output = self._policy.forward(obs)\n                actions = {i: a['action'] for i, a in policy_output.items()}\n                actions = to_ndarray(actions)\n                timesteps = self._env.step(actions)\n                timesteps = to_tensor(timesteps, dtype=torch.float32)\n                for env_id, t in timesteps.items():\n                    if t.info.get('abnormal', False):\n                        # If there is an abnormal timestep, reset all the related variables(including this env).\n                        self._policy.reset([env_id])\n                        continue\n                    if t.done:\n                        # Env reset is done by env_manager automatically.", "metadata": {"task_id": "opendilab_ACE/93", "ground_truth": "                        self._policy.reset([env_id])", "fpath_tuple": ["opendilab_ACE", "ding", "worker", "collector", "interaction_serial_evaluator.py"], "context_start_lineno": 105, "line_no": 195}}
{"prompt": " permissions and\n# limitations under the License.\n\n\"\"\"\nAbstracts for the Pipeline class.\n\"\"\"\nfrom __future__ import annotations\n\nimport typing\nfrom abc import abstractmethod\nfrom dataclasses import dataclass, field\nfrom time import time\nfrom typing import Any, Dict, List, Optional, Type, Union, cast\n\nimport torch\nimport torch.distributed as dist\nfrom rich.progress import (\n    BarColumn,\n    MofNCompleteColumn,\n    Progress,\n    TextColumn,\n    TimeElapsedColumn,\n)\nfrom torch import nn\nfrom torch.nn import Parameter\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nfrom typing_extensions import Literal\n\nfrom nerfstudio.configs import base_config as cfg\nfrom nerfstudio.data.datamanagers.base_datamanager import (\n    DataManager,\n    VanillaDataManager,\n    VanillaDataManagerConfig,\n)\nfrom nerfstudio.engine.callbacks import TrainingCallback, TrainingCallbackAttributes\nfrom nerfstudio.models.base_model import Model, ModelConfig\nfrom nerfstudio.utils import profiler\n\n\ndef module_wrapper(ddp_or_model: Union[DDP, Model]) -> Model:\n    \"\"\"\n    If DDP, then return the.module. Otherwise, return the model.\n    \"\"\"\n    if isinstance(ddp_or_model, DDP):\n        return cast(Model, ddp_or_model.module)\n    return ddp_or_model\n\n\nclass Pipeline(nn.Module):\n    \"\"\"The intent of this class is to provide a higher level interface for the Model\n    that will be easy to use for our Trainer class.\n\n    This class will contain high level functions for the model like getting the loss\n    dictionaries and visualization code. It should have ways to get the next iterations\n    training loss, evaluation loss, and generate whole images for visualization. Each model\n    class should be 1:1 with a pipeline that can act as a standardized interface and hide\n    differences in how each model takes in and outputs data.\n\n    This class's function is to hide the data manager and model classes from the trainer,\n    worrying about:\n    1) Fetching data with the data manager\n    2) Feeding the model the data and fetching the loss\n    Hopefully this provides a higher level interface for the trainer to use, and\n    simplifying the model classes, which each may have different forward() methods\n    and so on.\n\n    Args:\n        config: configuration to instantiate pipeline\n        device: location to place model and data\n        test_mode:\n            'train': loads train/eval datasets into memory\n            'test': loads train/test dataset into memory\n            'inference': does not load any dataset into memory\n        world_size: total number of machines available\n        local_rank: rank of current machine\n\n    Attributes:\n        datamanager: The data manager that will be used\n        model: The model that will be used\n    \"\"\"\n\n    # pylint: disable=abstract-method\n\n    datamanager: DataManager\n    _model: Model\n\n    @property\n    def model(self):\n        \"\"\"Returns the unwrapped model if in ddp\"\"\"\n        return module_wrapper(self._model)\n\n    @property\n    def device(self):\n        \"\"\"Returns the device that the model is on.\"\"\"\n        return self.model.device\n\n    @profiler.time_function\n    def get_train_loss_dict(self, step: int):\n        \"\"\"This function gets your training loss dict. This will be responsible for\n        getting the next batch of data from the DataManager and interfacing with the\n        Model class, feeding the data to the model's forward function.\n\n        Args:\n            step: current iteration step to update sampler if using DDP (distributed)\n        \"\"\"\n        if self.world_size > 1 and step:\n            assert self.datamanager.train_sampler is not None\n            self.datamanager.train_sampler.set_epoch(step)\n        ray_bundle, batch = self.datamanager.next_train(step)\n        model_outputs = self.model(ray_bundle, batch)\n        metrics_dict = self.model.get_metrics_dict(model_outputs, batch)", "metadata": {"task_id": "nerfstudio-project_nerfstudio/134", "ground_truth": "        loss_dict = self.model.get_loss_dict(model_outputs, batch, metrics_dict)", "fpath_tuple": ["nerfstudio-project_nerfstudio", "nerfstudio", "pipelines", "base_pipeline.py"], "context_start_lineno": 11, "line_no": 122}}
{"prompt": "MAXIMIZE\n            )\n        ]\n    )\n    root = py_study_config.search_space.root\n    root.add_float_param('learning_rate', 0.01, 3.0)\n\n    pytrial = vz.Trial(\n        id=1,\n        completion_time=datetime.datetime(\n            year=2021, month=12, day=2, hour=7, minute=31\n        ),\n        parameters={'learning_rate': vz.ParameterValue(0.5)},\n        final_measurement=vz.Measurement(\n            metrics={\n                'loss': vz.Metric(value=56.8),\n                'objective': vz.Metric(value=77.7),\n            },\n            elapsed_secs=67,\n            steps=101,\n        ),\n    )\n    parameters = py_study_config._pytrial_parameters(pytrial)\n    self.assertEqual({'learning_rate': 0.5}, parameters)\n    metrics = py_study_config._pytrial_metrics(pytrial)\n    self.assertEqual({'objective': 77.7}, metrics)\n    metrics = py_study_config._pytrial_metrics(\n        pytrial, include_all_metrics=True)\n    self.assertEqual({'objective': 77.7, 'loss': 56.8}, metrics)\n\n  def testTrialToDictWithFinalMetricsNotCompleted(self):\n    # Throw a Trial that has inconsistent field values.\n    # (ACTIVE but has final measurement).\n    # Pyvizier fixes the state.\n    py_study_config = vz.StudyConfig(\n        metric_information=[\n            vz.MetricInformation(\n                name='objective', goal=vz.ObjectiveMetricGoal.MAXIMIZE\n            )\n        ]\n    )\n    root = py_study_config.search_space.root\n    root.add_float_param('learning_rate', 0.01, 3.0)\n\n    trial_proto = study_pb2.Trial()\n    trial_proto.id = str(1)\n    trial_proto.state = study_pb2.Trial.State.ACTIVE\n    trial_proto.parameters.add(\n        parameter_id='learning_rate', value=struct_pb2.Value(number_value=0.5))\n    trial_proto.final_measurement.step_count = 101\n    trial_proto.final_measurement.elapsed_duration.seconds = 67\n    trial_proto.final_measurement.metrics.add(metric_id='loss', value=56.8)\n    trial_proto.final_measurement.metrics.add(metric_id='objective', value=77.7)\n\n    parameters = py_study_config.trial_parameters(trial_proto)\n    self.assertEqual({'learning_rate': 0.5}, parameters)\n    self.assertLen(\n        py_study_config.trial_metrics(trial_proto, include_all_metrics=True), 2)\n\n  def testTrialToDictWithFinalMetricsInfeasible(self):\n    py_study_config = vz.StudyConfig(\n        metric_information=[\n            vz.MetricInformation(\n                name='objective', goal=vz.ObjectiveMetricGoal.MAXIMIZE\n            )\n        ]\n    )\n    root = py_study_config.search_space.root\n    root.add_float_param('learning_rate', 0.01, 3.0)\n\n    trial_proto = study_pb2.Trial()\n    trial_proto.id = str(1)\n    trial_proto.state = study_pb2.Trial.State.INFEASIBLE\n    trial_proto.parameters.add(\n        parameter_id='learning_rate', value=struct_pb2.Value(number_value=0.5))\n    trial_proto.final_measurement.step_count = 101\n    trial_proto.final_measurement.elapsed_duration.seconds = 67", "metadata": {"task_id": "google_vizier/146", "ground_truth": "    trial_proto.final_measurement.metrics.add(metric_id='loss', value=56.8)", "fpath_tuple": ["google_vizier", "vizier", "_src", "pyvizier", "oss", "study_config_test.py"], "context_start_lineno": 741, "line_no": 818}}
{"prompt": "\", \"a\", \"nice\", \"City\", \".\"]]\n\n        # aligned start and words\n        predictions = [\n            [\n                {\"start\": 0, \"entity\": \"B-LOC\"},\n                {\"start\": 2, \"entity\": \"I-LOC\"},\n                {\"start\": 4, \"entity\": \"I-LOC\"},\n                {\"start\": 9, \"entity\": \"O\"},\n                {\"start\": 11, \"entity\": \"O\"},\n                {\"start\": 16, \"entity\": \"B-LOC\"},\n                {\"start\": 21, \"entity\": \"O\"},\n            ]\n        ]\n        predictions = task_evaluator.predictions_processor(predictions, words, join_by)\n        self.assertListEqual(predictions[\"predictions\"][0], [\"B-LOC\", \"I-LOC\", \"O\", \"O\", \"B-LOC\", \"O\"])\n\n        # non-aligned start and words\n        predictions = [\n            [\n                {\"start\": 0, \"entity\": \"B-LOC\"},\n                {\"start\": 2, \"entity\": \"I-LOC\"},\n                {\"start\": 9, \"entity\": \"O\"},\n                {\"start\": 11, \"entity\": \"O\"},\n                {\"start\": 16, \"entity\": \"B-LOC\"},\n                {\"start\": 21, \"entity\": \"O\"},\n            ]\n        ]\n        predictions = task_evaluator.predictions_processor(predictions, words, join_by)\n        self.assertListEqual(predictions[\"predictions\"][0], [\"B-LOC\", \"O\", \"O\", \"O\", \"B-LOC\", \"O\"])\n\n        # non-aligned start and words\n        predictions = [\n            [\n                {\"start\": 0, \"entity\": \"B-LOC\"},\n                {\"start\": 6, \"entity\": \"I-LOC\"},\n                {\"start\": 9, \"entity\": \"O\"},\n                {\"start\": 11, \"entity\": \"O\"},\n                {\"start\": 16, \"entity\": \"B-LOC\"},\n                {\"start\": 21, \"entity\": \"O\"},\n            ]\n        ]\n        predictions = task_evaluator.predictions_processor(predictions, words, join_by)\n        self.assertListEqual(predictions[\"predictions\"][0], [\"B-LOC\", \"O\", \"O\", \"O\", \"B-LOC\", \"O\"])\n\n        # non-aligned start and words\n        predictions = [\n            [\n                {\"start\": 0, \"entity\": \"B-LOC\"},\n                {\"start\": 9, \"entity\": \"O\"},\n                {\"start\": 11, \"entity\": \"O\"},\n                {\"start\": 16, \"entity\": \"B-LOC\"},\n                {\"start\": 21, \"entity\": \"O\"},\n            ]\n        ]\n        predictions = task_evaluator.predictions_processor(predictions, words, join_by)\n        self.assertListEqual(predictions[\"predictions\"][0], [\"B-LOC\", \"O\", \"O\", \"O\", \"B-LOC\", \"O\"])\n\n\nclass TestTextGenerationEvaluator(TestCase):\n    def setUp(self):\n        self.data = Dataset.from_dict({\"text\": [\"Lorem ipsum\"]})\n        self.pipe = DummyTextGenerationPipeline(num_return_sequences=4)\n        self.evaluator = evaluator(\"text-generation\")\n\n    def test_class_init(self):\n        evaluator = TextGenerationEvaluator()\n        self.assertEqual(evaluator.task, \"text-generation\")\n        self.assertIsNone(evaluator.default_metric_name)\n\n        results = evaluator.compute(\n            model_or_pipeline=self.pipe,\n            data=self.data,\n            metric=\"word_count\",\n        )\n        self.assertIsInstance(results[\"unique_words\"], int)\n\n    @slow\n    def test_default_pipe_init(self):\n        results = self.evaluator.compute(data=self.data)\n        self.assertIsInstance(results[\"unique_words\"], int)\n\n    def test_overwrite_default_metric(self):\n        word_length = load(\"word_length\")", "metadata": {"task_id": "huggingface_evaluate/177", "ground_truth": "        results = self.evaluator.compute(\n            model_or_pipeline=self.pipe,\n            data=self.data,\n            metric=word_length,\n        )", "fpath_tuple": ["huggingface_evaluate", "tests", "test_evaluator.py"], "context_start_lineno": 794, "line_no": 878}}
{"prompt": "_config=self.reg_fit_config_nodir_dump,\n                    calib_config=self.reg_calib_config_nodir_nodump,\n                )\n\n            # save dir, no dump\n            status = prob_reg.train(\n                train_data_loader=self.reg_train_data_loader,\n                calib_data_loader=self.reg_val_data_loader,\n                val_data_loader=self.reg_val_data_loader,\n                map_fit_config=self.reg_fit_config_nodir_nodump,\n                fit_config=self.reg_fit_config_dir_nodump(tmp_dir),\n                calib_config=self.reg_calib_config_nodir_nodump,\n            )\n            sample = prob_reg.posterior.sample()\n            prob_reg.posterior.load_state(tmp_dir)\n\n            # save dir and dump\n            status = prob_reg.train(\n                train_data_loader=self.reg_train_data_loader,\n                calib_data_loader=self.reg_val_data_loader,\n                val_data_loader=self.reg_val_data_loader,\n                map_fit_config=self.reg_fit_config_nodir_nodump,\n                fit_config=self.reg_fit_config_dir_nodump(tmp_dir),\n                calib_config=self.reg_calib_config_nodir_nodump,\n            )\n            sample = prob_reg.posterior.sample()\n            prob_reg.posterior.load_state(tmp_dir)\n\n            # restore from swag\n            status = prob_reg.train(\n                train_data_loader=self.reg_train_data_loader,\n                calib_data_loader=self.reg_val_data_loader,\n                val_data_loader=self.reg_val_data_loader,\n                map_fit_config=self.reg_fit_config_nodir_nodump,\n                fit_config=self.reg_fit_config_restore(tmp_dir),\n                calib_config=self.reg_calib_config_nodir_nodump,\n            )\n\n            # restore from map\n            prob_reg_map = ProbRegressor(\n                model=MyModel(self.reg_output_dim),\n                likelihood_log_variance_model=MyModel(self.reg_output_dim),\n                posterior_approximator=MAPPosteriorApproximator(),\n                output_calibrator=RegressionTemperatureScaler(),\n            )\n            status = prob_reg_map.train(\n                train_data_loader=self.reg_train_data_loader,\n                calib_data_loader=self.reg_val_data_loader,\n                val_data_loader=self.reg_val_data_loader,\n                map_fit_config=self.reg_fit_config_nodir_nodump,\n                fit_config=self.reg_fit_config_dir_dump(tmp_dir),\n                calib_config=self.reg_calib_config_nodir_nodump,\n            )\n            status = prob_reg.train(\n                train_data_loader=self.reg_train_data_loader,\n                calib_data_loader=self.reg_val_data_loader,\n                val_data_loader=self.reg_val_data_loader,\n                map_fit_config=self.reg_fit_config_nodir_nodump,\n                fit_config=self.reg_fit_config_restore(tmp_dir),\n                calib_config=self.reg_calib_config_nodir_nodump,\n            )\n\n            # load state\n            prob_reg.load_state(checkpoint_path=tmp_dir)\n\n            # save state\n            prob_reg.save_state(checkpoint_path=tmp_dir)\n\n    def test_dryrun_class_swag(self):\n        with tempfile.TemporaryDirectory() as tmp_dir:", "metadata": {"task_id": "awslabs_fortuna/152", "ground_truth": "            prob_class = ProbClassifier(\n                model=MyModel(self.class_output_dim),\n                posterior_approximator=SWAGPosteriorApproximator(rank=2),\n                output_calibrator=ClassificationTemperatureScaler(),\n            )", "fpath_tuple": ["awslabs_fortuna", "tests", "fortuna", "prob_model", "test_train.py"], "context_start_lineno": 790, "line_no": 860}}
{"prompt": "\n    @pytest.mark.parametrize(\"shape2\", [(), (10,)])\n    def test_onehot(self, shape1, shape2):\n        if shape1 is None:\n            shape1 = (15,)\n        else:\n            shape1 = (*shape1, 15)\n        spec = OneHotDiscreteTensorSpec(\n            n=15, shape=shape1, device=\"cpu\", dtype=torch.long\n        )\n        if shape1 is not None:\n            shape2_real = (*shape2, *shape1)\n        else:\n            shape2_real = shape2\n\n        spec2 = spec.expand(shape2_real)\n        assert spec2 is not spec\n        assert spec2.dtype == spec.dtype\n        assert (spec2.zero() == spec.zero()).all()\n        assert spec2.rand().shape == spec2.shape\n        assert spec2.zero().shape == spec2.shape\n        spec2 = spec.expand(*shape2_real)\n        assert spec2 is not spec\n        assert spec2.dtype == spec.dtype\n        assert (spec2.zero() == spec.zero()).all()\n        assert spec2.rand().shape == spec2.shape\n        assert spec2.zero().shape == spec2.shape\n\n    @pytest.mark.parametrize(\n        \"shape1\",\n        [\n            None,\n            (),\n            (5,),\n        ],\n    )\n    @pytest.mark.parametrize(\"shape2\", [(), (10,)])\n    def test_unbounded(self, shape1, shape2):\n        if shape1 is None:\n            shape1 = (15,)\n        else:\n            shape1 = (*shape1, 15)\n        spec = UnboundedContinuousTensorSpec(\n            shape=shape1, device=\"cpu\", dtype=torch.float64\n        )\n        if shape1 is not None:\n            shape2_real = (*shape2, *shape1)\n        else:\n            shape2_real = shape2\n\n        spec2 = spec.expand(shape2_real)\n        assert spec2 is not spec\n        assert spec2.dtype == spec.dtype\n        assert (spec2.zero() == spec.zero()).all()\n        assert spec2.rand().shape == spec2.shape\n        assert spec2.zero().shape == spec2.shape\n        spec2 = spec.expand(*shape2_real)\n        assert spec2 is not spec\n        assert spec2.dtype == spec.dtype\n        assert (spec2.zero() == spec.zero()).all()\n        assert spec2.rand().shape == spec2.shape\n        assert spec2.zero().shape == spec2.shape\n\n    @pytest.mark.parametrize(\n        \"shape1\",\n        [\n            None,\n            (),\n            (5,),\n        ],\n    )\n    @pytest.mark.parametrize(\"shape2\", [(), (10,)])\n    def test_unboundeddiscrete(self, shape1, shape2):\n        if shape1 is None:\n            shape1 = (15,)\n        else:\n            shape1 = (*shape1, 15)\n        spec = UnboundedDiscreteTensorSpec(shape=shape1, device=\"cpu\", dtype=torch.long)\n        if shape1 is not None:\n            shape2_real = (*shape2, *shape1)\n        else:\n            shape2_real = shape2\n\n        spec2 = spec.expand(shape2_real)\n        assert spec2 is not spec\n        assert spec2.dtype == spec.dtype\n        assert (spec2.zero() == spec.zero()).all()\n        spec2 = spec.expand(*shape2_real)\n        assert spec2 is not spec\n        assert spec2.dtype == spec.dtype\n        assert (spec2.zero() == spec.zero()).all()\n        assert spec2.rand().shape == spec2.shape\n        assert spec2.zero().shape == spec2.shape\n        spec2 = spec.expand(*shape2_real)\n        assert spec2 is not spec\n        assert spec2.dtype == spec.dtype", "metadata": {"task_id": "pytorch_rl/110", "ground_truth": "        assert spec2.rand().shape == spec2.shape", "fpath_tuple": ["pytorch_rl", "test", "test_specs.py"], "context_start_lineno": 1354, "line_no": 1450}}
{"prompt": "import time\nimport signal\nimport pytest\nimport torch\nimport numpy as np\n\nfrom..base_env_manager import EnvState\nfrom..subprocess_env_manager import AsyncSubprocessEnvManager, SyncSubprocessEnvManager\n\n\nclass TestSubprocessEnvManager:\n\n    @pytest.mark.unittest\n    def test_naive(self, setup_async_manager_cfg, setup_model_type):\n        env_fn = setup_async_manager_cfg.pop('env_fn')\n        env_manager = AsyncSubprocessEnvManager(env_fn, setup_async_manager_cfg)\n        model = setup_model_type()\n\n        env_manager.seed([314 for _ in range(env_manager.env_num)])\n        env_manager.launch(reset_param={i: {'stat':'stat_test'} for i in range(env_manager.env_num)})\n        assert all([s == 314 for s in env_manager._seed])\n        assert all([s =='stat_test'] for s in env_manager._stat)\n        # Test basic\n        name = env_manager._name\n        for i in range(env_manager.env_num):\n            assert name[i] == 'name{}'.format(i)\n        assert len(name) == env_manager.env_num\n        assert all([isinstance(n, str) for n in name])\n        name = env_manager.name\n        assert len(name) == env_manager.env_num\n        assert all([isinstance(n, str) for n in name])\n        assert env_manager._max_retry == 5\n        assert env_manager._reset_timeout == 10\n        # Test arribute\n        with pytest.raises(AttributeError):\n            data = env_manager.xxx\n        env_manager._env_ref.user_defined()\n        with pytest.raises(RuntimeError):\n            env_manager.user_defined()\n        # Test step\n        env_count = [0 for _ in range(env_manager.env_num)]\n        data_count = 0\n        start_time = time.time()\n        while not env_manager.done:\n            obs = env_manager.ready_obs\n            print('obs', obs.keys(), env_manager._env_states)\n            action = model.forward(obs)\n            assert 1 <= len(action) <= len(obs)\n            print('act', action.keys())", "metadata": {"task_id": "opendilab_ACE/9", "ground_truth": "            timestep = env_manager.step(action)", "fpath_tuple": ["opendilab_ACE", "ding", "envs", "env_manager", "tests", "test_subprocess_env_manager.py"], "context_start_lineno": 0, "line_no": 49}}
{"prompt": "import os\nimport logging\n\nimport numpy as np\ntry:\n    import torch\n    from torch.utils.data import DataLoader, Dataset\nexcept ImportError:\n    torch = None\n    DataLoader = None\n    Dataset = None\n\nfrom federatedscope.core.trainers.enums import MODE, LIFECYCLE\nfrom federatedscope.core.trainers.trainer import Trainer\nfrom federatedscope.core.trainers.context import CtxVar\nfrom federatedscope.core.auxiliaries.optimizer_builder import get_optimizer\nfrom federatedscope.core.auxiliaries.scheduler_builder import get_scheduler\nfrom federatedscope.core.data import ClientData\nfrom federatedscope.core.data.wrap_dataset import WrapDataset\nfrom federatedscope.core.auxiliaries.dataloader_builder import get_dataloader\nfrom federatedscope.core.auxiliaries.ReIterator import ReIterator\nfrom federatedscope.core.auxiliaries.utils import param2tensor, \\\n    merge_param_dict\nfrom federatedscope.core.monitors.monitor import Monitor\n\nlogger = logging.getLogger(__name__)\n\n\nclass GeneralTorchTrainer(Trainer):\n    def get_model_para(self):", "metadata": {"task_id": "alibaba_FederatedScope/140", "ground_truth": "        return self._param_filter(\n            self.ctx.model.state_dict() if self.cfg.federate.\n            share_local_model else self.ctx.model.cpu().state_dict())", "fpath_tuple": ["alibaba_FederatedScope", "federatedscope", "core", "trainers", "torch_trainer.py"], "context_start_lineno": 0, "line_no": 30}}
{"prompt": "                calib_config=self.reg_calib_config_nodir_nodump,\n            )\n\n            # load state\n            prob_reg.load_state(checkpoint_path=tmp_dir)\n\n            # save state\n            prob_reg.save_state(checkpoint_path=tmp_dir)\n\n    def test_dryrun_class_swag(self):\n        with tempfile.TemporaryDirectory() as tmp_dir:\n            prob_class = ProbClassifier(\n                model=MyModel(self.class_output_dim),\n                posterior_approximator=SWAGPosteriorApproximator(rank=2),\n                output_calibrator=ClassificationTemperatureScaler(),\n            )\n            # no save dir, no dump\n            status = prob_class.train(\n                train_data_loader=self.class_train_data_loader,\n                calib_data_loader=self.class_val_data_loader,\n                val_data_loader=self.class_val_data_loader,\n                map_fit_config=self.class_fit_config_nodir_nodump,\n                fit_config=self.class_fit_config_nodir_nodump,\n                calib_config=self.class_calib_config_nodir_nodump,\n            )\n            sample = prob_class.posterior.sample()\n\n            # no save dir but dump\n            with self.assertRaises(ValueError):\n                status = prob_class.train(\n                    train_data_loader=self.class_train_data_loader,\n                    calib_data_loader=self.class_val_data_loader,\n                    val_data_loader=self.class_val_data_loader,\n                    map_fit_config=self.class_fit_config_nodir_nodump,\n                    fit_config=self.class_fit_config_nodir_dump,\n                    calib_config=self.class_calib_config_nodir_nodump,\n                )\n\n            # save dir, no dump\n            status = prob_class.train(\n                train_data_loader=self.class_train_data_loader,\n                calib_data_loader=self.class_val_data_loader,\n                val_data_loader=self.class_val_data_loader,\n                map_fit_config=self.class_fit_config_nodir_nodump,\n                fit_config=self.class_fit_config_dir_nodump(tmp_dir),\n                calib_config=self.class_calib_config_nodir_nodump,\n            )\n            sample = prob_class.posterior.sample()\n            prob_class.posterior.load_state(tmp_dir)\n\n            # save dir and dump\n            status = prob_class.train(\n                train_data_loader=self.class_train_data_loader,\n                calib_data_loader=self.class_val_data_loader,\n                val_data_loader=self.class_val_data_loader,\n                map_fit_config=self.class_fit_config_nodir_nodump,\n                fit_config=self.class_fit_config_dir_dump(tmp_dir),\n                calib_config=self.class_calib_config_nodir_nodump,\n            )\n            sample = prob_class.posterior.sample()\n            prob_class.posterior.load_state(tmp_dir)\n\n            # restore from swag\n            status = prob_class.train(\n                train_data_loader=self.class_train_data_loader,\n                calib_data_loader=self.class_val_data_loader,\n                val_data_loader=self.class_val_data_loader,\n                map_fit_config=self.class_fit_config_nodir_nodump,\n                fit_config=self.class_fit_config_restore(tmp_dir),\n                calib_config=self.class_calib_config_nodir_nodump,\n            )\n\n            # restore from map", "metadata": {"task_id": "awslabs_fortuna/49", "ground_truth": "            prob_class_map = ProbClassifier(\n                model=MyModel(self.class_output_dim),\n                posterior_approximator=MAPPosteriorApproximator(),\n                output_calibrator=ClassificationTemperatureScaler(),\n            )", "fpath_tuple": ["awslabs_fortuna", "tests", "fortuna", "prob_model", "test_train.py"], "context_start_lineno": 849, "line_no": 922}}
{"prompt": " set(obs.keys()).difference(ready_env_id)\n                ready_env_id = ready_env_id.union(set(list(new_available_env_id)[:remain_episode]))\n                remain_episode -= min(len(new_available_env_id), remain_episode)\n                obs = {env_id: obs[env_id] for env_id in ready_env_id}\n                # Policy forward.\n                self._obs_pool.update(obs)\n                if self._transform_obs:\n                    obs = to_tensor(obs, dtype=torch.float32)\n                obs = dicts_to_lists(obs)\n                policy_output = [p.forward(obs[i], **policy_kwargs) for i, p in enumerate(self._policy)]\n                self._policy_output_pool.update(policy_output)\n                # Interact with env.\n                actions = {}\n                for env_id in ready_env_id:\n                    actions[env_id] = []\n                    for output in policy_output:\n                        actions[env_id].append(output[env_id]['action'])\n                actions = to_ndarray(actions)\n                # temporally for viz\n                probs0 = torch.softmax(torch.stack([o['logit'] for o in policy_output[0].values()], 0), 1).mean(0)\n                probs1 = torch.softmax(torch.stack([o['logit'] for o in policy_output[1].values()], 0), 1).mean(0)\n                timesteps = self._env.step(actions)\n\n            # TODO(nyz) this duration may be inaccurate in async env\n            interaction_duration = self._timer.value / len(timesteps)\n\n            # TODO(nyz) vectorize this for loop\n            for env_id, timestep in timesteps.items():\n                self._env_info[env_id]['step'] += 1\n                self._total_envstep_count += 1\n                with self._timer:\n                    for policy_id, policy in enumerate(self._policy):\n                        policy_timestep_data = [d[policy_id] if not isinstance(d, bool) else d for d in timestep]\n                        policy_timestep = type(timestep)(*policy_timestep_data)\n                        transition = self._policy[policy_id].process_transition(\n                            self._obs_pool[env_id][policy_id], self._policy_output_pool[env_id][policy_id],\n                            policy_timestep\n                        )\n                        transition['collect_iter'] = train_iter\n                        self._traj_buffer[env_id][policy_id].append(transition)\n                        # prepare data\n                        if timestep.done:\n                            transitions = to_tensor_transitions(self._traj_buffer[env_id][policy_id])\n                            if self._cfg.get_train_sample:\n                                train_sample = self._policy[policy_id].get_train_sample(transitions)\n                                return_data[policy_id].extend(train_sample)\n                            else:\n                                return_data[policy_id].append(transitions)\n                            self._traj_buffer[env_id][policy_id].clear()\n\n                self._env_info[env_id]['time'] += self._timer.value + interaction_duration\n\n                # If env is done, record episode info and reset\n                if timestep.done:\n                    self._total_episode_count += 1\n                    info = {\n                       'reward0': timestep.info[0]['final_eval_reward'],\n                       'reward1': timestep.info[1]['final_eval_reward'],\n                        'time': self._env_info[env_id]['time'],\n                       'step': self._env_info[env_id]['step'],\n                        'probs0': probs0,\n                        'probs1': probs1,\n                    }\n                    collected_episode += 1\n                    self._episode_info.append(info)\n                    for i, p in enumerate(self._policy):", "metadata": {"task_id": "opendilab_ACE/2", "ground_truth": "                        p.reset([env_id])", "fpath_tuple": ["opendilab_ACE", "ding", "worker", "collector", "battle_episode_serial_collector.py"], "context_start_lineno": 223, "line_no": 289}}
{"prompt": "],\n                )\n            except ValueError:\n                # We are fine with either raising a ValueError or computing well the metric\n                # Being sure we raise the error would means making the dummy dataset bigger\n                # and the test longer...\n                pass\n            else:\n                self.assertDictEqual(expected_results, results[0])\n                self.assertDictEqual(expected_results, results[2])\n                self.assertIsNone(results[1])\n                self.assertIsNone(results[3])\n                del results\n\n            results = pool.map(\n                metric_add_and_compute,\n                [\n                    (2, 0, preds_0, refs_0, \"exp_0\", tmp_dir, 0),\n                    (2, 1, preds_1, refs_1, \"exp_0\", tmp_dir, 0),\n                    (2, 0, preds_0, refs_0, \"exp_1\", tmp_dir, 0),\n                    (2, 1, preds_1, refs_1, \"exp_1\", tmp_dir, 0),\n                ],\n            )\n            self.assertDictEqual(expected_results, results[0])\n            self.assertDictEqual(expected_results, results[2])\n            self.assertIsNone(results[1])\n            self.assertIsNone(results[3])\n            del results\n\n            # With keep_in_memory is not allowed\n            with self.assertRaises(ValueError):\n                DummyMetric(\n                    experiment_id=\"test_distributed_metrics_4\",\n                    keep_in_memory=True,\n                    num_process=2,\n                    process_id=0,\n                    cache_dir=tmp_dir,\n                )\n\n    def test_dummy_metric_pickle(self):\n        with tempfile.TemporaryDirectory() as tmp_dir:\n            tmp_file = os.path.join(tmp_dir, \"metric.pt\")\n            preds, refs = DummyMetric.predictions_and_references()\n            expected_results = DummyMetric.expected_results()\n\n            metric = DummyMetric(experiment_id=\"test_dummy_metric_pickle\")\n\n            with open(tmp_file, \"wb\") as f:\n                pickle.dump(metric, f)\n            del metric\n\n            with open(tmp_file, \"rb\") as f:\n                metric = pickle.load(f)\n            self.assertDictEqual(expected_results, metric.compute(predictions=preds, references=refs))\n            del metric\n\n    def test_input_numpy(self):\n        import numpy as np\n\n        preds, refs = DummyMetric.predictions_and_references()\n        expected_results = DummyMetric.expected_results()\n        preds, refs = np.array(preds), np.array(refs)\n\n        metric = DummyMetric(experiment_id=\"test_input_numpy\")\n        self.assertDictEqual(expected_results, metric.compute(predictions=preds, references=refs))\n        del metric\n\n        metric = DummyMetric(experiment_id=\"test_input_numpy\")\n        metric.add_batch(predictions=preds, references=refs)\n        self.assertDictEqual(expected_results, metric.compute())\n        del metric\n\n        metric = DummyMetric(experiment_id=\"test_input_numpy\")\n        for pred, ref in zip(preds, refs):\n            metric.add(prediction=pred, reference=ref)\n        self.assertDictEqual(expected_results, metric.compute())\n        del metric\n\n    @require_torch\n    def test_input_torch(self):\n        import torch\n\n        preds, refs = DummyMetric.predictions_and_references()\n        expected_results = DummyMetric.expected_results()\n        preds, refs = torch.tensor(preds), torch.tensor(refs)\n\n        metric = DummyMetric(experiment_id=\"test_input_torch\")", "metadata": {"task_id": "huggingface_evaluate/64", "ground_truth": "        self.assertDictEqual(expected_results, metric.compute(predictions=preds, references=refs))", "fpath_tuple": ["huggingface_evaluate", "tests", "test_metric.py"], "context_start_lineno": 378, "line_no": 465}}
{"prompt": "from typing import List, Optional\nimport gym\nimport copy\nimport numpy as np\nfrom ding.envs.common.env_element import EnvElementInfo\nfrom ding.torch_utils import to_ndarray\nfrom.base_env import BaseEnv, BaseEnvTimestep, BaseEnvInfo\n\n\nclass DingEnvWrapper(BaseEnv):\n\n    def __init__(self, env: gym.Env, cfg: dict = None) -> None:\n        self._cfg = cfg\n        if self._cfg is None:\n            self._cfg = dict()\n        self._env = env\n\n    # override\n    def reset(self) -> None:\n        if hasattr(self, '_seed') and hasattr(self, '_dynamic_seed') and self._dynamic_seed:\n            np_seed = 100 * np.random.randint(1, 1000)\n            self._env.seed(self._seed + np_seed)\n        elif hasattr(self, '_seed'):\n            self._env.seed(self._seed)", "metadata": {"task_id": "opendilab_ACE/173", "ground_truth": "        obs = self._env.reset()", "fpath_tuple": ["opendilab_ACE", "ding", "envs", "env", "ding_env_wrapper.py"], "context_start_lineno": 0, "line_no": 24}}
{"prompt": "_collate(list(data.values()))\n        if self._cuda:\n            data = to_device(data, self._device)\n        self._collect_model.eval()\n        with torch.no_grad():\n            (mu, sigma) = self._collect_model.forward(data, mode='compute_actor')['logit']\n            dist = Independent(Normal(mu, sigma), 1)\n            action = torch.tanh(dist.rsample())\n            output = {'logit': (mu, sigma), 'action': action}\n        if self._cuda:\n            output = to_device(output, 'cpu')\n        output = default_decollate(output)\n        return {i: d for i, d in zip(data_id, output)}\n\n    def _process_transition(self, obs: Any, model_output: dict, timestep: namedtuple) -> dict:\n        r\"\"\"\n        Overview:\n            Generate dict type transition data from inputs.\n        Arguments:\n            - obs (:obj:`Any`): Env observation\n            - model_output (:obj:`dict`): Output of collect model, including at least ['action']\n            - timestep (:obj:`namedtuple`): Output after env step, including at least ['obs','reward', 'done'] \\\n                (here 'obs' indicates obs after env step, i.e. next_obs).\n        Return:\n            - transition (:obj:`Dict[str, Any]`): Dict type transition data.\n        \"\"\"\n        transition = {\n            'obs': obs,\n            'next_obs': timestep.obs,\n            'action': model_output['action'],\n           'reward': timestep.reward,\n            'done': timestep.done,\n        }\n        return transition\n\n    def _get_train_sample(self, data: list) -> Union[None, List[Any]]:\n        return get_train_sample(data, self._unroll_len)\n\n    def _init_eval(self) -> None:\n        r\"\"\"\n        Overview:\n            Evaluate mode init method. Called by ``self.__init__``.\n            Init eval model. Unlike learn and collect model, eval model does not need noise.\n        \"\"\"\n        self._eval_model = model_wrap(self._model, wrapper_name='base')\n        self._eval_model.reset()\n\n    def _forward_eval(self, data: dict) -> dict:\n        r\"\"\"\n        Overview:\n            Forward function for eval mode, similar to ``self._forward_collect``.\n        Arguments:\n            - data (:obj:`dict`): Dict type data, including at least ['obs'].\n        Returns:\n            - output (:obj:`dict`): Dict type data, including at least inferred action according to input obs.\n        \"\"\"\n        data_id = list(data.keys())\n        data = default_collate(list(data.values()))\n        if self._cuda:\n            data = to_device(data, self._device)\n        self._eval_model.eval()\n        with torch.no_grad():\n            (mu, sigma) = self._eval_model.forward(data, mode='compute_actor')['logit']\n            action = torch.tanh(mu)  # deterministic_eval\n            output = {'action': action}\n        if self._cuda:\n            output = to_device(output, 'cpu')\n        output = default_decollate(output)\n        return {i: d for i, d in zip(data_id, output)}\n\n    def default_model(self) -> Tuple[str, List[str]]:\n        return 'qac', ['ding.model.template.qac']\n\n    def _monitor_vars_learn(self) -> List[str]:\n        r\"\"\"\n        Overview:\n            Return variables' name if variables are to used in monitor.\n        Returns:\n            - vars (:obj:`List[str]`): Variables' name list.\n        \"\"\"\n        twin_critic = ['twin_critic_loss'] if self._twin_critic else []\n        if self._auto_alpha:", "metadata": {"task_id": "opendilab_ACE/189", "ground_truth": "            return super()._monitor_vars_learn() + [", "fpath_tuple": ["opendilab_ACE", "ding", "policy", "cql.py"], "context_start_lineno": 558, "line_no": 640}}
{"prompt": "prompt, str) else len(prompt)\n        device = self._execution_device\n        # here `guidance_scale` is defined analog to the guidance weight `w` of equation (2)\n        # of the Imagen paper: https://arxiv.org/pdf/2205.11487.pdf. `guidance_scale = 1`\n        # corresponds to doing no classifier free guidance.\n        do_classifier_free_guidance = guidance_scale > 1.0\n\n        # 3. Encode input prompt\n        prompt_embeds = self._encode_prompt(\n            prompt,\n            device,\n            num_images_per_prompt,\n            do_classifier_free_guidance,\n            negative_prompt,\n            prompt_embeds=prompt_embeds,\n            negative_prompt_embeds=negative_prompt_embeds,\n        )\n\n        # 4. Prepare depth mask\n        depth_mask = self.prepare_depth_map(\n            image,\n            depth_map,\n            batch_size * num_images_per_prompt,\n            do_classifier_free_guidance,\n            prompt_embeds.dtype,\n            device,\n        )\n\n        # 5. Preprocess image\n        image = preprocess(image)\n\n        # 6. Set timesteps\n        self.scheduler.set_timesteps(num_inference_steps, device=device)\n        timesteps, num_inference_steps = self.get_timesteps(num_inference_steps, strength, device)\n        latent_timestep = timesteps[:1].repeat(batch_size * num_images_per_prompt)\n\n        # 7. Prepare latent variables\n        latents = self.prepare_latents(\n            image, latent_timestep, batch_size, num_images_per_prompt, prompt_embeds.dtype, device, generator\n        )\n\n        # 8. Prepare extra step kwargs. TODO: Logic should ideally just be moved out of the pipeline\n        extra_step_kwargs = self.prepare_extra_step_kwargs(generator, eta)\n\n        # 9. Denoising loop\n        num_warmup_steps = len(timesteps) - num_inference_steps * self.scheduler.order\n        with self.progress_bar(total=num_inference_steps) as progress_bar:\n            for i, t in enumerate(timesteps):\n                # expand the latents if we are doing classifier free guidance\n                latent_model_input = torch.cat([latents] * 2) if do_classifier_free_guidance else latents\n                latent_model_input = self.scheduler.scale_model_input(latent_model_input, t)\n                latent_model_input = torch.cat([latent_model_input, depth_mask], dim=1)\n\n                # predict the noise residual\n                noise_pred = self.unet(latent_model_input, t, encoder_hidden_states=prompt_embeds).sample\n\n                # perform guidance\n                if do_classifier_free_guidance:\n                    noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n                    noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n\n                # compute the previous noisy sample x_t -> x_t-1\n                latents = self.scheduler.step(noise_pred, t, latents, **extra_step_kwargs).prev_sample\n\n                # call the callback, if provided\n                if i == len(timesteps) - 1 or ((i + 1) > num_warmup_steps and (i + 1) % self.scheduler.order == 0):\n                    progress_bar.update()\n                    if callback is not None and i % callback_steps == 0:\n                        callback(i, t, latents)\n\n        # 10. Post-processing\n        image = self.decode_latents(latents)\n\n        # 11. Convert to PIL\n        if output_type == \"pil\":", "metadata": {"task_id": "huggingface_diffusers/114", "ground_truth": "            image = self.numpy_to_pil(image)", "fpath_tuple": ["huggingface_diffusers", "src", "diffusers", "pipelines", "stable_diffusion", "pipeline_stable_diffusion_depth2img.py"], "context_start_lineno": 599, "line_no": 674}}
{"prompt": "\n    step = 0\n    predict_fn = lambda *x: x[-1]\n\n\nclass FakeTrainer(TrainerABC):\n    def init_state(\n        self,\n        prob_model_state: JointState,\n        predict_fn: Callable[[jnp.ndarray], jnp.ndarray],\n        optimizer: GradientTransformation,\n        rng: Optional[PRNGKeyArray] = None,\n        **kwargs\n    ) -> TrainState:\n        return FakeTrainState()\n\n    def training_step(\n        self,\n        state: TrainState,\n        batch: Tuple[Union[jnp.ndarray, np.ndarray], Union[jnp.ndarray, np.ndarray]],\n        log_joint_prob: Callable[[Any], Union[float, Tuple[float, dict]]],\n        rng: jnp.ndarray,\n        n_data: int,\n        unravel: Optional[Callable[[any], PyTree]] = None,\n        kwargs: FrozenDict[str, Any] = FrozenDict(),\n    ) -> Tuple[TrainState, Dict[str, Any]]:\n        state.step += 1\n        return state, {\"loss\": 4.2, \"logging_kwargs\": None,}\n\n    def training_loss_step(\n        self,\n        log_joint_prob: Callable[[Any], Union[float, Tuple[float, dict]]],\n        params: Union[PyTree, jnp.ndarray, Tuple[jnp.ndarray,...]],\n        batch: Tuple[Union[jnp.ndarray, np.ndarray], Union[jnp.ndarray, np.ndarray]],\n        mutable: FrozenDict[str, FrozenDict],\n        rng: jnp.ndarray,\n        n_data: int,\n        unravel: Optional[Callable[[any], PyTree]] = None,\n        kwargs: FrozenDict[str, Any] = FrozenDict(),\n    ) -> Tuple[jnp.ndarray, Dict[str, Any]]:\n        pass\n\n    def validation_step(\n        self,\n        state: TrainState,\n        batch: Tuple[Union[jnp.ndarray, np.ndarray], Union[jnp.ndarray, np.ndarray]],\n        log_joint_prob: Callable[[Any], Union[float, Tuple[float, dict]]],\n        rng: jnp.ndarray,\n        n_data: int,\n        metrics: Optional[Tuple[str]] = None,\n        unravel: Optional[Callable[[any], PyTree]] = None,\n        kwargs: FrozenDict[str, Any] = FrozenDict(),\n    ) -> Dict[str, jnp.ndarray]:\n        return {\"val_loss\": jnp.array(0.1), \"val_accuracy\": jnp.array(0.5)}\n\n\nclass TestTrainer(unittest.TestCase):\n    def test_default_init(self):\n        trainer = FakeTrainer(predict_fn=lambda x: x)\n        self.assertFalse(trainer.is_early_stopping_active)\n\n    def test_training_step_end_missing_keys(self):\n        trainer = FakeTrainer(\n            predict_fn=lambda x: x,\n            disable_training_metrics_computation=False,\n            save_checkpoint_dir=\"tmp_dir\",\n            save_every_n_steps=1,\n            keep_top_n_checkpoints=3,\n        )\n        state = FakeTrainState()\n        batch = [[1, 2, 3], [0, 0, 1]]\n        with unittest.mock.patch.object(trainer, \"save_checkpoint\") as msc:\n            with self.assertRaises(KeyError):\n                trainer.training_step_end(1, state, {}, batch, (), {})\n        msc.assert_called_once_with(state, \"tmp_dir\", keep=3)\n\n        with unittest.mock.patch.object(trainer, \"save_checkpoint\") as msc:\n            with self.assertRaises(KeyError):", "metadata": {"task_id": "awslabs_fortuna/69", "ground_truth": "                trainer.training_step_end(1, state, {\"loss\": 1}, batch, (), {})", "fpath_tuple": ["awslabs_fortuna", "tests", "fortuna", "test_trainer.py"], "context_start_lineno": 21, "line_no": 99}}
{"prompt": "=\"test_dummy_metric\")\n        self.assertDictEqual(expected_results, metric.compute(predictions=preds, references=refs))\n        del metric\n\n        metric = DummyMetric(experiment_id=\"test_dummy_metric\")\n        metric.add_batch(predictions=preds, references=refs)\n        self.assertDictEqual(expected_results, metric.compute())\n        del metric\n\n        metric = DummyMetric(experiment_id=\"test_dummy_metric\")\n        for pred, ref in zip(preds, refs):\n            metric.add(prediction=pred, reference=ref)\n        self.assertDictEqual(expected_results, metric.compute())\n        del metric\n\n        # With keep_in_memory\n        metric = DummyMetric(keep_in_memory=True, experiment_id=\"test_dummy_metric\")\n        self.assertDictEqual(expected_results, metric.compute(predictions=preds, references=refs))\n        del metric\n\n        metric = DummyMetric(keep_in_memory=True, experiment_id=\"test_dummy_metric\")\n        metric.add_batch(predictions=preds, references=refs)\n        self.assertDictEqual(expected_results, metric.compute())\n        del metric\n\n        metric = DummyMetric(keep_in_memory=True, experiment_id=\"test_dummy_metric\")\n        for pred, ref in zip(preds, refs):\n            metric.add(prediction=pred, reference=ref)\n        self.assertDictEqual(expected_results, metric.compute())\n        del metric\n\n        metric = DummyMetric(keep_in_memory=True, experiment_id=\"test_dummy_metric\")\n        self.assertDictEqual({}, metric.compute(predictions=[], references=[]))\n        del metric\n\n        metric = DummyMetric(keep_in_memory=True, experiment_id=\"test_dummy_metric\")\n        with self.assertRaisesRegex(ValueError, \"Mismatch in the number\"):\n            metric.add_batch(predictions=[1, 2, 3], references=[1, 2, 3, 4])\n        del metric\n\n    def test_metric_with_cache_dir(self):\n        preds, refs = DummyMetric.predictions_and_references()\n        expected_results = DummyMetric.expected_results()\n\n        with tempfile.TemporaryDirectory() as tmp_dir:\n            metric = DummyMetric(experiment_id=\"test_dummy_metric\", cache_dir=tmp_dir)\n            self.assertDictEqual(expected_results, metric.compute(predictions=preds, references=refs))\n            del metric\n\n    def test_concurrent_metrics(self):\n        preds, refs = DummyMetric.predictions_and_references()\n        other_preds, other_refs = DummyMetric.other_predictions_and_references()\n        expected_results = DummyMetric.expected_results()\n        other_expected_results = DummyMetric.other_expected_results()\n\n        metric = DummyMetric(experiment_id=\"test_concurrent_metrics\")\n        other_metric = DummyMetric(\n            experiment_id=\"test_concurrent_metrics\",\n        )\n\n        self.assertDictEqual(expected_results, metric.compute(predictions=preds, references=refs))\n        self.assertDictEqual(\n            other_expected_results, other_metric.compute(predictions=other_preds, references=other_refs)\n        )\n        del metric, other_metric\n\n        metric = DummyMetric(\n            experiment_id=\"test_concurrent_metrics\",\n        )\n        other_metric = DummyMetric(\n            experiment_id=\"test_concurrent_metrics\",\n        )\n        metric.add_batch(predictions=preds, references=refs)", "metadata": {"task_id": "huggingface_evaluate/46", "ground_truth": "        other_metric.add_batch(predictions=other_preds, references=other_refs)", "fpath_tuple": ["huggingface_evaluate", "tests", "test_metric.py"], "context_start_lineno": 159, "line_no": 232}}
{"prompt": "ns: \":ns\"\n        value: \"ns-value\"\n      }\n      metadata {\n        key: \"key\"\n        value: \"value\"\n      }\n      metadata {\n        key: \"proto\"\n        proto {\n          [type.googleapis.com/vizier.Trial] {\n            id: '1'\n          }\n        }\n      }\n    \"\"\", proto)\n    from_proto = sc.from_proto(proto).metadata\n    self.assertCountEqual(sc.metadata, from_proto)\n\n  def testCreation(self):\n    sc = vz.StudyConfig()\n    sc.algorithm = vz.Algorithm.RANDOM_SEARCH\n    sc.metric_information.append(\n        vz.MetricInformation(\n            name='pr-auc', goal=vz.ObjectiveMetricGoal.MAXIMIZE\n        )\n    )\n    root = sc.search_space.root\n    root.add_float_param(\n        'learning_rate', 0.00001, 1.0, scale_type=vz.ScaleType.LINEAR\n    )\n    root.add_categorical_param('optimizer', ['adagrad', 'adam', 'experimental'])\n\n    sc.automated_stopping_config = (\n        vz.AutomatedStoppingConfig.default_stopping_spec()\n    )\n\n    # Test all proprties.\n    self.assertEqual(sc.algorithm, 'RANDOM_SEARCH')\n    expected = vz.MetricsConfig(\n        [\n            vz.MetricInformation(\n                name='pr-auc', goal=vz.ObjectiveMetricGoal.MAXIMIZE\n            )\n        ]\n    )\n    self.assertEqual(sc.metric_information, expected)\n    self.assertEqual(sc.single_objective_metric_name, 'pr-auc')\n    self.assertTrue(sc.is_single_objective)\n\n    expected = study_pb2.StudySpec(\n        algorithm='RANDOM_SEARCH',\n        metrics=[\n            study_pb2.StudySpec.MetricSpec(\n                metric_id='pr-auc',\n                goal=study_pb2.StudySpec.MetricSpec.GoalType.MAXIMIZE)\n        ],\n        default_stopping_spec=study_pb2.StudySpec.DefaultEarlyStoppingSpec(),\n        observation_noise=study_pb2.StudySpec.ObservationNoise\n       .OBSERVATION_NOISE_UNSPECIFIED,\n    )\n    expected.parameters.extend(self.pconfigs)\n    compare.assertProto2Equal(self, expected, sc.to_proto())\n\n  @absltest.skip('???')\n  def testTrialToDict(self):\n    py_study_config = vz.StudyConfig(\n        metric_information=[\n            vz.MetricInformation(\n                name='objective', goal=vz.ObjectiveMetricGoal.MAXIMIZE\n            )\n        ]\n    )\n    root = py_study_config.search_space.root\n    root.add_float_param('learning_rate', 0.01, 3.0)\n    root.add_int_param('units', 10, 1000, scale_type=vz.ScaleType.LOG)\n    root.add_discrete_param('batch_size', [8, 16, 32])\n    root.add_discrete_param(\n        'floating_point_param', [8., 16., 32.], auto_cast=False)\n    root.add_categorical_param('activation', ['tanh','relu'])\n    root.add_bool_param('synchronous')\n\n    trial_proto = study_pb2.Trial(id=str(1))\n    trial_proto.parameters.add(\n        parameter_id='activation', value=struct_pb2.Value(string_value='relu'))\n    trial_proto.parameters.add(\n        parameter_id='synchronus', value=struct_pb2.Value(string_value='true'))", "metadata": {"task_id": "google_vizier/21", "ground_truth": "    trial_proto.parameters.add(\n        parameter_id='batch_size', value=struct_pb2.Value(number_value=32))", "fpath_tuple": ["google_vizier", "vizier", "_src", "pyvizier", "oss", "study_config_test.py"], "context_start_lineno": 236, "line_no": 323}}
{"prompt": "output_dim),\n                posterior_approximator=MAPPosteriorApproximator(),\n                output_calibrator=RegressionTemperatureScaler(),\n            )\n            status = prob_reg_map.train(\n                train_data_loader=self.reg_train_data_loader,\n                calib_data_loader=self.reg_val_data_loader,\n                val_data_loader=self.reg_val_data_loader,\n                fit_config=self.reg_fit_config_dir_dump(tmp_dir),\n                calib_config=self.reg_calib_config_nodir_nodump,\n            )\n            prob_reg.train(\n                train_data_loader=self.reg_train_data_loader,\n                calib_data_loader=self.reg_val_data_loader,\n                val_data_loader=self.reg_val_data_loader,\n                fit_config=self.reg_fit_config_restore(tmp_dir),\n                calib_config=self.reg_calib_config_nodir_nodump,\n            )\n\n            # load state\n            prob_reg.load_state(checkpoint_path=tmp_dir)\n\n            # save state\n            prob_reg.save_state(checkpoint_path=tmp_dir)\n\n    def test_dryrun_class_advi(self):\n        with tempfile.TemporaryDirectory() as tmp_dir:\n            prob_class = ProbClassifier(\n                model=MyModel(self.class_output_dim),\n                posterior_approximator=ADVIPosteriorApproximator(),\n                output_calibrator=ClassificationTemperatureScaler(),\n            )\n            # no save dir, no dump\n            status = prob_class.train(\n                train_data_loader=self.class_train_data_loader,\n                calib_data_loader=self.class_val_data_loader,\n                val_data_loader=self.class_val_data_loader,\n                fit_config=self.class_fit_config_nodir_nodump,\n                calib_config=self.class_calib_config_nodir_nodump,\n            )\n            sample = prob_class.posterior.sample()\n\n            # no save dir but dump\n            with self.assertRaises(ValueError):\n                status = prob_class.train(\n                    train_data_loader=self.class_train_data_loader,\n                    calib_data_loader=self.class_val_data_loader,\n                    val_data_loader=self.class_val_data_loader,\n                    fit_config=self.class_fit_config_nodir_dump,\n                    calib_config=self.class_calib_config_nodir_nodump,\n                )\n\n            # save dir, no dump\n            status = prob_class.train(\n                train_data_loader=self.class_train_data_loader,\n                calib_data_loader=self.class_val_data_loader,\n                val_data_loader=self.class_val_data_loader,\n                fit_config=self.class_fit_config_dir_nodump(tmp_dir),\n                calib_config=self.class_calib_config_nodir_nodump,\n            )\n            sample = prob_class.posterior.sample()\n            prob_class.posterior.load_state(tmp_dir)\n\n            # save dir and dump\n            status = prob_class.train(\n                train_data_loader=self.class_train_data_loader,\n                calib_data_loader=self.class_val_data_loader,\n                val_data_loader=self.class_val_data_loader,\n                fit_config=self.class_fit_config_dir_dump(tmp_dir),\n                calib_config=self.class_calib_config_nodir_nodump,\n            )\n            sample = prob_class.posterior.sample()\n            prob_class.posterior.load_state(tmp_dir)\n\n            # restore from advi", "metadata": {"task_id": "awslabs_fortuna/148", "ground_truth": "            status = prob_class.train(\n                train_data_loader=self.class_train_data_loader,\n                calib_data_loader=self.class_val_data_loader,\n                val_data_loader=self.class_val_data_loader,\n                fit_config=self.class_fit_config_restore(tmp_dir),\n                calib_config=self.class_calib_config_nodir_nodump,\n            )", "fpath_tuple": ["awslabs_fortuna", "tests", "fortuna", "prob_model", "test_train.py"], "context_start_lineno": 339, "line_no": 414}}
{"prompt": "=utf-8\n# Copyright 2022 HuggingFace Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport tempfile\nimport unittest\n\nfrom diffusers import (\n    DDIMScheduler,\n    DDPMScheduler,\n    DPMSolverMultistepScheduler,\n    EulerAncestralDiscreteScheduler,\n    EulerDiscreteScheduler,\n    PNDMScheduler,\n    logging,\n)\nfrom diffusers.configuration_utils import ConfigMixin, register_to_config\nfrom diffusers.utils.testing_utils import CaptureLogger\n\n\nclass SampleObject(ConfigMixin):\n    config_name = \"config.json\"\n\n    @register_to_config\n    def __init__(\n        self,\n        a=2,\n        b=5,\n        c=(2, 5),\n        d=\"for diffusion\",\n        e=[1, 3],\n    ):\n        pass\n\n\nclass SampleObject2(ConfigMixin):\n    config_name = \"config.json\"\n\n    @register_to_config\n    def __init__(\n        self,\n        a=2,\n        b=5,\n        c=(2, 5),\n        d=\"for diffusion\",\n        f=[1, 3],\n    ):\n        pass\n\n\nclass SampleObject3(ConfigMixin):\n    config_name = \"config.json\"\n\n    @register_to_config\n    def __init__(\n        self,\n        a=2,\n        b=5,\n        c=(2, 5),\n        d=\"for diffusion\",\n        e=[1, 3],\n        f=[1, 3],\n    ):\n        pass\n\n\nclass ConfigTester(unittest.TestCase):\n    def test_load_not_from_mixin(self):\n        with self.assertRaises(ValueError):\n            ConfigMixin.load_config(\"dummy_path\")\n\n    def test_register_to_config(self):\n        obj = SampleObject()\n        config = obj.config\n        assert config[\"a\"] == 2\n        assert config[\"b\"] == 5\n        assert config[\"c\"] == (2, 5)\n        assert config[\"d\"] == \"for diffusion\"\n        assert config[\"e\"] == [1, 3]\n\n        # init ignore private arguments\n        obj = SampleObject(_name_or_path=\"lalala\")\n        config = obj.config\n        assert config[\"a\"] == 2\n        assert config[\"b\"] == 5\n        assert config[\"c\"] == (2, 5)\n        assert config[\"d\"] == \"for diffusion\"\n        assert config[\"e\"] == [1, 3]\n\n        # can override default\n        obj = SampleObject(c=6)\n        config = obj.config\n        assert config[\"a\"] == 2\n        assert config[\"b\"] == 5\n        assert config[\"c\"] == 6\n        assert config[\"d\"] == \"for diffusion\"\n        assert config[\"e\"] == [1, 3]\n\n        # can use positional arguments.\n        obj = SampleObject(1, c=6)\n        config = obj.config\n        assert config[\"a\"] == 1\n        assert config[\"b\"] == 5\n        assert config[\"c\"] == 6\n        assert config[\"d\"] == \"for diffusion\"\n        assert config[\"e\"] == [1, 3]\n\n    def test_save_load(self):\n        obj = SampleObject()\n        config = obj.config\n\n        assert config[\"a\"] == 2\n        assert config[\"b\"] == 5\n        assert config[\"c\"] == (2, 5)\n        assert config[\"d\"] == \"for diffusion\"\n        assert config[\"e\"] == [1, 3]\n\n        with tempfile.TemporaryDirectory() as tmpdirname:", "metadata": {"task_id": "huggingface_diffusers/135", "ground_truth": "            obj.save_config(tmpdirname)", "fpath_tuple": ["huggingface_diffusers", "tests", "test_config.py"], "context_start_lineno": 0, "line_no": 129}}
{"prompt": "ets is None and val_outputs is not None\n        ):\n            raise ValueError(\n                \"For validation, both `val_outputs` and `val_targets` must be passed as arguments.\"\n            )\n        trainer_cls = select_trainer_given_devices(\n            devices=calib_config.processor.devices,\n            BaseTrainer=CalibModelCalibrator,\n            JittedTrainer=JittedCalibModelCalibrator,\n            MultiDeviceTrainer=MultiDeviceCalibModelCalibrator,\n            disable_jit=calib_config.processor.disable_jit,\n        )\n\n        calibrator = trainer_cls(\n            calib_outputs=calib_outputs,\n            calib_targets=calib_targets,\n            val_outputs=val_outputs,\n            val_targets=val_targets,\n            predict_fn=self.prob_output_layer.predict,\n            uncertainty_fn=uncertainty_fn,\n            save_checkpoint_dir=calib_config.checkpointer.save_checkpoint_dir,\n            save_every_n_steps=calib_config.checkpointer.save_every_n_steps,\n            keep_top_n_checkpoints=calib_config.checkpointer.keep_top_n_checkpoints,\n            disable_training_metrics_computation=calib_config.monitor.disable_calibration_metrics_computation,\n            eval_every_n_epochs=calib_config.monitor.eval_every_n_epochs,\n            early_stopping_monitor=calib_config.monitor.early_stopping_monitor,\n            early_stopping_min_delta=calib_config.monitor.early_stopping_min_delta,\n            early_stopping_patience=calib_config.monitor.early_stopping_patience,\n        )\n\n        if calib_config.checkpointer.restore_checkpoint_path is None:\n            state = OutputCalibManagerState.init_from_dict(\n                d=FrozenDict(\n                    output_calibrator=self.output_calib_manager.init(\n                        output_dim=calib_outputs.shape[-1]\n                    )\n                ),\n            )\n            state = CalibState.init(\n                params=state.params,\n                mutable=state.mutable,\n                optimizer=calib_config.optimizer.method,\n            )\n        else:\n            state = self.restore_checkpoint(\n                calib_config.checkpointer.restore_checkpoint_path,\n                optimizer=calib_config.optimizer.method,\n            )\n\n        if calib_config.monitor.verbose:\n            logging.info(\"Start calibration.\")\n        state, status = calibrator.train(\n            rng=self.rng.get(),\n            state=state,\n            fun=self.predictive._log_joint_prob,\n            n_epochs=calib_config.optimizer.n_epochs,\n            metrics=calib_config.monitor.metrics,\n            verbose=calib_config.monitor.verbose,\n        )\n\n        self.predictive.state = TrainStateRepository(\n            calib_config.checkpointer.save_checkpoint_dir\n            if calib_config.checkpointer.dump_state is True\n            else None\n        )\n        self.predictive.state.put(\n            state, keep=calib_config.checkpointer.keep_top_n_checkpoints\n        )\n        return status\n\n    def load_state(self, checkpoint_path: Path) -> None:\n        \"\"\"\n        Load a calibration state from a checkpoint path.\n        The checkpoint must be compatible with the calibration model.\n\n        Parameters\n        ----------\n        checkpoint_path : Path\n            Path to a checkpoint file or directory to restore.\n        \"\"\"\n        try:\n            self.restore_checkpoint(checkpoint_path)\n        except ValueError:\n            raise ValueError(\n                f\"No checkpoint was found in `checkpoint_path={checkpoint_path}`.\"\n            )", "metadata": {"task_id": "awslabs_fortuna/32", "ground_truth": "        self.predictive.state = TrainStateRepository(checkpoint_dir=checkpoint_path)", "fpath_tuple": ["awslabs_fortuna", "fortuna", "calib_model", "base.py"], "context_start_lineno": 46, "line_no": 132}}
{"prompt": "_msgs.append(msg)\n                    # assume there is at least one client\n                    msg = cached_bc_msgs[0]\n                    self._handle_msg(msg, rcv=msg.receiver[cur_idx])\n                    cur_idx += 1\n                    if cur_idx >= len(msg.receiver):\n                        del cached_bc_msgs[0]\n                        cur_idx = 0\n                else:\n                    self._handle_msg(msg)\n            elif len(cached_bc_msgs) > 0:\n                msg = cached_bc_msgs[0]\n                self._handle_msg(msg, rcv=msg.receiver[cur_idx])\n                cur_idx += 1\n                if cur_idx >= len(msg.receiver):\n                    del cached_bc_msgs[0]\n                    cur_idx = 0\n            else:\n                # finished\n                break\n\n    def _run_simulation(self):\n        server_msg_cache = list()\n        while True:\n            if len(self.shared_comm_queue) > 0:\n                msg = self.shared_comm_queue.popleft()\n                if msg.receiver == [self.server_id]:\n                    # For the server, move the received message to a\n                    # cache for reordering the messages according to\n                    # the timestamps\n                    heapq.heappush(server_msg_cache, msg)\n                else:\n                    self._handle_msg(msg)\n            elif len(server_msg_cache) > 0:\n                msg = heapq.heappop(server_msg_cache)\n                if self.cfg.asyn.use and self.cfg.asyn.aggregator \\\n                        == 'time_up':\n                    # When the timestamp of the received message beyond\n                    # the deadline for the currency round, trigger the\n                    # time up event first and push the message back to\n                    # the cache\n                    if self.server.trigger_for_time_up(msg.timestamp):\n                        heapq.heappush(server_msg_cache, msg)\n                    else:\n                        self._handle_msg(msg)\n                else:\n                    self._handle_msg(msg)\n            else:\n                if self.cfg.asyn.use and self.cfg.asyn.aggregator \\\n                        == 'time_up':\n                    self.server.trigger_for_time_up()\n                    if len(self.shared_comm_queue) == 0 and \\\n                            len(server_msg_cache) == 0:\n                        break\n                else:\n                    # terminate when shared_comm_queue and\n                    # server_msg_cache are all empty\n                    break\n\n    def _setup_server(self, resource_info=None, client_resource_info=None):\n        \"\"\"\n        Set up the server\n        \"\"\"\n        self.server_id = 0\n        if self.mode =='standalone':\n            if self.server_id in self.data:\n                server_data = self.data[self.server_id]\n                model = get_model(self.cfg.model,\n                                  server_data,\n                                  backend=self.cfg.backend)\n            else:\n                server_data = None\n                data_representative = self.data[1]\n                model = get_model(\n                    self.cfg.model,\n                    data_representative,\n                    backend=self.cfg.backend\n                )  # get the model according to client's data if the server\n                # does not own data\n            kw = {\n               'shared_comm_queue': self.shared_comm_queue,\n               'resource_info': resource_info,\n                'client_resource_info': client_resource_info\n            }\n        elif self.mode == 'distributed':\n            server_data = self.data\n            model = get_model(self.cfg.model,\n                              server_data,\n                              backend=self.cfg.backend)\n            kw = self.server_address\n            kw.update({'resource_info': resource_info})\n        else:\n            raise ValueError('Mode {} is not provided'.format(\n                self.cfg.mode.type))\n\n        if self.server_class:", "metadata": {"task_id": "alibaba_FederatedScope/161", "ground_truth": "            self._server_device = self.gpu_manager.auto_choice()", "fpath_tuple": ["alibaba_FederatedScope", "federatedscope", "core", "fed_runner.py"], "context_start_lineno": 753, "line_no": 849}}
{"prompt": "from.registry import Registry\n\nPOLICY_REGISTRY = Registry()\nENV_REGISTRY = Registry()\nLEARNER_REGISTRY = Registry()\nCOMM_LEARNER_REGISTRY = Registry()\nSERIAL_COLLECTOR_REGISTRY = Registry()\nPARALLEL_COLLECTOR_REGISTRY = Registry()\nCOMM_COLLECTOR_REGISTRY = Registry()\nBUFFER_REGISTRY = Registry()\nCOMMANDER_REGISTRY = Registry()\nLEAGUE_REGISTRY = Registry()\nPLAYER_REGISTRY = Registry()\nMODEL_REGISTRY = Registry()\nENV_MANAGER_REGISTRY = Registry()", "metadata": {"task_id": "opendilab_ACE/70", "ground_truth": "REWARD_MODEL_REGISTRY = Registry()", "fpath_tuple": ["opendilab_ACE", "ding", "utils", "registry_factory.py"], "context_start_lineno": 0, "line_no": 15}}
{"prompt": "import json\nimport os\nimport shutil\nimport subprocess\nimport tempfile\nimport unittest\n\nimport numpy as np\nimport torch\nimport transformers\nfrom datasets import load_dataset\nfrom transformers import AutoFeatureExtractor, AutoModelForImageClassification, Trainer, TrainingArguments, pipeline\n\nfrom evaluate import evaluator, load\n\nfrom.utils import slow\n\n\nclass TestEvaluatorTrainerParity(unittest.TestCase):\n    def setUp(self):\n        self.dir_path = tempfile.mkdtemp(\"evaluator_trainer_parity_test\")\n\n        transformers_version = transformers.__version__\n        branch = \"\"\n        if not transformers_version.endswith(\".dev0\"):\n            branch = f\"--branch v{transformers_version}\"\n        subprocess.run(\n            f\"git clone --depth 3 --filter=blob:none --sparse {branch} https://github.com/huggingface/transformers\",\n            shell=True,\n            cwd=self.dir_path,\n        )\n\n    def tearDown(self):\n        shutil.rmtree(self.dir_path, ignore_errors=True)\n\n    def test_text_classification_parity(self):\n        model_name = \"philschmid/tiny-bert-sst2-distilled\"\n\n        subprocess.run(\n            \"git sparse-checkout set examples/pytorch/text-classification\",\n            shell=True,\n            cwd=os.path.join(self.dir_path, \"transformers\"),\n        )\n\n        subprocess.run(\n            f\"python examples/pytorch/text-classification/run_glue.py\"\n            f\" --model_name_or_path {model_name}\"\n            f\" --task_name sst2\"\n            f\" --do_eval\"\n            f\" --max_seq_length 9999999999\"  # rely on tokenizer.model_max_length for max_length\n            f\" --output_dir {os.path.join(self.dir_path, 'textclassification_sst2_transformers')}\"\n            f\" --max_eval_samples 80\",\n            shell=True,\n            cwd=os.path.join(self.dir_path, \"transformers\"),\n        )\n\n        with open(\n            f\"{os.path.join(self.dir_path, 'textclassification_sst2_transformers', 'eval_results.json')}\", \"r\"\n        ) as f:\n            transformers_results = json.load(f)\n\n        eval_dataset = load_dataset(\"glue\", \"sst2\", split=\"validation[:80]\")\n\n        pipe = pipeline(task=\"text-classification\", model=model_name, tokenizer=model_name)\n\n        task_evaluator = evaluator(task=\"text-classification\")", "metadata": {"task_id": "huggingface_evaluate/44", "ground_truth": "        evaluator_results = task_evaluator.compute(\n            model_or_pipeline=pipe,\n            data=eval_dataset,\n            metric=\"accuracy\",\n            input_column=\"sentence\",\n            label_column=\"label\",\n            label_mapping={\"negative\": 0, \"positive\": 1},\n            strategy=\"simple\",\n        )", "fpath_tuple": ["huggingface_evaluate", "tests", "test_trainer_evaluator_parity.py"], "context_start_lineno": 0, "line_no": 66}}
{"prompt": "import abc\nfrom typing import Callable, Dict, List, Optional, Tuple, Union\n\nimport jax\nimport jax.numpy as jnp\nimport jax.scipy as jsp\nfrom jax import jit, lax, pmap, random\nfrom jax._src.prng import PRNGKeyArray\nfrom jax.tree_util import tree_map\n\nfrom fortuna.data.loader import (DataLoader,\n                                 DeviceDimensionAugmentedDataLoader,\n                                 DeviceDimensionAugmentedInputsLoader,\n                                 InputsLoader, TargetsLoader)\nfrom fortuna.prob_model.posterior.base import Posterior\nfrom fortuna.typing import Array, Batch, CalibMutable, CalibParams\nfrom fortuna.utils.random import WithRNG\n\n\nclass Predictive(WithRNG):\n    def __init__(self, posterior: Posterior):\n        \"\"\"\n        Predictive distribution abstract class.\n\n        Parameters\n        ----------\n        posterior : Posterior\n             A posterior distribution object.\n        \"\"\"\n        self.likelihood = posterior.joint.likelihood\n        self.posterior = posterior\n\n    def log_prob(\n        self,\n        data_loader: DataLoader,\n        n_posterior_samples: int = 30,\n        rng: Optional[PRNGKeyArray] = None,\n        distribute: bool = True,\n        **kwargs\n    ) -> jnp.ndarray:\n\n        r\"\"\"\n        Estimate the predictive log-probability density function (a.k.a. log-pdf), that is\n\n       .. math::\n            \\log p(y|x, \\mathcal{D}),\n\n        where:\n         - :math:`x` is an observed input variable;\n         - :math:`y` is an observed target variable;\n         - :math:`\\mathcal{D}` is the observed training data set.\n\n        Parameters\n        ----------\n        data_loader : DataLoader\n            A data loader.\n        n_posterior_samples : int\n            Number of posterior samples to draw in order to approximate the predictive log-pdf.\n            that would be produced using the posterior distribution state.\n        rng : Optional[PRNGKeyArray]\n            A random number generator. If not passed, this will be taken from the attributes of this class.\n        distribute: bool\n            Whether to distribute computation over multiple devices, if available.\n\n        Returns\n        -------\n        jnp.ndarray\n            An estimate of the predictive log-pdf for each data point.\n        \"\"\"\n        if rng is None:\n            rng = self.rng.get()\n\n        return self._loop_fun_through_data_loader(\n            self._batched_log_prob,\n            data_loader,\n            n_posterior_samples,\n            rng,\n            distribute,\n            **kwargs\n        )\n\n    def _batched_log_prob(\n        self,\n        batch: Batch,\n        n_posterior_samples: int = 30,\n        rng: Optional[PRNGKeyArray] = None,\n        **kwargs\n    ) -> Union[jnp.ndarray, Tuple[jnp.ndarray, dict]]:\n        if rng is None:\n            rng = self.rng.get()\n        keys = random.split(rng, n_posterior_samples)\n\n        def _lik_log_batched_prob(key):", "metadata": {"task_id": "awslabs_fortuna/35", "ground_truth": "            sample = self.posterior.sample(inputs=batch[0], rng=key)", "fpath_tuple": ["awslabs_fortuna", "fortuna", "prob_model", "predictive", "base.py"], "context_start_lineno": 0, "line_no": 93}}
{"prompt": " 1, dtype=torch.bool, device=device)\n        mask = ~torch.zeros(batch, T, dtype=torch.bool, device=device)\n        td = TensorDict(\n            batch_size=(batch, T),\n            source={\n                \"observation\": obs.masked_fill_(~mask.unsqueeze(-1), 0.0),\n                \"next\": {\n                    \"observation\": next_obs.masked_fill_(~mask.unsqueeze(-1), 0.0)\n                },\n                \"done\": done,\n                \"collector\": {\"mask\": mask},\n                \"reward\": reward.masked_fill_(~mask.unsqueeze(-1), 0.0),\n                \"action\": action.masked_fill_(~mask.unsqueeze(-1), 0.0),\n            },\n            device=device,\n        )\n        return td\n\n    @pytest.mark.parametrize(\"delay_qvalue\", (True, False))\n    @pytest.mark.parametrize(\"num_qvalue\", [1, 2, 4, 8])\n    @pytest.mark.parametrize(\"device\", get_available_devices())\n    def test_redq(self, delay_qvalue, num_qvalue, device):\n\n        torch.manual_seed(self.seed)\n        td = self._create_mock_data_redq(device=device)\n\n        actor = self._create_mock_actor(device=device)\n        qvalue = self._create_mock_qvalue(device=device)\n\n        loss_fn = REDQLoss(\n            actor_network=actor,\n            qvalue_network=qvalue,\n            num_qvalue_nets=num_qvalue,\n            gamma=0.9,\n            loss_function=\"l2\",\n            delay_qvalue=delay_qvalue,\n        )\n\n        with _check_td_steady(td):\n            loss = loss_fn(td)\n\n        # check td is left untouched\n        assert loss_fn.priority_key in td.keys()\n\n        # check that losses are independent\n        for k in loss.keys():\n            if not k.startswith(\"loss\"):\n                continue\n            loss[k].sum().backward(retain_graph=True)\n            if k == \"loss_actor\":\n                assert all(\n                    (p.grad is None) or (p.grad == 0).all()\n                    for p in loss_fn.qvalue_network_params.values(\n                        include_nested=True, leaves_only=True\n                    )\n                )\n                assert not any(\n                    (p.grad is None) or (p.grad == 0).all()\n                    for p in loss_fn.actor_network_params.values(\n                        include_nested=True, leaves_only=True\n                    )\n                )\n            elif k == \"loss_qvalue\":\n                assert all(\n                    (p.grad is None) or (p.grad == 0).all()\n                    for p in loss_fn.actor_network_params.values(\n                        include_nested=True, leaves_only=True\n                    )\n                )\n                assert not any(\n                    (p.grad is None) or (p.grad == 0).all()\n                    for p in loss_fn.qvalue_network_params.values(\n                        include_nested=True, leaves_only=True\n                    )\n                )\n            elif k == \"loss_alpha\":\n                assert all(\n                    (p.grad is None) or (p.grad == 0).all()\n                    for p in loss_fn.actor_network_params.values(\n                        include_nested=True, leaves_only=True\n                    )\n                )\n                assert all(\n                    (p.grad is None) or (p.grad == 0).all()\n                    for p in loss_fn.qvalue_network_params.values(\n                        include_nested=True, leaves_only=True\n                    )\n                )\n            else:\n                raise NotImplementedError(k)\n            loss_fn.zero_grad()\n\n        sum([item for _, item in loss.items()]).backward()", "metadata": {"task_id": "pytorch_rl/142", "ground_truth": "        named_parameters = list(loss_fn.named_parameters())", "fpath_tuple": ["pytorch_rl", "test", "test_cost.py"], "context_start_lineno": 1435, "line_no": 1528}}
{"prompt": "umpy as np\ntry:\n    import torch\n    from torch.utils.data import DataLoader, Dataset\nexcept ImportError:\n    torch = None\n    DataLoader = None\n    Dataset = None\n\nfrom federatedscope.core.trainers.enums import MODE, LIFECYCLE\nfrom federatedscope.core.trainers.trainer import Trainer\nfrom federatedscope.core.trainers.context import CtxVar\nfrom federatedscope.core.auxiliaries.optimizer_builder import get_optimizer\nfrom federatedscope.core.auxiliaries.scheduler_builder import get_scheduler\nfrom federatedscope.core.data import ClientData\nfrom federatedscope.core.data.wrap_dataset import WrapDataset\nfrom federatedscope.core.auxiliaries.dataloader_builder import get_dataloader\nfrom federatedscope.core.auxiliaries.ReIterator import ReIterator\nfrom federatedscope.core.auxiliaries.utils import param2tensor, \\\n    merge_param_dict\nfrom federatedscope.core.monitors.monitor import Monitor\n\nlogger = logging.getLogger(__name__)\n\n\nclass GeneralTorchTrainer(Trainer):\n    def get_model_para(self):\n        return self._param_filter(\n            self.ctx.model.state_dict() if self.cfg.federate.\n            share_local_model else self.ctx.model.cpu().state_dict())\n\n    def setup_data(self, ctx):\n        \"\"\"\n        Initialization data by ``cfg``.\n        \"\"\"\n        if isinstance(ctx.data, ClientData):\n            ctx.data.setup(ctx.cfg)\n        else:\n            logger.warning(f'The data type should be `ClientData` to '\n                           f'enable new `config`, but got '\n                           f'{type(ctx.data)} instead.')\n\n    def parse_data(self, data):\n        \"\"\"Populate \"${split}_data\", \"${split}_loader\" and \"num_${\n        split}_data\" for different data splits\n        \"\"\"\n        init_dict = dict()\n        if isinstance(data, dict):\n            for split in data.keys():\n                if split not in ['train', 'val', 'test']:\n                    continue\n                init_dict[\"{}_data\".format(split)] = None\n                init_dict[\"{}_loader\".format(split)] = None\n                init_dict[\"num_{}_data\".format(split)] = 0\n                if data.get(split, None) is not None:\n                    if isinstance(data.get(split), Dataset):\n                        init_dict[\"{}_data\".format(split)] = data.get(split)\n                        init_dict[\"num_{}_data\".format(split)] = len(\n                            data.get(split))\n                    elif isinstance(data.get(split), DataLoader):\n                        init_dict[\"{}_loader\".format(split)] = data.get(split)\n                        init_dict[\"num_{}_data\".format(split)] = len(\n                            data.get(split).dataset)\n                    elif isinstance(data.get(split), dict):\n                        init_dict[\"{}_data\".format(split)] = data.get(split)\n                        init_dict[\"num_{}_data\".format(split)] = len(\n                            data.get(split)['y'])\n                    else:\n                        raise TypeError(\"Type {} is not supported.\".format(\n                            type(data.get(split))))\n        else:\n            raise TypeError(\"Type of data should be dict.\")\n        return init_dict\n\n    def update(self, model_parameters, strict=False):\n        \"\"\"\n            Called by the FL client to update the model parameters\n        Arguments:\n            model_parameters (dict): PyTorch Module object's state_dict.\n        \"\"\"\n        for key in model_parameters:\n            model_parameters[key] = param2tensor(model_parameters[key])\n        # Due to lazy load, we merge two state dict\n        merged_param = merge_param_dict(self.ctx.model.state_dict().copy(),", "metadata": {"task_id": "alibaba_FederatedScope/22", "ground_truth": "                                        self._param_filter(model_parameters))", "fpath_tuple": ["alibaba_FederatedScope", "federatedscope", "core", "trainers", "torch_trainer.py"], "context_start_lineno": 3, "line_no": 87}}
{"prompt": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import annotations\n\n\"\"\"Comparison test for algorithms using score analysis.\n\nEx: Typical Efficiency Convergence Test Example\n-----------------------------------------------\nbaseline_factory = benchmarks.BenchmarkStateFactory(...)\ncandidate_factory = benchmarks.BenchmarkStateFactory(...)\n\n# Run each algorithm for 100 Trials with 5 repeats each.\ncomparator = comparator_runner.EfficiencyComparisonTester(\n        num_trials=100, num_repeats=5)\ncomparator.assert_better_efficiency(candidate_factory, baseline_factory)\n\nNOTE: assert_converges_faster is a generic method name that conveys the general\nuse of the class.\n\"\"\"\n\nimport logging\n\nimport attr\nimport numpy as np\nfrom vizier import benchmarks\nfrom vizier import pyvizier as vz\nfrom vizier._src.algorithms.optimizers import vectorized_base as vb\nfrom vizier._src.benchmarks.analyzers import simple_regret_score\nfrom vizier.pyvizier import converters\n\n\nclass FailedComparisonTestError(Exception):\n  \"\"\"Exception raised for comparison test fails.\"\"\"\n\n\nclass FailedSimpleRegretConvergenceTestError(Exception):\n  \"\"\"Exception raised for simple-regret convergence test fails.\"\"\"\n\n\n@attr.define\nclass EfficiencyComparisonTester:\n  \"\"\"Comparison test between algorithms using analysis scores.\"\"\"\n  num_trials: int = attr.field(\n      default=1, validator=attr.validators.instance_of(int))\n  num_repeats: int = attr.field(\n      default=1, validator=attr.validators.instance_of(int))\n\n  def assert_better_efficiency(\n      self,\n      candidate_state_factory: benchmarks.BenchmarkStateFactory,\n      baseline_state_factory: benchmarks.BenchmarkStateFactory,\n      score_threshold: float = 0.0) -> None:\n    \"\"\"Asserts that candidate is better than baseline via log_eff_score.\"\"\"\n    # TODO: Consider making this more flexible with more runners\n    # And enable multimetric.\n    runner = benchmarks.BenchmarkRunner(\n        benchmark_subroutines=[benchmarks.GenerateAndEvaluate()],\n        num_repeats=self.num_trials)\n\n    baseline_curves = []\n    candidate_curves = []\n    for _ in range(self.num_repeats):\n      baseline_state = baseline_state_factory()\n      candidate_state = candidate_state_factory()", "metadata": {"task_id": "google_vizier/10", "ground_truth": "      baseline_statement = baseline_state.experimenter.problem_statement()", "fpath_tuple": ["google_vizier", "vizier", "_src", "algorithms", "testing", "comparator_runner.py"], "context_start_lineno": 0, "line_no": 77}}
{"prompt": "),\n            total_frames=-1,\n            max_frames_per_traj=100,\n            frames_per_batch=21,\n            init_random_frames=-1,\n            reset_at_each_iter=False,\n            split_trajs=True,\n            devices=[device, device],\n            passing_devices=[device, device],\n            update_at_each_batch=False,\n            init_with_lag=False,\n            exploration_mode=\"random\",\n        )\n        for i, data in enumerate(collector):\n            if i == 3:\n                assert data.shape[0] == 3\n                assert data.shape[1] == 7\n                break\n        collector.shutdown()\n        del env\n\n\n@pytest.mark.skipif(not _has_habitat, reason=\"habitat not installed\")\n@pytest.mark.parametrize(\"envname\", [\"HabitatRenderPick-v0\", \"HabitatPick-v0\"])\nclass TestHabitat:\n    def test_habitat(self, envname):\n        env = HabitatEnv(envname)\n        _ = env.rollout(3)\n        check_env_specs(env)\n\n    @pytest.mark.parametrize(\"from_pixels\", [True, False])\n    def test_habitat_render(self, envname, from_pixels):\n        env = HabitatEnv(envname, from_pixels=from_pixels)\n        rollout = env.rollout(3)\n        check_env_specs(env)\n        if from_pixels:\n            assert \"pixels\" in rollout.keys()\n\n\n@pytest.mark.skipif(not _has_jumanji, reason=\"jumanji not installed\")\n@pytest.mark.parametrize(\n    \"envname\",\n    [\n        \"TSP50-v0\",\n        \"Snake-6x6-v0\",\n    ],\n)\nclass TestJumanji:\n    def test_jumanji_seeding(self, envname):\n        final_seed = []\n        tdreset = []\n        tdrollout = []\n        for _ in range(2):\n            env = JumanjiEnv(envname)\n            torch.manual_seed(0)\n            np.random.seed(0)\n            final_seed.append(env.set_seed(0))\n            tdreset.append(env.reset())\n            tdrollout.append(env.rollout(max_steps=50))\n            env.close()\n            del env\n        assert final_seed[0] == final_seed[1]\n        assert_allclose_td(*tdreset)\n        assert_allclose_td(*tdrollout)\n\n    @pytest.mark.parametrize(\"batch_size\", [(), (5,), (5, 4)])\n    def test_jumanji_batch_size(self, envname, batch_size):\n        env = JumanjiEnv(envname, batch_size=batch_size)\n        env.set_seed(0)\n        tdreset = env.reset()\n        tdrollout = env.rollout(max_steps=50)\n        env.close()\n        del env\n        assert tdreset.batch_size == batch_size\n        assert tdrollout.batch_size[:-1] == batch_size\n\n    @pytest.mark.parametrize(\"batch_size\", [(), (5,), (5, 4)])\n    def test_jumanji_spec_rollout(self, envname, batch_size):\n        env = JumanjiEnv(envname, batch_size=batch_size)\n        env.set_seed(0)\n        check_env_specs(env)\n\n    @pytest.mark.parametrize(\"batch_size\", [(), (5,), (5, 4)])\n    def test_jumanji_consistency(self, envname, batch_size):\n        import jax\n        import jax.numpy as jnp\n        import numpy as onp\n        from torchrl.envs.libs.jax_utils import _tree_flatten", "metadata": {"task_id": "pytorch_rl/116", "ground_truth": "        env = JumanjiEnv(envname, batch_size=batch_size)", "fpath_tuple": ["pytorch_rl", "test", "test_libs.py"], "context_start_lineno": 320, "line_no": 409}}
{"prompt": "        dtype=np.float32,\n    )\n    np.testing.assert_allclose(expected, actual)\n    self.assertEqual(expected.dtype, actual.dtype)\n    self.assertEqual(\n        converter.output_spec,\n        core.NumpyArraySpec(\n            core.NumpyArraySpecType.ONEHOT_EMBEDDING,\n            np.float32,\n            (0, 1),\n            4,\n            'x1',\n            1,\n        ),\n        msg=repr(converter.output_spec),\n    )\n\n  def test_integer_discretes_into_onehot_empty(self):\n    converter = core.DefaultModelInputConverter(\n        pyvizier.ParameterConfig.factory('x1', feasible_values=(1, 2, 3)),\n        scale=True,\n        onehot_embed=True,\n        max_discrete_indices=10,\n    )\n\n    actual = converter.convert([])\n    expected = np.zeros([0, 4], dtype=np.float32)\n    np.testing.assert_allclose(expected, actual)\n    self.assertEqual(expected.dtype, actual.dtype)\n\n  def test_discretes_into_discretes(self):\n    converter = core.DefaultModelInputConverter(\n        pyvizier.ParameterConfig.factory('x1', feasible_values=(1.0, 2.0, 3.0)),\n        max_discrete_indices=10,\n    )\n\n    actual = converter.convert([\n        Trial(parameters={'x1': pyvizier.ParameterValue(1.0)}),\n        Trial(parameters={'x1': pyvizier.ParameterValue(2.0)}),\n        Trial(parameters={'x1': pyvizier.ParameterValue(-3.0)}),\n        Trial(parameters={'x1': pyvizier.ParameterValue('a')}),\n        Trial(),\n    ])\n    expected = np.asarray([[0], [1], [3], [3], [3]], dtype=np.float32)\n    np.testing.assert_equal(expected, actual)\n\n  def test_discretes_into_doubles(self):\n    converter = core.DefaultModelInputConverter(\n        pyvizier.ParameterConfig.factory('x1', feasible_values=(1.0, 2.0, 3.0)),\n        max_discrete_indices=1,\n    )\n\n    actual = converter.convert([\n        Trial(parameters={'x1': pyvizier.ParameterValue(1.0)}),\n        Trial(parameters={'x1': pyvizier.ParameterValue(2.0)}),\n        Trial(parameters={'x1': pyvizier.ParameterValue(-3.0)}),\n        Trial(parameters={'x1': pyvizier.ParameterValue('a')}),\n        Trial(),\n    ])\n    expected = np.asarray(\n        [[1.0], [2.0], [-3.0], [np.NaN], [np.NaN]], dtype=np.float32\n    )\n    np.testing.assert_equal(expected, actual)\n\n  def test_discretes_into_scaled_doubles(self):\n    converter = core.DefaultModelInputConverter(\n        pyvizier.ParameterConfig.factory('x1', feasible_values=(1.0, 2.0, 3.0)),\n        max_discrete_indices=1,\n        scale=True,\n    )\n\n    actual = converter.convert([\n        Trial(parameters={'x1': pyvizier.ParameterValue(1.0)}),\n        Trial(parameters={'x1': pyvizier.ParameterValue(2.0)}),\n        Trial(parameters={'x1': pyvizier.ParameterValue(-3.0)}),\n        Trial(parameters={'x1': pyvizier.ParameterValue('a')}),\n        Trial(),\n    ])", "metadata": {"task_id": "google_vizier/139", "ground_truth": "    expected = np.asarray(\n        [[0.0], [0.5], [-2.0], [np.NaN], [np.NaN]], dtype=np.float32\n    )", "fpath_tuple": ["google_vizier", "vizier", "pyvizier", "converters", "core_test.py"], "context_start_lineno": 998, "line_no": 1076}}
{"prompt": "x: int, image: Optional[TensorType[\"height\", \"width\", 2]] = None, max_size: Optional[int] = None\n    ) -> Dict:\n        \"\"\"Convert a camera to a json dictionary.\n\n        Args:\n            camera_idx: Index of the camera to convert.\n            image: An image in range [0, 1] that is encoded to a base64 string.\n            max_size: Max size to resize the image to if present.\n\n        Returns:\n            A JSON representation of the camera\n        \"\"\"\n        flattened = self.flatten()\n        json_ = {\n            \"type\": \"PinholeCamera\",\n            \"cx\": flattened[camera_idx].cx.item(),\n            \"cy\": flattened[camera_idx].cy.item(),\n            \"fx\": flattened[camera_idx].fx.item(),\n            \"fy\": flattened[camera_idx].fy.item(),\n            \"camera_to_world\": self.camera_to_worlds[camera_idx].tolist(),\n            \"camera_index\": camera_idx,\n            \"times\": flattened[camera_idx].times.item() if self.times is not None else None,\n        }\n        if image is not None:\n            image_uint8 = (image * 255).detach().type(torch.uint8)\n            if max_size is not None:\n                image_uint8 = image_uint8.permute(2, 0, 1)\n                image_uint8 = torchvision.transforms.functional.resize(image_uint8, max_size)  # type: ignore\n                image_uint8 = image_uint8.permute(1, 2, 0)\n            image_uint8 = image_uint8.cpu().numpy()\n            data = cv2.imencode(\".jpg\", image_uint8)[1].tobytes()\n            json_[\"image\"] = str(\"data:image/jpeg;base64,\" + base64.b64encode(data).decode(\"ascii\"))\n        return json_\n\n    def get_intrinsics_matrices(self) -> TensorType[\"num_cameras\":..., 3, 3]:\n        \"\"\"Returns the intrinsic matrices for each camera.\n\n        Returns:\n            Pinhole camera intrinsics matrices\n        \"\"\"\n        K = torch.zeros((*self.shape, 3, 3), dtype=torch.float32)\n        K[..., 0, 0] = self.fx.squeeze(-1)\n        K[..., 1, 1] = self.fy.squeeze(-1)\n        K[..., 0, 2] = self.cx.squeeze(-1)\n        K[..., 1, 2] = self.cy.squeeze(-1)\n        K[..., 2, 2] = 1.0\n        return K\n\n    def rescale_output_resolution(\n        self, scaling_factor: Union[TensorType[\"num_cameras\":...], TensorType[\"num_cameras\":..., 1], float, int]\n    ) -> None:\n        \"\"\"Rescale the output resolution of the cameras.\n\n        Args:\n            scaling_factor: Scaling factor to apply to the output resolution.\n        \"\"\"\n        if isinstance(scaling_factor, (float, int)):\n            scaling_factor = torch.tensor([scaling_factor]).to(self.device).broadcast_to((self.cx.shape))\n        elif isinstance(scaling_factor, torch.Tensor) and scaling_factor.shape == self.shape:\n            scaling_factor = scaling_factor.unsqueeze(-1)\n        elif isinstance(scaling_factor, torch.Tensor) and scaling_factor.shape == (*self.shape, 1):\n            pass\n        else:\n            raise ValueError(\n                f\"Scaling factor must be a float, int, or a tensor of shape {self.shape} or {(*self.shape, 1)}.\"\n            )\n\n        self.fx = self.fx * scaling_factor\n        self.fy = self.fy * scaling_factor\n        self.cx = self.cx * scaling_factor\n        self.cy = self.cy * scaling_factor", "metadata": {"task_id": "nerfstudio-project_nerfstudio/92", "ground_truth": "        self.height = (self.height * scaling_factor).to(torch.int64)", "fpath_tuple": ["nerfstudio-project_nerfstudio", "nerfstudio", "cameras", "cameras.py"], "context_start_lineno": 729, "line_no": 800}}
{"prompt": "[1][i : i + bs])\n            for i in range(0, len(reg_val_data[0]), bs)\n        ]\n        self.reg_train_data_loader = DataLoader.from_iterable(reg_train_data)\n        self.reg_val_data_loader = DataLoader.from_iterable(reg_val_data)\n\n        self.class_input_shape = (2,)\n        self.class_output_dim = 2\n        class_train_data = make_array_random_data(\n            n_data=100,\n            shape_inputs=self.class_input_shape,\n            output_dim=self.class_output_dim,\n            output_type=\"discrete\",\n        )\n        class_val_data = make_array_random_data(\n            n_data=10,\n            shape_inputs=self.class_input_shape,\n            output_dim=self.class_output_dim,\n            output_type=\"discrete\",\n        )\n        class_train_data = [\n            (class_train_data[0][i : i + bs], class_train_data[1][i : i + bs])\n            for i in range(0, len(class_train_data[0]), bs)\n        ]\n        class_val_data = [\n            (class_val_data[0][i : i + bs], class_val_data[1][i : i + bs])\n            for i in range(0, len(class_val_data[0]), bs)\n        ]\n        self.class_train_data_loader = DataLoader.from_iterable(class_train_data)\n        self.class_val_data_loader = DataLoader.from_iterable(class_val_data)\n\n        self.class_fit_config_nodir_nodump = FitConfig(\n            optimizer=FitOptimizer(n_epochs=3), monitor=FitMonitor(metrics=(accuracy,))\n        )\n        self.class_fit_config_nodir_dump = FitConfig(\n            optimizer=FitOptimizer(n_epochs=3),\n            monitor=FitMonitor(metrics=(accuracy,)),\n            checkpointer=FitCheckpointer(dump_state=True),\n        )\n        self.class_fit_config_dir_nodump = lambda save_dir: FitConfig(\n            optimizer=FitOptimizer(n_epochs=3),\n            monitor=FitMonitor(metrics=(accuracy,)),\n            checkpointer=FitCheckpointer(save_checkpoint_dir=save_dir),\n        )\n        self.class_fit_config_dir_dump = lambda save_dir: FitConfig(\n            optimizer=FitOptimizer(n_epochs=3),\n            monitor=FitMonitor(metrics=(accuracy,)),\n            checkpointer=FitCheckpointer(save_checkpoint_dir=save_dir, dump_state=True),\n        )\n        self.class_fit_config_restore = lambda restore_dir: FitConfig(\n            optimizer=FitOptimizer(n_epochs=3),\n            checkpointer=FitCheckpointer(restore_checkpoint_path=restore_dir),\n        )\n        self.reg_fit_config_nodir_nodump = FitConfig(\n            optimizer=FitOptimizer(n_epochs=3), monitor=FitMonitor(metrics=(rmse,))\n        )\n        self.reg_fit_config_nodir_dump = FitConfig(\n            optimizer=FitOptimizer(n_epochs=3),\n            monitor=FitMonitor(metrics=(rmse,)),\n            checkpointer=FitCheckpointer(dump_state=True),\n        )\n        self.reg_fit_config_dir_nodump = lambda save_dir: FitConfig(\n            optimizer=FitOptimizer(n_epochs=3),\n            monitor=FitMonitor(metrics=(rmse,)),\n            checkpointer=FitCheckpointer(save_checkpoint_dir=save_dir),\n        )\n        self.reg_fit_config_dir_dump = lambda save_dir: FitConfig(", "metadata": {"task_id": "awslabs_fortuna/142", "ground_truth": "            optimizer=FitOptimizer(n_epochs=3),", "fpath_tuple": ["awslabs_fortuna", "tests", "fortuna", "prob_model", "test_train.py"], "context_start_lineno": 66, "line_no": 133}}
{"prompt": "_and_metrics, expected_losses_and_metrics)\n\n    def test__get_mean_losses_and_metrics_ko(self):\n        trainer = FakeTrainer(\n            predict_fn=lambda x: x, disable_training_metrics_computation=False\n        )\n        losses_and_metrics = [\n            {\n                \"train_loss\": jnp.array(0.1),\n                \"val_loss\": jnp.array(0.2),\n                \"val_accuracy\": jnp.array(0.1),\n            },\n            {\"train_loss\": jnp.array(0.05), \"val_accuracy\": jnp.array(0.1)},\n            {\n                \"train_loss\": jnp.array(0.0),\n                \"val_loss\": jnp.array(0.22),\n                \"val_accuracy\": jnp.array(0.1),\n            },\n        ]\n        with self.assertRaises(ValueError):\n            _ = trainer._get_mean_losses_and_metrics(losses_and_metrics)\n\n    def test_training_epoch_end(self):\n        trainer = FakeTrainer(\n            predict_fn=lambda x: x, disable_training_metrics_computation=False\n        )\n\n        losses_and_metrics = [\n            {\n                \"train_loss\": jnp.array(0.1),\n                \"val_loss\": jnp.array(0.2),\n                \"val_accuracy\": jnp.array(0.1),\n            },\n            {\n                \"train_loss\": jnp.array(0.0),\n                \"val_loss\": jnp.array(0.22),\n                \"val_accuracy\": jnp.array(0.1),\n            },\n        ]\n        fake_out = {\n            \"train_loss\": jnp.array(0.0),\n            \"val_loss\": jnp.array(0.22),\n            \"val_accuracy\": jnp.array(0.1),\n        }\n\n        with unittest.mock.patch.object(\n            trainer, \"_get_mean_losses_and_metrics\", return_value=fake_out\n        ) as m:\n            observed = trainer.training_epoch_end(losses_and_metrics)\n        m.assert_called_once_with(losses_and_metrics)\n        self.assertDictEqual(observed, fake_out)\n\n    def test_validation_epoch_end(self):\n        trainer = FakeTrainer(\n            predict_fn=lambda x: x,\n            disable_training_metrics_computation=False,\n            save_checkpoint_dir=\"tmp_dir\",\n            save_every_n_steps=1,\n        )\n\n        losses_and_metrics = [\n            {\n                \"train_loss\": jnp.array(0.1),\n                \"val_loss\": jnp.array(0.2),\n                \"val_accuracy\": jnp.array(0.1),\n            },\n            {\n                \"train_loss\": jnp.array(0.0),\n                \"val_loss\": jnp.array(0.22),\n                \"val_accuracy\": jnp.array(0.1),\n            },\n        ]\n        fake_out = {\n            \"train_loss\": jnp.array(0.0),\n            \"val_loss\": jnp.array(0.22),\n            \"val_accuracy\": jnp.array(0.1),\n        }\n\n        with unittest.mock.patch.multiple(\n            trainer,\n            _get_mean_losses_and_metrics=mock.DEFAULT,\n            early_stopping_update=mock.DEFAULT,\n            save_checkpoint=mock.DEFAULT,\n        ) as m:\n            m[\"_get_mean_losses_and_metrics\"].return_value = fake_out\n            m[\"early_stopping_update\"].return_value = False\n            m[\"save_checkpoint\"].return_value = fake_out", "metadata": {"task_id": "awslabs_fortuna/105", "ground_truth": "            observed = trainer.validation_epoch_end(losses_and_metrics, None)", "fpath_tuple": ["awslabs_fortuna", "tests", "fortuna", "test_trainer.py"], "context_start_lineno": 225, "line_no": 313}}
{"prompt": "import pytest\nimport torch\n\nfrom ding.torch_utils.distribution import Pd, CategoricalPd, CategoricalPdPytorch\n\n\n@pytest.mark.unittest\nclass TestProbDistribution:\n\n    def test_Pd(self):\n        pd = Pd()\n        with pytest.raises(NotImplementedError):\n            pd.neglogp(torch.randn(5, ))\n        with pytest.raises(NotImplementedError):", "metadata": {"task_id": "opendilab_ACE/125", "ground_truth": "            pd.noise_mode()", "fpath_tuple": ["opendilab_ACE", "ding", "torch_utils", "tests", "test_distribution.py"], "context_start_lineno": 0, "line_no": 14}}
{"prompt": "train_batch\n            self.ctx.num_total_train_batch = \\\n                self.ctx.num_train_epoch * self.ctx.num_train_batch\n\n        for hook in hooks_set['on_fit_start']:\n            hook(self.ctx)\n\n        self._run_epoch(hooks_set)\n\n        for hook in hooks_set[\"on_fit_end\"]:\n            hook(self.ctx)\n\n        if raw_num_train_epoch is not None and raw_num_train_batch is not None:\n            self.ctx.num_train_epoch = raw_num_train_epoch\n            self.ctx.num_train_batch = raw_num_train_batch\n            self.ctx.num_train_batch_last_epoch = self.ctx.num_train_batch\n            self.ctx.num_total_train_batch = \\\n                self.ctx.num_train_epoch * self.ctx.num_train_batch\n\n        return self.ctx.num_samples\n\n    @lifecycle(LIFECYCLE.BATCH)\n    def _run_batch(self, hooks_set):\n        for batch_i in tqdm(range(\n                getattr(self.ctx, f\"num_{self.ctx.cur_split}_batch\", None)),\n                            disable=not (self._in_contrast_prepare\n                                         or self.ctx.cur_split == \"test\")):\n            self.ctx.cur_batch_i = CtxVar(batch_i, LIFECYCLE.BATCH)\n\n            for hook in hooks_set[\"on_batch_start\"]:\n                hook(self.ctx)\n\n            for hook in hooks_set[\"on_batch_forward\"]:\n                hook(self.ctx)\n\n            for hook in hooks_set[\"on_batch_backward\"]:\n                hook(self.ctx)\n\n            for hook in hooks_set[\"on_batch_end\"]:\n                hook(self.ctx)\n\n            # Break in the final epoch\n            if self.ctx.cur_mode in [MODE.TRAIN, MODE.FINETUNE] and \\\n                self.ctx.cur_epoch_i == getattr(\n                    self.ctx, f'num_{self.ctx.cur_mode}_epoch', None) - 1:\n                if batch_i >= \\\n                        getattr(self.ctx,\n                                f'num_{self.ctx.cur_mode}_batch_last_epoch',\n                                None) - 1:\n                    break\n\n    def _hook_on_fit_start_init(self, ctx):\n        ctx.model.to(ctx.device)\n\n        if ctx.cur_mode in [MODE.TRAIN, MODE.FINETUNE]:\n            ctx.optimizer = ctx.get(f'{ctx.cur_mode}_optimizer', None)\n            ctx.scheduler = ctx.get(f'{ctx.cur_mode}_scheduler', None)\n            if ctx.optimizer is None or ctx.scheduler is None:\n                ctx.optimizer, ctx.scheduler = \\\n                    self.setup_optimizer_and_scheduler(ctx)\n                setattr(ctx, f'{ctx.cur_mode}_optimizer', ctx.optimizer)\n                setattr(ctx, f'{ctx.cur_mode}_scheduler', ctx.scheduler)\n            if ctx.cfg.federate.atc_load_from and self.load_ckpt:\n                self._load_model(ctx)\n                self.load_ckpt = False\n\n        if ctx.cur_split == 'train' and ctx.cfg.federate.atc_load_from \\\n                and self.load_ckpt:\n            self._load_model(ctx)\n            self.load_ckpt = False\n\n        # prepare statistics\n        ctx.loss_agg = CtxVar(AverageMeter(), LIFECYCLE.ROUTINE)\n        ctx.loss_batch_total = CtxVar(0, LIFECYCLE.ROUTINE)\n        ctx.loss_regular_total = CtxVar(0, LIFECYCLE.ROUTINE)", "metadata": {"task_id": "alibaba_FederatedScope/103", "ground_truth": "        ctx.num_samples = CtxVar(0, LIFECYCLE.ROUTINE)", "fpath_tuple": ["alibaba_FederatedScope", "federatedscope", "nlp", "hetero_tasks", "trainer", "trainer.py"], "context_start_lineno": 243, "line_no": 318}}
{"prompt": "from typing import Optional\n\nimport flax.linen as nn\nimport jax.numpy as jnp\nimport numpy as np\n\nfrom fortuna.calib_model.base import CalibModel\nfrom fortuna.calib_model.calib_config.base import CalibConfig\nfrom fortuna.calib_model.predictive.classification import \\\n    ClassificationPredictive\nfrom fortuna.output_calibrator.classification import \\\n    ClassificationTemperatureScaler\nfrom fortuna.output_calibrator.output_calib_manager.base import \\\n    OutputCalibManager\nfrom fortuna.prob_output_layer.classification import \\\n    ClassificationProbOutputLayer\nfrom fortuna.typing import Array, Status\n\n\nclass CalibClassifier(CalibModel):\n    def __init__(\n        self,\n        output_calibrator: Optional[nn.Module] = ClassificationTemperatureScaler(),\n        seed: int = 0,\n    ) -> None:\n        r\"\"\"\n        A calibration classifier class.\n\n        Parameters\n        ----------\n        output_calibrator : Optional[nn.Module]\n            An output calibrator object. The default is temperature scaling for classification, which rescales the\n            logits with a scalar temperature parameter. Given outputs :math:`o`,\n            the output calibrator is described by a function :math:`g(\\phi, o)`,\n            where `phi` are calibration parameters.\n        seed: int\n            A random seed.\n\n        Attributes\n        ----------\n        output_calibrator : nn.Module\n            See `output_calibrator` in `Parameters`.\n        output_calib_manager : OutputCalibManager\n            It manages the forward pass of the output calibrator.\n        prob_output_layer : ClassificationProbOutputLayer\n            A probabilistic output payer.\n            It characterizes the distribution of the target variables given the outputs.\n        predictive : ClassificationPredictive\n            The predictive distribution.\n        \"\"\"\n        self.output_calibrator = output_calibrator\n        self.output_calib_manager = OutputCalibManager(\n            output_calibrator=output_calibrator\n        )\n        self.prob_output_layer = ClassificationProbOutputLayer()\n        self.predictive = ClassificationPredictive(\n            output_calib_manager=self.output_calib_manager,\n            prob_output_layer=self.prob_output_layer,\n        )\n        super().__init__(seed=seed)\n\n    def calibrate(\n        self,\n        calib_outputs: Array,\n        calib_targets: Array,\n        val_outputs: Optional[Array] = None,\n        val_targets: Optional[Array] = None,\n        calib_config: CalibConfig = CalibConfig(),\n    ) -> Status:\n        \"\"\"\n        Calibrate the model outputs.\n\n        Parameters\n        ----------\n        calib_outputs: Array\n            Calibration model outputs.\n        calib_targets: Array\n            Calibration target variables.\n        val_outputs: Optional[Array]\n            Validation model outputs.\n        val_targets: Optional[Array]\n            Validation target variables.\n        calib_config : CalibConfig\n            An object to configure the calibration.\n\n        Returns\n        -------\n        Status\n            A calibration status object. It provides information about the calibration.\n        \"\"\"\n        self._check_output_dim(calib_outputs, calib_targets)\n        if val_outputs is not None:\n            self._check_output_dim(val_outputs, val_targets)", "metadata": {"task_id": "awslabs_fortuna/138", "ground_truth": "        return super()._calibrate(\n            uncertainty_fn=calib_config.monitor.uncertainty_fn\n            if calib_config.monitor.uncertainty_fn is not None\n            else self.prob_output_layer.mean,\n            calib_outputs=calib_outputs,\n            calib_targets=calib_targets,\n            val_outputs=val_outputs,\n            val_targets=val_targets,\n            calib_config=calib_config,\n        )", "fpath_tuple": ["awslabs_fortuna", "fortuna", "calib_model", "classification.py"], "context_start_lineno": 0, "line_no": 93}}
{"prompt": "import time\nimport logging\nfrom easydict import EasyDict\nimport pytest\nfrom functools import partial\nimport copy\n\nfrom ding.worker import SampleSerialCollector, NaiveReplayBuffer\nfrom ding.envs import get_vec_env_setting, create_env_manager, AsyncSubprocessEnvManager, SyncSubprocessEnvManager,\\\n    BaseEnvManager\nfrom ding.utils import deep_merge_dicts, set_pkg_seed\n\nfrom ding.worker.collector.tests.speed_test.fake_policy import FakePolicy\nfrom ding.worker.collector.tests.speed_test.fake_env import FakeEnv, env_sum\nfrom ding.worker.collector.tests.speed_test.test_config import test_config\n\n# SLOW MODE:\n#   - Repeat 3 times; Collect 300 iterations;\n#   - Test on small + middle + big env\n#   - Test on base + asynnc_subprocess + sync_subprocess env manager\n#   - Test with reset_ratio = 1 and 5.\n# FAST MODE:\n#   - Only once (No repeat); Collect 50 iterations;\n#   - Test on small env\n#   - Test on sync_subprocess env manager\n#   - Test with reset_ratio = 1.\nFAST_MODE = True\n\n\ndef compare_test(cfg, out_str, seed):\n    global FAST_MODE\n    duration_list = []\n    repeat_times = 1 if FAST_MODE else 3\n    for i in range(repeat_times):\n        env_fn = FakeEnv\n        collector_env_cfg = copy.deepcopy(cfg.env)\n        collector_env_num = collector_env_cfg.pop('collector_env_num')\n        collector_env_cfg.pop('manager')\n        collector_env_fns = [partial(env_fn, cfg=collector_env_cfg) for _ in range(collector_env_num)]\n        if cfg.env.manager.type == 'base':\n            env_manager_type = BaseEnvManager\n        elif cfg.env.manager.type == 'async_subprocess':\n            env_manager_type = AsyncSubprocessEnvManager\n        elif cfg.env.manager.type =='subprocess':\n            env_manager_type = SyncSubprocessEnvManager\n        env_manager_cfg = deep_merge_dicts(env_manager_type.default_config(), cfg.env.manager)\n        collector_env = env_manager_type(collector_env_fns, env_manager_cfg)\n        collector_env.seed(seed)\n\n        # cfg.policy.collect.collector = deep_merge_dicts(\n        #     SampleSerialCollector.default_config(), cfg.policy.collect.collector)\n        policy = FakePolicy(cfg.policy)\n        collector_cfg = deep_merge_dicts(SampleSerialCollector.default_config(), cfg.policy.collect.collector)\n        collector = SampleSerialCollector(collector_cfg, collector_env, policy.collect_mode)\n        buffer_cfg = deep_merge_dicts(cfg.policy.other.replay_buffer, NaiveReplayBuffer.default_config())\n        replay_buffer = NaiveReplayBuffer(buffer_cfg)\n\n        start = time.time()\n        iters = 50 if FAST_MODE else 300\n        for iter in range(iters):\n            if iter % 50 == 0:\n                print('\\t', iter)\n            new_data = collector.collect(train_iter=iter)\n            replay_buffer.push(new_data, cur_collector_envstep=iter * 8)\n        duration_list.append(time.time() - start)\n        print('\\tduration: {}'.format(time.time() - start))", "metadata": {"task_id": "opendilab_ACE/129", "ground_truth": "        collector.close()", "fpath_tuple": ["opendilab_ACE", "ding", "worker", "collector", "tests", "speed_test", "test_collector_profile.py"], "context_start_lineno": 0, "line_no": 66}}
{"prompt": "parameters, strict=False):\n        \"\"\"\n            Called by the FL client to update the model parameters\n        Arguments:\n            model_parameters (dict): PyTorch Module object's state_dict.\n        \"\"\"\n        for key in model_parameters:\n            model_parameters[key] = param2tensor(model_parameters[key])\n        # Due to lazy load, we merge two state dict\n        merged_param = merge_param_dict(self.ctx.model.state_dict().copy(),\n                                        self._param_filter(model_parameters))\n        self.ctx.model.load_state_dict(merged_param, strict=strict)\n\n    def evaluate(self, target_data_split_name=\"test\"):\n        with torch.no_grad():\n            super(GeneralTorchTrainer, self).evaluate(target_data_split_name)\n\n        return self.ctx.eval_metrics\n\n    def register_default_hooks_train(self):\n        self.register_hook_in_train(self._hook_on_fit_start_init,\n                                    \"on_fit_start\")\n        self.register_hook_in_train(\n            self._hook_on_fit_start_calculate_model_size, \"on_fit_start\")\n        self.register_hook_in_train(self._hook_on_epoch_start,\n                                    \"on_epoch_start\")\n        self.register_hook_in_train(self._hook_on_batch_start_init,\n                                    \"on_batch_start\")\n        self.register_hook_in_train(self._hook_on_batch_forward,\n                                    \"on_batch_forward\")\n        self.register_hook_in_train(self._hook_on_batch_forward_regularizer,\n                                    \"on_batch_forward\")\n        self.register_hook_in_train(self._hook_on_batch_forward_flop_count,\n                                    \"on_batch_forward\")\n        self.register_hook_in_train(self._hook_on_batch_backward,\n                                    \"on_batch_backward\")\n        self.register_hook_in_train(self._hook_on_batch_end, \"on_batch_end\")\n        self.register_hook_in_train(self._hook_on_fit_end, \"on_fit_end\")\n\n    def register_default_hooks_ft(self):\n        self.register_hook_in_ft(self._hook_on_fit_start_init, \"on_fit_start\")\n        self.register_hook_in_ft(self._hook_on_fit_start_calculate_model_size,\n                                 \"on_fit_start\")\n        self.register_hook_in_ft(self._hook_on_epoch_start, \"on_epoch_start\")\n        self.register_hook_in_ft(self._hook_on_batch_start_init,\n                                 \"on_batch_start\")\n        self.register_hook_in_ft(self._hook_on_batch_forward,\n                                 \"on_batch_forward\")\n        self.register_hook_in_ft(self._hook_on_batch_forward_regularizer,\n                                 \"on_batch_forward\")\n        self.register_hook_in_ft(self._hook_on_batch_forward_flop_count,\n                                 \"on_batch_forward\")\n        self.register_hook_in_ft(self._hook_on_batch_backward,\n                                 \"on_batch_backward\")\n        self.register_hook_in_ft(self._hook_on_batch_end, \"on_batch_end\")\n        self.register_hook_in_ft(self._hook_on_fit_end, \"on_fit_end\")\n\n    def register_default_hooks_eval(self):\n        # test/val\n        self.register_hook_in_eval(self._hook_on_fit_start_init,\n                                   \"on_fit_start\")\n        self.register_hook_in_eval(self._hook_on_epoch_start, \"on_epoch_start\")", "metadata": {"task_id": "alibaba_FederatedScope/197", "ground_truth": "        self.register_hook_in_eval(self._hook_on_batch_start_init,\n                                   \"on_batch_start\")", "fpath_tuple": ["alibaba_FederatedScope", "federatedscope", "core", "trainers", "torch_trainer.py"], "context_start_lineno": 77, "line_no": 139}}
{"prompt": ": int\n  candidate_num_trials: int\n  baseline_suggestion_batch_size: int\n  candidate_suggestion_batch_size: int\n  baseline_num_repeats: int\n  candidate_num_repeats: int\n  alpha: float = attr.field(\n      validator=attr.validators.and_(\n          attr.validators.ge(0), attr.validators.le(0.1)),\n      default=0.05)\n  goal: vz.ObjectiveMetricGoal\n\n  def assert_optimizer_better_simple_regret(\n      self,\n      converter: converters.TrialToArrayConverter,\n      score_fn: vb.BatchArrayScoreFunction,\n      baseline_optimizer_factory: vb.VectorizedOptimizerFactory,\n      candidate_optimizer_factory: vb.VectorizedOptimizerFactory,\n  ) -> None:\n    \"\"\"Assert if candidate optimizer has better simple regret than the baseline.\n    \"\"\"\n    baseline_obj_values = []\n    candidate_obj_values = []\n\n    baseline_optimizer = baseline_optimizer_factory(\n        suggestion_batch_size=self.baseline_suggestion_batch_size,\n        max_evaluations=self.baseline_num_trials)\n\n    candidate_optimizer = candidate_optimizer_factory(\n        suggestion_batch_size=self.candidate_suggestion_batch_size,\n        max_evaluations=self.candidate_num_trials)\n\n    for i in range(self.baseline_num_repeats):\n      trial = baseline_optimizer.optimize(converter, score_fn, count=1, seed=i)\n      baseline_obj_values.append(\n          trial[0].final_measurement.metrics['acquisition'].value)\n\n    for i in range(self.candidate_num_repeats):\n      trial = candidate_optimizer.optimize(converter, score_fn, count=1, seed=i)\n      candidate_obj_values.append(\n          trial[0].final_measurement.metrics['acquisition'].value)\n\n    self._conclude_test(baseline_obj_values, candidate_obj_values)\n\n  def assert_benchmark_state_better_simple_regret(\n      self,\n      baseline_benchmark_state_factory: benchmarks.BenchmarkStateFactory,\n      candidate_benchmark_state_factory: benchmarks.BenchmarkStateFactory,\n  ) -> None:\n    \"\"\"Runs simple-regret convergence test for benchmark state.\"\"\"\n\n    def _run_one(benchmark_state_factory: benchmarks.BenchmarkStateFactory,\n                 num_trials: int, batch_size: int, seed: int) -> float:\n      \"\"\"Run one benchmark run and returns simple regret.\"\"\"\n      benchmark_state = benchmark_state_factory(seed=seed)\n      baseline_runner = benchmarks.BenchmarkRunner(\n          benchmark_subroutines=[benchmarks.GenerateAndEvaluate(batch_size)],\n          num_repeats=num_trials // batch_size)\n      baseline_runner.run(benchmark_state)\n      # Extract best metric\n      best_trial = benchmark_state.algorithm.supporter.GetBestTrials(count=1)[0]\n      metric_name = benchmark_state.experimenter.problem_statement(\n      ).single_objective_metric_name\n      return best_trial.final_measurement.metrics[metric_name].value\n\n    baseline_obj_values = []\n    candidate_obj_values = []\n\n    for idx in range(self.baseline_num_repeats):\n      baseline_obj_values.append(\n          _run_one(\n              benchmark_state_factory=baseline_benchmark_state_factory,\n              num_trials=self.baseline_num_trials,\n              batch_size=self.baseline_suggestion_batch_size,\n              seed=idx))\n\n    for idx in range(self.candidate_num_repeats):", "metadata": {"task_id": "google_vizier/77", "ground_truth": "      candidate_obj_values.append(\n          _run_one(\n              benchmark_state_factory=candidate_benchmark_state_factory,\n              num_trials=self.candidate_num_trials,\n              batch_size=self.candidate_suggestion_batch_size,\n              seed=idx))", "fpath_tuple": ["google_vizier", "vizier", "_src", "algorithms", "testing", "comparator_runner.py"], "context_start_lineno": 125, "line_no": 202}}
{"prompt": " len(\n                            data.get(split).dataset)\n                    elif isinstance(data.get(split), dict):\n                        init_dict[\"{}_data\".format(split)] = data.get(split)\n                        init_dict[\"num_{}_data\".format(split)] = len(\n                            data.get(split)['y'])\n                    else:\n                        raise TypeError(\"Type {} is not supported.\".format(\n                            type(data.get(split))))\n        else:\n            raise TypeError(\"Type of data should be dict.\")\n        return init_dict\n\n    def update(self, model_parameters, strict=False):\n        \"\"\"\n            Called by the FL client to update the model parameters\n        Arguments:\n            model_parameters (dict): PyTorch Module object's state_dict.\n        \"\"\"\n        for key in model_parameters:\n            model_parameters[key] = param2tensor(model_parameters[key])\n        # Due to lazy load, we merge two state dict\n        merged_param = merge_param_dict(self.ctx.model.state_dict().copy(),\n                                        self._param_filter(model_parameters))\n        self.ctx.model.load_state_dict(merged_param, strict=strict)\n\n    def evaluate(self, target_data_split_name=\"test\"):\n        with torch.no_grad():\n            super(GeneralTorchTrainer, self).evaluate(target_data_split_name)\n\n        return self.ctx.eval_metrics\n\n    def register_default_hooks_train(self):\n        self.register_hook_in_train(self._hook_on_fit_start_init,\n                                    \"on_fit_start\")\n        self.register_hook_in_train(\n            self._hook_on_fit_start_calculate_model_size, \"on_fit_start\")\n        self.register_hook_in_train(self._hook_on_epoch_start,\n                                    \"on_epoch_start\")\n        self.register_hook_in_train(self._hook_on_batch_start_init,\n                                    \"on_batch_start\")\n        self.register_hook_in_train(self._hook_on_batch_forward,\n                                    \"on_batch_forward\")\n        self.register_hook_in_train(self._hook_on_batch_forward_regularizer,\n                                    \"on_batch_forward\")\n        self.register_hook_in_train(self._hook_on_batch_forward_flop_count,\n                                    \"on_batch_forward\")\n        self.register_hook_in_train(self._hook_on_batch_backward,\n                                    \"on_batch_backward\")\n        self.register_hook_in_train(self._hook_on_batch_end, \"on_batch_end\")\n        self.register_hook_in_train(self._hook_on_fit_end, \"on_fit_end\")\n\n    def register_default_hooks_ft(self):\n        self.register_hook_in_ft(self._hook_on_fit_start_init, \"on_fit_start\")\n        self.register_hook_in_ft(self._hook_on_fit_start_calculate_model_size,\n                                 \"on_fit_start\")\n        self.register_hook_in_ft(self._hook_on_epoch_start, \"on_epoch_start\")\n        self.register_hook_in_ft(self._hook_on_batch_start_init,\n                                 \"on_batch_start\")\n        self.register_hook_in_ft(self._hook_on_batch_forward,\n                                 \"on_batch_forward\")\n        self.register_hook_in_ft(self._hook_on_batch_forward_regularizer,\n                                 \"on_batch_forward\")\n        self.register_hook_in_ft(self._hook_on_batch_forward_flop_count,\n                                 \"on_batch_forward\")\n        self.register_hook_in_ft(self._hook_on_batch_backward,\n                                 \"on_batch_backward\")", "metadata": {"task_id": "alibaba_FederatedScope/195", "ground_truth": "        self.register_hook_in_ft(self._hook_on_batch_end, \"on_batch_end\")", "fpath_tuple": ["alibaba_FederatedScope", "federatedscope", "core", "trainers", "torch_trainer.py"], "context_start_lineno": 64, "line_no": 131}}
{"prompt": " * Fpen(arr)\n  return float(term1 + term2 + term3)\n\n\ndef LinearSlope(arr: np.ndarray, seed: int = 0) -> float:\n  \"\"\"Implementation for BBOB LinearSlope function.\"\"\"\n  dim = len(arr)\n  arr.shape = (dim, 1)\n  r = _R(dim, seed, b\"R\")\n  z = np.matmul(r, arr)\n  result = 0.0\n  for i in range(dim):\n    s = 10**(i / float(dim - 1) if dim > 1 else 1)\n    z_opt = 5 * np.sum(np.abs(r[i, :]))\n    result += float(s * (z_opt - z[i]))\n  return result\n\n\ndef AttractiveSector(arr: np.ndarray, seed: int = 0) -> float:\n  \"\"\"Implementation for BBOB Attractive Sector function.\"\"\"\n  dim = len(arr)\n  arr.shape = (dim, 1)\n  x_opt = np.array([1 if i % 2 == 0 else -1 for i in range(dim)])\n  x_opt.shape = (dim, 1)\n  z_vec = np.matmul(_R(dim, seed, b\"R\"), arr - x_opt)\n  z_vec = np.matmul(LambdaAlpha(10.0, dim), z_vec)\n  z_vec = np.matmul(_R(dim, seed, b\"Q\"), z_vec)\n\n  result = 0.0\n  for i in range(dim):\n    z = z_vec[i, 0]\n    s = 100 if z * x_opt[i] > 0 else 1\n    result += (s * z)**2\n\n  return math.pow(Tosz(result), 0.9)\n\n\ndef StepEllipsoidal(arr: np.ndarray, seed: int = 0) -> float:\n  \"\"\"Implementation for BBOB StepEllipsoidal function.\"\"\"\n  dim = len(arr)\n  arr.shape = (dim, 1)\n  z_hat = np.matmul(_R(dim, seed, b\"R\"), arr)\n  z_hat = np.matmul(LambdaAlpha(10.0, dim), z_hat)\n  z_tilde = np.array([\n      math.floor(0.5 + z) if (z > 0.5) else (math.floor(0.5 + 10 * z) / 10)\n      for z in z_hat.flat\n  ])\n  z_tilde = np.matmul(_R(dim, seed, b\"Q\"), z_tilde)\n  s = 0.0\n  for i, val in enumerate(z_tilde):\n    exponent = 2.0 * float(i) / (dim - 1.0) if dim > 1.0 else 2.0\n    s += 10.0**exponent * val**2\n  value = max(abs(z_hat[0, 0]) / 1000, s)\n  return 0.1 * value + Fpen(arr)\n\n\ndef RosenbrockRotated(arr: np.ndarray, seed: int = 0) -> float:\n  \"\"\"Implementation for BBOB RosenbrockRotated function.\"\"\"\n  dim = len(arr)\n  r_x = np.matmul(_R(dim, seed, b\"R\"), arr)\n  z = max(1.0, (dim**0.5) / 8.0) * r_x + 0.5 * np.ones((dim,))\n  return float(\n      sum([\n          100.0 * (z[i]**2 - z[i + 1])**2 + (z[i] - 1)**2\n          for i in range(dim - 1)\n      ]))\n\n\ndef Ellipsoidal(arr: np.ndarray, seed: int = 0) -> float:\n  \"\"\"Implementation for BBOB Ellipsoidal function.\"\"\"\n  del seed\n  dim = len(arr)\n  arr.shape = (dim, 1)\n  z_vec = ArrayMap(arr, Tosz)\n  s = 0.0", "metadata": {"task_id": "google_vizier/17", "ground_truth": "  for i in range(dim):", "fpath_tuple": ["google_vizier", "vizier", "_src", "benchmarks", "experimenters", "synthetic", "bbob.py"], "context_start_lineno": 226, "line_no": 301}}
{"prompt": " = task_evaluator.words_to_offsets(words, join_by)\n\n        self.assertListEqual([(0, 3), (5, 6), (8, 8), (10, 13), (15, 15)], offsets)\n\n        words = [\"\u65e5\", \"\u672c\", \"\u8a9e\", \"\u306f\u306a\u305b\u308b\u306e?\"]\n        join_by = \"\"\n\n        offsets = task_evaluator.words_to_offsets(words, join_by)\n\n        self.assertListEqual([(0, 0), (1, 1), (2, 2), (3, 8)], offsets)\n\n    def test_predictions_processor(self):\n        task_evaluator = evaluator(\"token-classification\")\n        join_by = \" \"\n        words = [[\"New\", \"York\", \"a\", \"nice\", \"City\", \".\"]]\n\n        # aligned start and words\n        predictions = [\n            [\n                {\"start\": 0, \"entity\": \"B-LOC\"},\n                {\"start\": 2, \"entity\": \"I-LOC\"},\n                {\"start\": 4, \"entity\": \"I-LOC\"},\n                {\"start\": 9, \"entity\": \"O\"},\n                {\"start\": 11, \"entity\": \"O\"},\n                {\"start\": 16, \"entity\": \"B-LOC\"},\n                {\"start\": 21, \"entity\": \"O\"},\n            ]\n        ]\n        predictions = task_evaluator.predictions_processor(predictions, words, join_by)\n        self.assertListEqual(predictions[\"predictions\"][0], [\"B-LOC\", \"I-LOC\", \"O\", \"O\", \"B-LOC\", \"O\"])\n\n        # non-aligned start and words\n        predictions = [\n            [\n                {\"start\": 0, \"entity\": \"B-LOC\"},\n                {\"start\": 2, \"entity\": \"I-LOC\"},\n                {\"start\": 9, \"entity\": \"O\"},\n                {\"start\": 11, \"entity\": \"O\"},\n                {\"start\": 16, \"entity\": \"B-LOC\"},\n                {\"start\": 21, \"entity\": \"O\"},\n            ]\n        ]\n        predictions = task_evaluator.predictions_processor(predictions, words, join_by)\n        self.assertListEqual(predictions[\"predictions\"][0], [\"B-LOC\", \"O\", \"O\", \"O\", \"B-LOC\", \"O\"])\n\n        # non-aligned start and words\n        predictions = [\n            [\n                {\"start\": 0, \"entity\": \"B-LOC\"},\n                {\"start\": 6, \"entity\": \"I-LOC\"},\n                {\"start\": 9, \"entity\": \"O\"},\n                {\"start\": 11, \"entity\": \"O\"},\n                {\"start\": 16, \"entity\": \"B-LOC\"},\n                {\"start\": 21, \"entity\": \"O\"},\n            ]\n        ]\n        predictions = task_evaluator.predictions_processor(predictions, words, join_by)\n        self.assertListEqual(predictions[\"predictions\"][0], [\"B-LOC\", \"O\", \"O\", \"O\", \"B-LOC\", \"O\"])\n\n        # non-aligned start and words\n        predictions = [\n            [\n                {\"start\": 0, \"entity\": \"B-LOC\"},\n                {\"start\": 9, \"entity\": \"O\"},\n                {\"start\": 11, \"entity\": \"O\"},\n                {\"start\": 16, \"entity\": \"B-LOC\"},\n                {\"start\": 21, \"entity\": \"O\"},\n            ]\n        ]\n        predictions = task_evaluator.predictions_processor(predictions, words, join_by)\n        self.assertListEqual(predictions[\"predictions\"][0], [\"B-LOC\", \"O\", \"O\", \"O\", \"B-LOC\", \"O\"])\n\n\nclass TestTextGenerationEvaluator(TestCase):\n    def setUp(self):\n        self.data = Dataset.from_dict({\"text\": [\"Lorem ipsum\"]})\n        self.pipe = DummyTextGenerationPipeline(num_return_sequences=4)\n        self.evaluator = evaluator(\"text-generation\")\n\n    def test_class_init(self):", "metadata": {"task_id": "huggingface_evaluate/141", "ground_truth": "        evaluator = TextGenerationEvaluator()", "fpath_tuple": ["huggingface_evaluate", "tests", "test_evaluator.py"], "context_start_lineno": 780, "line_no": 860}}
{"prompt": "# Copyright 2022 The Nerfstudio Team. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Classic NeRF field\"\"\"\n\n\nfrom typing import Dict, Optional, Tuple\n\nimport torch\nfrom torch import nn\nfrom torchtyping import TensorType\n\nfrom nerfstudio.cameras.rays import RaySamples\nfrom nerfstudio.field_components.encodings import Encoding, Identity\nfrom nerfstudio.field_components.field_heads import (\n    DensityFieldHead,\n    FieldHead,\n    FieldHeadNames,\n    RGBFieldHead,\n)\nfrom nerfstudio.field_components.mlp import MLP\nfrom nerfstudio.field_components.spatial_distortions import SpatialDistortion\nfrom nerfstudio.fields.base_field import Field\n\n\nclass NeRFField(Field):\n    \"\"\"NeRF Field\n\n    Args:\n        position_encoding: Position encoder.\n        direction_encoding: Direction encoder.\n        base_mlp_num_layers: Number of layers for base MLP.\n        base_mlp_layer_width: Width of base MLP layers.\n        head_mlp_num_layers: Number of layer for output head MLP.\n        head_mlp_layer_width: Width of output head MLP layers.\n        skip_connections: Where to add skip connection in base MLP.\n        use_integrated_encoding: Used integrated samples as encoding input.\n        spatial_distortion: Spatial distortion.\n    \"\"\"\n\n    def __init__(\n        self,\n        position_encoding: Encoding = Identity(in_dim=3),\n        direction_encoding: Encoding = Identity(in_dim=3),\n        base_mlp_num_layers: int = 8,\n        base_mlp_layer_width: int = 256,\n        head_mlp_num_layers: int = 2,\n        head_mlp_layer_width: int = 128,\n        skip_connections: Tuple[int] = (4,),\n        field_heads: Tuple[FieldHead] = (RGBFieldHead(),),\n        use_integrated_encoding: bool = False,\n        spatial_distortion: Optional[SpatialDistortion] = None,\n    ) -> None:\n        super().__init__()\n        self.position_encoding = position_encoding\n        self.direction_encoding = direction_encoding\n        self.use_integrated_encoding = use_integrated_encoding\n        self.spatial_distortion = spatial_distortion\n\n        self.mlp_base = MLP(\n            in_dim=self.position_encoding.get_out_dim(),\n            num_layers=base_mlp_num_layers,\n            layer_width=base_mlp_layer_width,\n            skip_connections=skip_connections,\n            out_activation=nn.ReLU(),\n        )\n\n        self.mlp_head = MLP(\n            in_dim=self.mlp_base.get_out_dim() + self.direction_encoding.get_out_dim(),\n            num_layers=head_mlp_num_layers,\n            layer_width=head_mlp_layer_width,\n            out_activation=nn.ReLU(),\n        )\n\n        self.field_output_density = DensityFieldHead(in_dim=self.mlp_base.get_out_dim())\n        self.field_heads = nn.ModuleList(field_heads)\n        for field_head in self.field_heads:", "metadata": {"task_id": "nerfstudio-project_nerfstudio/83", "ground_truth": "            field_head.set_in_dim(self.mlp_head.get_out_dim())  # type: ignore", "fpath_tuple": ["nerfstudio-project_nerfstudio", "nerfstudio", "fields", "vanilla_nerf_field.py"], "context_start_lineno": 0, "line_no": 88}}
{"prompt": "import os\nimport os.path as osp\nimport logging\nimport torch\nfrom federatedscope.nlp.hetero_tasks.dataset.utils import split_sent, \\\n    DatasetDict, NUM_DEBUG\n\nlogger = logging.getLogger(__name__)\n\n\ndef get_imdb_examples(data, is_debug=False):\n    if is_debug:\n        data = data[:NUM_DEBUG]\n    examples = []\n    for ex in data:\n        examples.append((ex['text'], ex['label']))\n    return examples\n\n\ndef process_imdb_dataset(data,\n                         split,\n                         tokenizer,\n                         max_seq_len,\n                         cache_dir='',\n                         client_id=None,\n                         pretrain=False,\n                         is_debug=False,\n                         **kwargs):\n    if pretrain:\n        return process_imdb_dataset_for_pretrain(data, split, tokenizer,\n                                                 max_seq_len, cache_dir,\n                                                 client_id, is_debug)\n\n    save_dir = osp.join(cache_dir, 'train', str(client_id))\n    cache_file = osp.join(save_dir, split + '.pt')\n    if osp.exists(cache_file):\n        logger.info('Loading cache file from \\'{}\\''.format(cache_file))\n        cache_data = torch.load(cache_file)\n        examples = cache_data['examples']\n        encoded_inputs = cache_data['encoded_inputs']\n    else:\n        examples = get_imdb_examples(data, is_debug)\n        texts = [ex[0] for ex in examples]\n        encoded_inputs = tokenizer(texts,\n                                   padding='max_length',\n                                   truncation=True,\n                                   max_length=max_seq_len,\n                                   return_tensors='pt')\n\n        if cache_dir:\n            logger.info('Saving cache file to \\'{}\\''.format(cache_file))\n            os.makedirs(save_dir, exist_ok=True)\n            torch.save({\n                'examples': examples,\n                'encoded_inputs': encoded_inputs\n            }, cache_file)\n\n    labels = [ex[1] for ex in examples]\n    example_indices = torch.arange(encoded_inputs.input_ids.size(0),\n                                   dtype=torch.long)", "metadata": {"task_id": "alibaba_FederatedScope/43", "ground_truth": "    dataset = DatasetDict({\n        'token_ids': encoded_inputs.input_ids,\n        'token_type_ids': encoded_inputs.token_type_ids,\n        'attention_mask': encoded_inputs.attention_mask,\n        'labels': torch.LongTensor(labels),\n        'example_indices': example_indices\n    })", "fpath_tuple": ["alibaba_FederatedScope", "federatedscope", "nlp", "hetero_tasks", "dataset", "imdb.py"], "context_start_lineno": 0, "line_no": 60}}
{"prompt": "\nfrom federatedscope.gfl.fedsageplus.trainer import LocalGenTrainer, \\\n    FedGenTrainer\n\nlogger = logging.getLogger(__name__)\n\n\nclass FedSagePlusServer(Server):\n    def __init__(self,\n                 ID=-1,\n                 state=0,\n                 config=None,\n                 data=None,\n                 model=None,\n                 client_num=5,\n                 total_round_num=10,\n                 device='cpu',\n                 strategy=None,\n                 **kwargs):\n        r\"\"\"\n        FedSage+ consists of three of training stages.\n        Stage1: 0, local pre-train for generator.\n        Stage2: -> 2 * fedgen_epoch, federated training for generator.\n        Stage3: -> 2 * fedgen_epoch + total_round_num: federated training\n        for GraphSAGE Classifier\n        \"\"\"\n        super(FedSagePlusServer,\n              self).__init__(ID, state, config, data, model, client_num,\n                             total_round_num, device, strategy, **kwargs)\n\n        assert self.model_num == 1, \"Not supported multi-model for \" \\\n                                    \"FedSagePlusServer\"\n\n        # If state < fedgen_epoch and state % 2 == 0:\n        #     Server receive [model, embedding, label]\n        # If state < fedgen_epoch and state % 2 == 1:\n        #     Server receive [gradient]\n        self.fedgen_epoch = 2 * self._cfg.fedsageplus.fedgen_epoch\n        self.total_round_num = total_round_num + self.fedgen_epoch\n        self.grad_cnt = 0\n\n    def _register_default_handlers(self):\n        self.register_handlers('join_in', self.callback_funcs_for_join_in)\n        self.register_handlers('join_in_info', self.callback_funcs_for_join_in)\n        self.register_handlers('clf_para', self.callback_funcs_model_para)\n        self.register_handlers('gen_para', self.callback_funcs_model_para)\n        self.register_handlers('gradient', self.callback_funcs_gradient)\n        self.register_handlers('metrics', self.callback_funcs_for_metrics)\n\n    def callback_funcs_for_join_in(self, message: Message):\n        if 'info' in message.msg_type:\n            sender, info = message.sender, message.content\n            for key in self._cfg.federate.join_in_info:\n                assert key in info\n            self.join_in_info[sender] = info\n            logger.info('Server: Client #{:d} has joined in!'.format(sender))\n        else:\n            self.join_in_client_num += 1\n            sender, address = message.sender, message.content\n            if int(sender) == -1:  # assign number to client\n                sender = self.join_in_client_num\n                self.comm_manager.add_neighbors(neighbor_id=sender,\n                                                address=address)\n                self.comm_manager.send(\n                    Message(msg_type='assign_client_id',\n                            sender=self.ID,\n                            receiver=[sender],\n                            state=self.state,\n                            content=str(sender)))\n            else:\n                self.comm_manager.add_neighbors(neighbor_id=sender,\n                                                address=address)\n\n            if len(self._cfg.federate.join_in_info)!= 0:\n                self.comm_manager.send(\n                    Message(msg_type='ask_for_join_in_info',\n                            sender=self.ID,\n                            receiver=[sender],\n                            state=self.state,\n                            content=self._cfg.federate.join_in_info.copy()))\n\n        if self.check_client_join_in():\n            if self._cfg.federate.use_ss:", "metadata": {"task_id": "alibaba_FederatedScope/10", "ground_truth": "                self.broadcast_client_address()", "fpath_tuple": ["alibaba_FederatedScope", "federatedscope", "gfl", "fedsageplus", "worker.py"], "context_start_lineno": 14, "line_no": 97}}
{"prompt": "precision=args.mixed_precision,\n        log_with=args.report_to,\n        logging_dir=logging_dir,\n    )\n    if args.report_to == \"wandb\":\n        if not is_wandb_available():\n            raise ImportError(\"Make sure to install wandb if you want to use it for logging during training.\")\n        import wandb\n\n    # Make one log on every process with the configuration for debugging.\n    logging.basicConfig(\n        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n        datefmt=\"%m/%d/%Y %H:%M:%S\",\n        level=logging.INFO,\n    )\n    logger.info(accelerator.state, main_process_only=False)\n    if accelerator.is_local_main_process:\n        datasets.utils.logging.set_verbosity_warning()\n        transformers.utils.logging.set_verbosity_warning()\n        diffusers.utils.logging.set_verbosity_info()\n    else:\n        datasets.utils.logging.set_verbosity_error()\n        transformers.utils.logging.set_verbosity_error()\n        diffusers.utils.logging.set_verbosity_error()\n\n    # If passed along, set the training seed now.\n    if args.seed is not None:\n        set_seed(args.seed)\n\n    # Handle the repository creation\n    if accelerator.is_main_process:\n        if args.push_to_hub:\n            if args.hub_model_id is None:\n                repo_name = get_full_repo_name(Path(args.output_dir).name, token=args.hub_token)\n            else:\n                repo_name = args.hub_model_id\n            repo_name = create_repo(repo_name, exist_ok=True)\n            repo = Repository(args.output_dir, clone_from=repo_name)\n\n            with open(os.path.join(args.output_dir, \".gitignore\"), \"w+\") as gitignore:\n                if \"step_*\" not in gitignore:\n                    gitignore.write(\"step_*\\n\")\n                if \"epoch_*\" not in gitignore:\n                    gitignore.write(\"epoch_*\\n\")\n        elif args.output_dir is not None:\n            os.makedirs(args.output_dir, exist_ok=True)\n\n    # Load scheduler, tokenizer and models.\n    noise_scheduler = DDPMScheduler.from_pretrained(args.pretrained_model_name_or_path, subfolder=\"scheduler\")\n    tokenizer = CLIPTokenizer.from_pretrained(\n        args.pretrained_model_name_or_path, subfolder=\"tokenizer\", revision=args.revision\n    )\n    text_encoder = CLIPTextModel.from_pretrained(\n        args.pretrained_model_name_or_path, subfolder=\"text_encoder\", revision=args.revision\n    )\n    vae = AutoencoderKL.from_pretrained(args.pretrained_model_name_or_path, subfolder=\"vae\", revision=args.revision)\n    unet = UNet2DConditionModel.from_pretrained(\n        args.pretrained_model_name_or_path, subfolder=\"unet\", revision=args.revision\n    )\n\n    # For mixed precision training we cast the text_encoder and vae weights to half-precision\n    # as these models are only used for inference, keeping weights in full precision is not required.\n    weight_dtype = torch.float32\n    if accelerator.mixed_precision == \"fp16\":\n        weight_dtype = torch.float16\n    elif accelerator.mixed_precision == \"bf16\":\n        weight_dtype = torch.bfloat16\n\n    # Move unet, vae and text_encoder to device and cast to weight_dtype\n    unet.to(accelerator.device, dtype=weight_dtype)", "metadata": {"task_id": "huggingface_diffusers/186", "ground_truth": "    vae.to(accelerator.device, dtype=weight_dtype)", "fpath_tuple": ["huggingface_diffusers", "examples", "text_to_image", "train_text_to_image_lora.py"], "context_start_lineno": 358, "line_no": 428}}
{"prompt": "_collate(list(data.values()))\n        if self._cuda:\n            data = to_device(data, self._device)\n        self._collect_model.eval()\n        with torch.no_grad():\n            (mu, sigma) = self._collect_model.forward(data, mode='compute_actor')['logit']\n            dist = Independent(Normal(mu, sigma), 1)\n            action = torch.tanh(dist.rsample())\n            output = {'logit': (mu, sigma), 'action': action}\n        if self._cuda:\n            output = to_device(output, 'cpu')\n        output = default_decollate(output)\n        return {i: d for i, d in zip(data_id, output)}\n\n    def _process_transition(self, obs: Any, model_output: dict, timestep: namedtuple) -> dict:\n        r\"\"\"\n        Overview:\n            Generate dict type transition data from inputs.\n        Arguments:\n            - obs (:obj:`Any`): Env observation\n            - model_output (:obj:`dict`): Output of collect model, including at least ['action']\n            - timestep (:obj:`namedtuple`): Output after env step, including at least ['obs','reward', 'done'] \\\n                (here 'obs' indicates obs after env step, i.e. next_obs).\n        Return:\n            - transition (:obj:`Dict[str, Any]`): Dict type transition data.\n        \"\"\"\n        transition = {\n            'obs': obs,\n            'next_obs': timestep.obs,\n            'action': model_output['action'],\n           'reward': timestep.reward,\n            'done': timestep.done,\n        }\n        return transition\n\n    def _get_train_sample(self, data: list) -> Union[None, List[Any]]:\n        return get_train_sample(data, self._unroll_len)\n\n    def _init_eval(self) -> None:\n        r\"\"\"\n        Overview:\n            Evaluate mode init method. Called by ``self.__init__``.\n            Init eval model. Unlike learn and collect model, eval model does not need noise.\n        \"\"\"\n        self._eval_model = model_wrap(self._model, wrapper_name='base')\n        self._eval_model.reset()\n\n    def _forward_eval(self, data: dict) -> dict:\n        r\"\"\"\n        Overview:\n            Forward function for eval mode, similar to ``self._forward_collect``.\n        Arguments:\n            - data (:obj:`dict`): Dict type data, including at least ['obs'].\n        Returns:\n            - output (:obj:`dict`): Dict type data, including at least inferred action according to input obs.\n        \"\"\"\n        data_id = list(data.keys())\n        data = default_collate(list(data.values()))\n        if self._cuda:\n            data = to_device(data, self._device)\n        self._eval_model.eval()\n        with torch.no_grad():\n            (mu, sigma) = self._eval_model.forward(data, mode='compute_actor')['logit']\n            action = torch.tanh(mu)  # deterministic_eval\n            output = {'action': action}\n        if self._cuda:\n            output = to_device(output, 'cpu')\n        output = default_decollate(output)\n        return {i: d for i, d in zip(data_id, output)}\n\n    def default_model(self) -> Tuple[str, List[str]]:\n        return 'qac', ['ding.model.template.qac']\n\n    def _monitor_vars_learn(self) -> List[str]:\n        r\"\"\"\n        Overview:\n            Return variables' name if variables are to used in monitor.\n        Returns:\n            - vars (:obj:`List[str]`): Variables' name list.\n        \"\"\"\n        twin_critic = ['twin_critic_loss'] if self._twin_critic else []\n        if self._auto_alpha:", "metadata": {"task_id": "opendilab_ACE/147", "ground_truth": "            return super()._monitor_vars_learn() + [", "fpath_tuple": ["opendilab_ACE", "ding", "policy", "sac.py"], "context_start_lineno": 470, "line_no": 552}}
{"prompt": "if args.train_data_dir is not None:\n            data_files[\"train\"] = os.path.join(args.train_data_dir, \"**\")\n        dataset = load_dataset(\n            \"imagefolder\",\n            data_files=data_files,\n            cache_dir=args.cache_dir,\n        )\n        # See more about loading custom images at\n        # https://huggingface.co/docs/datasets/v2.4.0/en/image_load#imagefolder\n\n    # Preprocessing the datasets.\n    # We need to tokenize inputs and targets.\n    column_names = dataset[\"train\"].column_names\n\n    # 6. Get the column names for input/target.\n    dataset_columns = DATASET_NAME_MAPPING.get(args.dataset_name, None)\n    if args.image_column is None:\n        image_column = dataset_columns[0] if dataset_columns is not None else column_names[0]\n    else:\n        image_column = args.image_column\n        if image_column not in column_names:\n            raise ValueError(\n                f\"--image_column' value '{args.image_column}' needs to be one of: {', '.join(column_names)}\"\n            )\n    if args.caption_column is None:\n        caption_column = dataset_columns[1] if dataset_columns is not None else column_names[1]\n    else:\n        caption_column = args.caption_column\n        if caption_column not in column_names:\n            raise ValueError(\n                f\"--caption_column' value '{args.caption_column}' needs to be one of: {', '.join(column_names)}\"\n            )\n\n    # Preprocessing the datasets.\n    # We need to tokenize input captions and transform the images.\n    def tokenize_captions(examples, is_train=True):\n        captions = []\n        for caption in examples[caption_column]:\n            if isinstance(caption, str):\n                captions.append(caption)\n            elif isinstance(caption, (list, np.ndarray)):\n                # take a random caption if there are multiple\n                captions.append(random.choice(caption) if is_train else caption[0])\n            else:\n                raise ValueError(\n                    f\"Caption column `{caption_column}` should contain either strings or lists of strings.\"\n                )\n        inputs = tokenizer(\n            captions, max_length=tokenizer.model_max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n        )\n        return inputs.input_ids\n\n    # Preprocessing the datasets.\n    train_transforms = transforms.Compose(\n        [\n            transforms.Resize(args.resolution, interpolation=transforms.InterpolationMode.BILINEAR),\n            transforms.CenterCrop(args.resolution) if args.center_crop else transforms.RandomCrop(args.resolution),\n            transforms.RandomHorizontalFlip() if args.random_flip else transforms.Lambda(lambda x: x),\n            transforms.ToTensor(),\n            transforms.Normalize([0.5], [0.5]),\n        ]\n    )\n\n    def preprocess_train(examples):\n        images = [image.convert(\"RGB\") for image in examples[image_column]]\n        examples[\"pixel_values\"] = [train_transforms(image) for image in images]\n        examples[\"input_ids\"] = tokenize_captions(examples)\n        return examples\n\n    with accelerator.main_process_first():\n        if args.max_train_samples is not None:\n            dataset[\"train\"] = dataset[\"train\"].shuffle(seed=args.seed).select(range(args.max_train_samples))\n        # Set the training transforms\n        train_dataset = dataset[\"train\"].with_transform(preprocess_train)\n\n    def collate_fn(examples):\n        pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])", "metadata": {"task_id": "huggingface_diffusers/96", "ground_truth": "        pixel_values = pixel_values.to(memory_format=torch.contiguous_format).float()", "fpath_tuple": ["huggingface_diffusers", "examples", "text_to_image", "train_text_to_image_lora.py"], "context_start_lineno": 515, "line_no": 592}}
{"prompt": " run every X iterations after the training iteration\n        def reinitialize_optimizer(\n            self, training_callback_attributes: TrainingCallbackAttributes, step: int  # pylint: disable=unused-argument\n        ):\n            index = self.upsampling_iters.index(step)\n            resolution = self.upsampling_steps[index]\n\n            # upsample the position and direction grids\n            self.field.density_encoding.upsample_grid(resolution)\n            self.field.color_encoding.upsample_grid(resolution)\n\n            # reinitialize the encodings optimizer\n            optimizers_config = training_callback_attributes.optimizers.config\n            enc = training_callback_attributes.pipeline.get_param_groups()[\"encodings\"]\n            lr_init = optimizers_config[\"encodings\"][\"optimizer\"].lr\n\n            training_callback_attributes.optimizers.optimizers[\"encodings\"] = optimizers_config[\"encodings\"][\n                \"optimizer\"\n            ].setup(params=enc)\n            if optimizers_config[\"encodings\"][\"scheduler\"]:\n                training_callback_attributes.optimizers.schedulers[\"encodings\"] = optimizers_config[\"encodings\"][\n                    \"scheduler\"\n                ].setup(optimizer=training_callback_attributes.optimizers.optimizers[\"encodings\"], lr_init=lr_init)\n\n        callbacks = [\n            TrainingCallback(\n                where_to_run=[TrainingCallbackLocation.AFTER_TRAIN_ITERATION],\n                iters=self.upsampling_iters,\n                func=reinitialize_optimizer,\n                args=[self, training_callback_attributes],\n            )\n        ]\n        return callbacks\n\n    def update_to_step(self, step: int) -> None:\n        if step < self.upsampling_iters[0]:\n            return\n\n        new_iters = list(self.upsampling_iters) + [step + 1]\n        new_iters.sort()\n\n        index = new_iters.index(step + 1)\n        new_grid_resolution = self.upsampling_steps[index - 1]\n\n        self.field.density_encoding.upsample_grid(new_grid_resolution)  # type: ignore\n        self.field.color_encoding.upsample_grid(new_grid_resolution)  # type: ignore\n\n    def populate_modules(self):\n        \"\"\"Set the fields and modules\"\"\"\n        super().populate_modules()\n\n        # setting up fields\n        density_encoding = TensorVMEncoding(\n            resolution=self.init_resolution,\n            num_components=self.num_den_components,\n        )\n        color_encoding = TensorVMEncoding(\n            resolution=self.init_resolution,\n            num_components=self.num_color_components,\n        )\n\n        feature_encoding = NeRFEncoding(in_dim=self.appearance_dim, num_frequencies=2, min_freq_exp=0, max_freq_exp=2)\n        direction_encoding = NeRFEncoding(in_dim=3, num_frequencies=2, min_freq_exp=0, max_freq_exp=2)\n\n        self.field = TensoRFField(\n            self.scene_box.aabb,\n            feature_encoding=feature_encoding,\n            direction_encoding=direction_encoding,\n            density_encoding=density_encoding,\n            color_encoding=color_encoding,\n            appearance_dim=self.appearance_dim,\n            head_mlp_num_layers=2,\n            head_mlp_layer_width=128,\n            use_sh=False,\n        )\n\n        # samplers\n        self.sampler_uniform = UniformSampler(num_samples=self.config.num_samples, single_jitter=True)\n        self.sampler_pdf = PDFSampler(num_samples=self.config.num_samples // 2, single_jitter=True)\n\n        # renderers", "metadata": {"task_id": "nerfstudio-project_nerfstudio/24", "ground_truth": "        self.renderer_rgb = RGBRenderer(background_color=colors.WHITE)", "fpath_tuple": ["nerfstudio-project_nerfstudio", "nerfstudio", "models", "tensorf.py"], "context_start_lineno": 112, "line_no": 193}}
{"prompt": " on rays/sec calclation for training\n\n        Args:\n            train_t: timer object carrying time to execute total training iteration\n            vis_t: timer object carrying time to execute visualization step\n            step: current step\n        \"\"\"\n        train_num_rays_per_batch = self.config.pipeline.datamanager.train_num_rays_per_batch\n        writer.put_time(\n            name=EventName.TRAIN_RAYS_PER_SEC,\n            duration=train_num_rays_per_batch / (train_t.duration - vis_t.duration),\n            step=step,\n            avg_over_steps=True,\n        )\n\n    def _load_checkpoint(self) -> None:\n        \"\"\"Helper function to load pipeline and optimizer from prespecified checkpoint\"\"\"\n        load_dir = self.config.load_dir\n        if load_dir is not None:\n            load_step = self.config.load_step\n            if load_step is None:\n                print(\"Loading latest checkpoint from load_dir\")\n                # NOTE: this is specific to the checkpoint name format\n                load_step = sorted(int(x[x.find(\"-\") + 1 : x.find(\".\")]) for x in os.listdir(load_dir))[-1]\n            load_path = load_dir / f\"step-{load_step:09d}.ckpt\"\n            assert load_path.exists(), f\"Checkpoint {load_path} does not exist\"\n            loaded_state = torch.load(load_path, map_location=\"cpu\")\n            self._start_step = loaded_state[\"step\"] + 1\n            # load the checkpoints for pipeline, optimizers, and gradient scalar\n            self.pipeline.load_pipeline(loaded_state[\"pipeline\"], loaded_state[\"step\"])\n            self.optimizers.load_optimizers(loaded_state[\"optimizers\"])\n            self.grad_scaler.load_state_dict(loaded_state[\"scalers\"])\n            CONSOLE.print(f\"done loading checkpoint from {load_path}\")\n        else:\n            CONSOLE.print(\"No checkpoints to load, training from scratch\")\n\n    @check_main_thread\n    def save_checkpoint(self, step: int) -> None:\n        \"\"\"Save the model and optimizers\n\n        Args:\n            step: number of steps in training for given checkpoint\n        \"\"\"\n        # possibly make the checkpoint directory\n        if not self.checkpoint_dir.exists():\n            self.checkpoint_dir.mkdir(parents=True, exist_ok=True)\n        # save the checkpoint\n        ckpt_path = self.checkpoint_dir / f\"step-{step:09d}.ckpt\"\n        torch.save(\n            {\n                \"step\": step,\n                \"pipeline\": self.pipeline.module.state_dict()  # type: ignore\n                if hasattr(self.pipeline, \"module\")\n                else self.pipeline.state_dict(),\n                \"optimizers\": {k: v.state_dict() for (k, v) in self.optimizers.optimizers.items()},\n                \"scalers\": self.grad_scaler.state_dict(),\n            },\n            ckpt_path,\n        )\n        # possibly delete old checkpoints\n        if self.config.save_only_latest_checkpoint:\n            # delete everything else in the checkpoint folder\n            for f in self.checkpoint_dir.glob(\"*\"):\n                if f!= ckpt_path:\n                    f.unlink()\n\n    @profiler.time_function\n    def train_iteration(self, step: int) -> Tuple[torch.Tensor, Dict[str, torch.Tensor], Dict[str, torch.Tensor]]:\n        \"\"\"Run one iteration with a batch of inputs. Returns dictionary of model losses.\n\n        Args:\n            step: Current training step.\n        \"\"\"\n        self.optimizers.zero_grad_all()\n        cpu_or_cuda_str = self.device.split(\":\")[0]\n        with torch.autocast(device_type=cpu_or_cuda_str, enabled=self.mixed_precision):", "metadata": {"task_id": "nerfstudio-project_nerfstudio/113", "ground_truth": "            _, loss_dict, metrics_dict = self.pipeline.get_train_loss_dict(step=step)", "fpath_tuple": ["nerfstudio-project_nerfstudio", "nerfstudio", "engine", "trainer.py"], "context_start_lineno": 292, "line_no": 368}}
{"prompt": "                    path.join(folder, filename),\n                    size_guidance={\n                        event_accumulator.IMAGES: 0,\n                    },\n                )\n                ea.Reload()\n                img = ea.Images(\"tmp_ALE/Pong-v5_video\")\n                try:\n                    assert len(img) == N // args.record_interval\n                    break\n                except AssertionError:\n                    sleep(0.1)\n\n    @pytest.mark.parametrize(\n        \"backend\",\n        [\n            \"torchsnapshot\",\n            \"torch\",\n        ],\n    )\n    def test_recorder_load(self, backend, N=8):\n        if not _has_ts and backend == \"torchsnapshot\":\n            pytest.skip(\"torchsnapshot not found\")\n\n        os.environ[\"CKPT_BACKEND\"] = backend\n        state_dict_has_been_called = [False]\n        load_state_dict_has_been_called = [False]\n        Recorder.state_dict, Recorder_state_dict = _fun_checker(\n            Recorder.state_dict, state_dict_has_been_called\n        )\n        (\n            Recorder.load_state_dict,\n            Recorder_load_state_dict,\n        ) = _fun_checker(Recorder.load_state_dict, load_state_dict_has_been_called)\n\n        args = self._get_args()\n\n        def _make_recorder_and_trainer(tmpdirname):\n            logger = TensorboardLogger(exp_name=f\"{tmpdirname}/tb\")\n            if backend == \"torch\":\n                file = path.join(tmpdirname, \"file.pt\")\n            elif backend == \"torchsnapshot\":\n                file = tmpdirname\n            else:\n                raise NotImplementedError\n            trainer = mocking_trainer(file)\n\n            recorder = transformed_env_constructor(\n                args,\n                video_tag=\"tmp\",\n                norm_obs_only=True,\n                stats={\"loc\": 0, \"scale\": 1},\n                logger=logger,\n            )()\n\n            recorder = Recorder(\n                record_frames=args.record_frames,\n                frame_skip=args.frame_skip,\n                policy_exploration=None,\n                recorder=recorder,\n                record_interval=args.record_interval,\n            )\n            recorder.register(trainer)\n            return trainer, recorder, file\n\n        with tempfile.TemporaryDirectory() as tmpdirname:\n            trainer, recorder, file = _make_recorder_and_trainer(tmpdirname)\n            for _ in range(N):\n                recorder(None)\n            trainer.save_trainer(True)\n            with tempfile.TemporaryDirectory() as tmpdirname2:\n                trainer2, recorder2, _ = _make_recorder_and_trainer(tmpdirname2)\n                trainer2.load_from_file(file)\n            assert recorder2._count == 8\n            assert state_dict_has_been_called[0]\n            assert load_state_dict_has_been_called[0]\n        Recorder.state_dict = Recorder_state_dict\n        Recorder.load_state_dict = Recorder_load_state_dict\n\n\ndef test_updateweights():\n    torch.manual_seed(0)\n    trainer = mocking_trainer()\n\n    T = 5\n    update_weights = UpdateWeights(trainer.collector, T)\n    update_weights.register(trainer)\n    for t in range(T):\n        trainer._post_steps_hook()\n        assert trainer.collector.called_update_policy_weights_ is (t == T - 1)\n    assert trainer.collector.called_update_policy_weights_\n\n\nclass TestCountFrames:\n    def test_countframes(self):\n        torch.manual_seed(0)\n        trainer = mocking_trainer()\n\n        frame_skip = 3\n        batch = 10\n        count_frames = CountFramesLog(frame_skip=frame_skip)", "metadata": {"task_id": "pytorch_rl/85", "ground_truth": "        count_frames.register(trainer)", "fpath_tuple": ["pytorch_rl", "test", "test_trainer.py"], "context_start_lineno": 888, "line_no": 989}}
{"prompt": " None:\n            raise AttributeError(\"Camera indices are not provided.\")\n        camera_indices = ray_samples.camera_indices.squeeze()\n        directions = get_normalized_directions(ray_samples.frustums.directions)\n        directions_flat = directions.view(-1, 3)\n        d = self.direction_encoding(directions_flat)\n\n        outputs_shape = ray_samples.frustums.directions.shape[:-1]\n\n        # appearance\n        if self.training:\n            embedded_appearance = self.embedding_appearance(camera_indices)\n        else:\n            if self.use_average_appearance_embedding:\n                embedded_appearance = torch.ones(\n                    (*directions.shape[:-1], self.appearance_embedding_dim), device=directions.device\n                ) * self.embedding_appearance.mean(dim=0)\n            else:\n                embedded_appearance = torch.zeros(\n                    (*directions.shape[:-1], self.appearance_embedding_dim), device=directions.device\n                )\n\n        # transients\n        if self.use_transient_embedding and self.training:\n            embedded_transient = self.embedding_transient(camera_indices)\n            transient_input = torch.cat(\n                [\n                    density_embedding.view(-1, self.geo_feat_dim),\n                    embedded_transient.view(-1, self.transient_embedding_dim),\n                ],\n                dim=-1,\n            )\n            x = self.mlp_transient(transient_input).view(*outputs_shape, -1).to(directions)\n            outputs[FieldHeadNames.UNCERTAINTY] = self.field_head_transient_uncertainty(x)\n            outputs[FieldHeadNames.TRANSIENT_RGB] = self.field_head_transient_rgb(x)\n            outputs[FieldHeadNames.TRANSIENT_DENSITY] = self.field_head_transient_density(x)\n\n        # semantics\n        if self.use_semantics:\n            density_embedding_copy = density_embedding.clone().detach()\n            semantics_input = torch.cat(\n                [\n                    density_embedding_copy.view(-1, self.geo_feat_dim),\n                ],\n                dim=-1,\n            )\n            x = self.mlp_semantics(semantics_input).view(*outputs_shape, -1).to(directions)\n            outputs[FieldHeadNames.SEMANTICS] = self.field_head_semantics(x)\n\n        # predicted normals\n        if self.use_pred_normals:\n            positions = ray_samples.frustums.get_positions()\n\n            positions_flat = self.position_encoding(positions.view(-1, 3))\n            pred_normals_inp = torch.cat([positions_flat, density_embedding.view(-1, self.geo_feat_dim)], dim=-1)\n\n            x = self.mlp_pred_normals(pred_normals_inp).view(*outputs_shape, -1).to(directions)\n            outputs[FieldHeadNames.PRED_NORMALS] = self.field_head_pred_normals(x)\n\n        h = torch.cat(\n            [\n                d,\n                density_embedding.view(-1, self.geo_feat_dim),\n                embedded_appearance.view(-1, self.appearance_embedding_dim),\n            ],\n            dim=-1,\n        )\n        rgb = self.mlp_head(h).view(*outputs_shape, -1).to(directions)\n        outputs.update({FieldHeadNames.RGB: rgb})\n\n        return outputs\n\n\nclass TorchNerfactoField(Field):\n    \"\"\"\n    PyTorch implementation of the compound field.\n    \"\"\"\n\n    def __init__(\n        self,\n        aabb,\n        num_images: int,\n        position_encoding: Encoding = HashEncoding(),", "metadata": {"task_id": "nerfstudio-project_nerfstudio/127", "ground_truth": "        direction_encoding: Encoding = SHEncoding(),", "fpath_tuple": ["nerfstudio-project_nerfstudio", "nerfstudio", "fields", "nerfacto_field.py"], "context_start_lineno": 254, "line_no": 337}}
{"prompt": "if shape1 is None:\n            shape1 = (15,)\n        else:\n            shape1 = (*shape1, 15)\n        spec = UnboundedDiscreteTensorSpec(shape=shape1, device=\"cpu\", dtype=torch.long)\n        if shape1 is not None:\n            shape2_real = (*shape2, *shape1)\n        else:\n            shape2_real = shape2\n\n        spec2 = spec.expand(shape2_real)\n        assert spec2 is not spec\n        assert spec2.dtype == spec.dtype\n        assert (spec2.zero() == spec.zero()).all()\n        spec2 = spec.expand(*shape2_real)\n        assert spec2 is not spec\n        assert spec2.dtype == spec.dtype\n        assert (spec2.zero() == spec.zero()).all()\n        assert spec2.rand().shape == spec2.shape\n        assert spec2.zero().shape == spec2.shape\n        spec2 = spec.expand(*shape2_real)\n        assert spec2 is not spec\n        assert spec2.dtype == spec.dtype\n        assert spec2.rand().shape == spec2.shape\n        assert spec2.zero().shape == spec2.shape\n\n\nclass TestClone:\n    @pytest.mark.parametrize(\n        \"shape1\",\n        [\n            None,\n            (4,),\n            (5, 4),\n        ],\n    )\n    def test_binary(self, shape1):\n        spec = BinaryDiscreteTensorSpec(\n            n=4, shape=shape1, device=\"cpu\", dtype=torch.bool\n        )\n        assert spec == spec.clone()\n        assert spec is not spec.clone()\n\n    @pytest.mark.parametrize(\n        \"shape1,mini,maxi\",\n        [\n            [(10,), -torch.ones([]), torch.ones([])],\n            [None, -torch.ones([10]), torch.ones([])],\n            [None, -torch.ones([]), torch.ones([10])],\n            [(10,), -torch.ones([]), torch.ones([10])],\n            [(10,), -torch.ones([10]), torch.ones([])],\n            [(10,), -torch.ones([10]), torch.ones([10])],\n        ],\n    )\n    def test_bounded(self, shape1, mini, maxi):\n        spec = BoundedTensorSpec(\n            mini, maxi, shape=shape1, device=\"cpu\", dtype=torch.bool\n        )\n        assert spec == spec.clone()\n        assert spec is not spec.clone()\n\n    def test_composite(self):\n        batch_size = (5,)\n        spec1 = BoundedTensorSpec(\n            -torch.ones([*batch_size, 10]),\n            torch.ones([*batch_size, 10]),\n            shape=(\n                *batch_size,\n                10,\n            ),\n            device=\"cpu\",\n            dtype=torch.bool,\n        )\n        spec2 = BinaryDiscreteTensorSpec(\n            n=4, shape=(*batch_size, 4), device=\"cpu\", dtype=torch.bool\n        )\n        spec3 = DiscreteTensorSpec(\n            n=4, shape=batch_size, device=\"cpu\", dtype=torch.long\n        )\n        spec4 = MultiDiscreteTensorSpec(\n            nvec=(4, 5, 6), shape=(*batch_size, 3), device=\"cpu\", dtype=torch.long\n        )\n        spec5 = MultiOneHotDiscreteTensorSpec(\n            nvec=(4, 5, 6), shape=(*batch_size, 15), device=\"cpu\", dtype=torch.long\n        )\n        spec6 = OneHotDiscreteTensorSpec(\n            n=15, shape=(*batch_size, 15), device=\"cpu\", dtype=torch.long\n        )\n        spec7 = UnboundedContinuousTensorSpec(\n            shape=(*batch_size, 9),\n            device=\"cpu\",\n            dtype=torch.float64,\n        )", "metadata": {"task_id": "pytorch_rl/193", "ground_truth": "        spec8 = UnboundedDiscreteTensorSpec(\n            shape=(*batch_size, 9),\n            device=\"cpu\",\n            dtype=torch.long,\n        )", "fpath_tuple": ["pytorch_rl", "test", "test_specs.py"], "context_start_lineno": 1427, "line_no": 1520}}
{"prompt": "),\n        SafeProbabilisticModule(\n            in_keys=[\"loc\", \"scale\"],\n            out_keys=[action_key],\n            default_interaction_mode=\"random\",\n            distribution_class=TanhNormal,\n            spec=CompositeSpec(**{action_key: proof_environment.action_spec}),\n        ),\n    )\n    return actor_simulator\n\n\ndef _dreamer_make_actor_real(\n    obs_encoder, rssm_prior, rssm_posterior, actor_module, action_key, proof_environment\n):\n    # actor for real world: interacts with states ~ posterior\n    # Out actor differs from the original paper where first they compute prior and posterior and then act on it\n    # but we found that this approach worked better.\n    actor_realworld = SafeSequential(\n        SafeModule(\n            obs_encoder,\n            in_keys=[\"pixels\"],\n            out_keys=[\"encoded_latents\"],\n        ),\n        SafeModule(\n            rssm_posterior,\n            in_keys=[\"belief\", \"encoded_latents\"],\n            out_keys=[\n                \"_\",\n                \"_\",\n                \"state\",\n            ],\n        ),\n        SafeProbabilisticSequential(\n            SafeModule(\n                actor_module,\n                in_keys=[\"state\", \"belief\"],\n                out_keys=[\"loc\", \"scale\"],\n                spec=CompositeSpec(\n                    **{\n                        \"loc\": UnboundedContinuousTensorSpec(\n                            proof_environment.action_spec.shape,\n                        ),\n                        \"scale\": UnboundedContinuousTensorSpec(\n                            proof_environment.action_spec.shape,\n                        ),\n                    }\n                ),\n            ),\n            SafeProbabilisticModule(\n                in_keys=[\"loc\", \"scale\"],\n                out_keys=[action_key],\n                default_interaction_mode=\"random\",\n                distribution_class=TanhNormal,\n                spec=CompositeSpec(\n                    **{action_key: proof_environment.action_spec.to(\"cpu\")}\n                ),\n            ),\n        ),\n        SafeModule(\n            rssm_prior,\n            in_keys=[\"state\", \"belief\", action_key],\n            out_keys=[\n                \"_\",\n                \"_\",\n                \"_\",  # we don't need the prior state\n                (\"next\", \"belief\"),\n            ],\n        ),\n    )\n    return actor_realworld\n\n\ndef _dreamer_make_value_model(mlp_num_units, value_key):\n    # actor for simulator: interacts with states ~ prior\n    value_model = SafeModule(\n        MLP(\n            out_features=1,\n            depth=3,\n            num_cells=mlp_num_units,\n            activation_class=nn.ELU,\n        ),\n        in_keys=[\"state\", \"belief\"],\n        out_keys=[value_key],\n    )\n    return value_model\n\n\ndef _dreamer_make_mbenv(\n    reward_module,\n    rssm_prior,\n    obs_decoder,\n    proof_environment,\n    use_decoder_in_env,\n    state_dim,\n    rssm_hidden_dim,\n):\n    # MB environment\n    if use_decoder_in_env:\n        mb_env_obs_decoder = SafeModule(\n            obs_decoder,\n            in_keys=[(\"next\", \"state\"), (\"next\", \"belief\")],\n            out_keys=[(\"next\", \"reco_pixels\")],\n        )\n    else:\n        mb_env_obs_decoder = None\n\n    transition_model = SafeSequential(\n        SafeModule(\n            rssm_prior,\n            in_keys=[\"state\", \"belief\", \"action\"],\n            out_keys=[\n                \"_\",\n                \"_\",\n                \"state\",\n                \"belief\",\n            ],\n        ),\n    )\n    reward_model = SafeModule(\n        reward_module,\n        in_keys=[\"state\", \"belief\"],\n        out_keys=[\"reward\"],\n    )", "metadata": {"task_id": "pytorch_rl/21", "ground_truth": "    model_based_env = DreamerEnv(\n        world_model=WorldModelWrapper(\n            transition_model,\n            reward_model,\n        ),\n        prior_shape=torch.Size([state_dim]),\n        belief_shape=torch.Size([rssm_hidden_dim]),\n        obs_decoder=mb_env_obs_decoder,\n    )", "fpath_tuple": ["pytorch_rl", "torchrl", "trainers", "helpers", "models.py"], "context_start_lineno": 1663, "line_no": 1787}}
{"prompt": "result_sum.item() - 20.4125) < 1e-2\n            assert abs(result_mean.item() - 0.0266) < 1e-3\n        else:\n            # CUDA\n            assert abs(result_sum.item() - 20.4125) < 1e-2\n            assert abs(result_mean.item() - 0.0266) < 1e-3\n\n\nclass DEISMultistepSchedulerTest(SchedulerCommonTest):\n    scheduler_classes = (DEISMultistepScheduler,)\n    forward_default_kwargs = ((\"num_inference_steps\", 25),)\n\n    def get_scheduler_config(self, **kwargs):\n        config = {\n            \"num_train_timesteps\": 1000,\n            \"beta_start\": 0.0001,\n            \"beta_end\": 0.02,\n            \"beta_schedule\": \"linear\",\n            \"solver_order\": 2,\n        }\n\n        config.update(**kwargs)\n        return config\n\n    def check_over_configs(self, time_step=0, **config):\n        kwargs = dict(self.forward_default_kwargs)\n        num_inference_steps = kwargs.pop(\"num_inference_steps\", None)\n        sample = self.dummy_sample\n        residual = 0.1 * sample\n        dummy_past_residuals = [residual + 0.2, residual + 0.15, residual + 0.10]\n\n        for scheduler_class in self.scheduler_classes:\n            scheduler_config = self.get_scheduler_config(**config)\n            scheduler = scheduler_class(**scheduler_config)\n            scheduler.set_timesteps(num_inference_steps)\n            # copy over dummy past residuals\n            scheduler.model_outputs = dummy_past_residuals[: scheduler.config.solver_order]\n\n            with tempfile.TemporaryDirectory() as tmpdirname:\n                scheduler.save_config(tmpdirname)\n                new_scheduler = scheduler_class.from_pretrained(tmpdirname)\n                new_scheduler.set_timesteps(num_inference_steps)\n                # copy over dummy past residuals\n                new_scheduler.model_outputs = dummy_past_residuals[: new_scheduler.config.solver_order]\n\n            output, new_output = sample, sample\n            for t in range(time_step, time_step + scheduler.config.solver_order + 1):\n                output = scheduler.step(residual, t, output, **kwargs).prev_sample\n                new_output = new_scheduler.step(residual, t, new_output, **kwargs).prev_sample\n\n                assert torch.sum(torch.abs(output - new_output)) < 1e-5, \"Scheduler outputs are not identical\"\n\n    def test_from_save_pretrained(self):\n        pass\n\n    def check_over_forward(self, time_step=0, **forward_kwargs):\n        kwargs = dict(self.forward_default_kwargs)\n        num_inference_steps = kwargs.pop(\"num_inference_steps\", None)\n        sample = self.dummy_sample\n        residual = 0.1 * sample\n        dummy_past_residuals = [residual + 0.2, residual + 0.15, residual + 0.10]\n\n        for scheduler_class in self.scheduler_classes:\n            scheduler_config = self.get_scheduler_config()\n            scheduler = scheduler_class(**scheduler_config)\n            scheduler.set_timesteps(num_inference_steps)\n\n            # copy over dummy past residuals (must be after setting timesteps)\n            scheduler.model_outputs = dummy_past_residuals[: scheduler.config.solver_order]\n\n            with tempfile.TemporaryDirectory() as tmpdirname:\n                scheduler.save_config(tmpdirname)", "metadata": {"task_id": "huggingface_diffusers/130", "ground_truth": "                new_scheduler = scheduler_class.from_pretrained(tmpdirname)", "fpath_tuple": ["huggingface_diffusers", "tests", "test_scheduler.py"], "context_start_lineno": 2429, "line_no": 2501}}
{"prompt": "0, \"test_distributed_metrics_2\", tmp_dir, 0),\n                    (2, 1, preds_1, refs_1, \"test_distributed_metrics_2\", tmp_dir, 0),\n                ],\n            )\n            self.assertDictEqual(expected_results, results[0])\n            self.assertIsNone(results[1])\n            del results\n\n            # To use several distributed metrics on the same local file system, need to specify an experiment_id\n            try:\n                results = pool.map(\n                    metric_add_and_compute,\n                    [\n                        (2, 0, preds_0, refs_0, \"test_distributed_metrics_3\", tmp_dir, 0),\n                        (2, 1, preds_1, refs_1, \"test_distributed_metrics_3\", tmp_dir, 0),\n                        (2, 0, preds_0, refs_0, \"test_distributed_metrics_3\", tmp_dir, 0),\n                        (2, 1, preds_1, refs_1, \"test_distributed_metrics_3\", tmp_dir, 0),\n                    ],\n                )\n            except ValueError:\n                # We are fine with either raising a ValueError or computing well the metric\n                # Being sure we raise the error would means making the dummy dataset bigger\n                # and the test longer...\n                pass\n            else:\n                self.assertDictEqual(expected_results, results[0])\n                self.assertDictEqual(expected_results, results[2])\n                self.assertIsNone(results[1])\n                self.assertIsNone(results[3])\n                del results\n\n            results = pool.map(\n                metric_add_and_compute,\n                [\n                    (2, 0, preds_0, refs_0, \"exp_0\", tmp_dir, 0),\n                    (2, 1, preds_1, refs_1, \"exp_0\", tmp_dir, 0),\n                    (2, 0, preds_0, refs_0, \"exp_1\", tmp_dir, 0),\n                    (2, 1, preds_1, refs_1, \"exp_1\", tmp_dir, 0),\n                ],\n            )\n            self.assertDictEqual(expected_results, results[0])\n            self.assertDictEqual(expected_results, results[2])\n            self.assertIsNone(results[1])\n            self.assertIsNone(results[3])\n            del results\n\n            # With keep_in_memory is not allowed\n            with self.assertRaises(ValueError):\n                DummyMetric(\n                    experiment_id=\"test_distributed_metrics_4\",\n                    keep_in_memory=True,\n                    num_process=2,\n                    process_id=0,\n                    cache_dir=tmp_dir,\n                )\n\n    def test_dummy_metric_pickle(self):\n        with tempfile.TemporaryDirectory() as tmp_dir:\n            tmp_file = os.path.join(tmp_dir, \"metric.pt\")\n            preds, refs = DummyMetric.predictions_and_references()\n            expected_results = DummyMetric.expected_results()\n\n            metric = DummyMetric(experiment_id=\"test_dummy_metric_pickle\")\n\n            with open(tmp_file, \"wb\") as f:\n                pickle.dump(metric, f)\n            del metric\n\n            with open(tmp_file, \"rb\") as f:\n                metric = pickle.load(f)\n            self.assertDictEqual(expected_results, metric.compute(predictions=preds, references=refs))\n            del metric\n\n    def test_input_numpy(self):\n        import numpy as np\n\n        preds, refs = DummyMetric.predictions_and_references()\n        expected_results = DummyMetric.expected_results()\n        preds, refs = np.array(preds), np.array(refs)\n\n        metric = DummyMetric(experiment_id=\"test_input_numpy\")", "metadata": {"task_id": "huggingface_evaluate/188", "ground_truth": "        self.assertDictEqual(expected_results, metric.compute(predictions=preds, references=refs))", "fpath_tuple": ["huggingface_evaluate", "tests", "test_metric.py"], "context_start_lineno": 361, "line_no": 442}}
{"prompt": "=\"squad_v2\",\n        )\n        self.assertDictEqual(\n            {key: results[key] for key in [\"HasAns_f1\", \"NoAns_f1\"]}, {\"HasAns_f1\": 100.0, \"NoAns_f1\": 100.0}\n        )\n\n    @slow\n    def test_default_pipe_init(self):\n        # squad_v1-like dataset\n        results = self.evaluator.compute(\n            data=self.data,\n        )\n        self.assertEqual(results[\"exact_match\"], 100.0)\n        self.assertEqual(results[\"f1\"], 100.0)\n\n        # squad_v2-like dataset\n        results = self.evaluator.compute(\n            data=self.data_v2,\n            metric=\"squad_v2\",\n        )\n        self.assertDictEqual(\n            {key: results[key] for key in [\"HasAns_f1\", \"NoAns_f1\"]}, {\"HasAns_f1\": 100.0, \"NoAns_f1\": 0.0}\n        )\n\n    def test_data_loading(self):\n        # Test passing in dataset by name with data_split\n        data = self.evaluator.load_data(\"evaluate/squad-ci\", split=\"validation[:1]\")\n        self.evaluator.prepare_data(\n            data=data, question_column=\"question\", context_column=\"context\", id_column=\"id\", label_column=\"answers\"\n        )\n\n        # Test passing in dataset by name without data_split and inferring the optimal split\n        data = self.evaluator.load_data(\"evaluate/squad-ci\")\n        self.evaluator.prepare_data(\n            data=data, question_column=\"question\", context_column=\"context\", id_column=\"id\", label_column=\"answers\"\n        )\n\n        # Test that it chooses the correct one (e.g. squad only has train and validation, but no test)\n        self.assertEqual(data.split, \"validation\")\n\n        # Test that the data point returned is correct; this maps to the first example in the squad-ci dataset\n        self.assertEqual(data[0][\"id\"], \"56be4db0acb8001400a502ec\")\n\n    def test_overwrite_default_metric(self):\n        # squad_v1-like dataset\n        squad = load(\"squad\")\n        results = self.evaluator.compute(\n            model_or_pipeline=self.pipe,\n            data=self.data,\n            metric=squad,\n        )\n        self.assertEqual(results[\"exact_match\"], 100.0)\n        self.assertEqual(results[\"f1\"], 100.0)\n\n        results = self.evaluator.compute(\n            model_or_pipeline=self.pipe,\n            data=self.data,\n            metric=\"squad\",\n        )\n        self.assertEqual(results[\"exact_match\"], 100.0)\n        self.assertEqual(results[\"f1\"], 100.0)\n\n\nclass TestTokenClassificationEvaluator(TestCase):\n    def setUp(self):\n        features = Features(\n            {\n                \"tokens\": Sequence(feature=Value(dtype=\"string\")),\n                \"ner_tags\": Sequence(feature=ClassLabel(names=[\"O\", \"B-LOC\", \"I-LOC\"])),\n            }\n        )\n\n        self.data = Dataset.from_dict(\n            {\n                \"tokens\": [[\"New\", \"York\", \"a\", \"nice\", \"City\", \".\"]],\n                \"ner_tags\": [[1, 2, 0, 0, 1, 0]],\n            },\n            features=features,\n        )\n        self.default_model = \"hf-internal-testing/tiny-bert-for-token-classification\"\n        self.pipe = DummyTokenClassificationPipeline()\n        self.evaluator = evaluator(\"token-classification\")\n\n    @slow\n    def test_model_init(self):", "metadata": {"task_id": "huggingface_evaluate/181", "ground_truth": "        results = self.evaluator.compute(\n            model_or_pipeline=self.default_model,\n            data=self.data,\n            metric=\"seqeval\",\n        )", "fpath_tuple": ["huggingface_evaluate", "tests", "test_evaluator.py"], "context_start_lineno": 610, "line_no": 695}}
{"prompt": "\n        update_at_each_batch: bool = False,\n        init_with_lag: bool = False,\n        exploration_mode: str = DEFAULT_EXPLORATION_MODE,\n        reset_when_done: bool = True,\n    ):\n        self.closed = True\n        self.create_env_fn = create_env_fn\n        self.num_workers = len(create_env_fn)\n        self.create_env_kwargs = (\n            create_env_kwargs\n            if create_env_kwargs is not None\n            else [{} for _ in range(self.num_workers)]\n        )\n        # Preparing devices:\n        # We want the user to be able to choose, for each worker, on which\n        # device will the policy live and which device will be used to store\n        # data. Those devices may or may not match.\n        # One caveat is that, if there is only one device for the policy, and\n        # if there are multiple workers, sending the same device and policy\n        # to be copied to each worker will result in multiple copies of the\n        # same policy on the same device.\n        # To go around this, we do the copies of the policy in the server\n        # (this object) to each possible device, and send to all the\n        # processes their copy of the policy.\n\n        def device_err_msg(device_name, devices_list):\n            return (\n                f\"The length of the {device_name} argument should match the \"\n                f\"number of workers of the collector. Got len(\"\n                f\"create_env_fn)={self.num_workers} and len(\"\n                f\"passing_devices)={len(devices_list)}\"\n            )\n\n        if isinstance(devices, (str, int, torch.device)):\n            devices = [torch.device(devices) for _ in range(self.num_workers)]\n        elif devices is None:\n            devices = [None for _ in range(self.num_workers)]\n        elif isinstance(devices, Sequence):\n            if len(devices)!= self.num_workers:\n                raise RuntimeError(device_err_msg(\"devices\", devices))\n            devices = [torch.device(_device) for _device in devices]\n        else:\n            raise ValueError(\n                \"devices should be either None, a torch.device or equivalent \"\n                \"or an iterable of devices. \"\n                f\"Found {type(devices)} instead.\"\n            )\n        self._policy_dict = {}\n        self._get_weights_fn_dict = {}\n\n        for i, (_device, create_env, kwargs) in enumerate(\n            zip(devices, self.create_env_fn, self.create_env_kwargs)\n        ):\n            if _device in self._policy_dict:\n                devices[i] = _device\n                continue\n\n            if hasattr(create_env, \"observation_spec\"):\n                observation_spec = create_env.observation_spec\n            else:\n                try:\n                    observation_spec = create_env(**kwargs).observation_spec\n                except:  # noqa\n                    observation_spec = None\n\n            _policy, _device, _get_weight_fn = self._get_policy_and_device(\n                policy=policy, device=_device, observation_spec=observation_spec\n            )\n            self._policy_dict[_device] = _policy\n            self._get_weights_fn_dict[_device] = _get_weight_fn\n            devices[i] = _device\n        self.devices = devices\n\n        if passing_devices is None:\n            self.passing_devices = self.devices\n        else:\n            if isinstance(passing_devices, (str, int, torch.device)):\n                self.passing_devices = [\n                    torch.device(passing_devices) for _ in range(self.num_workers)\n                ]\n            elif isinstance(passing_devices, Sequence):\n                if len(passing_devices)!= self.num_workers:\n                    raise RuntimeError(\n                        device_err_msg(\"passing_devices\", passing_devices)\n                    )\n                self.passing_devices = [", "metadata": {"task_id": "pytorch_rl/20", "ground_truth": "                    torch.device(_passing_device) for _passing_device in passing_devices", "fpath_tuple": ["pytorch_rl", "torchrl", "collectors", "collectors.py"], "context_start_lineno": 870, "line_no": 957}}
{"prompt": "# Copyright 2023 Google LLC.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import annotations\n\n\"\"\"Comparison test for algorithms using score analysis.\n\nEx: Typical Efficiency Convergence Test Example\n-----------------------------------------------\nbaseline_factory = benchmarks.BenchmarkStateFactory(...)\ncandidate_factory = benchmarks.BenchmarkStateFactory(...)\n\n# Run each algorithm for 100 Trials with 5 repeats each.\ncomparator = comparator_runner.EfficiencyComparisonTester(\n        num_trials=100, num_repeats=5)\ncomparator.assert_better_efficiency(candidate_factory, baseline_factory)\n\nNOTE: assert_converges_faster is a generic method name that conveys the general\nuse of the class.\n\"\"\"\n\nimport logging\n\nimport attr\nimport numpy as np\nfrom vizier import benchmarks\nfrom vizier import pyvizier as vz\nfrom vizier._src.algorithms.optimizers import vectorized_base as vb\nfrom vizier._src.benchmarks.analyzers import simple_regret_score\nfrom vizier.pyvizier import converters\n\n\nclass FailedComparisonTestError(Exception):\n  \"\"\"Exception raised for comparison test fails.\"\"\"\n\n\nclass FailedSimpleRegretConvergenceTestError(Exception):\n  \"\"\"Exception raised for simple-regret convergence test fails.\"\"\"\n\n\n@attr.define\nclass EfficiencyComparisonTester:\n  \"\"\"Comparison test between algorithms using analysis scores.\"\"\"\n  num_trials: int = attr.field(\n      default=1, validator=attr.validators.instance_of(int))\n  num_repeats: int = attr.field(\n      default=1, validator=attr.validators.instance_of(int))\n\n  def assert_better_efficiency(\n      self,\n      candidate_state_factory: benchmarks.BenchmarkStateFactory,\n      baseline_state_factory: benchmarks.BenchmarkStateFactory,\n      score_threshold: float = 0.0) -> None:\n    \"\"\"Asserts that candidate is better than baseline via log_eff_score.\"\"\"\n    # TODO: Consider making this more flexible with more runners\n    # And enable multimetric.\n    runner = benchmarks.BenchmarkRunner(\n        benchmark_subroutines=[benchmarks.GenerateAndEvaluate()],\n        num_repeats=self.num_trials)\n\n    baseline_curves = []\n    candidate_curves = []\n    for _ in range(self.num_repeats):\n      baseline_state = baseline_state_factory()\n      candidate_state = candidate_state_factory()\n\n      baseline_statement = baseline_state.experimenter.problem_statement()\n      if len(baseline_statement.metric_information) > 1:\n        raise ValueError('Support for multimetric is not yet')\n      if baseline_statement!= (\n          candidate_statement :=\n          candidate_state.experimenter.problem_statement()):\n        raise ValueError('Comparison tests done for different statements: '\n                         f'{baseline_statement} vs {candidate_statement}')\n\n      runner.run(baseline_state)\n      runner.run(candidate_state)\n      baseline_curves.append(\n          benchmarks.ConvergenceCurveConverter(\n              baseline_statement.metric_information.item()).convert(\n                  baseline_state.algorithm.supporter.GetTrials()))\n      candidate_curves.append(\n          benchmarks.ConvergenceCurveConverter(", "metadata": {"task_id": "google_vizier/158", "ground_truth": "              baseline_statement.metric_information.item()).convert(", "fpath_tuple": ["google_vizier", "vizier", "_src", "algorithms", "testing", "comparator_runner.py"], "context_start_lineno": 0, "line_no": 94}}
{"prompt": " input.\"\n                    \"Please check the forward format or implement your own \"\n                    \"flop_count function\")\n                ctx.monitor.flops_per_sample = -1\n\n        # by default, we assume the data has the same input shape,\n        # thus simply multiply the flops to avoid redundant forward\n        ctx.monitor.total_flops += ctx.monitor.flops_per_sample * \\\n            ctx.batch_size\n\n    def _hook_on_batch_forward_regularizer(self, ctx):\n        \"\"\"\n        Note:\n          The modified attributes and according operations are shown below:\n            ==================================  ===========================\n            Attribute                           Operation\n            ==================================  ===========================\n            ``ctx.loss_regular``                Calculate the regular loss\n            ``ctx.loss_task``                   Sum the ``ctx.loss_regular`` \\\n            and ``ctx.loss``\n            ==================================  ===========================\n        \"\"\"\n        ctx.loss_regular = CtxVar(\n            self.cfg.regularizer.mu * ctx.regularizer(ctx), LIFECYCLE.BATCH)\n        ctx.loss_task = CtxVar(ctx.loss_batch + ctx.loss_regular,\n                               LIFECYCLE.BATCH)\n\n    def _hook_on_batch_backward(self, ctx):\n        \"\"\"\n        Note:\n          The modified attributes and according operations are shown below:\n            ==================================  ===========================\n            Attribute                           Operation\n            ==================================  ===========================\n            ``ctx.optimizer``                   Update by gradient\n            ``ctx.loss_task``                   Backward propagation\n            ``ctx.scheduler``                   Update by gradient\n            ==================================  ===========================\n        \"\"\"\n        ctx.optimizer.zero_grad()\n        ctx.loss_task.backward()\n        if ctx.grad_clip > 0:\n            torch.nn.utils.clip_grad_norm_(ctx.model.parameters(),\n                                           ctx.grad_clip)\n\n        ctx.optimizer.step()\n        if ctx.scheduler is not None:\n            ctx.scheduler.step()\n\n    def _hook_on_batch_end(self, ctx):\n        \"\"\"\n        Note:\n          The modified attributes and according operations are shown below:\n            ==================================  ===========================\n            Attribute                           Operation\n            ==================================  ===========================\n            ``ctx.num_samples``                 Add ``ctx.batch_size``\n            ``ctx.loss_batch_total``            Add batch loss\n            ``ctx.loss_regular_total``          Add batch regular loss\n            ``ctx.ys_true``                     Append ``ctx.y_true``\n            ``ctx.ys_prob``                     Append ``ctx.ys_prob``\n            ==================================  ===========================\n        \"\"\"\n        # update statistics\n        ctx.num_samples += ctx.batch_size\n        ctx.loss_batch_total += ctx.loss_batch.item() * ctx.batch_size\n        ctx.loss_regular_total += float(ctx.get(\"loss_regular\", 0.))\n        # cache label for evaluate\n        ctx.ys_true.append(ctx.y_true.detach().cpu().numpy())\n        ctx.ys_prob.append(ctx.y_prob.detach().cpu().numpy())\n\n    def _hook_on_fit_end(self, ctx):\n        \"\"\"\n        Evaluate metrics.\n\n        Note:\n          The modified attributes and according operations are shown below:\n            ==================================  ===========================\n            Attribute                           Operation\n            ==================================  ===========================\n            ``ctx.ys_true``                     Convert to ``numpy.array``\n            ``ctx.ys_prob``                     Convert to ``numpy.array``\n            ``ctx.monitor``                     Evaluate the results\n            ``ctx.eval_metrics``                Get evaluated results from \\\n            ``ctx.monitor``\n            ==================================  ===========================\n        \"\"\"\n        ctx.ys_true = CtxVar(np.concatenate(ctx.ys_true), LIFECYCLE.ROUTINE)\n        ctx.ys_prob = CtxVar(np.concatenate(ctx.ys_prob), LIFECYCLE.ROUTINE)", "metadata": {"task_id": "alibaba_FederatedScope/83", "ground_truth": "        results = ctx.monitor.eval(ctx)", "fpath_tuple": ["alibaba_FederatedScope", "federatedscope", "core", "trainers", "torch_trainer.py"], "context_start_lineno": 313, "line_no": 402}}
{"prompt": "\nfrom fortuna.model.mlp import MLP\nfrom fortuna.output_calibrator.regression import RegressionTemperatureScaler\nfrom fortuna.prob_model.classification import ProbClassifier\nfrom fortuna.prob_model.fit_config import FitConfig, FitMonitor\nfrom fortuna.prob_model.fit_config.optimizer import FitOptimizer\nfrom fortuna.prob_model.posterior.map.map_approximator import \\\n    MAPPosteriorApproximator\nfrom fortuna.prob_model.prior import IsotropicGaussianPrior\nfrom fortuna.prob_model.regression import ProbRegressor\nfrom tests.make_data import make_array_random_data\nfrom tests.make_model import MyModel\n\nlogging.basicConfig(level=logging.INFO)\n\n\ndef brier(dummy, p, y):\n    return brier_score(p, y)\n\n\ndef standard_error(m, v, y):\n    return jnp.sum((y - m) ** 2 / v) / m.shape[0]\n\n\nclass TestApproximations(unittest.TestCase):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.prob_class = ProbClassifier(\n            model=MyModel(2), prior=IsotropicGaussianPrior()\n        )\n\n        self.reg_input_shape = (3,)\n        self.reg_output_dim = 2\n        self.class_input_shape = (2,)\n        self.class_output_dim = 2\n        bs = 32\n        x, y = make_array_random_data(\n            n_data=100,\n            shape_inputs=self.reg_input_shape,\n            output_dim=self.reg_output_dim,\n            output_type=\"continuous\",\n        )\n        x /= x.max(0)\n        y /= y.max(0)\n        reg_train_data = x, y\n        reg_val_data = make_array_random_data(\n            n_data=100,\n            shape_inputs=self.reg_input_shape,\n            output_dim=self.reg_output_dim,\n            output_type=\"continuous\",\n        )\n        reg_train_data = [\n            (reg_train_data[0][i : i + bs], reg_train_data[1][i : i + bs])\n            for i in range(0, len(reg_train_data[0]), bs)\n        ]\n        reg_val_data = [\n            (reg_val_data[0][i : i + bs], reg_val_data[1][i : i + bs])\n            for i in range(0, len(reg_val_data[0]), bs)\n        ]\n        self.reg_train_data_loader = DataLoader.from_iterable(reg_train_data)\n        self.reg_val_data_loader = DataLoader.from_iterable(reg_val_data)\n\n        class_train_data = make_array_random_data(\n            n_data=100,\n            shape_inputs=self.class_input_shape,\n            output_dim=self.class_output_dim,\n            output_type=\"discrete\",\n        )\n        class_val_data = make_array_random_data(\n            n_data=100,\n            shape_inputs=self.class_input_shape,\n            output_dim=self.class_output_dim,\n            output_type=\"discrete\",\n        )\n        class_train_data = [\n            (class_train_data[0][i : i + bs], class_train_data[1][i : i + bs])\n            for i in range(0, len(class_train_data[0]), bs)\n        ]\n        class_val_data = [\n            (class_val_data[0][i : i + bs], class_val_data[1][i : i + bs])\n            for i in range(0, len(class_val_data[0]), bs)\n        ]", "metadata": {"task_id": "awslabs_fortuna/114", "ground_truth": "        self.class_train_data_loader = DataLoader.from_iterable(class_train_data)", "fpath_tuple": ["awslabs_fortuna", "tests", "fortuna", "calib_model", "test_calibrate.py"], "context_start_lineno": 14, "line_no": 96}}
{"prompt": "_b(cfg_other, self, self, [])\n        self.__cfg_check_funcs__.clear()\n        self.__cfg_check_funcs__.extend(cfg_check_funcs)\n        self.assert_cfg(check_cfg)\n        set_help_info(self, self.__help_info__)\n\n    def merge_from_list(self, cfg_list, check_cfg=True):\n        \"\"\"\n        load configs from a list stores the keys and values. \\\n        modified ``merge_from_list`` in ``yacs.config.py`` to allow adding \\\n        new keys if ``is_new_allowed()`` returns True \\\n\n        Args:\n            cfg_list: list of pairs of cfg name and value\n            check_cfg: whether enable ``assert_cfg()``\n        \"\"\"\n        cfg_check_funcs = copy.copy(self.__cfg_check_funcs__)\n        super().merge_from_list(cfg_list)\n        self.__cfg_check_funcs__.clear()\n        self.__cfg_check_funcs__.extend(cfg_check_funcs)\n        self.assert_cfg(check_cfg)\n        set_help_info(self, self.__help_info__)\n\n    def assert_cfg(self, check_cfg=True):\n        \"\"\"\n        check the validness of the configuration instance\n\n        Args:\n            check_cfg: whether enable checks\n        \"\"\"\n        if check_cfg:\n            for check_func in self.__cfg_check_funcs__:\n                check_func(self)\n\n    def clean_unused_sub_cfgs(self):\n        \"\"\"\n        Clean the un-used secondary-level CfgNode, whose ``.use`` \\\n        attribute is ``True``\n        \"\"\"\n        for v in self.values():\n            if isinstance(v, CfgNode) or isinstance(v, CN):\n                # sub-config\n                if hasattr(v, \"use\") and v.use is False:\n                    for k in copy.deepcopy(v).keys():\n                        # delete the un-used attributes\n                        if k == \"use\":\n                            continue\n                        else:\n                            del v[k]\n\n    def check_required_args(self):\n        \"\"\"\n        Check required arguments.\n        \"\"\"\n        for k, v in self.items():\n            if isinstance(v, CN):\n                v.check_required_args()\n            if isinstance(v, Argument) and v.required and v.value is None:\n                logger.warning(f\"You have not set the required argument {k}\")\n\n    def de_arguments(self):\n        \"\"\"\n        some config values are managed via ``Argument`` class, this function \\\n        is used to make these values clean without the ``Argument`` class, \\\n        such that the potential type-specific methods work correctly, \\\n        e.g., ``len(cfg.federate.method)`` for a string config\n        \"\"\"\n        for k, v in copy.deepcopy(self).items():\n            if isinstance(v, CN):\n                self[k].de_arguments()\n            if isinstance(v, Argument):\n                self[k] = v.value\n\n    def ready_for_run(self, check_cfg=True):\n        \"\"\"\n        Check and cleans up the internal state of cfg and save cfg.\n\n        Args:\n            check_cfg: whether enable ``assert_cfg()``\n        \"\"\"\n        self.assert_cfg(check_cfg)\n        self.clean_unused_sub_cfgs()\n        self.check_required_args()\n        self.de_arguments()\n        self.is_ready_for_run = True\n\n    def freeze(self, inform=True, save=True, check_cfg=True):\n        \"\"\"\n        (1) make the cfg attributes immutable;\n        (2) if ``save==True``, save the frozen cfg_check_funcs into \\\n            ``self.outdir/config.yaml`` for better reproducibility;\n        (3) if ``self.wandb.use==True``, update the frozen config\n        \"\"\"\n        self.ready_for_run(check_cfg)", "metadata": {"task_id": "alibaba_FederatedScope/133", "ground_truth": "        super(CN, self).freeze()", "fpath_tuple": ["alibaba_FederatedScope", "federatedscope", "core", "configs", "config.py"], "context_start_lineno": 123, "line_no": 217}}
{"prompt": "# Copyright 2022 The HuggingFace Team. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom typing import List, Optional, Tuple, Union\n\nimport torch\n\nfrom...models import UNet2DModel\nfrom...schedulers import ScoreSdeVeScheduler\nfrom...utils import randn_tensor\nfrom..pipeline_utils import DiffusionPipeline, ImagePipelineOutput\n\n\nclass ScoreSdeVePipeline(DiffusionPipeline):\n    r\"\"\"\n    Parameters:\n    This model inherits from [`DiffusionPipeline`]. Check the superclass documentation for the generic methods the\n    library implements for all the pipelines (such as downloading or saving, running on a particular device, etc.)\n        unet ([`UNet2DModel`]): U-Net architecture to denoise the encoded image. scheduler ([`SchedulerMixin`]):\n            The [`ScoreSdeVeScheduler`] scheduler to be used in combination with `unet` to denoise the encoded image.\n    \"\"\"\n    unet: UNet2DModel\n    scheduler: ScoreSdeVeScheduler\n\n    def __init__(self, unet: UNet2DModel, scheduler: DiffusionPipeline):\n        super().__init__()\n        self.register_modules(unet=unet, scheduler=scheduler)\n\n    @torch.no_grad()\n    def __call__(\n        self,\n        batch_size: int = 1,\n        num_inference_steps: int = 2000,\n        generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,\n        output_type: Optional[str] = \"pil\",\n        return_dict: bool = True,\n        **kwargs,\n    ) -> Union[ImagePipelineOutput, Tuple]:\n        r\"\"\"\n        Args:\n            batch_size (`int`, *optional*, defaults to 1):\n                The number of images to generate.\n            generator (`torch.Generator`, *optional*):\n                One or a list of [torch generator(s)](https://pytorch.org/docs/stable/generated/torch.Generator.html)\n                to make generation deterministic.\n            output_type (`str`, *optional*, defaults to `\"pil\"`):\n                The output format of the generate image. Choose between\n                [PIL](https://pillow.readthedocs.io/en/stable/): `PIL.Image.Image` or `np.array`.\n            return_dict (`bool`, *optional*, defaults to `True`):\n                Whether or not to return a [`~pipelines.ImagePipelineOutput`] instead of a plain tuple.\n\n        Returns:\n            [`~pipelines.ImagePipelineOutput`] or `tuple`: [`~pipelines.utils.ImagePipelineOutput`] if `return_dict` is\n            True, otherwise a `tuple. When returning a tuple, the first element is a list with the generated images.\n        \"\"\"\n\n        img_size = self.unet.config.sample_size\n        shape = (batch_size, 3, img_size, img_size)\n\n        model = self.unet\n\n        sample = randn_tensor(shape, generator=generator) * self.scheduler.init_noise_sigma", "metadata": {"task_id": "huggingface_diffusers/23", "ground_truth": "        sample = sample.to(self.device)", "fpath_tuple": ["huggingface_diffusers", "src", "diffusers", "pipelines", "score_sde_ve", "pipeline_score_sde_ve.py"], "context_start_lineno": 0, "line_no": 73}}
{"prompt": "qa\n        \"\"\"\n        Overview:\n            Init method.\n        Arguments:\n            - cfg (:obj:`EasyDict`): Config dict.\n        \"\"\"\n        BaseCommLearner.__init__(self, cfg)\n\n        # Callback functions for message passing between comm learner and coordinator.\n        self._callback_fn = {\n            'deal_with_resource': self.deal_with_resource,\n            'deal_with_learner_start': self.deal_with_learner_start,\n            'deal_with_get_data': self.deal_with_get_data,\n            'deal_with_learner_learn': self.deal_with_learner_learn,\n            'deal_with_learner_close': self.deal_with_learner_close,\n        }\n        # Learner slave to implement those callback functions. Host and port is used to build connection with master.\n        host, port = cfg.host, cfg.port\n        if isinstance(port, list):\n            port = port[self._rank]\n        elif isinstance(port, int) and self._world_size > 1:\n            port = port + self._rank\n        self._slave = LearnerSlave(host, port, callback_fn=self._callback_fn)\n\n        self._path_data = cfg.path_data  # path to read data from\n        self._path_policy = cfg.path_policy  # path to save policy\n\n        # Queues to store info dicts. Only one info is needed to pass between learner and coordinator at a time.\n        self._data_demand_queue = Queue(maxsize=1)\n        self._data_result_queue = Queue(maxsize=1)\n        self._learn_info_queue = Queue(maxsize=1)\n\n        # Task-level learner and policy will only be set once received the task.\n        self._learner = None\n        self._policy_id = None\n\n    def start(self) -> None:\n        \"\"\"\n        Overview:\n            Start comm learner itself and the learner slave.\n        \"\"\"\n        BaseCommLearner.start(self)\n        self._slave.start()\n\n    def close(self) -> None:\n        \"\"\"\n        Overview:\n            Join learner thread and close learner if still running.\n            Then close learner slave and comm learner itself.\n        \"\"\"\n        if self._end_flag:\n            return\n        if self._learner is not None:\n            self.deal_with_learner_close()\n        self._slave.close()\n        BaseCommLearner.close(self)\n\n    def __del__(self) -> None:\n        \"\"\"\n        Overview:\n            Call ``close`` for deletion.\n        \"\"\"\n        self.close()\n\n    def deal_with_resource(self) -> dict:\n        \"\"\"\n        Overview:\n            Callback function. Return how many resources are needed to start current learner.\n        Returns:\n            - resource (:obj:`dict`): Resource info dict, including [\"gpu\"].\n        \"\"\"\n        return {'gpu': self._world_size}\n\n    def deal_with_learner_start(self, task_info: dict) -> None:\n        \"\"\"\n        Overview:\n            Callback function. Create a learner and help register its hooks. Start a learner thread of the created one.\n        Arguments:\n            - task_info (:obj:`dict`): Task info dict.\n\n       .. note::\n            In ``_create_learner`` method in base class ``BaseCommLearner``, 3 methods\n            ('get_data','send_policy','send_learn_info'), dataloader and policy are set.\n            You can refer to it for details.\n        \"\"\"\n        self._policy_id = task_info['policy_id']\n        self._league_save_checkpoint_path = task_info.get('league_save_checkpoint_path', None)\n        self._learner = self._create_learner(task_info)\n        for h in self.hooks4call:\n            self._learner.register_hook(h)\n        self._learner_thread = Thread(target=self._learner.start, args=(), daemon=True, name='learner_start')", "metadata": {"task_id": "opendilab_ACE/76", "ground_truth": "        self._learner_thread.start()", "fpath_tuple": ["opendilab_ACE", "ding", "worker", "learner", "comm", "flask_fs_learner.py"], "context_start_lineno": 81, "line_no": 173}}
{"prompt": "\n\nimport torch\nfrom rich.console import Console\nfrom torch.cuda.amp.grad_scaler import GradScaler\nfrom typing_extensions import Literal\n\nfrom nerfstudio.configs.experiment_config import ExperimentConfig\nfrom nerfstudio.engine.callbacks import (\n    TrainingCallback,\n    TrainingCallbackAttributes,\n    TrainingCallbackLocation,\n)\nfrom nerfstudio.engine.optimizers import Optimizers\nfrom nerfstudio.pipelines.base_pipeline import VanillaPipeline\nfrom nerfstudio.utils import profiler, writer\nfrom nerfstudio.utils.decorators import (\n    check_eval_enabled,\n    check_main_thread,\n    check_viewer_enabled,\n)\nfrom nerfstudio.utils.misc import step_check\nfrom nerfstudio.utils.writer import EventName, TimeWriter\nfrom nerfstudio.viewer.server import viewer_utils\n\nCONSOLE = Console(width=120)\n\n\n@dataclass\nclass TrainerConfig(ExperimentConfig):\n    \"\"\"Configuration for training regimen\"\"\"\n\n    _target: Type = field(default_factory=lambda: Trainer)\n    \"\"\"target class to instantiate\"\"\"\n    steps_per_save: int = 1000\n    \"\"\"Number of steps between saves.\"\"\"\n    steps_per_eval_batch: int = 500\n    \"\"\"Number of steps between randomly sampled batches of rays.\"\"\"\n    steps_per_eval_image: int = 500\n    \"\"\"Number of steps between single eval images.\"\"\"\n    steps_per_eval_all_images: int = 25000\n    \"\"\"Number of steps between eval all images.\"\"\"\n    max_num_iterations: int = 1000000\n    \"\"\"Maximum number of iterations to run.\"\"\"\n    mixed_precision: bool = False\n    \"\"\"Whether or not to use mixed precision for training.\"\"\"\n    save_only_latest_checkpoint: bool = True\n    \"\"\"Whether to only save the latest checkpoint or all checkpoints.\"\"\"\n    # optional parameters if we want to resume training\n    load_dir: Optional[Path] = None\n    \"\"\"Optionally specify a pre-trained model directory to load from.\"\"\"\n    load_step: Optional[int] = None\n    \"\"\"Optionally specify model step to load from; if none, will find most recent model in load_dir.\"\"\"\n    load_config: Optional[Path] = None\n\n\nclass Trainer:\n    \"\"\"Trainer class\n\n    Args:\n        config: The configuration object.\n        local_rank: Local rank of the process.\n        world_size: World size of the process.\n\n    Attributes:\n        config: The configuration object.\n        local_rank: Local rank of the process.\n        world_size: World size of the process.\n        device: The device to run the training on.\n        pipeline: The pipeline object.\n        optimizers: The optimizers object.\n        callbacks: The callbacks object.\n    \"\"\"\n\n    pipeline: VanillaPipeline\n    optimizers: Optimizers\n    callbacks: List[TrainingCallback]\n\n    def __init__(self, config: TrainerConfig, local_rank: int = 0, world_size: int = 1):\n        self.config = config\n        self.local_rank = local_rank\n        self.world_size = world_size\n        self.device = \"cpu\" if world_size == 0 else f\"cuda:{local_rank}\"\n        self.mixed_precision = self.config.mixed_precision\n        if self.device == \"cpu\":\n            self.mixed_precision = False\n            CONSOLE.print(\"Mixed precision is disabled for CPU training.\")\n        self._start_step = 0\n        # optimizers\n        self.grad_scaler = GradScaler(enabled=self.mixed_precision)\n\n        self.base_dir = config.get_base_dir()\n        # directory to save checkpoints\n        self.checkpoint_dir = config.get_checkpoint_dir()\n        CONSOLE.log(f\"Saving checkpoints to: {self.checkpoint_dir}\")\n        # set up viewer if enabled\n        viewer_log_path = self.base_dir / config.viewer.relative_log_filename\n        self.viewer_state, banner_messages = None, None", "metadata": {"task_id": "nerfstudio-project_nerfstudio/34", "ground_truth": "        if self.config.is_viewer_enabled() and local_rank == 0:", "fpath_tuple": ["nerfstudio-project_nerfstudio", "nerfstudio", "engine", "trainer.py"], "context_start_lineno": 25, "line_no": 123}}
{"prompt": "in_keys=[\"action\", gSDE_state_key, \"_eps_gSDE\"],\n                out_keys=[\"loc\", \"scale\", \"action\", \"_eps_gSDE\"],\n            ),\n        )\n\n    actor = ProbabilisticActor(\n        spec=action_spec,\n        in_keys=[\"loc\", \"scale\"],\n        module=actor_module,\n        distribution_class=dist_class,\n        distribution_kwargs=dist_kwargs,\n        default_interaction_mode=\"random\",\n        return_log_prob=True,\n    )\n    qvalue = ValueOperator(\n        in_keys=in_keys_qvalue,\n        module=qvalue_net,\n    )\n    model = nn.ModuleList([actor, qvalue]).to(device)\n\n    # init nets\n    with torch.no_grad(), set_exploration_mode(\"random\"):\n        td = proof_environment.rollout(1000)\n        td = td.to(device)\n        for net in model:\n            net(td)\n    del td\n    return model\n\n\ndef make_dreamer(\n    cfg: \"DictConfig\",  # noqa: F821\n    proof_environment: EnvBase = None,\n    device: DEVICE_TYPING = \"cpu\",\n    action_key: str = \"action\",\n    value_key: str = \"state_value\",\n    use_decoder_in_env: bool = False,\n    obs_norm_state_dict=None,\n) -> nn.ModuleList:\n    \"\"\"Create Dreamer components.\n\n    Args:\n        cfg (DictConfig): Config object.\n        proof_environment (EnvBase): Environment to initialize the model.\n        device (DEVICE_TYPING, optional): Device to use.\n            Defaults to \"cpu\".\n        action_key (str, optional): Key to use for the action.\n            Defaults to \"action\".\n        value_key (str, optional): Key to use for the value.\n            Defaults to \"state_value\".\n        use_decoder_in_env (bool, optional): Whether to use the decoder in the model based dreamer env.\n            Defaults to False.\n        obs_norm_state_dict (dict, optional): the state_dict of the ObservationNorm transform used\n            when proof_environment is missing. Defaults to None.\n\n    Returns:\n        nn.TensorDictModel: Dreamer World model.\n        nn.TensorDictModel: Dreamer Model based environnement.\n        nn.TensorDictModel: Dreamer Actor the world model space.\n        nn.TensorDictModel: Dreamer Value model.\n        nn.TensorDictModel: Dreamer Actor for the real world space.\n\n    \"\"\"\n    proof_env_is_none = proof_environment is None\n    if proof_env_is_none:\n        proof_environment = transformed_env_constructor(\n            cfg=cfg, use_env_creator=False, obs_norm_state_dict=obs_norm_state_dict\n        )()\n\n    # Modules\n    obs_encoder = ObsEncoder()\n    obs_decoder = ObsDecoder()\n\n    rssm_prior = RSSMPrior(\n        hidden_dim=cfg.rssm_hidden_dim,\n        rnn_hidden_dim=cfg.rssm_hidden_dim,\n        state_dim=cfg.state_dim,\n        action_spec=proof_environment.action_spec,\n    )\n    rssm_posterior = RSSMPosterior(\n        hidden_dim=cfg.rssm_hidden_dim, state_dim=cfg.state_dim\n    )\n    reward_module = MLP(\n        out_features=1, depth=2, num_cells=cfg.mlp_num_units, activation_class=nn.ELU\n    )\n\n    world_model = _dreamer_make_world_model(\n        obs_encoder, obs_decoder, rssm_prior, rssm_posterior, reward_module\n    ).to(device)\n    with torch.no_grad(), set_exploration_mode(\"random\"):\n        tensordict = proof_environment.rollout(4)", "metadata": {"task_id": "pytorch_rl/196", "ground_truth": "        tensordict = tensordict.to_tensordict().to(device)", "fpath_tuple": ["pytorch_rl", "torchrl", "trainers", "helpers", "models.py"], "context_start_lineno": 1432, "line_no": 1523}}
{"prompt": "we need gather the loss before clean it)\n        self.register_hook_in_eval(\n            new_hook=self._hook_on_batch_start_track_batch_idx,\n            trigger=\"on_batch_start\",\n            insert_pos=0)  # insert at the front\n        # replace the original evaluation into the ensemble one\n        self.replace_hook_in_eval(new_hook=self._hook_on_fit_end_ensemble_eval,\n                                  target_trigger=\"on_fit_end\",\n                                  target_hook_name=\"_hook_on_fit_end\")\n\n        # Then for other models, set the same hooks as model 0\n        # since we differentiate different models in the hook\n        # implementations via ctx.cur_model_idx\n        self.hooks_in_train_multiple_models.extend([\n            self.hooks_in_train_multiple_models[0]\n            for _ in range(1, self.model_nums)\n        ])\n        self.hooks_in_eval_multiple_models.extend([\n            self.hooks_in_eval_multiple_models[0]\n            for _ in range(1, self.model_nums)\n        ])\n\n    def _hook_on_batch_start_track_batch_idx(self, ctx):\n        \"\"\"\n        Note:\n          The modified attributes and according operations are shown below:\n            ==================================  ===========================\n            Attribute                           Operation\n            ==================================  ===========================\n            ``ctx.optimizer_for_global_model``  False\n            ==================================  ===========================\n        \"\"\"\n        # for both train & eval\n        ctx.cur_batch_idx = (self.ctx.cur_batch_idx +\n                             1) % self.ctx.num_train_batch\n\n    def _hook_on_batch_forward_weighted_loss(self, ctx):\n        \"\"\"\n        Note:\n          The modified attributes and according operations are shown below:\n            ==================================  ===========================\n            Attribute                           Operation\n            ==================================  ===========================\n            ``ctx.loss_batch``                  Multiply by \\\n            ``weights_internal_models``\n            ==================================  ===========================\n        \"\"\"\n        # for only train\n        ctx.loss_batch *= self.weights_internal_models[ctx.cur_model_idx]\n\n    def _hook_on_batch_end_gather_loss(self, ctx):\n        \"\"\"\n        Note:\n          The modified attributes and according operations are shown below:\n            ==================================  ===========================\n            Attribute                           Operation\n            ==================================  ===========================\n            ``ctx.all_losses_model_batch``      Gather loss\n            ==================================  ===========================\n        \"\"\"\n        # for only eval\n        # before clean the loss_batch; we record it\n        # for further weights_data_sample update\n        ctx.all_losses_model_batch[ctx.cur_model_idx][\n            ctx.cur_batch_idx] = ctx.loss_batch.item()\n\n    def _hook_on_fit_start_mixture_weights_update(self, ctx):\n        \"\"\"\n        Note:\n          The modified attributes and according operations are shown below:\n            ==================================  ===========================\n            Attribute                           Operation\n            ==================================  ===========================\n            ``ctx.mode``                        Evaluate\n            ==================================  ===========================\n        \"\"\"\n        # for only train\n        if ctx.cur_model_idx!= 0:\n            # do the mixture_weights_update once\n            pass\n        else:\n            # gathers losses for all sample in iterator\n            # for each internal model, calling `evaluate()`\n            for model_idx in range(self.model_nums):\n                self._switch_model_ctx(model_idx)\n                self.evaluate(target_data_split_name=\"train\")\n\n            self.weights_data_sample = f_softmax(\n                (torch.log(self.weights_internal_models) -\n                 ctx.all_losses_model_batch.T),\n                dim=1).T\n            self.weights_internal_models = self.weights_data_sample.mean(dim=1)\n\n            # restore the model_ctx", "metadata": {"task_id": "alibaba_FederatedScope/99", "ground_truth": "            self._switch_model_ctx(0)", "fpath_tuple": ["alibaba_FederatedScope", "federatedscope", "core", "trainers", "trainer_FedEM.py"], "context_start_lineno": 86, "line_no": 180}}
{"prompt": " for s in env_manager._stat)\n        # Test arribute\n        with pytest.raises(AttributeError):\n            _ = env_manager.xxx\n        with pytest.raises(RuntimeError):\n            env_manager.user_defined()\n        # Test step\n        count = 1\n        start_time = time.time()\n        while not env_manager.done:\n            env_id = env_manager.ready_obs.keys()\n            action = {i: np.random.randn(4) for i in env_id}\n            timestep = env_manager.step(action)\n            assert len(timestep) == len(env_id)\n            print('Count {}'.format(count))\n            print([v.info for v in timestep.values()])\n            print([v.done for v in timestep.values()])\n            count += 1\n        end_time = time.time()\n        print('total step time: {}'.format(end_time - start_time))\n        assert all([env_manager._env_states[env_id] == EnvState.DONE for env_id in range(env_manager.env_num)])\n        assert all([c == setup_base_manager_cfg.episode_num for c in env_manager._env_episode_count.values()])\n        # Test close\n        env_manager.close()\n        assert env_manager._closed\n        assert all([not env_manager._envs[env_id]._launched for env_id in range(env_manager.env_num)])\n        assert all([env_manager._env_states[env_id] == EnvState.VOID for env_id in range(env_manager.env_num)])\n        with pytest.raises(AssertionError):\n            env_manager.reset([])\n        with pytest.raises(AssertionError):\n            env_manager.step([])\n\n    def test_error(self, setup_base_manager_cfg):\n        env_fn = setup_base_manager_cfg.pop('env_fn')\n        env_manager = BaseEnvManager(env_fn, setup_base_manager_cfg)\n        # Test reset error\n        with pytest.raises(RuntimeError):\n            reset_param = {i: {'stat': 'error'} for i in range(env_manager.env_num)}\n            obs = env_manager.launch(reset_param=reset_param)\n        assert env_manager._closed\n        reset_param = {i: {'stat':'stat_test'} for i in range(env_manager.env_num)}\n        obs = env_manager.launch(reset_param=reset_param)\n        assert not env_manager._closed\n\n        timestep = env_manager.step({i: np.random.randn(4) for i in range(env_manager.env_num)})\n        assert len(timestep) == env_manager.env_num\n\n        # Test step catched error\n        action = {i: np.random.randn(4) for i in range(env_manager.env_num)}\n        action[0] = 'catched_error'\n        timestep = env_manager.step(action)\n        assert timestep[0].info['abnormal']\n        assert all(['abnormal' not in timestep[i].info for i in range(1, env_manager.env_num)])\n        assert all([env_manager._env_states[i] == EnvState.RUN for i in range(env_manager.env_num)])\n        assert len(env_manager.ready_obs) == 4\n        timestep = env_manager.step({i: np.random.randn(4) for i in range(env_manager.env_num)})\n        # Test step error\n        action[0] = 'error'\n        with pytest.raises(RuntimeError):\n            timestep = env_manager.step(action)\n        assert env_manager._env_states[0] == EnvState.ERROR\n        assert all([env_manager._env_states[i] == EnvState.RUN for i in range(1, env_manager.env_num)])", "metadata": {"task_id": "opendilab_ACE/137", "ground_truth": "        obs = env_manager.reset(reset_param)", "fpath_tuple": ["opendilab_ACE", "ding", "envs", "env_manager", "tests", "test_base_env_manager.py"], "context_start_lineno": 29, "line_no": 91}}
{"prompt": "name = instance_name\n        self._collect_print_freq = cfg.collect_print_freq\n        self._deepcopy_obs = cfg.deepcopy_obs\n        self._transform_obs = cfg.transform_obs\n        self._cfg = cfg\n        self._timer = EasyTimer()\n        self._end_flag = False\n\n        if tb_logger is not None:\n            self._logger, _ = build_logger(\n                path='./{}/log/{}'.format(self._exp_name, self._instance_name), name=self._instance_name, need_tb=False\n            )\n            self._tb_logger = tb_logger\n        else:\n            self._logger, self._tb_logger = build_logger(\n                path='./{}/log/{}'.format(self._exp_name, self._instance_name), name=self._instance_name\n            )\n        self.reset(policy, env)\n\n    def reset_env(self, _env: Optional[BaseEnvManager] = None) -> None:\n        \"\"\"\n        Overview:\n            Reset the environment.\n            If _env is None, reset the old environment.\n            If _env is not None, replace the old environment in the collector with the new passed \\\n                in environment and launch.\n        Arguments:\n            - env (:obj:`Optional[BaseEnvManager]`): instance of the subclass of vectorized \\\n                env_manager(BaseEnvManager)\n        \"\"\"\n        if _env is not None:\n            self._env = _env\n            self._env.launch()\n            self._env_num = self._env.env_num\n        else:\n            self._env.reset()\n\n    def reset_policy(self, _policy: Optional[namedtuple] = None) -> None:\n        \"\"\"\n        Overview:\n            Reset the policy.\n            If _policy is None, reset the old policy.\n            If _policy is not None, replace the old policy in the collector with the new passed in policy.\n        Arguments:\n            - policy (:obj:`Optional[namedtuple]`): the api namedtuple of collect_mode policy\n        \"\"\"\n        assert hasattr(self, '_env'), \"please set env first\"\n        if _policy is not None:\n            self._policy = _policy\n            self._default_n_sample = _policy.get_attribute('cfg').collect.get('n_sample', None)\n            self._unroll_len = _policy.get_attribute('unroll_len')\n            self._on_policy = _policy.get_attribute('on_policy')\n            if self._default_n_sample is not None:\n                self._traj_len = max(\n                    self._unroll_len,\n                    self._default_n_sample // self._env_num + int(self._default_n_sample % self._env_num!= 0)\n                )\n                self._logger.debug(\n                    'Set default n_sample mode(n_sample({}), env_num({}), traj_len({}))'.format(\n                        self._default_n_sample, self._env_num, self._traj_len\n                    )\n                )\n            else:\n                self._traj_len = INF\n        self._policy.reset()\n\n    def reset(self, _policy: Optional[namedtuple] = None, _env: Optional[BaseEnvManager] = None) -> None:\n        \"\"\"\n        Overview:\n            Reset the environment and policy.\n            If _env is None, reset the old environment.\n            If _env is not None, replace the old environment in the collector with the new passed \\\n                in environment and launch.\n            If _policy is None, reset the old policy.\n            If _policy is not None, replace the old policy in the collector with the new passed in policy.\n        Arguments:\n            - policy (:obj:`Optional[namedtuple]`): the api namedtuple of collect_mode policy\n            - env (:obj:`Optional[BaseEnvManager]`): instance of the subclass of vectorized \\\n                env_manager(BaseEnvManager)\n        \"\"\"\n        if _env is not None:", "metadata": {"task_id": "opendilab_ACE/174", "ground_truth": "            self.reset_env(_env)", "fpath_tuple": ["opendilab_ACE", "ding", "worker", "collector", "sample_serial_collector.py"], "context_start_lineno": 46, "line_no": 127}}
{"prompt": "mark.parametrize(\n        \"shape1\",\n        [\n            None,\n            (),\n            (5,),\n        ],\n    )\n    @pytest.mark.parametrize(\"shape2\", [(), (10,)])\n    def test_multidiscrete(self, shape1, shape2):\n        if shape1 is None:\n            shape1 = (3,)\n        else:\n            shape1 = (*shape1, 3)\n        spec = MultiDiscreteTensorSpec(\n            nvec=(4, 5, 6), shape=shape1, device=\"cpu\", dtype=torch.long\n        )\n        if shape1 is not None:\n            shape2_real = (*shape2, *shape1)\n        else:\n            shape2_real = shape2\n\n        spec2 = spec.expand(shape2_real)\n        assert spec2 is not spec\n        assert spec2.dtype == spec.dtype\n        assert (spec2.zero() == spec.zero()).all()\n        assert spec2.rand().shape == spec2.shape\n        assert spec2.zero().shape == spec2.shape\n        spec2 = spec.expand(*shape2_real)\n        assert spec2 is not spec\n        assert spec2.dtype == spec.dtype\n        assert (spec2.zero() == spec.zero()).all()\n        assert spec2.rand().shape == spec2.shape\n        assert spec2.zero().shape == spec2.shape\n\n    @pytest.mark.parametrize(\n        \"shape1\",\n        [\n            None,\n            (),\n            (5,),\n        ],\n    )\n    @pytest.mark.parametrize(\"shape2\", [(), (10,)])\n    def test_multionehot(self, shape1, shape2):\n        if shape1 is None:\n            shape1 = (15,)\n        else:\n            shape1 = (*shape1, 15)\n        spec = MultiOneHotDiscreteTensorSpec(\n            nvec=(4, 5, 6), shape=shape1, device=\"cpu\", dtype=torch.long\n        )\n        if shape1 is not None:\n            shape2_real = (*shape2, *shape1)\n        else:\n            shape2_real = shape2\n\n        spec2 = spec.expand(shape2_real)\n        assert spec2 is not spec\n        assert spec2.dtype == spec.dtype\n        assert (spec2.zero() == spec.zero()).all()\n        assert spec2.rand().shape == spec2.shape\n        assert spec2.zero().shape == spec2.shape\n        spec2 = spec.expand(*shape2_real)\n        assert spec2 is not spec\n        assert spec2.dtype == spec.dtype\n        assert (spec2.zero() == spec.zero()).all()\n        assert spec2.rand().shape == spec2.shape\n        assert spec2.zero().shape == spec2.shape\n\n    @pytest.mark.parametrize(\n        \"shape1\",\n        [\n            None,\n            (),\n            (5,),\n        ],\n    )\n    @pytest.mark.parametrize(\"shape2\", [(), (10,)])\n    def test_onehot(self, shape1, shape2):\n        if shape1 is None:\n            shape1 = (15,)\n        else:\n            shape1 = (*shape1, 15)\n        spec = OneHotDiscreteTensorSpec(\n            n=15, shape=shape1, device=\"cpu\", dtype=torch.long\n        )\n        if shape1 is not None:\n            shape2_real = (*shape2, *shape1)\n        else:\n            shape2_real = shape2\n\n        spec2 = spec.expand(shape2_real)\n        assert spec2 is not spec\n        assert spec2.dtype == spec.dtype\n        assert (spec2.zero() == spec.zero()).all()\n        assert spec2.rand().shape == spec2.shape\n        assert spec2.zero().shape == spec2.shape\n        spec2 = spec.expand(*shape2_real)\n        assert spec2 is not spec\n        assert spec2.dtype == spec.dtype", "metadata": {"task_id": "pytorch_rl/112", "ground_truth": "        assert (spec2.zero() == spec.zero()).all()", "fpath_tuple": ["pytorch_rl", "test", "test_specs.py"], "context_start_lineno": 1277, "line_no": 1378}}
