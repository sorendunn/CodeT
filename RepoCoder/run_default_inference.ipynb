{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Subsets for Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "# Define the seed for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "# Define the fraction of lines you want in the random subset\n",
    "FRACTION = 0.1\n",
    "\n",
    "def create_random_subset_jsonl(input_filepath, output_filepath, seed):\n",
    "    random.seed(seed)  # Set the seed for reproducible results\n",
    "    lines = []\n",
    "\n",
    "    # Step 1: Read the original JSONL file and store the lines\n",
    "    with open(input_filepath, 'r') as infile:\n",
    "        for line in infile:\n",
    "            lines.append(line.strip())  # Strip to remove the newline at the end\n",
    "\n",
    "    # Step 2: Randomly select a subset of the lines\n",
    "    subset_size = int(FRACTION * len(lines))\n",
    "    random_subset = random.sample(lines, subset_size)\n",
    "\n",
    "    # Step 3: Write the random subset to the output JSONL file\n",
    "    with open(output_filepath, 'w') as outfile:\n",
    "        for line in random_subset:\n",
    "            outfile.write(line + '\\n')  # Add a newline at the end\n",
    "\n",
    "# Replace with your input and output file paths\n",
    "base_jsonl_name = \"line_level_completion_2k_context_codegen.test\"\n",
    "input_jsonl_fp = 'datasets/' + base_jsonl_name + '.jsonl'\n",
    "output_jsonl_fp = 'subsets/' + base_jsonl_name + \"_\" + str(FRACTION) + '.jsonl'\n",
    "\n",
    "# Create a random subset JSONL file\n",
    "create_random_subset_jsonl(input_jsonl_fp, output_jsonl_fp, RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        timesteps = np.array([timesteps] * batch_size * num_images_per_prompt)\n",
      "\n",
      "        # add noise to latents using the timesteps\n",
      "        noise = generator.randn(*init_latents.shape).astype(latents_dtype)\n",
      "        init_latents = self.scheduler.add_noise(\n",
      "            torch.from_numpy(init_latents), torch.from_numpy(noise), torch.from_numpy(timesteps)\n",
      "        )\n",
      "        init_latents = init_latents.numpy()\n",
      "\n",
      "        # prepare extra kwargs for the scheduler step, since not all schedulers have the same signature\n",
      "        # eta (?) is only used with the DDIMScheduler, it will be ignored for other schedulers.\n",
      "        # eta corresponds to? in DDIM paper: https://arxiv.org/abs/2010.02502\n",
      "        # and should be between [0, 1]\n",
      "        accepts_eta = \"eta\" in set(inspect.signature(self.scheduler.step).parameters.keys())\n",
      "        extra_step_kwargs = {}\n",
      "        if accepts_eta:\n",
      "            extra_step_kwargs[\"eta\"] = eta\n",
      "\n",
      "        latents = init_latents\n",
      "\n",
      "        t_start = max(num_inference_steps - init_timestep + offset, 0)\n",
      "        timesteps = self.scheduler.timesteps[t_start:].numpy()\n",
      "\n",
      "        for i, t in enumerate(self.progress_bar(timesteps)):\n",
      "            # expand the latents if we are doing classifier free guidance\n",
      "            latent_model_input = np.concatenate([latents] * 2) if do_classifier_free_guidance else latents\n",
      "            latent_model_input = self.scheduler.scale_model_input(latent_model_input, t)\n",
      "\n",
      "            # predict the noise residual\n",
      "            noise_pred = self.unet(\n",
      "                sample=latent_model_input, timestep=np.array([t]), encoder_hidden_states=prompt_embeds\n",
      "            )[0]\n",
      "\n",
      "            # perform guidance\n",
      "            if do_classifier_free_guidance:\n",
      "                noise_pred_uncond, noise_pred_text = np.split(noise_pred, 2)\n",
      "                noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
      "\n",
      "            # compute the previous noisy sample x_t -> x_t-1\n",
      "            latents = self.scheduler.step(\n",
      "                torch.from_numpy(noise_pred), t, torch.from_numpy(latents), **extra_step_kwargs\n",
      "            ).prev_sample\n",
      "\n",
      "            latents = latents.numpy()\n",
      "\n",
      "            init_latents_proper = self.scheduler.add_noise(\n",
      "                torch.from_numpy(init_latents_orig), torch.from_numpy(noise), torch.from_numpy(np.array([t]))\n",
      "            )\n",
      "\n",
      "            init_latents_proper = init_latents_proper.numpy()\n",
      "\n",
      "            latents = (init_latents_proper * mask) + (latents * (1 - mask))\n",
      "\n",
      "            # call the callback, if provided\n",
      "            if callback is not None and i % callback_steps == 0:\n",
      "                callback(i, t, latents)\n",
      "\n",
      "        latents = 1 / 0.18215 * latents\n",
      "        # image = self.vae_decoder(latent_sample=latents)[0]\n",
      "        # it seems likes there is a strange result for using half-precision vae decoder if batchsize>1\n",
      "        image = np.concatenate(\n",
      "            [self.vae_decoder(latent_sample=latents[i : i + 1])[0] for i in range(latents.shape[0])]\n",
      "        )\n",
      "\n",
      "        image = np.clip(image / 2 + 0.5, 0, 1)\n",
      "        image = image.transpose((0, 2, 3, 1))\n",
      "\n",
      "        if self.safety_checker is not None:\n",
      "            safety_checker_input = self.feature_extractor(\n"
     ]
    }
   ],
   "source": [
    "## View Prompt Examples\n",
    "import json\n",
    "\n",
    "# Function to read a JSON Lines file and return the prompt of a desired line\n",
    "def get_prompt_at_line(jsonl_file_path, desired_line_no):\n",
    "    with open(jsonl_file_path, 'r') as file:\n",
    "        for line_no, line in enumerate(file, start=1):\n",
    "            if line_no == desired_line_no:\n",
    "                json_object = json.loads(line)\n",
    "                prompt = json_object.get('prompt', None)\n",
    "                return prompt\n",
    "    return None  # Return None if the desired line was not found\n",
    "\n",
    "# Specify the .jsonl file path and the desired line number\n",
    "jsonl_file_path = 'datasets/api_level_completion_1k_context_codegen.test.jsonl'\n",
    "desired_line_no = 5  # For example, we want the prompt at line 10\n",
    "\n",
    "# Get the prompt at the desired line\n",
    "prompt_at_desired_line = get_prompt_at_line(jsonl_file_path, desired_line_no)\n",
    "\n",
    "if prompt_at_desired_line:\n",
    "    print(prompt_at_desired_line)\n",
    "else:\n",
    "    print(f\"No prompt found at line {desired_line_no}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Inference with the OpenAI API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess the completion string\n",
    "def preprocess_completion(completion):\n",
    "    # Extract content within ``\n",
    "    if '```python' in completion:\n",
    "        start = completion.find('```python') + 9  # Start index of content inside ``\n",
    "        end = completion.rfind('```')  # End index of content inside ``\n",
    "        completion = completion[start:end]\n",
    "    elif '```' in completion:\n",
    "        start = completion.find('```') + 3  # Start index of content inside ``\n",
    "        end = completion.rfind('```')  # End index of content inside ``\n",
    "        completion = completion[start:end]\n",
    "    elif '`' in completion:\n",
    "        start = completion.find('`') + 1  # Start index of content inside ``\n",
    "        end = completion.rfind('`')  # End index of content inside ``\n",
    "        completion = completion[start:end]\n",
    "\n",
    "    # Remove lines starting with '#'\n",
    "    lines = [line.split('#', 1)[0].rstrip() for line in completion.split('\\n')]\n",
    "    # Save only the first non-empty line\n",
    "    for line in lines:\n",
    "        if line.strip():\n",
    "            return line\n",
    "    return \"\"  # Return empty string if no non-empty lines are found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'safety_checker_input = self.feature_extractor(image)'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_completion('''\n",
    "## Assistant:\n",
    "\n",
    "`\n",
    "safety_checker_input = self.feature_extractor(image)\n",
    "safety_score = self.safety_checker(safety_checker_input)\n",
    "`''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import json\n",
    "\n",
    "client = openai.OpenAI()\n",
    "\n",
    "# Set OpenAI API key\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Input and output JSONL file paths\n",
    "base_jsonl_name = \"line_level_completion_2k_context_codegen.test_0.1\"\n",
    "\n",
    "# Original and preprocessed JSONL file paths\n",
    "original_responses_path = 'raw_generations/' + base_jsonl_name + '_raw_generations.jsonl'\n",
    "input_jsonl_file_path = 'subsets/' + base_jsonl_name + '.jsonl'\n",
    "output_jsonl_file_path = \"processed_generations/\" + base_jsonl_name + \"_generations.jsonl\"\n",
    "\n",
    "# Predefined system message\n",
    "system_message = '''Return your proposed next line completion inside of a code block\n",
    "\n",
    "```python\n",
    "YOUR_CODE_HERE\n",
    "```'''\n",
    "\n",
    "# Function to process a single JSONL entry\n",
    "def process_entry(entry, model_name=\"gpt-3.5-turbo-0613\"):\n",
    "    prompt = entry['prompt']\n",
    "    messages = [{\"role\": \"system\", \"content\": system_message}, {\"role\": \"user\", \"content\": prompt}]\n",
    "    \n",
    "    # Generate the completion with the OpenAI API\n",
    "    response = client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=messages,\n",
    "        max_tokens=250,  # Limit the number of generated tokens (adjust as needed)\n",
    "        temperature=1,  # Adjust for creativity of the response\n",
    "        seed = 1,\n",
    "        n=1,  # Number of completions to generate\n",
    "    )\n",
    "    \n",
    "    # Extract the text of the completion generated by the model\n",
    "    generated_completion = response.choices[0].message.content\n",
    "    return generated_completion\n",
    "\n",
    "# Read the input JSONL file and generate completions\n",
    "with open(input_jsonl_file_path, 'r') as input_file, open(original_responses_path, 'w') as original_file:\n",
    "    for line in input_file:\n",
    "        entry = json.loads(line.strip())\n",
    "        metadata = entry[\"metadata\"]\n",
    "        output_entry = process_entry(entry)\n",
    "        \n",
    "        # Save the original response to a new JSONL\n",
    "        original_file.write(json.dumps({\"prompt\": entry['prompt'], \"completion\": output_entry, \"metadata\" : metadata}) + \"\\n\")\n",
    "\n",
    "# Now, read the original responses JSONL, preprocess, and write back out to the second JSONL\n",
    "with open(original_responses_path, 'r') as original_file, open(output_jsonl_file_path, 'w') as output_file:\n",
    "    for line in original_file:\n",
    "        entry = json.loads(line.strip())\n",
    "        completion = entry['completion']\n",
    "        metadata = entry[\"metadata\"]\n",
    "        preprocessed_completion = preprocess_completion(completion)\n",
    "        \n",
    "        output_entry = {\n",
    "            \"prompt\": entry['prompt'],\n",
    "            \"choices\": [{\"text\": preprocessed_completion}],\n",
    "            \"metadata\": metadata\n",
    "        }\n",
    "        \n",
    "        output_file.write(json.dumps(output_entry) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Subsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use compute_score.py to evalaute "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
