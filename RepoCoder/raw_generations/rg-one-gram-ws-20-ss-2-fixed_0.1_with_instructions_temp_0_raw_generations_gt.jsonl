{"completion": "\"-Wno-deprecated-declarations\",\n            \"-Wno-unused-function\",\n            \"-Wno-write-strings\",\n            \"-Wno-unknown-pragmas\",\n            \"-Wno-unused-variable\",\n            \"-Wno-unused-result\",\n            \"-Wno-unused-but-set-variable\",\n            \"-Wno-strict-overflow\",\n            \"-Wno-format\",\n            \"-Wno-format-security\",\n            \"-Wno-maybe-uninitialized\",\n            \"-Wno-sign-compare\",\n            \"-Wno-strict-aliasing\",\n            \"-Wno-strict-overflow\",\n            \"-Wno-strict-prototypes\",\n            \"-Wno-unused-function\",\n            \"-Wno-unused-label\",\n            \"-Wno-unused-parameter\",\n            \"-Wno-unused-variable\",\n            \"-Wno-unused-value\",\n            \"-Wno-unused-but-set-variable\",\n            \"-Wno-unused-result\",\n            \"-Wno-unused-local-typedefs\",\n            \"-Wno-unused-local-typedefs\",\n            \"-Wno-unused-private-field\",\n            \"-Wno-unused-private-field\",\n            \"-Wno-unused-private-field\",\n            \"-Wno-unused-private-field\",\n            \"-Wno-unused-private-field\",\n            \"-Wno-unused-private-field\",\n            \"-Wno-unused", "ground_truth": "        ]"}
{"completion": "parser.add_argument(\n        \"--prior_generation_precision\",\n        type=str,\n        default=None,\n        choices=[\"no\", \"fp16\", \"bf16\"],\n        help=(\n            \"Whether to use mixed precision. Choose\"\n            \"between fp16 and bf16 (bfloat16). Bf16 requires PyTorch >= 1.10.\"\n            \"and an Nvidia Ampere GPU.\"\n        ),\n    )", "ground_truth": "    parser.add_argument("}
{"completion": "\"type {type(transform)}.\"", "ground_truth": "                f\"type {type(transform)} instead.\""}
{"completion": "get_collector_task, judge_collector_finish", "ground_truth": "        __init__, step"}
{"completion": ")\n                ),\n                lik_log_var=self.class_lik.model_manager.likelihood_log_variance_model.init(\n                    self.rng, jnp.zeros((1,) + self.shape_inputs)\n                ),\n            )\n        )\n\n        samples = self.class_lik.sample(10, params, self.class_inputs_arr)\n        assert samples.shape == (10, self.n_inputs, self.output_dim)", "ground_truth": "                ),"}
{"completion": "yield from self._targets_loader()", "ground_truth": "        yield from self._targets_loader()"}
{"completion": "prob_output_layer: ProbOutputLayer,\n        output_calib_manager: OutputCalibManager,\n    ):\n        \"\"\"\n        Likelihood function abstract class. It characterizes the distribution of the target variable given the model\n        parameters and inputs. It can be seen as :math:`p(y|w, x)`, where :math:`y` is a target variable, :math:`w` are\n        model parameters, and :math:`x` are inputs.\n\n        Parameters\n        ----------\n        model_manager : ModelManager\n            A model manager object.\n        prob_output_layer : ProbOutputLayer\n            A probabilistic output layer object.\n        output_calib_manager : OutputCalibManager\n            An output calibration manager object.\n        \"\"\"\n        self.model_manager = model_manager\n        self.prob_output_layer = prob_output_layer\n        self.output_calib_manager = output_calib_manager\n\n    def log_prob(\n        self,\n        params: Params,\n        inputs: Array,\n        targets: Array,\n        calibrated: bool = True,\n        **kwargs\n    ) -> jnp.ndarray:\n        \"\"\"\n        Compute the log probability of the targets given the inputs and model parameters.\n\n        Parameters\n        ----------\n        params : Params\n            Model", "ground_truth": "        prob_output_layer: ProbOutputLayer,"}
{"completion": "class ResConvBlock(nn.Module):\n    def __init__(self, in_channels, mid_channels, out_channels, is_last=False):\n        super().__init__()\n        self.is_last = is_last\n        self.has_conv_skip = in_channels != out_channels\n\n        if self.has_conv_skip:\n            self.conv_skip = nn.Conv1d(in_channels, out_channels, 1, bias=False)\n\n        self.conv_1 = nn.Conv1d(in_channels, mid_channels, 5, padding=2)\n        self.group_norm_1 = nn.GroupNorm(1, mid_channels)\n        self.gelu_1 = nn.GELU()\n        self.conv_2 = nn.Conv1d(mid_channels, out_channels, 5, padding=2)\n\n        if not self.is_last:\n            self.group_norm_2 = nn.GroupNorm(1, out_channels)\n            self.gelu_2 = nn.GELU()\n\n    def forward(self, hidden_states):\n        residual = self.conv_skip(hidden_states) if self.has_conv_skip else hidden_states\n\n        hidden_states = self.conv_1(hidden_states)\n        hidden_states = self.group_norm_1(hidden_states)\n        hidden_states = self.gelu_1(hidden_states)\n", "ground_truth": "            self.conv = conv"}
{"completion": "{\n                'a': 1,\n                'b': 2,\n                'sum': 3,\n            }\n        )\n\n        response = client.get('/fail')\n        assert response.status_code == 404\n        assert get_values_from_response(response) == (\n            404,\n            False,\n            233,\n            'This is failure message.',\n            {\n                'a': 2,\n                'b': 3,\n                'sum': 5,\n            }\n        )\n\n    def test_responsible_decorator(self):\n        app = Flask('_test_responsible_decorator')\n\n        @app.route('/success', methods=['GET'])\n        @responsible\n        def success_method():\n            return {\n                'a': 1,\n                'b': 2,\n                'sum': 3,\n            }\n\n        @app.route('/fail', methods=['GET'])\n        @responsible\n        def fail_method():\n            raise ResponsibleException(\n                code=233,\n                message='This is failure message.',\n                data={\n                    'a': 2,\n                    'b': 3,\n                    'sum': 5,\n                },\n            )\n\n        client = app.test_client()\n\n        response = client.get('/success')\n        assert response.status_code", "ground_truth": "            {"}
{"completion": "scheduler_config = self.get_scheduler_config()\n\n            scheduler = scheduler_class(**scheduler_config)\n\n            assert all(c is not None for c in scheduler.compatibles)\n\n            for comp_scheduler_cls in scheduler.compatibles:\n                comp_scheduler = comp_scheduler_cls.from_config(scheduler.config)\n                assert comp_scheduler is not None\n\n            new_scheduler = scheduler_class.from_config(comp_scheduler.config)\n\n            new_scheduler_config = {k: v for k, v in new_scheduler.config.items() if k in scheduler.config}\n            scheduler_diff = {k: v for k, v in new_scheduler.config.items() if k not in scheduler.config}\n\n            # make sure that configs are essentially identical\n            assert new_scheduler_config == dict(scheduler.config)\n\n            # make sure that only differences are for configs that are not in init\n            init_keys = inspect.signature(scheduler_class.__init__).parameters.keys()\n            assert set(scheduler_diff.keys()).intersection(set(init_keys)) == set()", "ground_truth": "            scheduler_config = self.get_scheduler_config()"}
{"completion": "import torch\nimport torch.distributed as dist\nimport torch.multiprocessing as mp\nimport torch.nn as nn", "ground_truth": "from functools import wraps"}
{"completion": "can be used for training directly. A train sample can be a processed transition(DQN with nstep TD) or some continuous transitions(DRQN).\n        Arguments:\n            - data (:obj:`List[Dict[str, Any]`): The trajectory data(a list of transition), each element is the same \\\n                format as the return value of ``self._process_transition`` method.\n        Returns:\n            - samples (:obj:`dict`): The list of training samples.\n\n        .. note::\n            We will vectorize ``process_transition`` and ``get_train_sample`` method in the following release version. \\\n            And the user can customize the this data processing procecure by overriding this two methods and collector \\\n            itself.", "ground_truth": "            can be used for training directly. A train sample can be a processed transition(DQN with nstep TD) \\"}
{"completion": "new_hook=hook_on_batch_start_inject_data,\n                                        trigger='on_batch_start',\n                                        insert_mode=-1)\n\n    base_trainer.register_hook_in_train(\n        new_hook=hook_on_batch_forward_inject_data,\n        trigger='on_batch_forward',\n        insert_mode=-1)\n\n    base_trainer.register_hook_in_train(\n        new_hook=hook_on_fit_end_save_data,\n        trigger='on_fit_end',\n        insert_mode=-1)\n\n    return base_trainer", "ground_truth": "        new_hook=hook_on_batch_start_replace_data_batch,"}
{"completion": "{\n    \"count\": BoundedTensorSpec(0, max_steps, shape=()),\n    \"done\": BoundedTensorSpec(0, 1, shape=())\n}", "ground_truth": "            observation=UnboundedContinuousTensorSpec("}
{"completion": "```python\n        if self.is_functional and params is None:\n            raise RuntimeError(\n                \"Expected params to be passed to advantage module but got none.\"\n            )\n        if params is not None:\n            kwargs[\"params\"] = params.detach()\n        with hold_out_net(self.value_network):\n            self.value_network(tensordict, **kwargs)\n            value = tensordict.get(self.value_key)\n```\n", "ground_truth": "        if self.is_functional and params is None:"}
{"completion": "validator=attr.validators.optional(\n          attr.validators.deep_iterable(\n              member_validator=attr.validators.instance_of((int, float, str)),\n              iterable_validator=attr.validators.instance_of((list, tuple)))),\n      kw_only=True)\n  _default_value: Optional[ParameterValueTypes] = attr.ib(\n      init=True,\n      validator=attr.validators.optional(\n          attr.validators.instance_of((int, float, str, bool))),\n      kw_only=True)\n  _scale_type: Optional[ScaleType] = attr.ib(\n      init=True,\n      validator=attr.validators.optional(\n          attr.validators.instance_of(ScaleType)),\n      repr=lambda v: v.name if v is not None else 'None',\n      kw_only=True)\n  _fidelity_config: Optional[FidelityConfig] = attr.ib(\n      init=True,\n      validator=attr.validators.optional(\n          attr.validators.instance_of(FidelityConfig)),\n      kw_only=True)\n  _description: Optional[str] = attr.ib(\n      init=True,\n      validator=attr.validators.optional(attr.validators.instance_of(str)),\n      kw_only=True)\n  _is_mutable: bool = attr.ib(\n      init=True,\n      default=True,\n      validator=attr.validators.instance_of(bool),\n      kw_only=True", "ground_truth": "      kw_only=True)"}
{"completion": "options = rpc.TensorPipeRpcBackendOptions(\n        num_worker_threads=16, init_method=str_init_method\n    )\n\n    if rank == 0:\n        # rank0 is the trainer\n        rpc.init_rpc(\n            AGENT_NAME,\n            rank=rank,\n            world_size=world_size,\n            backend=rpc.BackendType.TENSORPIPE,\n            rpc_backend_options=options,\n        )\n\n        if args.task == 0:\n            time.sleep(1)\n            t0 = time.time()\n            for w in range(1, args.world_size):\n                fut0 = rpc.rpc_async(f\"worker{w}\", get_tensordict, args=())\n                fut0.wait()\n                fut1 = rpc.rpc_async(f\"worker{w}\", tensordict_add, args=())\n                tensordict2 = fut1.wait()\n                tensordict2.clone()\n            print(\"time: \", time.time() - t0)\n        elif args.task == 1:\n            time.sleep(1)\n            t0 = time.time()", "ground_truth": "    if rank == 0:"}
{"completion": "```\n# Create Agent\n\n# Define Actor Network\nin_keys = [\"observation\"]\naction_spec = train_env.action_spec\nactor_net_kwargs = {\n    \"num_cells\": [256, 256],\n    \"out_features\": action_spec.shape[-1],\n    \"activation_class\": nn.ReLU,\n}\n\nactor_net = MLP(**actor_net_kwargs)\n\ndist_class = TanhDelta\ndist_kwargs = {\n    \"min\": action_spec.space.minimum,\n    \"max\": action_spec.space.maximum,\n    \"tanh_loc\": False,\n}\n\nin_keys_actor = in_keys\nactor_module = SafeModule(\n    actor_net,\n    in_keys=in_keys_actor,\n    out_keys=[\n        \"param\",\n    ],\n)\nactor = ProbabilisticActor(\n    spec=action_spec,\n    in_keys=[\"param\"],\n    module=actor_module,\n    distribution_class=dist_class,\n    distribution_kwargs=dist_kwargs,\n    default_interaction_mode=\"random\",\n    return_log_prob=False,\n)\n\n# Define Critic Network\nqvalue_net_kwargs = {\n    \"num_cells\": [256, 256],\n    \"out_features\": 1,\n    \"activation_class\": nn.ReLU,\n}\n\nqvalue_net = MLP(\n    **qvalue_net_kwargs,\n)\n\n", "ground_truth": "    del td"}
{"completion": "prohibited.add(\"done\")", "ground_truth": "        prohibited.add(\"done\")"}
{"completion": "calib_data_loader=calib_data_loader,\n            val_data_loader=val_data_loader,\n            calib_config=calib_config,\n        )", "ground_truth": "            calib_data_loader=calib_data_loader,"}
{"completion": "(\n                    validation_losses_and_metrics_current_epoch,\n                    validation_epoch_metrics_str,\n                ) = self._validation_loop(\n                    fun=fun,\n                    metrics=metrics,\n                    rng=rng,\n                    state=state,\n                    training_kwargs=training_kwargs,\n                    validation_dataloader=validation_dataloader,\n                    validation_dataset_size=validation_dataset_size,\n                    verbose=verbose,\n                    unravel=unravel,\n                )\n                if verbose:\n                    logging.info(\n                        f\"Epoch: {epoch + 1} | \" + validation_epoch_metrics_str\n                    )\n                # keep track of training losses and metrics [granularity=epoch] and check for early stopping", "ground_truth": "                ("}
{"completion": "cfg.federate.sample_client_num = participated_client_num", "ground_truth": "    non_sample_case = cfg.federate.method in [\"local\", \"global\"]"}
{"completion": "raise ModuleNotFoundError", "ground_truth": "        return \"missing\""}
{"completion": "self, tensordict: Optional[TensorDictBase] = None, **kwargs\n    ) -> TensorDictBase:\n\n        obs = self._env.reset(**kwargs)\n        obs_dict = self.read_obs(obs)\n\n        tensordict_out = TensorDict(\n            obs_dict, batch_size=tensordict.batch_size, device=self.device\n        )\n\n        tensordict_out.set(\"reward\", self.reward_spec.zero())\n        tensordict_out.set(\"done\", torch.zeros(tensordict.batch_size, dtype=torch.bool))\n        if self.info_dict_reader is not None:\n            self.info_dict_reader({}, tensordict_out)\n\n        return tensordict_out", "ground_truth": "        self, tensordict: Optional[TensorDictBase] = None, **kwargs"}
{"completion": "self.comm_manager.send(\n            Message(msg_type='eval_result',\n                    sender=self.ID,\n                    state=state,\n                    receiver=[self.client_id],\n                    content=self.msg_buffer['eval'][state]))\n\n    def callback_func_for_eval_metric(self, message: Message):\n        state = message.state\n        eval_metric = message.content\n        self.msg_buffer['eval'][state] = eval_metric\n        if len(self.msg_buffer['eval']) == self._cfg.model.num_of_trees:\n            self._check_and_save_result()\n\n    def callback_func_for_feature_importance(self, message: Message):\n        state = message.state\n        feature_importance = message.content\n        self.msg_buffer['feature_importance'][state] = feature_importance\n        if len(self.msg_buffer['feature_importance']) == self._cfg.model.num_of_trees:\n            self._check_and_save_result()\n\n    # Bind method to instance\n    server._check_and_save_result = types.MethodType(_check_and_save_result,\n                                                     server)\n    server.callback_func_for_eval_metric = types.MethodType(\n        callback_func_for_eval_metric, server)\n    server.callback_func_for_feature_importance = types.MethodType(\n        callback_func_for_feature_importance, server)\n\n    #", "ground_truth": "        buffer = self.msg_buffer['eval'][state]"}
{"completion": "conv: ModuleDef = nn.Conv", "ground_truth": "    conv: ModuleDef = nn.Conv"}
{"completion": "_init_collect, _forward_collect, _reset_collect, _process_transition, _init_eval, _forward_eval\\\n            _reset_eval, _get_train_sample, default_model\n    Config:\n        == ==================== ======== ============== ======================================== =======================\n        ID Symbol               Type     Default Value  Description                              Other(Shape)\n        == ==================== ======== ============== ======================================== =======================\n        1  ``type``             str      qtran          | RL policy register name, refer to      | this arg is optional,\n                                                        | registry ``POLICY_REGISTRY``           | a placeholder\n        2  ``cuda``             bool     True           | Whether to use cuda for network        | this arg can be diff-", "ground_truth": "            _init_collect, _forward_collect, _reset_collect, _process_transition, _init_eval, _forward_eval\\"}
{"completion": "f\"Invalid destination '{dest}', please choose a valid destination.\"", "ground_truth": "                f\"The hook collection {dest} is not recognised. Choose from:\""}
{"completion": "def test_impala():\n    config = [deepcopy(cartpole_impala_config), deepcopy(cartpole_impala_create_config)]\n    try:\n        serial_pipeline(config, seed=0)\n    except Exception:\n        assert False, \"pipeline fail\"\n    with open(\"./algo_record.log\", \"a+\") as f:\n        f.write(\"13. impala\\n\")", "ground_truth": "def test_atoc():"}
{"completion": "self._studies_table.c.study_name == study.name)\n    update_query = self._studies_table.update().where(\n        self._studies_table.c.study_name == study.name\n    ).values(serialized_study=study.SerializeToString())\n\n    with self._lock:\n        exists = self._connection.execute(exists_query).scalar()\n        if not exists:\n            raise custom_errors.NotFoundError(\n                'Study with name %s does not exist.' % study.name\n            )\n        self._connection.execute(update_query)\n        return study_resource", "ground_truth": "            self._studies_table.c.study_name == study.name"}
{"completion": "model must have been calibrated beforehand.\n\n        Returns\n        -------\n        jnp.ndarray\n            The estimated variance for each output.\n        \"\"\"\n        return super().variance(outputs, calibrated, **kwargs)\n\n    def std(\n        self,\n        outputs: jnp.ndarray,\n        variances: Optional[jnp.ndarray] = None,\n        calibrated: bool = True,\n    ) -> jnp.ndarray:\n        \"\"\"\n        Estimate the standard deviation of the target variable given the output, with respect to the predictive distribution.\n\n        Parameters\n        ----------\n        outputs : jnp.ndarray\n            Model outputs.\n        variances: Optional[jnp.ndarray]\n            The estimated variance for each output.\n        calibrated : bool\n            Whether the outputs should be calibrated when computing this method. If `calibrated` is set to True, the\n            model must have been calibrated beforehand.\n\n        Returns\n        -------\n        jnp.ndarray\n            The estimated standard deviation for each output.\n        \"\"\"\n        return super().std(outputs, variances, calibrated)", "ground_truth": "            model must have been calibrated beforehand."}
{"completion": "      int_max_value = int(max_value)\n    if not math.isclose(max_value, int_max_value):\n      raise ValueError('max_value for an INTEGER parameter should be an integer'\n                       ', got: [{}]'.format(max_value))\n\n    bounds = (int_min_value, int_max_value)\n    param_names = self._get_parameter_names_to_create(name=name, index=index)\n\n    new_params = []\n    for param_name in param_names:\n      new_pc = ParameterConfig.factory(\n          name=param_name,\n          bounds=bounds,\n          scale_type=scale_type,\n          default_value=default_value,\n          fidelity_config=experimental_fidelity_config)\n      new_params.append(new_pc)\n    return self._add_parameters(new_params)\n\n  def add_discrete_param(\n      self,\n      name: str,\n      feasible_values: Sequence[Union[int, float]],\n      *,\n      default_value: Optional[Union[int, float]] = None,\n      index: Optional[int] = None,\n      experimental_fidelity_config: Optional[FidelityConfig] = None,\n  ) -> 'ParameterConfigSelector':\n    \"\"\"Adds discrete parameter config(s) to the selected search space(s).\n\n    Args:\n      name: The parameter's name. Cannot be empty.\n", "ground_truth": "    int_max_value = int(max_value)"}
{"completion": "def reset(self, tensordict: TensorDictBase) -> TensorDictBase:\n    \"\"\"Resets the environment to the initial state.\n\n    Args:\n        tensordict (TensorDictBase): The initial state of the environment.\n\n    Returns:\n        TensorDictBase: The reset state of the environment.\n\n    \"\"\"\n    pass", "ground_truth": "        if \"_observation_spec\" not in self.__dir__():"}
{"completion": "val_data_loader=self.reg_val_data_loader,\n                fit_config=self.reg_fit_config_restore(tmp_dir),\n                calib_config=self.reg_calib_config_nodir_nodump,\n            )\n\n            # load state\n            prob_reg.load_state(checkpoint_path=tmp_dir)\n\n            # save state\n            prob_reg.save_state(checkpoint_path=tmp_dir)\n\n    def test_dryrun_class_advi(self):\n        with tempfile.TemporaryDirectory() as tmp_dir:\n            prob_class = ProbClassifier(\n                model=MyModel(self.class_output_dim),\n                posterior_approximator=ADVIPosteriorApproximator(),\n                output_calibrator=ClassificationTemperatureScaler(),\n            )\n            # no save dir, no dump\n            status = prob_class.train(\n                train_data_loader=self.class_train_data_loader,\n                calib_data_loader=self.class_val_data_loader,\n                val_data_loader=self.class_val_data_loader,\n                fit_config=self.class_fit_config_nodir_nodump,\n                calib_config=self.class_calib_config_nodir_nodump,\n            )\n            sample = prob_class.posterior.sample()\n\n            # no save dir but dump\n            with self.assertRaises(ValueError):\n                status = prob_class.train(\n                    train_data_loader=self.class_train_data_loader", "ground_truth": "                val_data_loader=self.reg_val_data_loader,"}
{"completion": "return tensordict[self.priority_key].flatten()", "ground_truth": "            tensordict = tensordict.clone(recurse=False)"}
{"completion": "```\n    def _async_loop(self, p: tm.multiprocessing.connection, c: tm.multiprocessing.connection) -> None:\n        \"\"\"\n        Overview:\n            Main worker process. Run through ``self.async_process``.\n            Firstly, get data from ``self.get_data_thread``.\n            If multiple workers, put data in ``self.job_queue`` for further multiprocessing operation;\n            If only one worker, process data and put directly into ``self.async_train_queue``.\n        Arguments:\n            - p (:obj:`tm.multiprocessing.connection`): Parent connection.\n            - c (:obj:`tm.multiprocessing.connection`): Child connection.\n        \"\"\"\n        p.close()  # Close unused p, only use c\n        while not self.end_flag:\n            data = self.get_data_thread.get()\n            if self.num_workers > 1:\n                self.job_queue.put(data)\n            else:\n                self.async_train_queue.put(data)\n        c.close()\n```", "ground_truth": "    def send_learn_info(self, learn_info: dict) -> None:"}
{"completion": "images = images.cpu().permute(0, 2, 3, 1).numpy()\n\nif return_dict:\n    return ImagePipelineOutput(images=images)\nelse:\n    return (images,)", "ground_truth": "        images = images.cpu().permute(0, 2, 3, 1).numpy()"}
{"completion": "Callable, Tuple, Optional, Any", "ground_truth": "import torch"}
{"completion": "self._algorithm.update(tuner_trial.dna, reward)\n    self._incorporated_trial_ids.add(tuner_trial.id)\n    return True\n\n  def recover(self, vizier_trials: Sequence[pg.tuning.Trial]) -> TunerPolicy:\n    \"\"\"Recover the tuner policy from a set of vizier trials.\n\n    Args:\n      vizier_trials: Trials to recover from.\n\n    Returns:\n      TunerPolicy.\n    \"\"\"\n    prior_trials = []\n    for trial in vizier_trials:\n      tuner_trial = core.VizierTrial(self._converter, trial)\n      reward = tuner_trial.get_reward_for_feedback(\n          self._converter.metrics_to_optimize\n      )\n      prior_trials.append((tuner_trial.dna, reward))\n\n    self._algorithm.recover(get_trial_history(prior_trials))\n\n    return TunerPolicy(\n        self.tuner.pythia_supporter(self._study),\n        self._converter,\n        self._algorithm,\n        early_stopping_policy=early_stopping_policy,\n    )\n\n  def _get_chief_tuner_id(self) -> str:\n    metadata = self._study.materialize_problem_statement().metadata.ns(\n        constants.METADATA_NAMESPACE\n    )\n    try:\n      return str", "ground_truth": "      self._algorithm.feedback(tuner_trial.dna, reward)"}
{"completion": "```py\n>>> from evaluate import load\n>>> accuracy = evaluate.load(\"accuracy\")\n```", "ground_truth": "    evaluation_module = evaluation_module_factory("}
{"completion": "scheduler = scheduler_class(**scheduler_config)\n\n        assert torch.sum(torch.abs(scheduler._get_variance(0) - 0.0)) < 1e-5\n        assert torch.sum(torch.abs(scheduler._get_variance(487) - 0.00979)) < 1e-5\n        assert torch.sum(torch.abs(scheduler._get_variance(999) - 0.02)) < 1e-5\n\n    def test_full_loop_no_noise(self):\n        scheduler_class = self.scheduler_classes[0]\n        scheduler_config = self.get_scheduler_config()\n        scheduler = scheduler_class(**scheduler_config)\n\n        num_trained_timesteps = len(scheduler)\n\n        model = self.dummy_model()\n        sample = self.dummy_sample_deter\n        generator = torch.manual_seed(0)", "ground_truth": "        scheduler = scheduler_class(**scheduler_config)"}
{"completion": ") -> Dict[str, jnp.ndarray]:\n        val_loss, aux = self.val_loss_step(state, batch, outputs, fun, rng, n_data)\n        val_metrics = self.val_metrics_step(aux, batch[1], metrics)\n        return {\"val_loss\": val_loss, **val_metrics}", "ground_truth": "    ) -> Dict[str, jnp.ndarray]:"}
{"completion": "hpc_action = hpc_action.cuda()\n        hpc_rewards = hpc_rewards.cuda()\n        hpc_bootstrap_values = hpc_bootstrap_values.cuda()\n        hpc_upgo = hpc_upgo.cuda()\n\n    ori_target_output.requires_grad_(True)\n    for i in range(times):\n        t = time.time()\n        ori_loss = upgo_loss(ori_target_output, ori_rhos, ori_action, ori_rewards, ori_bootstrap_values)\n        ori_loss = ori_loss.mean()\n        ori_loss.backward()\n        if use_cuda:\n            torch.cuda.synchronize()\n        print('epoch: {}, original upgo cost time: {}'.format(i, time.time() - t))\n\n    hpc_target_output.requires_grad_(True)", "ground_truth": "        hpc_action = hpc_action.cuda()"}
{"completion": "self.evaluation_module_names = [module.name for module in self.evaluation_modules]", "ground_truth": "            self.evaluation_module_names = [module.name for module in self.evaluation_modules]"}
{"completion": "default=\"LazyMemmapStorage\",\n    choices=[\"LazyMemmapStorage\", \"LazyTensorStorage\", \"ListStorage\"],\n    help=\"Storage type for the replay buffer\",\n)\n\nargs = parser.parse_args()\n\n# Set up RPC\nrpc.init_rpc(\n    name=REPLAY_BUFFER_NODE if args.rank == 0 else TRAINER_NODE,\n    rank=args.rank,\n    world_size=2,\n    rpc_backend_options=rpc.TensorPipeRpcBackendOptions(\n        num_worker_threads=16,\n        rpc_timeout=0,\n        init_method=\"tcp://localhost:29500\",\n    ),\n)\n\n# Create the replay buffer\nif args.rank == 0:\n    storage_type = storage_options[args.storage]\n    storage_args = storage_arg_options[args.storage]\n    replay_buffer = RemoteTensorDictReplayBuffer(\n        BUFFER_SIZE,\n        storage_type=storage_type,\n        storage_args=storage_args,\n        writer=RoundRobinWriter(),\n        sampler=RandomSampler(),\n    )\nelse:\n    replay_buffer = rpc.remote(\n        REPLAY_BUFFER_NODE,\n        RemoteTensorDictReplayBuffer.construct_buffer,\n        args=(BUFFER_SIZE,),\n    )\n\n# Benchmark sample latency\nif args.rank == 1:\n    print(\"Starting sample latency benchmark...\")\n    start_time", "ground_truth": "    default=\"LazyMemmapStorage\","}
{"completion": "```python\ndef compute(predictions, references, num_buckets='auto', pca_max_data=-1, kmeans_explained_var=0.9, kmeans_num_redo=5, kmeans_max_iter=500, featurize_model_name='gpt2-large'):\n    return compute_mauve(predictions, references, num_buckets=num_buckets, pca_max_data=pca_max_data, kmeans_explained_var=kmeans_explained_var, kmeans_num_redo=kmeans_num_redo, kmeans_max_iter=kmeans_max_iter, featurize_model_name=featurize_model_name)\n```", "ground_truth": "    device_id: Device for featurization. Supply a GPU id (e.g. 0 or 3) to use GPU. If no GPU with this id is found, use CPU"}
{"completion": "import torch\nimport torch.cuda\nimport tqdm\n\nfrom torch import nn, optim\nfrom torchrl.collectors import MultiSyncDataCollector\nfrom torchrl.data import TensorDictPrioritizedReplayBuffer, TensorDictReplayBuffer\n\nfrom torchrl.data.replay_buffers.storages import LazyMemmapStorage\nfrom torchrl.envs import (\n    Compose,", "ground_truth": "import platform"}
{"completion": "```python\n    model_name (str): the name or path of the pre-trained model to use for scoring.\n    tokenizer_name (str): the name or path of the pre-trained tokenizer to use for tokenizing the input.\n    device (str): the device to run the model on (e.g., \"cpu\", \"cuda\").\nReturns:\n    frugal_score (float): the FrugalScore metric.\nExamples:\n    >>> frugal_score = evaluate.load(\"frugal_score\")\n    >>> preds = [\"This is a prediction\"]\n    >>> refs = [\"This is a reference\"]\n    >>> frugal_score.compute(predictions=preds, references=refs)\n    0.8\n```\n\"\"\"", "ground_truth": "    max_length (int): maximum sequence length."}
{"completion": "results = recall_metric.compute(references=[0, 0, 1, 1, 1], predictions=[0, 1, 0, 1, 1], sample_weight=sample_weight)\n        >>> print(results)\n        {'recall': 0.6}", "ground_truth": "        >>> results = recall_metric.compute(references=[0, 0, 1, 1, 1], predictions=[0, 1, 0, 1, 1], sample_weight=sample_weight)"}
{"completion": "5.0, 4.0, 3.0, 2.0, -5.0],\n                ]\n            )\n        )\n        action, values = hook(net=None, observation=None, values=in_values)\n        expected_action = torch.tensor(expected_action, dtype=torch.long)\n\n        assert action.shape == expected_action.shape\n        assert (action == expected_action).all()\n        assert values.shape == in_values.shape\n        assert (values == in_values).all()", "ground_truth": "                        [11.0, -1.0, 7.0, -1.0, 20.0],"}
{"completion": "        self._fun = fun\n\n    def __call__(self, *args, **kwargs):\n        return self._fun()", "ground_truth": "        self._fun = fun"}
{"completion": "mystorage.set(idx, td)\n        assert (mystorage._storage[idx] == td[idx]).all()\n\n    def test_get(self, max_size, shape, storage):\n        td = self._get_nested_td(shape)\n        mystorage = storage(max_size=max_size)\n        mystorage.set(list(range(td.shape[0])), td)\n        assert (mystorage.get(list(range(td.shape[0]))) == td).all()\n\n    def test_getitem(self, max_size, shape, storage):\n        td = self._get_nested_td(shape)\n        mystorage = storage(max_size=max_size)\n        mystorage.set(list(range(td.shape[0])), td)\n        assert (mystorage[0] == td[0]).all()\n\n    def test_len(self, max_size, shape, storage):\n        td = self._get_nested_td(shape)\n        mystorage = storage(max_size=max_size)\n        mystorage.set(list(range(td.shape[0])), td)\n        assert len(mystorage) == td.shape[0]\n\n    def test_iter(self, max_size, shape, storage):\n        td = self._get_nested_td(shape)\n        mystorage = storage(max_size=max_size)\n        mystorage.set(list(range", "ground_truth": "        tc_sample = mystorage.get(idx)"}
{"completion": "from ding.config import read_config, compile_config\nfrom ding.policy import create_policy, PolicyFactory\nfrom ding.utils import set_pkg_seed\nfrom ding.utils.data import create_dataset\n\nfrom torch.utils.data import DataLoader\n\n\ndef serial_pipeline_offline(\n        input_cfg: Union[str, Tuple[dict, dict]],\n        seed: int = 0,\n        env_setting: Optional[List[Any]] = None,\n        model: Optional[torch.nn.Module] = None,\n        max_iterations: Optional[int] = int(1e10),\n) -> 'Policy':  # noqa\n    \"\"\"\n    Overview:\n        Serial pipeline entry for offline algorithm.\n    Arguments:\n        - input_cfg (:obj:`Union[str, Tuple[dict, dict]]`): Config in dict type. \\\n            ``str`` type means config file path. \\\n            ``Tuple[dict, dict]`` type means [user_config, create_cfg].\n        - seed (:obj:`int`): Random seed.\n        - env_setting (:obj:`Optional[List[Any]]`): Environment setting.\n        - model (:obj:`Optional[torch.nn.Module]`): Model instance.\n        - max_iterations (:obj:`Optional[int]`): Maximum number of iterations.\n    Returns:\n", "ground_truth": "from ding.config import read_config, compile_config"}
{"completion": "```python\n    self.assertTrue(np.isfinite(lp))\n    self.assertGreater(lp, -np.inf)\n    self.assertGreaterEqual(losses, 0.0)\n    self.assertLessEqual(losses, 1.0)\n    self.assertSequenceEqual(dist.batch_shape, ())\n    self.assertSequenceEqual(dist.event_shape, ())\n    self.assertSequenceEqual(dist.sample().shape, ())\n    self.assertSequenceEqual(dist.mean().shape, ())\n    self.assertSequenceEqual(dist.variance().shape, ())\n    self.assertSequenceEqual(dist.stddev().shape, ())\n    self.assertSequenceEqual(dist.mode().shape, ())\n    self.assertSequenceEqual(dist.median().shape, ())\n    self.assertSequenceEqual(dist.quantile(0.5).shape, ())\n    self.assertSequenceEqual(dist.cdf(0.0).shape, ())\n    self.assertSequenceEqual(dist.survival_function(0.0).shape, ())\n    self.assertSequenceEqual(dist.prob(0.0).shape, ())\n    self.assertSequenceEqual(dist.log_prob(0.0).shape, ())\n    self.assertSequenceEqual(dist.entropy().shape, ())\n    self.assertSequenceEqual(dist.kl_divergence(dist).shape, ())\n    self.assertSequenceEqual(dist", "ground_truth": "    self.assertTrue(np.isfinite(lp))"}
{"completion": "from federatedscope.core.auxiliaries.runner_builder import get_runner", "ground_truth": "from federatedscope.core.auxiliaries.runner_builder import get_runner"}
{"completion": "# The callback function `callback_fn` has been defined.", "ground_truth": "            if step == 1:"}
{"completion": "metric.compute()", "ground_truth": "            metric.add_batch(references=refs, predictions=preds)"}
{"completion": "", "ground_truth": "        except StopIteration:"}
{"completion": "dataset = TUDataset(root=self.root, name=name)\n                dataset = [data for data in dataset]\n                for i in range(len(dataset)):\n                    dataset[i].y = dataset[i].y.long()\n                dataset = dataset[:100]\n                dataset = DummyDataTranslator(dataset)\n                dataset = dataset.translate()\n                dataset = dataset['train']\n                dataset = [data for data in dataset]\n                for i in range(len(dataset)):\n                    dataset[i].y = dataset[i].y.long()\n                dataset = dataset[:100]\n                dataset = DummyDataTranslator(dataset)\n                dataset = dataset.translate()\n                dataset = dataset['train']\n                dataset = [data for data in dataset]\n                for i in range(len(dataset)):\n                    dataset[i].y = dataset[i].y.long()\n                dataset = dataset[:100]\n                dataset = DummyDataTranslator(dataset)\n                dataset = dataset.translate()\n                dataset = dataset['train']\n                dataset = [data for data in dataset]\n                for i in range(len(dataset)):\n                    dataset[i].y = dataset[i].y.long()\n                dataset = dataset[:100]\n                dataset = DummyDataTranslator(dataset)\n                dataset = dataset.translate()\n                dataset = dataset['train']\n                dataset = [data for", "ground_truth": "                    dataset = TUDataset(self.root, name)"}
{"completion": "for line in lines:\n        line = line.strip()\n        if line.startswith(\"#\"):\n            if line.startswith(\"# From:\"):\n                url_path = line.split(\"# From:\")[1].strip()\n                url_path, sub_directory = convert_github_url(url_path)\n                imports.append((\"external\", url_path, sub_directory, None))\n            continue\n        if line.startswith(\"import \") or line.startswith(\"from \"):\n            if is_in_docstring:\n                continue\n            if line.startswith(\"import \"):\n                import_type, import_path = line.split(\"import \")[1].split()[0].split(\".\")[0:2]\n            else:\n                import_type, import_path = line.split(\"from \")[1].split()[0].split(\".\")[0:2]\n            if import_type in [\"datasets\", \"datasets.utils\", \"datasets.features\"]:\n                continue\n            if import_type == \"datasets\":\n                import_type = \"library\"\n            if import_type == \"datasets.utils.file_utils\":\n                import_type = \"library\"\n                import_path = \"datasets.utils.file_utils\"\n            if import_type == \"datasets.utils.logging\":\n                import_type = \"library\"\n                import_path = \"datasets.utils.logging\"\n            if import_type == \"datasets", "ground_truth": "    for line in lines:"}
{"completion": "if not initialized:\n                raise RuntimeError(\"call 'init' before resetting\")\n            if verbose:\n                print(f\"resetting {pid}\")\n            tensordict = env.reset(**data)\n            reset_keys = set(tensordict.keys())\n            child_pipe.send((\"reset_obs\", reset_keys))\n\n        elif cmd == \"step\":\n            if not initialized:\n                raise RuntimeError(\"call 'init' before stepping\")\n            if verbose:\n                print(f\"stepping {pid}\")\n            tensordict = env.step(**data)\n            step_keys = set(tensordict.keys())\n            child_pipe.send((\"step_result\", step_keys))\n\n        elif cmd == \"close\":\n            if verbose:\n                print(f\"closing {pid}\")\n            child_pipe.send((\"closing\", None))\n            child_pipe.close()\n            return\n\n        elif cmd == \"state_dict\":\n            if not initialized:\n                raise RuntimeError(\"call 'init' before getting state dict\")\n            state_dict = env.state_dict()\n            state_dict = _recursively_strip_locks_from_state_dict(state_dict)\n            child_pipe.send((\"state_dict\", state_dict))\n\n        elif cmd == \"load_state_dict\":\n            if not initialized:\n                raise RuntimeError(\"call 'init' before loading state", "ground_truth": "            reset_kwargs = data"}
{"completion": "def test_mujoco(self, env_name, frame_skip, from_pixels, pixels_only):\n        if from_pixels and (not torch.has_cuda or not torch.cuda.device_count()):\n            raise pytest.skip(\"no cuda device\")\n\n        tds = []\n        tds_reset = []\n        final_seed = []\n        for _ in range(2):\n            env0 = MujocoEnv(\n                env_name,\n                frame_skip=frame_skip,\n                from_pixels=from_pixels,\n                pixels_only=pixels_only,\n            )\n            torch.manual_seed(0)\n            np.random.seed(0)\n            final_seed0 = env0.set_seed(0)\n            tdreset0 = env0.reset()\n            rollout0 = env0.rollout(max_steps=50)\n            env0.close()\n            del env0\n            tds_reset.append(tdreset0)\n            tds.append(rollout0)\n            final_seed.append(final_seed0)\n\n        tdreset1, tdreset0 = tds_reset\n        rollout0, rollout1 = tds\n        final_seed0, final_seed1 = final_seed\n\n        assert_allclose_td(tdreset1, tdreset0)\n        assert final_seed0 == final_seed1\n        assert_allclose", "ground_truth": "@pytest.mark.skipif(not (_has_dmc and _has_gym), reason=\"gym or dm_control not present\")"}
{"completion": "{\"start\": 0, \"entity\": \"B-LOC\"},\n                {\"start\": 2, \"entity\": \"I-LOC\"},\n                {\"start\": 4, \"entity\": \"I-LOC\"},\n                {\"start\": 9, \"entity\": \"O\"},\n                {\"start\": 11, \"entity\": \"O\"},\n                {\"start\": 16, \"entity\": \"B-LOC\"},\n                {\"start\": 21, \"entity\": \"O\"},\n            ]\n        ]\n        predictions = task_evaluator.predictions_processor(predictions, words, join_by)\n        self.assertListEqual(predictions[\"predictions\"][0], [\"B-LOC\", \"I-LOC\", \"O\", \"O\", \"B-LOC\", \"O\"])", "ground_truth": "                {\"start\": 0, \"entity\": \"B-LOC\"},"}
{"completion": "return results", "ground_truth": "    return results"}
{"completion": "return (pred_prev_sample,)", "ground_truth": "            return (pred_prev_sample, state)"}
{"completion": "return size + idx", "ground_truth": "        return size + idx"}
{"completion": "plt.title(\"value\")\nplt.legend()\nplt.subplot(3, 2, 5)\nplt.plot(frames[-len(grad_vals_td0) :], grad_vals_td0, label=\"grad norm (td0)\")\nplt.plot(\n    frames[-len(grad_vals_tdlambda) :],\n    grad_vals_tdlambda,\n    label=\"grad norm (td(lambda))\",\n)\nplt.xlabel(\"frames collected\")\nplt.title(\"grad norm\")\nif len(traj_lengths_td0):\n    plt.subplot(3, 2, 6)\n    plt.plot(traj_lengths_td0, label=\"traj length (training) (td0)\")\n    plt.plot(\n        traj_lengths_tdlambda, label=\"traj length (training) (td(lambda))\"\n    )\n    plt.xlabel(\"batches\")\n    plt.title(\"traj length (training)\")\nplt.savefig(\"dqn_comparison.png\")\nif is_notebook():\n    plt.show()", "ground_truth": "plt.title(\"value\")"}
{"completion": "tsf_loc = (\n                    actor.module[0].module[-1].module.transform(td_clone.get(\"loc\"))\n                )\n\n            if exploration == \"random\":\n                with pytest.raises(AssertionError):\n                    torch.testing.assert_close(td_clone.get(\"action\"), tsf_loc)\n            else:\n                torch.testing.assert_close(td_clone.get(\"action\"), tsf_loc)\n\n        value = actor_value.get_value_operator()\n        expected_keys = [\n            \"done\",\n            \"pixels\" if len(from_pixels) else \"observation_vector\",\n            \"pixels_orig\" if len(from_pixels) else \"observation_orig\",\n            \"state_value\",\n        ]\n        if from_pixels:\n            # for CatFrames\n            expected_keys += [\"_reset\"]\n        if shared_mapping:\n            expected_keys += [\"hidden\"]\n        if len(gsde):\n            expected_keys += [\"_eps_gSDE\"]\n\n        td_clone = td.clone()\n        if UNSQUEEZE_SINGLETON and not td_clone.ndimension():\n            # Linear and conv used to break for non-batched data\n            value(td_clone.unsqueeze(0))\n        else:\n            value(td_clone)\n        try:\n            assert set(td_clone.keys()) == set(expected_keys)\n        except AssertionError:\n            proof", "ground_truth": "                tsf_loc = ("}
{"completion": "action_dim_gsde, state_dim_gsde = None, None", "ground_truth": "        action_dim_gsde, state_dim_gsde = None, None"}
{"completion": "dropout_rate=self.dropout_rate,\n            )(x, train=train)\n        return x\n\n\nclass WideResNet(nn.Module):\n    \"\"\"\n    Wide ResNet model.\n\n    Attributes\n    ----------\n    depth: int\n        Depth of the model.\n    width_factor: int\n        Width factor of the model.\n    num_classes: int\n        Number of output classes.\n    dropout_rate: float\n        Dropout rate.\n    \"\"\"\n\n    depth: int\n    width_factor: int\n    num_classes: int\n    dropout_rate: float = 0.0\n\n    @nn.compact\n    def __call__(self, x: Array, train: bool = True) -> jnp.ndarray:\n        \"\"\"\n        Forward pass.\n\n        Parameters\n        ----------\n        x: Array\n            Input data.\n        train: bool\n            Whether the call is performed during training.\n\n        Returns\n        -------\n        jnp.ndarray\n            Outputs.\n        \"\"\"\n        conv = partial(nn.Conv, kernel_size=(3, 3), padding=\"SAME\")\n        norm = partial(\n            nn.BatchNorm,\n            use_running_average=not train,\n            momentum=0.9,\n            epsilon=1e-5,\n        )\n        activation = nn.relu\n\n       ", "ground_truth": "                dropout_rate=self.dropout_rate,"}
{"completion": "Temperature parameter determines the relative importance of the entropy term against the reward.", "ground_truth": "           12 | ``learn.``          bool        False          | Determine whether to use        | Temperature parameter"}
{"completion": "calib_config=self.class_calib_config_nodir_nodump,\n            )\n            sample = prob_class.posterior.sample()\n            prob_class.posterior.load_state(tmp_dir)\n\n            # save dir and dump\n            status = prob_class.train(\n                train_data_loader=self.class_train_data_loader,\n                calib_data_loader=self.class_val_data_loader,\n                val_data_loader=self.class_val_data_loader,\n                fit_config=self.class_fit_config_dir_dump(tmp_dir),\n                calib_config=self.class_calib_config_nodir_nodump,\n            )\n            sample = prob_class.posterior.sample()\n            prob_class.posterior.load_state(tmp_dir)\n\n            # restore\n            prob_class.train(\n                train_data_loader=self.class_train_data_loader,\n                calib_data_loader=self.class_val_data_loader,\n                val_data_loader=self.class_val_data_loader,\n                fit_config=self.class_fit_config_restore(tmp_dir),\n                calib_config=self.class_calib_config_nodir_nodump,\n            )\n\n            # load state\n            prob_class.load_state(checkpoint_path=tmp_dir)\n\n            # save state\n            prob_class.save_state(checkpoint_path=tmp_dir)", "ground_truth": "                calib_config=self.class_calib_config_nodir_nodump,"}
{"completion": "return EnvMetaData.build_metadata_from_env(env_or_creator(**kwargs))", "ground_truth": "        if kwargs is None:"}
{"completion": "                kwargs[\"num_inference_steps\"] = num_inference_steps", "ground_truth": "                kwargs[\"num_inference_steps\"] = num_inference_steps"}
{"completion": "return list(self._child_parameter_configs)", "ground_truth": "    return copy.deepcopy(list(self._child_parameter_configs))"}
{"completion": ". to_tensordict()\n        )\n        _action_means = torch.zeros(\n            *action_stats_shape,\n            device=tensordict.device,\n            dtype=self.env.action_spec.dtype,\n        )\n        _action_stds = torch.ones_like(_action_means)\n        container = TensorDict(\n            {\n                \"tensordict\": expanded_original_tensordict,\n                \"stats\": TensorDict(\n                    {\n                        \"_action_means\": _action_means,\n                        \"_action_stds\": _action_stds,\n                    },\n                    [*batch_size, 1, self.planning_horizon],\n                ),\n            },\n            batch_size,\n        )\n\n        for _ in range(self.optim_steps):\n            actions_means = container.get((\"stats\", \"_action_means\"))\n            actions_stds = container.get((\"stats\", \"_action_stds\"))\n            actions = actions_means + actions_stds * torch.randn(\n                *action_shape,\n                device=actions_means.device,\n                dtype=actions_means.dtype,\n            )\n            actions = self.env.action_spec.project(actions)\n            optim_tensordict = container.get(\"tensordict\").clone()\n            policy = _PrecomputedActionsSequentialSetter(actions)\n            optim_tensordict = self.env.roll", "ground_truth": "            .to_tensordict()"}
{"completion": "def readable(self, *args, **kwargs):\n        \"\"\"Returns True if the IO object can be read.\"\"\"\n        return False", "ground_truth": "        return False"}
{"completion": "jnp.float32", "ground_truth": "            Parameters `dtype`"}
{"completion": "tempfile.TemporaryDirectory() as temp_dir:\n        try:\n            yield temp_dir\n        finally:\n            shutil.rmtree(temp_dir)", "ground_truth": "    with tempfile.TemporaryDirectory() as dirname:"}
{"completion": "}\n}\n\nSUPPORTED_TASKS = SUPPORTED_EVALUATOR_TASKS.copy()\n\nif TRANSFORMERS_AVAILABLE:\n    SUPPORTED_TASKS.update(SUPPORTED_PIPELINE_TASKS)\n    SUPPORTED_TASKS.update(TASK_ALIASES)\n\ndef check_task(task: str) -> bool:\n    return task in SUPPORTED_TASKS\n\ndef load(task: str) -> Evaluator:\n    if not check_task(task):\n        raise ValueError(f\"Task '{task}' not found. Available tasks: {list(SUPPORTED_TASKS.keys())}\")\n\n    if task in SUPPORTED_EVALUATOR_TASKS:\n        evaluator_class = SUPPORTED_EVALUATOR_TASKS[task][\"implementation\"]\n        default_metric_name = SUPPORTED_EVALUATOR_TASKS[task][\"default_metric_name\"]\n        return evaluator_class(default_metric_name=default_metric_name)\n    else:\n        return check_pipeline_task(task)", "ground_truth": "    },"}
{"completion": "calib_config=self.reg_calib_config_nodir_nodump,\n                )\n\n            # save dir, no dump\n            status = prob_reg.train(\n                train_data_loader=self.reg_train_data_loader,\n                calib_data_loader=self.reg_val_data_loader,\n                val_data_loader=self.reg_val_data_loader,\n                map_fit_config=self.reg_fit_config_nodir_nodump,\n                fit_config=self.reg_fit_config_dir_nodump(tmp_dir),\n                calib_config=self.reg_calib_config_nodir_nodump,\n            )\n            sample = prob_reg.posterior.sample()\n            prob_reg.posterior.load_state(tmp_dir)\n\n            # save dir and dump\n            status = prob_reg.train(\n                train_data_loader=self.reg_train_data_loader,\n                calib_data_loader=self.reg_val_data_loader,\n                val_data_loader=self.reg_val_data_loader,\n                map_fit_config=self.reg_fit_config_nodir_nodump,\n                fit_config=self.reg_fit_config_dir_dump(tmp_dir),\n                calib_config=self.reg_calib_config_nodir_nodump,\n            )\n            sample = prob_reg.posterior.sample()\n            prob_reg.posterior.load_state(tmp_dir)\n\n            # restore\n            status = prob_reg.train(\n               ", "ground_truth": "                    calib_config=self.reg_calib_config_nodir_nodump,"}
{"completion": "    _has_ts = False", "ground_truth": "    _has_ts = False"}
{"completion": "import jax\nimport jax.numpy as jnp\nfrom flax import jax_utils\nfrom jax.tree_util import tree_map\n\nfrom fortuna.typing import Array, Batch\n\n\nclass DataLoader:\n    def __init__(\n        self,\n        data_loader: Union[\n            FromIterableToDataLoader,\n            FromCallableIterableToDataLoader,\n            FromArrayDataToDataLoader,\n            FromTensorFlowDataLoaderToDataLoader,\n", "ground_truth": "from jax._src.prng import PRNGKeyArray"}
{"completion": "config = [deepcopy(cooperative_navigation_coma_config), deepcopy(cooperative_navigation_coma_create_config)]\n    try:\n        serial_pipeline(config, seed=0)\n    except Exception:\n        assert False, \"pipeline fail\"\n    with open(\"./algo_record.log\", \"a+\") as f:\n        f.write(\"8. coma\\n\")", "ground_truth": "    config = [deepcopy(cooperative_navigation_coma_config), deepcopy(cooperative_navigation_coma_create_config)]"}
{"completion": "def make_env(\n    cfg: \"DictConfig\",  # noqa: F821\n    env_creator: EnvCreator = env_creator,\n    env_kwargs: Optional[Dict[str, Any]] = None,\n) -> Union[EnvBase, ParallelEnv]:\n    \"\"\"Create the environment(s) based on the configuration.\n\n    Args:\n        cfg (DictConfig): DictConfig containing the environment configuration.\n        env_creator (EnvCreator, optional): Function used to create the environment. Defaults to env_creator.\n        env_kwargs (Optional[Dict[str, Any]], optional): Additional keyword arguments to pass to the environment creator. Defaults to None.\n\n    Returns:\n        Union[EnvBase, ParallelEnv]: The created environment(s).\n\n    \"\"\"\n    if env_kwargs is None:\n        env_kwargs = {}\n\n    if cfg.env_type == \"parallel\":\n        envs = []\n        for _ in range(cfg.num_envs):\n            env = env_creator(cfg.env_name, **env_kwargs)\n            envs.append(env)\n        env = ParallelEnv(envs)\n    else:\n        env = env_creator(cfg.env_name, **env_kwargs)\n\n    if cfg.env_transforms is not None:\n        env = TransformedEnv(env, cfg.env_transforms)\n\n   ", "ground_truth": "    return cfg"}
{"completion": "True", "ground_truth": "print(\"projected action: \\n\", env.action_spec.project(action))"}
{"completion": "\"_convert_to_functional\", self._convert_to_functional)", "ground_truth": "            param_name,"}
{"completion": "del tdmodule[3]\n        del params[\"module\", \"3\"]\n        assert len(tdmodule) == 3\n\n        assert hasattr(tdmodule, \"__getitem__\")\n        assert tdmodule[0] is tdmodule1\n        assert tdmodule[1] is tdmodule2\n        assert tdmodule[2] is prob_module\n\n        td = TensorDict({\"in\": torch.randn(3, 7)}, [3])\n        tdmodule(td, params=params)\n        assert td.shape == torch.Size([3])\n        assert td.get(\"out\").shape == torch.Size([3, 7])\n\n        dist = tdmodule.get_dist(td, params=params)\n        assert dist.rsample().shape[: td.ndimension()] == td.shape\n\n        # test bounds\n        if not safe and spec_type == \"bounded\":\n            assert ((td.get(\"out\") > 0.1) | (td.get(\"out\") < -0.1)).any()\n        elif safe and spec_type == \"bounded\":\n            assert ((td.get(\"out\") < 0.1) | (td.get(\"out\") > -0.1)).all()", "ground_truth": "        del tdmodule[3]"}
{"completion": "*args,\n        observation_spec=None,\n        action_spec=None,\n        input_spec=None,\n        reward_spec=None,\n        from_pixels=True,\n        pixel_shape=None,\n        **kwargs,\n    ):\n        batch_size = kwargs.setdefault(\"batch_size\", torch.Size([]))\n        if pixel_shape is None:\n            pixel_shape = [1, 7, 7]\n        if observation_spec is None:\n            cls.out_key = \"pixels\"\n            observation_spec = CompositeSpec(\n                pixels=UnboundedContinuousTensorSpec(\n                    shape=torch.Size([*batch_size, *pixel_shape])\n                ),\n                pixels_orig=UnboundedContinuousTensorSpec(\n                    shape=torch.Size([*batch_size, *pixel_shape])\n                ),\n                shape=batch_size,\n            )\n\n        if action_spec is None:\n            action_spec = BoundedTensorSpec(-1, 1, [*batch_size, pixel_shape[-1]])\n\n        if reward_spec is None:\n            reward_spec = UnboundedContinuousTensorSpec(shape=(*batch_size, 1))\n        if input_spec is None:\n            cls._out_key = \"pixels_orig\"\n            input_spec = CompositeSpec(\n                **{cls._out_key: observation_spec[\"pixels\"], \"action\": action_spec},\n               ", "ground_truth": "        cls,"}
{"completion": "inputs = {\n            \"prompt\": \"Face of a yellow cat, high resolution, sitting on a park bench\",\n            \"image\": init_image,\n            \"mask_image\": mask_image,\n            \"generator\": generator,\n            \"num_inference_steps\": 3,\n            \"guidance_scale\": 7.5,\n            \"output_type\": \"numpy\",\n        }\n        return inputs\n\n    def test_stable_diffusion_inpaint_ddim(self):\n        pipe = StableDiffusionInpaintPipeline.from_pretrained(\n            \"runwayml/stable-diffusion-inpainting\", safety_checker=None\n        )\n        pipe.to(torch_device)\n        pipe.set_progress_bar_config(disable=None)\n        pipe.enable_attention_slicing()\n\n        inputs = self.get_inputs(torch_device)\n        image = pipe(**inputs).images\n        image_slice = image[0, 253:256, 253:256, -1].flatten()\n\n        assert image.shape == (1, 512, 512, 3)\n        expected_slice = np.array([0.0427, 0.0460, 0.0483, 0.0460, 0.0584, 0.0521, 0", "ground_truth": "        mask_image = load_image("}
{"completion": "x = self.encoder_model(impared_data)\ndegree = self.reg_model(x)\ngen_feat = self.gen(x)\nmend_feats, mend_edge_index = self.mend_graph(raw_data.x,\n                                              raw_data.edge_index,\n                                              degree, gen_feat)\nnc_pred = self.classifier(\n    Data(x=mend_feats, edge_index=mend_edge_index))", "ground_truth": "        return degree, gen_feat, nc_pred[:raw_data.num_nodes]"}
{"completion": "collect_demo_data(\n            config, seed=0, collect_count=collect_count, expert_data_path=expert_data_path, state_dict=state_dict\n        )\n    except Exception:\n        assert False, \"pipeline fail\"\n\n    # train cql\n    config = [deepcopy(pendulum_cql_default_config), deepcopy(pendulum_cql_default_create_config)]\n    try:\n        serial_pipeline_offline(config, seed=0)\n    except Exception:\n        assert False, \"pipeline fail\"", "ground_truth": "        collect_demo_data("}
{"completion": "image_mask = image_mask.astype(np.float32) / 255.0\n    image_mask = image_mask[None, None]\n    image_mask[image_mask < 0.5] = 0\n    image_mask[image_mask >= 0.5] = 1\n    image_mask = torch.from_numpy(image_mask)\n\n    masked_image = image * (image_mask < 0.5)\n\n    return image_mask, masked_image\n\n\ndef check_size(image, height, width):\n    if isinstance(image, PIL.Image.Image):\n        w, h = image.size\n        if w != width or h != height:\n            image = image.resize((width, height), resample=PIL_INTERPOLATION[\"lanczos\"])\n    return image\n\n\ndef preprocess(image):\n    w, h = image.size\n    w, h = map(lambda x: x - x % 32, (w, h))  # resize to integer multiple of 32\n    image = image.resize((w, h), resample=PIL_INTERPOLATION[\"lanczos\"])\n    image = np.array(image).astype(np.float32) / 255.0\n    image = image[None].transpose(0, 3, ", "ground_truth": "    masked_image = image * (image_mask < 127.5)"}
{"completion": "kernel: tfpk.PositiveSemidefiniteKernel,\n      noise_variance: float,\n      jitter: float = 1e-6,\n      num_samples: int = 1,\n      num_features: int = 1,\n      num_latent_gps: int = 1,\n      num_data: int = 1,\n      num_inducing_points: int = 1,\n      inducing_index_points: chex.Array = None,\n      inducing_observations: chex.Array = None,\n      inducing_variable_constraints: sp.Constraint = None,\n      inducing_variable_prior: tfd.Distribution = None,\n      inducing_variable_bijector: tfb.Bijector = None,\n      inducing_variable_constraint_bijector: tfb.Bijector = None,\n      inducing_variable_constraint: sp.Constraint = None,\n      inducing_variable_constraint_bijector_kwargs: dict = None,\n      inducing_variable_constraint_kwargs: dict = None,\n      inducing_variable_constraint_bijector_kwargs_override: dict = None,\n      inducing_variable_constraint_kwargs_override: dict = None,\n      inducing_variable_constraint_bijector_kwargs_override_fn: callable = None,\n      inducing_variable_constraint_kwargs_override_fn: callable = None,\n      inducing_variable_constraint_bijector_kwargs_override_fn_kwargs:", "ground_truth": "      use_retrying_cholesky: bool = True,"}
{"completion": "predictions: array-like of shape (n_samples,) or (n_samples, n_outputs)\n    Estimated target values.\nreferences: array-like of shape (n_samples,) or (n_samples, n_outputs)\n    Ground truth (correct) target values.\nsample_weight: array-like of shape (n_samples,), default=None\n    Sample weights.\nmultioutput: {\"raw_values\", \"uniform_average\"} or array-like of shape (n_outputs,), default=\"uniform_average\"\n    Defines aggregating of multiple output values. Array-like value defines weights used to average errors.\n    If input is list then the shape must be (n_outputs,).\n    \"raw_values\" : Returns a full set of errors in case of multioutput input.\n    \"uniform_average\" : Errors of all outputs are averaged with uniform weight.\n\"\"\"", "ground_truth": "    predictions: array-like of shape (n_samples,) or (n_samples, n_outputs)"}
{"completion": "yield from self._inputs_loader()", "ground_truth": "        yield from self._inputs_loader()"}
{"completion": "UpdateWeights,\n    Trainer,\n    OptimizerHook,\n    _has_tqdm,\n    _has_ts,\n    _fun_checker,\n    MockingOptim,\n    REPLAY_BUFFER_CLASS,\n    _CKPT_BACKEND,\n    KeyDependentDefaultDict,\n    _DataCollector,\n    TensorDictPrioritizedReplayBuffer,\n    TensorDictReplayBuffer,\n    DEVICE_TYPING,\n    EnvBase,\n    set_exploration_mode,\n    SafeModule,\n    LossModule,\n    Logger,\n    Snapshot,\n    StateDict,\n    CosineAnnealingLR,\n    warn,\n    torch,\n    optim,\n    dataclass,\n    List,\n    Optional,\n    Union,\n    reset_noise,\n    TensorDictModuleWrapper", "ground_truth": "    Trainer,"}
{"completion": "elif isinstance(elem, Mapping):\n        return {key: default_collate([d[key] for d in batch], cat_1dim=cat_1dim) for key in elem}\n    elif isinstance(elem, tuple) and hasattr(elem, '_fields'):  # namedtuple\n        return elem_type(*(default_collate(samples) for samples in zip(*batch)))\n    elif isinstance(elem, container_abcs.Sequence) and not isinstance(elem, string_classes):\n        transposed = zip(*batch)\n        return [default_collate(samples, cat_1dim=cat_1dim) for samples in transposed]\n    else:\n        raise TypeError(default_collate_err_msg_format.format(elem_type))", "ground_truth": "    elif isinstance(elem, float):"}
{"completion": "*args,\n                 **kwargs):\n\n        super(ATCClient,\n              self).__init__(ID, server_id, state, config, data, model, device,\n                             strategy, *args, **kwargs)\n\n        self.data = data\n        self.own_label = ('y' in data['train'])\n        self.msg_buffer = dict()\n        self.client_num = self._cfg.federate.client_num", "ground_truth": "                 *args,"}
{"completion": "The estimated log-pdf for each output.", "ground_truth": "            An evaluation of the log-pdf for each data point."}
{"completion": "self._total_step = 0\n        self._total_sample = 0\n        self._total_episode = 0\n\n    def _setup_env_manager(self, env_cfg: EasyDict) -> BaseEnvManager:\n        env_fn = partial(create_env_manager, cfg=env_cfg)\n        env_manager = env_fn()\n        return env_manager\n\n    def _update_policy_periodically(self) -> None:\n        while not self._end_flag:\n            time.sleep(self._cfg.update_policy_second)\n            self.policy = create_policy(self._cfg.policy, enable_field=['collect']).collect_mode\n\n    def _join_thread(self) -> None:\n        if not self._eval_flag:\n            self._update_policy_thread.join()\n            del self._update_policy_thread\n\n    # override\n    def close(self) -> None:\n        if self._end_flag:\n            return\n        self._end_flag = True\n        time.sleep(1)\n        if hasattr(self, '_env_manager'):\n            self._env_manager.close()\n        self._join_thread()\n\n    # override\n    def _policy_inference(self, obs: Dict[int, Any]) -> Dict[int, Any]:\n        env_ids = list(obs.keys())\n        policy_input = self", "ground_truth": "        self._traj_buffer = {env_id: TrajBuffer(self._traj_len) for env_id in range(self._env_num)}"}
{"completion": "```\n        if ftp_head(url):\n            connected = True\n            if use_etag:\n                etag = request_etag(url, use_auth_token=use_auth_token)\n        else:\n            head_error = ConnectionError(f\"Could not connect to {url}\")\n    else:\n        head_error = ConnectionError(f\"Could not connect to {url}\")\n\n    if not connected:\n        if head_error is not None:\n            raise head_error\n        else:\n            raise ConnectionError(f\"Could not connect to {url}\")\n\n    if etag is not None and not force_download:\n        # Check if file already exists with the same eTag\n        filename = hash_url_to_filename(cached_url, etag=etag)\n        cache_path = os.path.join(cache_dir, filename)\n        if os.path.exists(cache_path):\n            return cache_path\n\n    # Download the file\n    with tempfile.NamedTemporaryFile(delete=False) as temp_file:\n        if url.startswith(\"ftp://\"):\n            ftp_get(url, temp_file)\n        else:\n            http_get(\n                url,\n                temp_file,\n                proxies=proxies,\n                headers=headers,\n                cookies=cookies,\n                max_retries=max_retries,\n                desc=download_desc,\n", "ground_truth": "            connected = ftp_head(url)"}
{"completion": "if isinstance(replay_path, str):\n            replay_path = [replay_path] * self.env_num\n        self._env_replay_path = replay_path\n\n    # override\n    def close(self) -> None:\n        \"\"\"\n        Overview:\n            CLose the env manager and release all related resources.\n        \"\"\"\n        if self._closed:\n            return\n        self._closed = True\n        self._env_ref.close()\n        for _, p in self._pipe_parents.items():\n            p.send(['close', None, None])\n        for _, p in self._pipe_parents.items():\n            p.recv()\n        for i in range(self._env_num):\n            self._env_states[i] = EnvState.VOID\n        # disable process join for avoiding hang\n        # for p in self._subprocesses:\n        #     p.join()\n        for _, p in self._subprocesses.items():", "ground_truth": "        if isinstance(replay_path, str):"}
{"completion": "return Loader(lambda value: _COMPARE_OPERATORS[op](first(value), second(value)))", "ground_truth": "        return Loader("}
{"completion": "return torch.tensor(value, dtype=dtype, device=device)", "ground_truth": "            if dtype is not None:"}
{"completion": "n_data=100,\n            shape_inputs=self.class_input_shape,\n            output_dim=self.class_output_dim,\n            output_type=\"discrete\",\n        )\n        class_train_data = [\n            (class_train_data[0][i : i + bs], class_train_data[1][i : i + bs])\n            for i in range(0, len(class_train_data[0]), bs)\n        ]\n        class_val_data = [\n            (class_val_data[0][i : i + bs], class_val_data[1][i : i + bs])\n            for i in range(0, len(class_val_data[0]), bs)\n        ]\n        self.class_train_data_loader = DataLoader.from_iterable(class_train_data)\n        self.class_val_data_loader = DataLoader.from_iterable(class_val_data)\n\n        self.class_fit_config_nodir_nodump = FitConfig(\n            optimizer=FitOptimizer(n_epochs=3), monitor=FitMonitor(metrics=(accuracy,))\n        )\n        self.reg_fit_config_nodir_nodump = FitConfig(\n            optimizer=FitOptimizer(n_epochs=3), monitor=FitMonitor(metrics=(rmse,))\n        )\n        self.calib_config_dir_nodump = lambda directory, metric: Calib", "ground_truth": "            n_data=10,"}
{"completion": "download_config=download_config,\n        )\n        if sub_directory is not None:\n            local_import_path = os.path.join(local_import_path, sub_directory)\n        local_imports.append((import_name, local_import_path))\n \n    # Check library imports\n    needs_to_be_installed = set()\n    for library_import_name, library_import_path in library_imports:\n        try:\n            lib = importlib.import_module(library_import_name)  # noqa F841\n        except ImportError:\n            needs_to_be_installed.add((library_import_name, library_import_path))\n    if needs_to_be_installed:\n        raise ImportError(\n            f\"To be able to use {name}, you need to install the following dependencies\"\n            f\"{[lib_name for lib_name, lib_path in needs_to_be_installed]} using 'pip install \"\n            f\"{' '.join([lib_path for lib_name, lib_path in needs_to_be_installed])}' for instance'\"\n        )\n    return local_imports", "ground_truth": "            download_desc=download_config.download_desc,"}
{"completion": "while True:\n        param = self.param(p.name, p.init_fn)\n        value = self.param(p.name)\n        if p.regularizer is not None:\n            regularization_loss = p.regularizer(value)\n            self.losses[p.name] = regularization_loss\n        p = gen.send(value)\n    except StopIteration:\n        break\n\n    dist = gen.value\n    if self.mean_fn is not None:\n        mean = self.mean_fn(x)\n        dist = tfd.TransformedDistribution(\n            distribution=dist,\n            bijector=tfb.Shift(mean),\n            name='stochastic_process')\n    return dist", "ground_truth": "      while True:"}
{"completion": "file.write(data)\n                progress_bar.update(len(data))\n        progress_bar.close()\n\n        # if download was successful, rename temp file to final file\n        os.rename(ckpt_file_temp, ckpt_file)\n    return ckpt_file\n\n\nclass TestDownload(unittest.TestCase):\n    def test_download(self):\n        ckpt_dir = tempfile.gettempdir()\n        url = \"https://github.com/google/flax/blob/main/examples/imagenet/imagenet_models.py\"\n        ckpt_file = download(ckpt_dir, url)\n        self.assertTrue(os.path.exists(ckpt_file))\n\n\nclass TestCNN(unittest.TestCase):\n    def test_cnn(self):\n        rng = random.PRNGKey(0)\n        x = make_array_random_inputs(rng, (10, 3, 32, 32))\n        model = CNN(num_classes=10)\n        params = model.init(rng, x)\n        y = model.apply(params, x)\n        self.assertEqual(y.shape, (10, 10))\n\n\nclass TestMLP(unittest.TestCase):\n    def test_mlp(self):\n        rng = random.PRNGKey(0)\n        x = make_array_random_inputs(rng, (10, 784))\n        model = MLP(num_classes=10)\n        params = model", "ground_truth": "                progress_bar.update(len(data))"}
{"completion": "```\n                raise RuntimeError(\n                    \"`SafeProbabilisticModule(spec=None, safe=True)` is not a valid configuration as the tensor \"\n                    \"specs are not specified\"\n                )\n            self.register_forward_hook(_forward_hook_safe_action)\n```", "ground_truth": "                raise RuntimeError("}
{"completion": "MultiDeviceCalibModelCalibrator)\nfrom fortuna.calib_model.calib_model_manager.state import CalibModelManagerState\nfrom fortuna.calib_model.calib_model_manager.base import CalibModelManager\nfrom fortuna.calib_model.calib_model_manager.utils import (\n    get_calib_model_manager_from_config)\nfrom fortuna.calib_model.calib_model_manager.config import (\n    CalibModelManagerConfig)\nfrom fortuna.calib_model.calib_model_manager.fit_config import (\n    CalibModelManagerFitConfig)\nfrom fortuna.calib_model.calib_model_manager.predict_config import (\n    CalibModelManagerPredictConfig)\nfrom fortuna.calib_model.calib_model_manager.train_config import (\n    CalibModelManagerTrainConfig)\nfrom fortuna.calib_model.calib_model_manager.utils import (\n    get_calib_model_manager_from_config)\nfrom fortuna.calib_model.calib_model_manager.config import (\n    CalibModelManagerConfig)\nfrom fortuna.calib_model.calib_model_manager.fit_config import (\n    CalibModelManagerFitConfig)\nfrom fortuna.calib_model.calib_model_manager.predict_config import (\n    CalibModelManagerPredictConfig)\nfrom fortuna.calib_model.calib_model_manager.train_config import", "ground_truth": "    MultiDeviceCalibModelCalibrator)"}
{"completion": "cfg.data.type = 'toy'\n        cfg.trainer.type = 'general'\n        cfg.model.type = 'lr'\n\n        cfg.early_stop.patience = 5\n        cfg.federate.method = \"global\"\n\n    def test_toy_example_standalone(self):\n        init_cfg = global_cfg.clone()\n        setup_seed(init_cfg.seed)\n        update_logger(init_cfg)\n        data = get_data(init_cfg)\n        server_cls = get_server_cls(init_cfg)\n        client_cls = get_client_cls(init_cfg)\n        runner = get_runner(init_cfg, server_cls, client_cls, data)\n        runner.run()", "ground_truth": "        cfg.data.type = 'toy'"}
{"completion": "global_obs_shape: int,\n            action_shape: int,\n            hidden_size_list: list,\n            mixer: bool = True,\n            lstm_type: str = 'gru',\n            dueling: bool = False\n    ) -> None:\n        \"\"\"\n        Overview:\n            initialize WQMix network\n        Arguments:\n            - agent_num (:obj:`int`): the number of agent\n            - obs_shape (:obj:`int`): the dimension of each agent's observation state\n            - global_obs_shape (:obj:`int`): the dimension of global observation state\n            - action_shape (:obj:`int`): the dimension of action shape\n            - hidden_size_list (:obj:`list`): the list of hidden size\n            - mixer (:obj:`bool`): whether to use mixer network\n            - lstm_type (:obj:`str`): the type of lstm, default is 'gru'\n            - dueling (:obj:`bool`): whether to use dueling network, default is False\n        \"\"\"\n        super(WQMix, self).__init__()\n        self.agent_num = agent_num\n        self.obs_shape = obs_shape\n        self.global_obs_shape = global_obs_shape\n        self.action_shape = action", "ground_truth": "            global_obs_shape: int,"}
{"completion": "normalized_sent = sacremoses.MosesTokenizer().tokenize(sentence, return_str=True, escape=False)", "ground_truth": "    elif tokenizer == \"penn\":"}
{"completion": "def __call__(self, x: jnp.ndarray, train: bool = True) -> jnp.ndarray:\n        \"\"\"\n        Block forward pass.\n\n        Parameters\n        ----------\n        x: jnp.ndarray\n            Block inputs.\n        train: bool\n            Whether the call is performed during training.\n\n        Returns\n        -------\n        jnp.ndarray\n            Block outputs.\n        \"\"\"\n        residual = x\n\n        y = self.conv(self.filters, (3, 3), strides=self.strides, padding=[(1, 1), (1, 1)])(x)\n        y = self.norm()(y, train=train)\n        y = self.activation(y)\n\n        y = self.conv(self.filters, (3, 3), padding=[(1, 1), (1, 1)])(y)\n        y = self.norm()(y, train=train)\n\n        if self.strides != (1, 1) or residual.shape[-1] != self.filters:\n            residual = self.conv(self.filters, (1, 1), strides=self.strides)(residual)\n            residual = self.norm()(residual, train=train)\n\n        y = y + residual\n        y = self.activation(y)\n\n        return y", "ground_truth": "    def __call__(self, x: jnp.ndarray,) -> jnp.ndarray:"}
{"completion": "plt.title(\"value\")\nplt.legend()\nplt.subplot(3, 2, 5)\nplt.plot(frames[-len(grad_vals_td0) :], grad_vals_td0, label=\"grad norm (td0)\")\nplt.plot(\n    frames[-len(grad_vals_tdlambda) :],\n    grad_vals_tdlambda,\n    label=\"grad norm (td(lambda))\",\n)\nplt.xlabel(\"frames collected\")\nplt.title(\"grad norm\")\nif len(traj_lengths_td0):\n    plt.subplot(3, 2, 6)\n    plt.plot(traj_lengths_td0, label=\"traj length (training) (td0)\")\n    plt.plot(\n        traj_lengths_tdlambda, label=\"traj length (training) (td(lambda))\"\n    )\n    plt.xlabel(\"batches\")\n    plt.title(\"traj length (training)\")\nplt.savefig(\"dqn_comparison.png\")\nif is_notebook():\n    plt.show()", "ground_truth": "plt.title(\"value\")"}
{"completion": "for f in onlyfiles:\n        if f.endswith(\".yaml\"):\n            file_cnt += 1\n            for seed in seed_sets:\n                cmd = f\"python main.py --config {join(best_cfg_path, f)} --seed {seed}\"\n                print(cmd)", "ground_truth": "    for file_name in onlyfiles:"}
{"completion": "```python\n    return self._trial_client.get_checkpoint_to_warm_start_from()\n```", "ground_truth": "    self._trial = self._trial_client.materialize()"}
{"completion": "ParallelEnv(\n                env0,\n                t_out(),\n            )\n        )\n        env_parallel = TransformedEnv(\n            env_parallel,\n            t_out(),\n        )\n        env_serial = TransformedEnv(\n            env_serial,\n            t_out(),\n        )\n    else:\n        def t_out():\n            return (\n                Compose(\n                    ObservationNorm(in_keys=[\"observation\"], loc=0.5, scale=1.1),\n                    RewardClipping(0, 0.1),\n                )\n                if not transformed_in\n                else Compose(\n                    ObservationNorm(in_keys=[\"observation\"], loc=1.0, scale=1.0)\n                )\n            )\n\n        env0 = TransformedEnv(\n            env0,\n            t_out(),\n        )\n        env_parallel = TransformedEnv(\n            env_parallel,\n            t_out(),\n        )\n        env_serial = TransformedEnv(\n            env_serial,\n            t_out(),\n        )\n    return env_parallel, env_serial, env0\n\n\nclass TestModelBasedEnvBase:\n    @pytest.mark.parametrize(\"device\", get_available_devices())\n    def test_mb_rollout(self, device, seed=0):\n        torch.manual_seed(seed)", "ground_truth": "                spec=None,"}
{"completion": "", "ground_truth": "  s = 0.0"}
{"completion": "size = 4\n        num_channels = 3\n        height = 8\n        width = 8\n\n        num_elems = batch_size * num_channels * height * width\n        sample = torch.arange(num_elems)\n        sample = sample.reshape(num_channels, height, width, batch_size)\n        sample = sample / num_elems\n        sample = sample.permute(3, 0, 1, 2)\n\n        return sample\n\n    def dummy_model(self):\n        def model(sample, t, *args):\n            return sample * t / (t + 1)\n\n        return model\n\n    def get_scheduler_config(self, **kwargs):\n        config = {\n            \"num_train_timesteps\": 2000,\n            \"snr\": 0.15,\n            \"sigma_min\": 0.01,\n            \"sigma_max\": 1348,\n            \"sampling_eps\": 1e-5,\n        }\n\n        config.update(**kwargs)\n        return config\n\n    def check_over_configs(self, time_step=0, **config):\n        kwargs = dict(self.forward_default_kwargs)\n\n        for scheduler_class in self.scheduler_classes:\n            sample = self.dummy_sample\n            residual = 0.1 * sample", "ground_truth": "    num_inference_steps = 10"}
{"completion": "Number of target samples to draw when computing quantiles.", "ground_truth": "            Number of target samples to sample for each input data point."}
{"completion": "self.assertFalse(improved)\n        self.assertTrue(trainer._early_stopping.should_stop)\n\n        trainer = FakeTrainerWithEarlyStopping(\n            early_stopping_monitor=\"metric1\",\n            early_stopping_min_delta=0,\n            early_stopping_patience=3,\n            early_stopping_mode=\"max\",\n        )\n        improved = trainer.early_stopping_update(validation_metrics_step1)\n        self.assertTrue(improved)\n        improved = trainer.early_stopping_update(validation_metrics_step2)\n        self.assertTrue(improved)\n        improved = trainer.early_stopping_update(validation_metrics_step3)\n        self.assertTrue(improved)\n        improved = trainer.early_stopping_update(validation_metrics_step4)\n        self.assertFalse(improved)\n        self.assertFalse(trainer._early_stopping.should_stop)\n        improved = trainer.early_stopping_update(validation_metrics_step5)\n        self.assertFalse(improved)\n        self.assertFalse(trainer._early_stopping.should_stop)\n        improved = trainer.early_stopping_update(validation_metrics_step5)\n        self.assertFalse(improved)\n        self.assertTrue(trainer._early_stopping.should_stop)\n\n\nif __name__ == \"__main__\":\n    unittest.main()", "ground_truth": "        self.assertFalse(improved)"}
{"completion": "config = [deepcopy(bitflip_her_dqn_config), deepcopy(bitflip_her_dqn_create_config)]\n    config[0].policy.learn.update_per_collect = 1\n    try:\n        bitflip_dqn_main(config[0], seed=0, max_iterations=1)\n    except Exception:\n        assert False, \"pipeline fail\"", "ground_truth": "    try:"}
{"completion": "c.NotebookApp.ip = '0.0.0.0'\nc.NotebookApp.port = 8888\nc.NotebookApp.open_browser = False\nc.NotebookApp.allow_root = True\nc.NotebookApp.allow_origin = '*'\nc.NotebookApp.allow_remote_access = True\nc.NotebookApp.notebook_dir = '/notebooks'\nc.NotebookApp.password = 'sha1:...'\nc.NotebookApp.token = ''\nc.NotebookApp.disable_check_xsrf = True\nc.NotebookApp.tornado_settings = {\n    'headers': {\n        'Content-Security-Policy': \"frame-ancestors 'self' http://localhost:* http://127.0.0.1:*\",\n    },\n}\nc.NotebookApp.nbserver_extensions = {\n    'jupyter_nbextensions_configurator': True,\n    'jupyter_dashboards': True,\n    'jupyter_bokeh': True,\n    'jupyterlab': True,\n}\nc.NotebookApp.server_extensions = [\n    'jupyter_nbextensions_configurator',\n    'jupyter_dashboards',\n    'jupyter_bokeh',\n    'jupyterlab',\n]\n\n# Set the default permissions for the created notebooks and directories\nnote", "ground_truth": "c.ServerApp.ip = \"0.0.0.0\""}
{"completion": "scores: Optional[Array] = None,\n    ) -> Array:\n        \"\"\"\n        Compute a quantile of the scores.\n\n        Parameters\n        ----------\n        val_probs: Array\n            A two-dimensional array of class probabilities for each validation data point.\n        val_targets: Array\n            A one-dimensional array of validation target variables.\n        error: float\n            Coverage error. This must be a scalar between 0 and 1, extremes included.\n        scores: Optional[Array]\n            The conformal scores. This should be the output of\n            :meth:`~fortuna.conformal.classification.simple_prediction.SimplePredictionConformalClassifier.score`.\n\n        Returns\n        -------\n        Array\n            The conformal quantiles.\n        \"\"\"\n        if val_probs.ndim != 2:\n            raise ValueError(\n                \"\"\"`val_probs` must be a two-dimensional array. The first dimension is over the validation\n            inputs. The second is over the classes.\"\"\"\n            )\n\n        if scores is None:\n            scores = self.score(val_probs, val_targets)\n\n        return np.quantile(scores, 1 - error)", "ground_truth": "        scores: Optional[Array] = None,"}
{"completion": "policy_output = self._policy[1].forward(obs, **self._cfg.collect_setting)\n        else:\n            policy_output = self._policy[0].forward(obs, **self._cfg.collect_setting)\n        self._policy_output_pool.update(policy_output)\n        actions = {env_id: output['action'] for env_id, output in policy_output.items()}\n        return actions", "ground_truth": "            assert not self._eval_flag"}
{"completion": "self.msg_buffer['eval'][self.state]['feature_importance'][message.sender] = feature_importance\n        self._check_and_save_result()\n\n    def callback_func_for_eval_metric(self, message: Message):\n        # Save the evaluation metrics\n        metrics = message.content\n        self.msg_buffer['eval'][message.state]['metrics'] = metrics\n        self._check_and_save_result()\n\n    # Bind method to instance\n    server._check_and_save_result = types.MethodType(_check_and_save_result,\n                                                     server)\n    server.callback_func_for_feature_importance = types.MethodType(\n        callback_func_for_feature_importance, server)\n    server.callback_func_for_eval_metric = types.MethodType(\n        callback_func_for_eval_metric, server)\n\n    # Register handler functions\n    server.register_handlers('feature_importance',\n                             server.callback_func_for_feature_importance)\n    server.register_handlers('eval_metric',\n                             server.callback_func_for_eval_metric)\n\n    return server", "ground_truth": "        sender = message.sender"}
{"completion": "py_study_config = vz.StudyConfig(\n        metric_information=[\n            vz.MetricInformation(\n                name='objective', goal=vz.ObjectiveMetricGoal.MAXIMIZE\n            )\n        ]\n    )\n    root = py_study_config.search_space.root\n    for index in (0, 1):\n        root.add_float_param('learning_rate', 0.01, 3.0, index=index)\n        root.add_int_param(\n            'units', 10, 1000, scale_type=vz.ScaleType.LOG, index=index\n        )\n        root.add_categorical_param('activation', ['tanh', 'relu'], index=index)\n        root.add_bool_param('synchronous', index=index)\n        root.add_discrete_param('batch_size', [8, 16, 32], index=index)\n    root.add_discrete_param(\n        'floating_point_param', [8., 16., 32.], auto_cast=False)\n\n    pytrial = vz.Trial(id=2)\n    pytrial.parameters = {\n        'learning_rate[0]': vz.ParameterValue(value=0.5),\n        'learning_rate[1]': vz.ParameterValue(value=0.1),\n        'units[0]': vz", "ground_truth": "    py_study_config = vz.StudyConfig("}
{"completion": "'please make sure to switch to the correct mode before accessing the batch and epoch numbers.'\n        )\n        num_samples = self.num_samples\n        batch_size = self.cfg.batch_size\n        num_batches = math.ceil(num_samples / batch_size)\n        num_batches_last_epoch = num_samples % batch_size\n        num_epochs = self.cfg.num_epochs\n        num_total_batches = num_batches * num_epochs\n\n        if mode == 'train':\n            self.num_train_batch = num_batches\n            self.num_train_batch_last_epoch = num_batches_last_epoch\n            self.num_train_epoch = num_epochs\n            self.num_total_train_batch = num_total_batches\n            return num_batches, num_batches_last_epoch, num_epochs, num_total_batches\n        elif mode == 'val':\n            self.num_val_batch = num_batches\n            self.num_val_epoch = num_epochs\n            return num_batches, num_batches_last_epoch, num_epochs\n        elif mode == 'test':\n            self.num_test_batch = num_batches\n            self.num_test_epoch = num_epochs\n            return num_batches, num_batches_last_epoch, num_epochs\n        else:\n            raise ValueError(f'Invalid mode: {mode}.')", "ground_truth": "                f'will use `{mode}` to calculate `ctx.var`.')"}
{"completion": "    'tensorflow': ['tensorflow>=2.0.0'],\n    'torch': ['torch>=1.0.0'],\n    'scipy': ['scipy>=1.0.0'],\n    'pandas': ['pandas>=1.0.0'],\n    'matplotlib': ['matplotlib>=3.0.0'],\n    'seaborn': ['seaborn>=0.10.0'],\n    'sklearn': ['scikit-learn>=0.20.0'],\n    'xgboost': ['xgboost>=1.0.0'],\n    'lightgbm': ['lightgbm>=2.0.0'],\n    'catboost': ['catboost>=0.20.0'],\n    'gpyopt': ['GPyOpt>=1.2.0'],\n    'optuna': ['optuna>=2.0.0'],\n    'ray': ['ray>=1.0.0'],\n    'dask': ['dask>=2.0.0'],\n    'pytorch-ignite': ['pytorch-ignite>=0.4.0'],\n    'pytorch-lightning': ['pytorch-lightning>=0.7.0'],\n", "ground_truth": "        'jax': _parse_requirements('requirements-jax.txt'),"}
{"completion": "self.assertEqual(results[\"accuracy\"], 1.0)", "ground_truth": "        self.assertAlmostEqual(results[\"total_time_in_seconds\"], 0.1, 1)"}
{"completion": "safety_checker is not None and feature_extractor is not None:\n            image = self._preprocess_image(image)\n            image_latents = self.vae.encode(image)\n            image_latents = self._upscale_latents(image_latents)\n            image_latents = self._denoise_latents(\n                image_latents,\n                num_inference_steps=num_inference_steps,\n                guidance_scale=image_guidance_scale,\n                negative_prompt=negative_prompt,\n                num_images_per_prompt=num_images_per_prompt,\n                eta=eta,\n                generator=generator,\n                latents=latents,\n                prompt_embeds=prompt_embeds,\n                negative_prompt_embeds=negative_prompt_embeds,\n                callback=callback,\n                callback_steps=callback_steps,\n            )\n            image_latents = self._downscale_latents(image_latents)\n            generated_image = self.vae.decode(image_latents)\n            generated_image = self._postprocess_image(generated_image, output_type=output_type)\n            return generated_image\n\n        elif prompt is not None and image is not None:\n            raise ValueError(\"You can only pass either `prompt` or `image`, not both.\")\n\n        elif prompt is not None:\n            prompt_embeds = self.text_encoder.encode_text", "ground_truth": "                image quality. This pipeline requires a value of at least `1`."}
{"completion": "test_data", "ground_truth": "        test_targets: y of test data"}
{"completion": "start_index = batch * batch_size\n            end_index = min(data_size, (batch + 1) * batch_size)\n            sample_index = shuffled_index[start_index:end_index]\n            yield sample_index, data_x[sample_index], data_y[sample_index]", "ground_truth": "            start_index = batch * batch_size"}
{"completion": "spec = None\n        elif spec_type == \"bounded\":\n            spec = BoundedTensorSpec(-0.1, 0.1, 4)\n        elif spec_type == \"unbounded\":\n            spec = UnboundedContinuousTensorSpec(4)\n\n        kwargs = {}\n\n        if safe and spec is None:\n            pytest.skip(\"safe and spec is None is checked elsewhere\")\n        else:\n            tdmodule1 = SafeModule(\n                net1,\n                spec=None,\n                in_keys=[\"in\"],\n                out_keys=[\"hidden\"],\n                safe=False,\n            )\n            dummy_tdmodule = SafeModule(\n                dummy_net,\n                spec=None,\n                in_keys=[\"hidden\"],\n                out_keys=[\"hidden\"],\n                safe=False,\n            )\n            tdmodule2 = SafeModule(\n                spec=spec,\n                module=net2,\n                in_keys=[\"hidden\"],\n                out_keys=[\"out\"],\n                safe=False,\n                **kwargs,\n            )\n            tdmodule = SafeProbabilisticSequential(tdmodule1, dummy_tdmodule, tdmodule2)\n\n        assert hasattr(tdmodule, \"__setitem__\")\n        assert len(tdmodule) == 3\n        tdmodule[1] = tdmodule2\n        assert len(tdmodule) == ", "ground_truth": "            spec = None"}
{"completion": "def create_evaluator_env_cfg(cfg: dict) -> List[dict]:\n        evaluator_cfg = copy.deepcopy(cfg)\n        evaluator_env_num = evaluator_cfg.pop('evaluator_env_num', 1)\n        evaluator_cfg.is_evaluator = True\n        return [evaluator_cfg for _ in range(evaluator_env_num)]", "ground_truth": "    def create_evaluator_env_cfg(cfg: dict) -> List[dict]:"}
{"completion": ".select(*self.qvalue_network.in_keys)\n            .expand(self.num_qvalue_nets, *tensordict_actor[0].batch_size)\n        )  # for actor loss\n        _qval_td = tensordict_select.select(*self.qvalue_network.in_keys).expand(\n            self.num_qvalue_nets,\n            *tensordict_select.select(*self.qvalue_network.in_keys).batch_size,\n        )  # for qvalue loss\n        _next_val_td = (\n            tensordict_actor[1]\n            .select(*self.qvalue_network.in_keys)\n            .expand(self.sub_sample_len, *tensordict_actor[1].batch_size)\n        )  # for next value estimation\n        tensordict_qval = torch.cat(\n            [\n                _actor_loss_td,\n                _next_val_td,\n                _qval_td,\n            ],\n            0,\n        )\n\n        tensordict_qval_expand = vmap(self.qvalue_network, (None, 0))(\n            tensordict_qval,\n            self.qvalue_network_params,\n        )\n        pred_val = tensordict_qval_expand.get(\"state_action_value\").squeeze(-1)\n        target_value", "ground_truth": "            .select(*self.qvalue_network.in_keys)"}
{"completion": "transformer=transformer,\n            scheduler=scheduler,\n            learned_classifier_free_sampling_embeddings=learned_classifier_free_sampling_embeddings,\n        )\n\n        prompt = \"This is a test prompt.\"\n        num_images_per_prompt = 1\n        do_classifier_free_guidance = False\n\n        encoded_prompt = pipe._encode_prompt(prompt, num_images_per_prompt, do_classifier_free_guidance)\n\n        self.assertEqual(len(encoded_prompt), num_images_per_prompt)\n        self.assertEqual(encoded_prompt[0][\"input_ids\"].shape, torch.Size([1, 77]))\n        self.assertEqual(encoded_prompt[0][\"attention_mask\"].shape, torch.Size([1, 77]))\n        self.assertEqual(encoded_prompt[0][\"image\"].shape, torch.Size([1, 3, 256, 256]))\n        self.assertEqual(encoded_prompt[0][\"text\"].shape, torch.Size([1, self.text_embedder_hidden_size]))", "ground_truth": "            transformer=transformer,"}
{"completion": "When `\"auto\"`, halves the input to the attention heads, so attention will be computed in two steps. If a number is provided, uses as many slices as `attention_head_dim // slice_size`. In this case, `attention_head_dim` must be a multiple of `slice_size`.", "ground_truth": "                When `\"auto\"`, halves the input to the attention heads, so attention will be computed in two steps. If"}
{"completion": "def _set_seed_initial(self, seed: int) -> None:  # noqa: F811\n        self._seed_calls_reset = True\n        self._env.seed(seed=seed)", "ground_truth": "    def _set_seed_initial(self, seed: int) -> None:  # noqa: F811"}
{"completion": "num_images_per_prompt (`int`, *optional*, defaults to 1):\n                The number of images to generate per prompt.\n            eta (`float`, *optional*, defaults to 0.0):\n                Corresponds to parameter eta (\u03b7) in the DDIM paper: https://arxiv.org/abs/2010.02502. Only applies to\n                [`schedulers.DDIMScheduler`], will be ignored for others.\n            generator (`torch.Generator`, *optional*):\n                A [torch generator](https://pytorch.org/docs/stable/generated/torch.Generator.html) to make generation\n                deterministic.\n            latents (`torch.FloatTensor`, *optional*):\n                Pre-generated noisy latents, sampled from a Gaussian distribution, to be used as inputs for image", "ground_truth": "            num_images_per_prompt (`int`, *optional*, defaults to 1):"}
{"completion": "bellman_max, c_val, dv_val, vtrace", "ground_truth": "    generalized_advantage_estimate,"}
{"completion": "not _has_functorch, reason=\"functorch not installed\")\n    @pytest.mark.parametrize(\"device\", get_available_devices())\n    @pytest.mark.parametrize(\n        \"delay_actor, delay_qvalue\", [(False, False), (True, True)]\n    )\n    @pytest.mark.parametrize(\"policy_noise\", [0.1, 1.0])\n    @pytest.mark.parametrize(\"noise_clip\", [0.1, 1.0])\n    def test_sac(\n        self,\n        delay_actor,\n        delay_qvalue,\n        device,\n        policy_noise,\n        noise_clip,\n        version,\n    ):\n        torch.manual_seed(self.seed)\n        actor = self._create_mock_actor(device=device)\n        qvalue = self._create_mock_qvalue(device=device)\n        value = self._create_mock_value(device=device)\n        td = self._create_mock_data_sac(device=device)\n        loss_fn = SACLoss(\n            actor,\n            qvalue,\n            value,\n            gamma=0.9,\n            alpha=0.2,\n            delay_actor=delay_actor,\n            delay_qvalue=delay_qvalue,\n            policy_noise=policy_noise,\n            noise_clip=noise_clip,\n            version=version,\n        )\n", "ground_truth": "        not _has_functorch, reason=f\"functorch not installed: {FUNCTORCH_ERR}\""}
{"completion": "f\"Invalid destination '{dest}', please choose a valid destination.\"", "ground_truth": "                f\"The hook collection {dest} is not recognised. Choose from:\""}
{"completion": "import os\nimport shutil\nimport tempfile\nimport logging\nimport requests\nimport hashlib\nimport zipfile\nimport tarfile\nimport gzip\nimport bz2\nimport lzma\nimport time\nimport datetime\nimport warnings\nimport functools\nimport inspect\nimport importlib.util\nimport contextlib\nimport sys\nimport subprocess\nimport platform\nimport tempfile\nimport collections\nimport itertools\nimport re\nimport glob\nimport fnmatch\nimport json\nimport csv\nimport random\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torch.autograd as autograd\nimport torch.cuda\nimport torch.distributed as dist\nimport torch.multiprocessing as mp\nimport torch.utils.data\nimport torch.utils.data.distributed\nimport torch.utils.data.sampler\nimport torch.utils.data.dataloader\nimport torch.utils.data.dataset\nimport torch.utils.data.distributed\nimport torch.utils.data.sampler\nimport torch.utils.data.dataloader\nimport torch.utils.data.dataset\nimport torch.utils.data.distributed\nimport torch.utils.data.sampler\nimport torch.utils.data.dataloader\nimport torch.utils.data.dataset\nimport torch.utils.data.distributed\nimport torch.utils.data.sampler", "ground_truth": "import os"}
{"completion": "model_id=\"username/repo\",\n            metric_value=self.result[\"accuracy\"],\n            metric_name=\"Pretty Metric Name\",\n            metric_type=self.metric.name,\n            dataset_name=\"dataset_name\",\n            dataset_type=\"dataset_type\",\n        )\n\n        metadata_update.assert_called_once_with(repo_id=\"username/repo\", metadata=minimum_metadata, overwrite=False)\n\n    def test_push_metric_with_extras(self, metadata_update):\n        push_to_hub(\n            model_id=\"username/repo\",\n            metric_value=self.result[\"accuracy\"],\n            metric_name=\"Pretty Metric Name\",\n            metric_type=self.metric.name,\n            dataset_name=\"dataset_name\",\n            dataset_type=\"dataset_type\",\n            task_type=\"dummy-task\",\n            dataset_config=\"fr\",\n            dataset_split=\"test\",\n            dataset_revision=\"abc\",\n            metric_config=\"default\",\n            metric_args=self.args,\n        )\n\n        metadata_update.assert_called_once_with(repo_id=\"username/repo\", metadata=extras_metadata, overwrite=False)\n\n    def test_push_metric_with_overwrite(self, metadata_update):\n        push_to_hub(\n            model_id=\"username/repo\",\n            metric_value=self.result[\"accuracy\"],\n            metric_name=\"Pretty Metric Name\",\n            metric_type=self.metric.name,\n            dataset_name=\"dataset_name\",\n            dataset", "ground_truth": "                model_id=\"username/repo\","}
{"completion": "SafeModule(\n                module=net,\n                spec=spec,\n                in_keys=in_keys,\n                out_keys=out_keys,\n                safe=safe,\n            )\n        else:\n            prob_module = SafeProbabilisticModule(\n                module=net,\n                spec=spec,\n                in_keys=in_keys,\n                out_keys=out_keys,\n                safe=safe,\n                **kwargs,\n            )\n\n        td = TensorDict({\"in\": torch.randn(3, 3)}, [3])\n        prob_module(td)\n        assert td.shape == torch.Size([3])\n        assert td.get(out_keys[0]).shape == torch.Size([3, 4])\n        assert td.get(out_keys[1]).shape == torch.Size([3, 4])\n\n        # test bounds\n        if not safe and spec_type == \"bounded\":\n            assert (\n                (td.get(out_keys[0]) > 0.1)\n                | (td.get(out_keys[0]) < -0.1)\n                | (td.get(out_keys[1]) > 0.1)\n                | (td.get(out_keys[1]) < -0.1)\n            ).any()\n        elif safe and spec_type == \"bounded\":\n            assert (\n               ", "ground_truth": "                    in_keys=dist_in_keys,"}
{"completion": "\"\"\"Values for Trial.Status.\"\"\"\n  RUNNING = 'RUNNING'\n  COMPLETED = 'COMPLETED'\n  FAILED = 'FAILED'\n  CANCELLED = 'CANCELLED'\n  PENDING = 'PENDING'\n  STOPPED = 'STOPPED'\n\n\nclass MetricType(enum.Enum):\n  \"\"\"Type of the metric.\n\n  OBJECTIVE: Objective to be maximized / minimized.\n  SAFETY: Objective to be kept above / below a certain threshold.\n  \"\"\"\n  OBJECTIVE = 'OBJECTIVE'\n  SAFETY = 'SAFETY'  # Soft constraint\n\n  # pylint: disable=comparison-with-callable\n  @property\n  def is_safety(self) -> bool:\n    return self == MetricType.SAFETY\n\n\nclass FidelityMode(enum.Enum):\n  \"\"\"Decides how the fidelity config should be interpreated.\n\n  SEQUENTIAL: A high fidelity measurement can be \"warm-started\" from a lower\n    fidelity measurement. Currently, no algorithms can take advatange of it, and\n    Vizier behaves exactly like NON_SEQUENTIAL case. This is for tracking\n    purposes only.\n\n  NOT_SEQUENTIAL: Each fidelity is separately measured. Example: Fidelity\n    is", "ground_truth": "  UNKNOWN = 'UNKNOWN'"}
{"completion": "config['federate.total_round_num'] = self._cfg.hpo.sha.budgets:\n                trial_cfg['federate.total_round_num'] = self._cfg.hpo.sha.budgets[self._stage]\n                trial_cfg['eval.freq'] = self._cfg.hpo.sha.budgets[self._stage]", "ground_truth": "                    self._cfg.hpo.sha.budgets):"}
{"completion": "Batch = Tuple[Array, Array]\nModelOutput = Union[Array, Tuple[Array, Mutable]]\nCalibOutput = Union[Array, Tuple[Array, CalibMutable]]\n\nclass TrainerABC:\n    def __init__(\n        self,\n        model_manager: ModelManager,\n        output_calib_manager: Optional[OutputCalibManager] = None,\n        rng: Optional[PRNGKeyArray] = None,\n    ):\n        self.model_manager = model_manager\n        self.output_calib_manager = output_calib_manager\n        self.rng = rng\n\n    def init_state(\n        self,\n        prob_model_state: JointState,\n        rng: Optional[PRNGKeyArray] = None,\n    ) -> TrainState:\n        \"\"\"\n        Initialize the training state.\n\n        Parameters\n        ----------\n        prob_model_state : JointState\n            The initial state of the probabilistic model.\n        rng : Optional[PRNGKeyArray], optional\n            The random number generator key, by default None.\n\n        Returns\n        -------\n        TrainState\n            The initialized training state.\n        \"\"\"\n        if rng is None:\n            rng = jax.random.PRNGKey(0)\n\n        params = self.model_manager.init_params(rng)\n        mutable", "ground_truth": "OptaxOptimizer = GradientTransformation"}
{"completion": "for i, pad_idx in enumerate(num_non_padding):\n            tgt_encoded.input_ids[i, 0] = tokenizer.bos_token_id\n            tgt_encoded.input_ids[i, pad_idx - 1] = tokenizer.eos_token_id\n\n        if raw_cache_dir:\n            logger.info('Saving cache file to \\'{}\\''.format(cache_dir))\n            os.makedirs(cache_dir, exist_ok=True)\n            token_ids = np.memmap(filename=osp.join(cache_dir,\n                                                    'token_ids.memmap'),\n                                  shape=(len(src_examples), max_src_len),\n                                  mode='w+',\n                                  dtype=np.int64)\n            attention_mask = np.memmap(filename=osp.join(\n                cache_dir, 'attention_mask.memmap'),\n                                       shape=(len(src_examples), max_src_len),\n                                       mode='w+',\n                                       dtype=np.int64)\n\n            for i in range(len(src_examples)):\n                token_ids[i] = src_encoded.input_ids[i]\n                attention_mask[i] = src_encoded.attention_mask[i]\n\n            token_ids = torch.from_numpy(token_ids)", "ground_truth": "        for i, pad_idx in enumerate(num_non_padding):"}
{"completion": "\"test_shared: shared=shared\"\n\"test_shared: shared=memmap\"", "ground_truth": "            },"}
{"completion": "Init eval model with argmax strategy and hidden_state plugin.", "ground_truth": "            Init eval model with argmax strategy and the hidden_state plugin."}
{"completion": "\"image\": init_image,\n            \"mask_image\": mask_image,\n            \"generator\": generator,\n            \"num_inference_steps\": 50,\n            \"strength\": 0.75,\n            \"guidance_scale\": 7.5,\n            \"output_type\": \"numpy\",\n        }\n        return inputs\n\n    def test_stable_diffusion_inpaint_slow(self):\n        device = torch_device\n        components = self.get_dummy_components()\n        sd_pipe = StableDiffusionInpaintPipeline(**components)\n        sd_pipe = sd_pipe.to(device)\n        sd_pipe.set_progress_bar_config(disable=None)\n\n        inputs = self.get_inputs(device)\n        image = sd_pipe(**inputs).images\n        image_slice = image[0, -3:, -3:, -1]\n\n        assert image.shape == (1, 256, 256, 3)\n        expected_slice = np.array(\n            [\n                0.0000,\n                0.0000,\n                0.0000,\n                0.0000,\n                0.0000,\n                0.0000,\n                0.0000,\n                0.0000,\n                0.0000,\n                0", "ground_truth": "            \"image\": init_image,"}
{"completion": "# test bounds\n        if not safe and spec_type == \"bounded\":\n            assert ((td.get(\"out\") > 0.1) | (td.get(\"out\") < -0.1)).any()\n        elif safe and spec_type == \"bounded\":\n            assert ((td.get(\"out\") < 0.1) | (td.get(\"out\") > -0.1)).all()", "ground_truth": "            assert \"b\" not in td.keys()"}
{"completion": "self.ID = message.content", "ground_truth": "        content = message.content"}
{"completion": "_, idx = model(data)\n                if use_cuda:\n                    idx = idx.cpu()\n                sorted_idx = torch.sort(idx)[0]\n                assert sorted_idx.eq(torch.arange(batch_size)).sum() == batch_size, idx\n            infer_time = timer.value\n            sum_time = data_time + infer_time\n            total_data_time += data_time\n            total_infer_time += infer_time\n            total_sum_time += sum_time\n            iter += 1\n            if iter == max_iter:\n                break\n        total_data_time_list.append(total_data_time)\n        total_infer_time_list.append(total_infer_time)\n        total_sum_time_list.append(total_sum_time)\n        our_dataloader.close()\n        time.sleep(0.5)\n        assert len(threading.enumerate()) <= 2, threading.enumerate()\n    avg_data_time = sum(total_data_time_list) / len(total_data_time_list)\n    avg_infer_time = sum(total_infer_time_list) / len(total_infer_time_list)\n    avg_sum_time = sum(total_sum_time_list) / len(total_sum_time_list)\n    out_str = '\\tOur DataLoader: avg_data_time: {:.4f}, avg_infer_time: {:.4f}, avg_sum", "ground_truth": "                    _, idx = model(data)"}
{"completion": "The user can define and use customized network model but must obey the same inferface definition indicated \\\nby import_names path. For this algorithm, the default model is 'qmix' and the import_names are ['ding.model.template.qmix'].", "ground_truth": "            The user can define and use customized network model but must obey the same inferface definition indicated \\"}
{"completion": "SafeProbabilisticSequential(\n        SafeModule(\n            actor_module,\n            in_keys=[\"state\", \"belief\"],\n            out_keys=[\"loc\", \"scale\"],\n            spec=CompositeSpec(\n                **{\n                    \"loc\": UnboundedContinuousTensorSpec(\n                        proof_environment.action_spec.shape,\n                        device=proof_environment.action_spec.device,\n                    ),\n                    \"scale\": UnboundedContinuousTensorSpec(\n                        proof_environment.action_spec.shape,\n                        device=proof_environment.action_spec.device,\n                    ),\n                }\n            ),\n        ),\n        SafeProbabilisticModule(\n            in_keys=[\"loc\", \"scale\"],\n            out_keys=[action_key],\n            default_interaction_mode=\"random\",\n            distribution_class=TanhNormal,\n            spec=CompositeSpec(**{action_key: proof_environment.action_spec}),\n        ),\n    )", "ground_truth": "        SafeProbabilisticSequential("}
